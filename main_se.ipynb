{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nhat/anaconda/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n",
      "/Users/nhat/anaconda/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import path, os, random, math\n",
    "from pathlib import Path\n",
    "\n",
    "from Source import SENet\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define paths:\n",
    "d = Path().resolve()\n",
    "data_path = str(d) + \"/Data\"\n",
    "train_path = data_path + \"/train.csv\"\n",
    "predictions_path = \"/output/\"\n",
    "sample_path = data_path + \"/sample_submission.csv\"\n",
    "weight_save_path = str(d) + \"/weights/model_se.ckpt\"\n",
    "weight_load_path = weight_save_path\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare data:\n",
    "df_train = pd.read_csv(data_path + \"/emnist-letters-train.csv\")\n",
    "df_test = pd.read_csv(data_path + \"/emnist-letters-test.csv\")\n",
    "\n",
    "y_train = df_train.iloc[:, 0].values\n",
    "y_test = df_test.iloc[:, 0].values\n",
    "\n",
    "X_train = df_train.iloc[:, 1:].values\n",
    "X_test = df_test.iloc[:, 1:].values\n",
    "X_train = X_train.reshape(-1, 28, 28, 1)\n",
    "X_test = X_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One-hot encode the y-values:\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(y_train)\n",
    "y_train_enc = lb.transform(y_train)\n",
    "y_test_enc = lb.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Characters Classifier for 15 epochs\n",
      "Iteration 0: with minibatch training loss = 3.66 and accuracy of 0.062\n",
      "Iteration 1: with minibatch training loss = 3.61 and accuracy of 0.078\n",
      "Iteration 2: with minibatch training loss = 3.8 and accuracy of 0.031\n",
      "Iteration 3: with minibatch training loss = 3.89 and accuracy of 0.016\n",
      "Iteration 4: with minibatch training loss = 3.72 and accuracy of 0.047\n",
      "Iteration 5: with minibatch training loss = 3.58 and accuracy of 0.062\n",
      "Iteration 6: with minibatch training loss = 3.79 and accuracy of 0.031\n",
      "Iteration 7: with minibatch training loss = 3.57 and accuracy of 0.14\n",
      "Iteration 8: with minibatch training loss = 3.41 and accuracy of 0.11\n",
      "Iteration 9: with minibatch training loss = 3.33 and accuracy of 0.14\n",
      "Iteration 10: with minibatch training loss = 3.48 and accuracy of 0.094\n",
      "Iteration 11: with minibatch training loss = 3.26 and accuracy of 0.14\n",
      "Iteration 12: with minibatch training loss = 3.21 and accuracy of 0.094\n",
      "Iteration 13: with minibatch training loss = 3.1 and accuracy of 0.12\n",
      "Iteration 14: with minibatch training loss = 3.3 and accuracy of 0.12\n",
      "Iteration 15: with minibatch training loss = 2.79 and accuracy of 0.25\n",
      "Iteration 16: with minibatch training loss = 3.18 and accuracy of 0.12\n",
      "Iteration 17: with minibatch training loss = 2.77 and accuracy of 0.25\n",
      "Iteration 18: with minibatch training loss = 2.89 and accuracy of 0.22\n",
      "Iteration 19: with minibatch training loss = 2.71 and accuracy of 0.23\n",
      "Iteration 20: with minibatch training loss = 2.7 and accuracy of 0.27\n",
      "Iteration 21: with minibatch training loss = 2.59 and accuracy of 0.39\n",
      "Iteration 22: with minibatch training loss = 2.8 and accuracy of 0.17\n",
      "Iteration 23: with minibatch training loss = 2.84 and accuracy of 0.17\n",
      "Iteration 24: with minibatch training loss = 2.82 and accuracy of 0.25\n",
      "Iteration 25: with minibatch training loss = 2.67 and accuracy of 0.31\n",
      "Iteration 26: with minibatch training loss = 2.69 and accuracy of 0.27\n",
      "Iteration 27: with minibatch training loss = 2.89 and accuracy of 0.25\n",
      "Iteration 28: with minibatch training loss = 2.72 and accuracy of 0.31\n",
      "Iteration 29: with minibatch training loss = 2.62 and accuracy of 0.31\n",
      "Iteration 30: with minibatch training loss = 2.7 and accuracy of 0.28\n",
      "Iteration 31: with minibatch training loss = 2.65 and accuracy of 0.28\n",
      "Iteration 32: with minibatch training loss = 2.24 and accuracy of 0.34\n",
      "Iteration 33: with minibatch training loss = 2.62 and accuracy of 0.27\n",
      "Iteration 34: with minibatch training loss = 2.65 and accuracy of 0.3\n",
      "Iteration 35: with minibatch training loss = 2.5 and accuracy of 0.36\n",
      "Iteration 36: with minibatch training loss = 2.17 and accuracy of 0.45\n",
      "Iteration 37: with minibatch training loss = 2.33 and accuracy of 0.44\n",
      "Iteration 38: with minibatch training loss = 2.27 and accuracy of 0.42\n",
      "Iteration 39: with minibatch training loss = 2.57 and accuracy of 0.3\n",
      "Iteration 40: with minibatch training loss = 2.58 and accuracy of 0.36\n",
      "Iteration 41: with minibatch training loss = 2.06 and accuracy of 0.45\n",
      "Iteration 42: with minibatch training loss = 2.26 and accuracy of 0.38\n",
      "Iteration 43: with minibatch training loss = 2.27 and accuracy of 0.47\n",
      "Iteration 44: with minibatch training loss = 2.4 and accuracy of 0.38\n",
      "Iteration 45: with minibatch training loss = 2.05 and accuracy of 0.47\n",
      "Iteration 46: with minibatch training loss = 2.19 and accuracy of 0.45\n",
      "Iteration 47: with minibatch training loss = 2.44 and accuracy of 0.33\n",
      "Iteration 48: with minibatch training loss = 2.38 and accuracy of 0.42\n",
      "Iteration 49: with minibatch training loss = 2.33 and accuracy of 0.36\n",
      "Iteration 50: with minibatch training loss = 1.96 and accuracy of 0.42\n",
      "Iteration 51: with minibatch training loss = 2.09 and accuracy of 0.48\n",
      "Iteration 52: with minibatch training loss = 2.16 and accuracy of 0.45\n",
      "Iteration 53: with minibatch training loss = 2.48 and accuracy of 0.36\n",
      "Iteration 54: with minibatch training loss = 2.12 and accuracy of 0.45\n",
      "Iteration 55: with minibatch training loss = 2.4 and accuracy of 0.39\n",
      "Iteration 56: with minibatch training loss = 2.43 and accuracy of 0.31\n",
      "Iteration 57: with minibatch training loss = 2.67 and accuracy of 0.28\n",
      "Iteration 58: with minibatch training loss = 2.03 and accuracy of 0.44\n",
      "Iteration 59: with minibatch training loss = 2.24 and accuracy of 0.48\n",
      "Iteration 60: with minibatch training loss = 2.32 and accuracy of 0.34\n",
      "Iteration 61: with minibatch training loss = 2 and accuracy of 0.48\n",
      "Iteration 62: with minibatch training loss = 2.24 and accuracy of 0.39\n",
      "Iteration 63: with minibatch training loss = 2 and accuracy of 0.52\n",
      "Iteration 64: with minibatch training loss = 1.97 and accuracy of 0.58\n",
      "Iteration 65: with minibatch training loss = 1.81 and accuracy of 0.53\n",
      "Iteration 66: with minibatch training loss = 2.06 and accuracy of 0.48\n",
      "Iteration 67: with minibatch training loss = 2.36 and accuracy of 0.38\n",
      "Iteration 68: with minibatch training loss = 2.21 and accuracy of 0.44\n",
      "Iteration 69: with minibatch training loss = 2.18 and accuracy of 0.45\n",
      "Iteration 70: with minibatch training loss = 1.96 and accuracy of 0.53\n",
      "Iteration 71: with minibatch training loss = 2.07 and accuracy of 0.52\n",
      "Iteration 72: with minibatch training loss = 2.22 and accuracy of 0.34\n",
      "Iteration 73: with minibatch training loss = 1.84 and accuracy of 0.56\n",
      "Iteration 74: with minibatch training loss = 1.99 and accuracy of 0.36\n",
      "Iteration 75: with minibatch training loss = 2.18 and accuracy of 0.41\n",
      "Iteration 76: with minibatch training loss = 2.13 and accuracy of 0.41\n",
      "Iteration 77: with minibatch training loss = 2.16 and accuracy of 0.41\n",
      "Iteration 78: with minibatch training loss = 1.86 and accuracy of 0.58\n",
      "Iteration 79: with minibatch training loss = 2.12 and accuracy of 0.41\n",
      "Iteration 80: with minibatch training loss = 1.88 and accuracy of 0.47\n",
      "Iteration 81: with minibatch training loss = 1.97 and accuracy of 0.52\n",
      "Iteration 82: with minibatch training loss = 1.91 and accuracy of 0.5\n",
      "Iteration 83: with minibatch training loss = 1.93 and accuracy of 0.55\n",
      "Iteration 84: with minibatch training loss = 2.04 and accuracy of 0.44\n",
      "Iteration 85: with minibatch training loss = 2.04 and accuracy of 0.52\n",
      "Iteration 86: with minibatch training loss = 1.9 and accuracy of 0.5\n",
      "Iteration 87: with minibatch training loss = 2.04 and accuracy of 0.48\n",
      "Iteration 88: with minibatch training loss = 1.85 and accuracy of 0.5\n",
      "Iteration 89: with minibatch training loss = 1.84 and accuracy of 0.55\n",
      "Iteration 90: with minibatch training loss = 1.87 and accuracy of 0.47\n",
      "Iteration 91: with minibatch training loss = 1.83 and accuracy of 0.59\n",
      "Iteration 92: with minibatch training loss = 1.96 and accuracy of 0.55\n",
      "Iteration 93: with minibatch training loss = 2.02 and accuracy of 0.44\n",
      "Iteration 94: with minibatch training loss = 1.89 and accuracy of 0.52\n",
      "Iteration 95: with minibatch training loss = 1.7 and accuracy of 0.59\n",
      "Iteration 96: with minibatch training loss = 1.82 and accuracy of 0.5\n",
      "Iteration 97: with minibatch training loss = 1.88 and accuracy of 0.52\n",
      "Iteration 98: with minibatch training loss = 2.01 and accuracy of 0.47\n",
      "Iteration 99: with minibatch training loss = 1.77 and accuracy of 0.61\n",
      "Iteration 100: with minibatch training loss = 1.9 and accuracy of 0.48\n",
      "Iteration 101: with minibatch training loss = 1.64 and accuracy of 0.58\n",
      "Iteration 102: with minibatch training loss = 1.55 and accuracy of 0.59\n",
      "Iteration 103: with minibatch training loss = 2.11 and accuracy of 0.45\n",
      "Iteration 104: with minibatch training loss = 1.72 and accuracy of 0.58\n",
      "Iteration 105: with minibatch training loss = 1.56 and accuracy of 0.67\n",
      "Iteration 106: with minibatch training loss = 1.7 and accuracy of 0.62\n",
      "Iteration 107: with minibatch training loss = 1.64 and accuracy of 0.66\n",
      "Iteration 108: with minibatch training loss = 1.83 and accuracy of 0.55\n",
      "Iteration 109: with minibatch training loss = 2.07 and accuracy of 0.42\n",
      "Iteration 110: with minibatch training loss = 1.78 and accuracy of 0.58\n",
      "Iteration 111: with minibatch training loss = 2.04 and accuracy of 0.5\n",
      "Iteration 112: with minibatch training loss = 1.91 and accuracy of 0.53\n",
      "Iteration 113: with minibatch training loss = 1.6 and accuracy of 0.64\n",
      "Iteration 114: with minibatch training loss = 1.63 and accuracy of 0.64\n",
      "Iteration 115: with minibatch training loss = 1.88 and accuracy of 0.44\n",
      "Iteration 116: with minibatch training loss = 1.69 and accuracy of 0.66\n",
      "Iteration 117: with minibatch training loss = 1.73 and accuracy of 0.58\n",
      "Iteration 118: with minibatch training loss = 2 and accuracy of 0.48\n",
      "Iteration 119: with minibatch training loss = 1.72 and accuracy of 0.66\n",
      "Iteration 120: with minibatch training loss = 1.74 and accuracy of 0.56\n",
      "Iteration 121: with minibatch training loss = 1.61 and accuracy of 0.66\n",
      "Iteration 122: with minibatch training loss = 1.66 and accuracy of 0.61\n",
      "Iteration 123: with minibatch training loss = 1.96 and accuracy of 0.45\n",
      "Iteration 124: with minibatch training loss = 1.51 and accuracy of 0.67\n",
      "Iteration 125: with minibatch training loss = 1.64 and accuracy of 0.61\n",
      "Iteration 126: with minibatch training loss = 1.83 and accuracy of 0.52\n",
      "Iteration 127: with minibatch training loss = 2.29 and accuracy of 0.52\n",
      "Iteration 128: with minibatch training loss = 1.7 and accuracy of 0.56\n",
      "Iteration 129: with minibatch training loss = 1.68 and accuracy of 0.61\n",
      "Iteration 130: with minibatch training loss = 2.01 and accuracy of 0.48\n",
      "Iteration 131: with minibatch training loss = 1.72 and accuracy of 0.67\n",
      "Iteration 132: with minibatch training loss = 1.54 and accuracy of 0.66\n",
      "Iteration 133: with minibatch training loss = 2 and accuracy of 0.5\n",
      "Iteration 134: with minibatch training loss = 1.75 and accuracy of 0.61\n",
      "Iteration 135: with minibatch training loss = 1.55 and accuracy of 0.69\n",
      "Iteration 136: with minibatch training loss = 1.95 and accuracy of 0.52\n",
      "Iteration 137: with minibatch training loss = 1.86 and accuracy of 0.53\n",
      "Iteration 138: with minibatch training loss = 1.94 and accuracy of 0.5\n",
      "Iteration 139: with minibatch training loss = 1.56 and accuracy of 0.61\n",
      "Iteration 140: with minibatch training loss = 1.53 and accuracy of 0.64\n",
      "Iteration 141: with minibatch training loss = 1.63 and accuracy of 0.62\n",
      "Iteration 142: with minibatch training loss = 1.98 and accuracy of 0.48\n",
      "Iteration 143: with minibatch training loss = 1.59 and accuracy of 0.59\n",
      "Iteration 144: with minibatch training loss = 1.8 and accuracy of 0.59\n",
      "Iteration 145: with minibatch training loss = 1.73 and accuracy of 0.55\n",
      "Iteration 146: with minibatch training loss = 1.64 and accuracy of 0.62\n",
      "Iteration 147: with minibatch training loss = 1.94 and accuracy of 0.48\n",
      "Iteration 148: with minibatch training loss = 1.81 and accuracy of 0.56\n",
      "Iteration 149: with minibatch training loss = 1.69 and accuracy of 0.66\n",
      "Iteration 150: with minibatch training loss = 1.68 and accuracy of 0.53\n",
      "Iteration 151: with minibatch training loss = 1.54 and accuracy of 0.64\n",
      "Iteration 152: with minibatch training loss = 1.51 and accuracy of 0.67\n",
      "Iteration 153: with minibatch training loss = 1.68 and accuracy of 0.62\n",
      "Iteration 154: with minibatch training loss = 1.71 and accuracy of 0.55\n",
      "Iteration 155: with minibatch training loss = 1.53 and accuracy of 0.64\n",
      "Iteration 156: with minibatch training loss = 1.82 and accuracy of 0.58\n",
      "Iteration 157: with minibatch training loss = 1.45 and accuracy of 0.69\n",
      "Iteration 158: with minibatch training loss = 2 and accuracy of 0.5\n",
      "Iteration 159: with minibatch training loss = 1.57 and accuracy of 0.59\n",
      "Iteration 160: with minibatch training loss = 1.63 and accuracy of 0.62\n",
      "Iteration 161: with minibatch training loss = 1.58 and accuracy of 0.67\n",
      "Iteration 162: with minibatch training loss = 1.44 and accuracy of 0.66\n",
      "Iteration 163: with minibatch training loss = 1.6 and accuracy of 0.61\n",
      "Iteration 164: with minibatch training loss = 1.3 and accuracy of 0.75\n",
      "Iteration 165: with minibatch training loss = 1.76 and accuracy of 0.58\n",
      "Iteration 166: with minibatch training loss = 1.71 and accuracy of 0.59\n",
      "Iteration 167: with minibatch training loss = 1.6 and accuracy of 0.62\n",
      "Iteration 168: with minibatch training loss = 1.81 and accuracy of 0.56\n",
      "Iteration 169: with minibatch training loss = 2.02 and accuracy of 0.52\n",
      "Iteration 170: with minibatch training loss = 1.46 and accuracy of 0.69\n",
      "Iteration 171: with minibatch training loss = 1.59 and accuracy of 0.66\n",
      "Iteration 172: with minibatch training loss = 1.5 and accuracy of 0.67\n",
      "Iteration 173: with minibatch training loss = 1.4 and accuracy of 0.7\n",
      "Iteration 174: with minibatch training loss = 1.33 and accuracy of 0.67\n",
      "Iteration 175: with minibatch training loss = 1.99 and accuracy of 0.42\n",
      "Iteration 176: with minibatch training loss = 1.51 and accuracy of 0.62\n",
      "Iteration 177: with minibatch training loss = 1.63 and accuracy of 0.59\n",
      "Iteration 178: with minibatch training loss = 1.62 and accuracy of 0.53\n",
      "Iteration 179: with minibatch training loss = 1.7 and accuracy of 0.58\n",
      "Iteration 180: with minibatch training loss = 1.58 and accuracy of 0.64\n",
      "Iteration 181: with minibatch training loss = 1.45 and accuracy of 0.64\n",
      "Iteration 182: with minibatch training loss = 1.66 and accuracy of 0.56\n",
      "Iteration 183: with minibatch training loss = 1.66 and accuracy of 0.59\n",
      "Iteration 184: with minibatch training loss = 1.72 and accuracy of 0.59\n",
      "Iteration 185: with minibatch training loss = 1.64 and accuracy of 0.62\n",
      "Iteration 186: with minibatch training loss = 2.06 and accuracy of 0.5\n",
      "Iteration 187: with minibatch training loss = 1.64 and accuracy of 0.58\n",
      "Iteration 188: with minibatch training loss = 1.77 and accuracy of 0.53\n",
      "Iteration 189: with minibatch training loss = 1.94 and accuracy of 0.56\n",
      "Iteration 190: with minibatch training loss = 1.54 and accuracy of 0.61\n",
      "Iteration 191: with minibatch training loss = 1.49 and accuracy of 0.61\n",
      "Iteration 192: with minibatch training loss = 1.42 and accuracy of 0.67\n",
      "Iteration 193: with minibatch training loss = 1.79 and accuracy of 0.59\n",
      "Iteration 194: with minibatch training loss = 1.45 and accuracy of 0.64\n",
      "Iteration 195: with minibatch training loss = 1.73 and accuracy of 0.52\n",
      "Iteration 196: with minibatch training loss = 1.82 and accuracy of 0.59\n",
      "Iteration 197: with minibatch training loss = 1.43 and accuracy of 0.69\n",
      "Iteration 198: with minibatch training loss = 1.51 and accuracy of 0.67\n",
      "Iteration 199: with minibatch training loss = 1.24 and accuracy of 0.67\n",
      "Iteration 200: with minibatch training loss = 1.34 and accuracy of 0.69\n",
      "Iteration 201: with minibatch training loss = 1.55 and accuracy of 0.59\n",
      "Iteration 202: with minibatch training loss = 2.01 and accuracy of 0.48\n",
      "Iteration 203: with minibatch training loss = 1.52 and accuracy of 0.67\n",
      "Iteration 204: with minibatch training loss = 1.42 and accuracy of 0.69\n",
      "Iteration 205: with minibatch training loss = 1.82 and accuracy of 0.5\n",
      "Iteration 206: with minibatch training loss = 1.69 and accuracy of 0.56\n",
      "Iteration 207: with minibatch training loss = 1.87 and accuracy of 0.53\n",
      "Iteration 208: with minibatch training loss = 1.97 and accuracy of 0.47\n",
      "Iteration 209: with minibatch training loss = 1.77 and accuracy of 0.56\n",
      "Iteration 210: with minibatch training loss = 1.47 and accuracy of 0.64\n",
      "Iteration 211: with minibatch training loss = 1.62 and accuracy of 0.62\n",
      "Iteration 212: with minibatch training loss = 1.98 and accuracy of 0.47\n",
      "Iteration 213: with minibatch training loss = 1.66 and accuracy of 0.62\n",
      "Iteration 214: with minibatch training loss = 1.71 and accuracy of 0.55\n",
      "Iteration 215: with minibatch training loss = 1.32 and accuracy of 0.73\n",
      "Iteration 216: with minibatch training loss = 1.8 and accuracy of 0.47\n",
      "Iteration 217: with minibatch training loss = 1.62 and accuracy of 0.58\n",
      "Iteration 218: with minibatch training loss = 1.34 and accuracy of 0.73\n",
      "Iteration 219: with minibatch training loss = 1.87 and accuracy of 0.56\n",
      "Iteration 220: with minibatch training loss = 1.58 and accuracy of 0.61\n",
      "Iteration 221: with minibatch training loss = 1.47 and accuracy of 0.58\n",
      "Iteration 222: with minibatch training loss = 1.25 and accuracy of 0.73\n",
      "Iteration 223: with minibatch training loss = 1.54 and accuracy of 0.67\n",
      "Iteration 224: with minibatch training loss = 1.61 and accuracy of 0.61\n",
      "Iteration 225: with minibatch training loss = 1.74 and accuracy of 0.56\n",
      "Iteration 226: with minibatch training loss = 1.5 and accuracy of 0.62\n",
      "Iteration 227: with minibatch training loss = 1.89 and accuracy of 0.48\n",
      "Iteration 228: with minibatch training loss = 1.47 and accuracy of 0.62\n",
      "Iteration 229: with minibatch training loss = 1.64 and accuracy of 0.61\n",
      "Iteration 230: with minibatch training loss = 1.42 and accuracy of 0.67\n",
      "Iteration 231: with minibatch training loss = 1.33 and accuracy of 0.72\n",
      "Iteration 232: with minibatch training loss = 1.41 and accuracy of 0.72\n",
      "Iteration 233: with minibatch training loss = 1.56 and accuracy of 0.58\n",
      "Iteration 234: with minibatch training loss = 1.43 and accuracy of 0.69\n",
      "Iteration 235: with minibatch training loss = 1.43 and accuracy of 0.64\n",
      "Iteration 236: with minibatch training loss = 1.65 and accuracy of 0.64\n",
      "Iteration 237: with minibatch training loss = 1.46 and accuracy of 0.67\n",
      "Iteration 238: with minibatch training loss = 1.51 and accuracy of 0.66\n",
      "Iteration 239: with minibatch training loss = 1.67 and accuracy of 0.58\n",
      "Iteration 240: with minibatch training loss = 1.54 and accuracy of 0.62\n",
      "Iteration 241: with minibatch training loss = 1.35 and accuracy of 0.72\n",
      "Iteration 242: with minibatch training loss = 1.36 and accuracy of 0.72\n",
      "Iteration 243: with minibatch training loss = 1.61 and accuracy of 0.53\n",
      "Iteration 244: with minibatch training loss = 1.31 and accuracy of 0.69\n",
      "Iteration 245: with minibatch training loss = 1.55 and accuracy of 0.62\n",
      "Iteration 246: with minibatch training loss = 1.44 and accuracy of 0.59\n",
      "Iteration 247: with minibatch training loss = 1.57 and accuracy of 0.62\n",
      "Iteration 248: with minibatch training loss = 1.58 and accuracy of 0.61\n",
      "Iteration 249: with minibatch training loss = 1.7 and accuracy of 0.59\n",
      "Iteration 250: with minibatch training loss = 1.55 and accuracy of 0.66\n",
      "Iteration 251: with minibatch training loss = 1.6 and accuracy of 0.53\n",
      "Iteration 252: with minibatch training loss = 1.58 and accuracy of 0.56\n",
      "Iteration 253: with minibatch training loss = 1.46 and accuracy of 0.66\n",
      "Iteration 254: with minibatch training loss = 1.52 and accuracy of 0.66\n",
      "Iteration 255: with minibatch training loss = 1.76 and accuracy of 0.56\n",
      "Iteration 256: with minibatch training loss = 1.47 and accuracy of 0.66\n",
      "Iteration 257: with minibatch training loss = 1.54 and accuracy of 0.59\n",
      "Iteration 258: with minibatch training loss = 1.49 and accuracy of 0.64\n",
      "Iteration 259: with minibatch training loss = 1.43 and accuracy of 0.66\n",
      "Iteration 260: with minibatch training loss = 1.49 and accuracy of 0.64\n",
      "Iteration 261: with minibatch training loss = 1.52 and accuracy of 0.59\n",
      "Iteration 262: with minibatch training loss = 1.61 and accuracy of 0.59\n",
      "Iteration 263: with minibatch training loss = 1.68 and accuracy of 0.58\n",
      "Iteration 264: with minibatch training loss = 1.48 and accuracy of 0.69\n",
      "Iteration 265: with minibatch training loss = 1.81 and accuracy of 0.48\n",
      "Iteration 266: with minibatch training loss = 1.41 and accuracy of 0.69\n",
      "Iteration 267: with minibatch training loss = 1.82 and accuracy of 0.56\n",
      "Iteration 268: with minibatch training loss = 1.88 and accuracy of 0.48\n",
      "Iteration 269: with minibatch training loss = 1.57 and accuracy of 0.62\n",
      "Iteration 270: with minibatch training loss = 1.22 and accuracy of 0.77\n",
      "Iteration 271: with minibatch training loss = 1.76 and accuracy of 0.52\n",
      "Iteration 272: with minibatch training loss = 1.51 and accuracy of 0.62\n",
      "Iteration 273: with minibatch training loss = 1.53 and accuracy of 0.61\n",
      "Iteration 274: with minibatch training loss = 1.48 and accuracy of 0.61\n",
      "Iteration 275: with minibatch training loss = 1.63 and accuracy of 0.58\n",
      "Iteration 276: with minibatch training loss = 1.31 and accuracy of 0.66\n",
      "Iteration 277: with minibatch training loss = 1.4 and accuracy of 0.69\n",
      "Iteration 278: with minibatch training loss = 1.61 and accuracy of 0.62\n",
      "Iteration 279: with minibatch training loss = 1.37 and accuracy of 0.67\n",
      "Iteration 280: with minibatch training loss = 1.95 and accuracy of 0.47\n",
      "Iteration 281: with minibatch training loss = 1.29 and accuracy of 0.66\n",
      "Iteration 282: with minibatch training loss = 1.6 and accuracy of 0.59\n",
      "Iteration 283: with minibatch training loss = 1.62 and accuracy of 0.61\n",
      "Iteration 284: with minibatch training loss = 1.39 and accuracy of 0.69\n",
      "Iteration 285: with minibatch training loss = 1.63 and accuracy of 0.64\n",
      "Iteration 286: with minibatch training loss = 1.58 and accuracy of 0.64\n",
      "Iteration 287: with minibatch training loss = 1.47 and accuracy of 0.69\n",
      "Iteration 288: with minibatch training loss = 1.57 and accuracy of 0.61\n",
      "Iteration 289: with minibatch training loss = 1.45 and accuracy of 0.66\n",
      "Iteration 290: with minibatch training loss = 1.53 and accuracy of 0.66\n",
      "Iteration 291: with minibatch training loss = 1.62 and accuracy of 0.58\n",
      "Iteration 292: with minibatch training loss = 1.36 and accuracy of 0.69\n",
      "Iteration 293: with minibatch training loss = 1.57 and accuracy of 0.64\n",
      "Iteration 294: with minibatch training loss = 1.54 and accuracy of 0.58\n",
      "Iteration 295: with minibatch training loss = 1.49 and accuracy of 0.59\n",
      "Iteration 296: with minibatch training loss = 1.62 and accuracy of 0.62\n",
      "Iteration 297: with minibatch training loss = 1.62 and accuracy of 0.55\n",
      "Iteration 298: with minibatch training loss = 1.39 and accuracy of 0.67\n",
      "Iteration 299: with minibatch training loss = 1.46 and accuracy of 0.64\n",
      "Iteration 300: with minibatch training loss = 1.15 and accuracy of 0.73\n",
      "Iteration 301: with minibatch training loss = 1.54 and accuracy of 0.59\n",
      "Iteration 302: with minibatch training loss = 1.41 and accuracy of 0.66\n",
      "Iteration 303: with minibatch training loss = 1.73 and accuracy of 0.58\n",
      "Iteration 304: with minibatch training loss = 1.52 and accuracy of 0.64\n",
      "Iteration 305: with minibatch training loss = 1.45 and accuracy of 0.64\n",
      "Iteration 306: with minibatch training loss = 2.01 and accuracy of 0.47\n",
      "Iteration 307: with minibatch training loss = 1.29 and accuracy of 0.69\n",
      "Iteration 308: with minibatch training loss = 1.68 and accuracy of 0.55\n",
      "Iteration 309: with minibatch training loss = 1.79 and accuracy of 0.55\n",
      "Iteration 310: with minibatch training loss = 1.68 and accuracy of 0.62\n",
      "Iteration 311: with minibatch training loss = 1.47 and accuracy of 0.67\n",
      "Iteration 312: with minibatch training loss = 1.61 and accuracy of 0.58\n",
      "Iteration 313: with minibatch training loss = 1.47 and accuracy of 0.66\n",
      "Iteration 314: with minibatch training loss = 1.53 and accuracy of 0.59\n",
      "Iteration 315: with minibatch training loss = 1.42 and accuracy of 0.69\n",
      "Iteration 316: with minibatch training loss = 1.21 and accuracy of 0.67\n",
      "Iteration 317: with minibatch training loss = 1.5 and accuracy of 0.66\n",
      "Iteration 318: with minibatch training loss = 1.4 and accuracy of 0.72\n",
      "Iteration 319: with minibatch training loss = 1.51 and accuracy of 0.62\n",
      "Iteration 320: with minibatch training loss = 1.54 and accuracy of 0.66\n",
      "Iteration 321: with minibatch training loss = 1.26 and accuracy of 0.69\n",
      "Iteration 322: with minibatch training loss = 1.61 and accuracy of 0.56\n",
      "Iteration 323: with minibatch training loss = 1.54 and accuracy of 0.56\n",
      "Iteration 324: with minibatch training loss = 1.11 and accuracy of 0.77\n",
      "Iteration 325: with minibatch training loss = 1.46 and accuracy of 0.66\n",
      "Iteration 326: with minibatch training loss = 1.26 and accuracy of 0.7\n",
      "Iteration 327: with minibatch training loss = 1.57 and accuracy of 0.59\n",
      "Iteration 328: with minibatch training loss = 1.21 and accuracy of 0.75\n",
      "Iteration 329: with minibatch training loss = 1.39 and accuracy of 0.69\n",
      "Iteration 330: with minibatch training loss = 1.81 and accuracy of 0.5\n",
      "Iteration 331: with minibatch training loss = 1.21 and accuracy of 0.77\n",
      "Iteration 332: with minibatch training loss = 1.3 and accuracy of 0.72\n",
      "Iteration 333: with minibatch training loss = 1.33 and accuracy of 0.67\n",
      "Iteration 334: with minibatch training loss = 1.7 and accuracy of 0.52\n",
      "Iteration 335: with minibatch training loss = 1.79 and accuracy of 0.58\n",
      "Iteration 336: with minibatch training loss = 1.11 and accuracy of 0.8\n",
      "Iteration 337: with minibatch training loss = 1.29 and accuracy of 0.69\n",
      "Iteration 338: with minibatch training loss = 1.3 and accuracy of 0.72\n",
      "Iteration 339: with minibatch training loss = 1.49 and accuracy of 0.62\n",
      "Iteration 340: with minibatch training loss = 1.74 and accuracy of 0.58\n",
      "Iteration 341: with minibatch training loss = 1.4 and accuracy of 0.62\n",
      "Iteration 342: with minibatch training loss = 1.4 and accuracy of 0.62\n",
      "Iteration 343: with minibatch training loss = 1.26 and accuracy of 0.66\n",
      "Iteration 344: with minibatch training loss = 1.73 and accuracy of 0.55\n",
      "Iteration 345: with minibatch training loss = 1.49 and accuracy of 0.64\n",
      "Iteration 346: with minibatch training loss = 1.21 and accuracy of 0.7\n",
      "Iteration 347: with minibatch training loss = 1.34 and accuracy of 0.64\n",
      "Iteration 348: with minibatch training loss = 1.26 and accuracy of 0.73\n",
      "Iteration 349: with minibatch training loss = 1.25 and accuracy of 0.73\n",
      "Iteration 350: with minibatch training loss = 1.39 and accuracy of 0.66\n",
      "Iteration 351: with minibatch training loss = 1.46 and accuracy of 0.64\n",
      "Iteration 352: with minibatch training loss = 1.33 and accuracy of 0.7\n",
      "Iteration 353: with minibatch training loss = 1.26 and accuracy of 0.72\n",
      "Iteration 354: with minibatch training loss = 1.38 and accuracy of 0.66\n",
      "Iteration 355: with minibatch training loss = 1.52 and accuracy of 0.62\n",
      "Iteration 356: with minibatch training loss = 1.38 and accuracy of 0.67\n",
      "Iteration 357: with minibatch training loss = 1.64 and accuracy of 0.55\n",
      "Iteration 358: with minibatch training loss = 1.3 and accuracy of 0.64\n",
      "Iteration 359: with minibatch training loss = 1.52 and accuracy of 0.67\n",
      "Iteration 360: with minibatch training loss = 1.37 and accuracy of 0.66\n",
      "Iteration 361: with minibatch training loss = 1.33 and accuracy of 0.67\n",
      "Iteration 362: with minibatch training loss = 1.62 and accuracy of 0.58\n",
      "Iteration 363: with minibatch training loss = 1.55 and accuracy of 0.66\n",
      "Iteration 364: with minibatch training loss = 1.6 and accuracy of 0.59\n",
      "Iteration 365: with minibatch training loss = 1.18 and accuracy of 0.75\n",
      "Iteration 366: with minibatch training loss = 1.39 and accuracy of 0.67\n",
      "Iteration 367: with minibatch training loss = 1.43 and accuracy of 0.62\n",
      "Iteration 368: with minibatch training loss = 1.43 and accuracy of 0.61\n",
      "Iteration 369: with minibatch training loss = 1.53 and accuracy of 0.64\n",
      "Iteration 370: with minibatch training loss = 1.27 and accuracy of 0.67\n",
      "Iteration 371: with minibatch training loss = 1.56 and accuracy of 0.62\n",
      "Iteration 372: with minibatch training loss = 1.43 and accuracy of 0.64\n",
      "Iteration 373: with minibatch training loss = 0.968 and accuracy of 0.84\n",
      "Iteration 374: with minibatch training loss = 1.6 and accuracy of 0.62\n",
      "Iteration 375: with minibatch training loss = 1.5 and accuracy of 0.67\n",
      "Iteration 376: with minibatch training loss = 1.18 and accuracy of 0.72\n",
      "Iteration 377: with minibatch training loss = 1.39 and accuracy of 0.66\n",
      "Iteration 378: with minibatch training loss = 1.49 and accuracy of 0.61\n",
      "Iteration 379: with minibatch training loss = 1.25 and accuracy of 0.75\n",
      "Iteration 380: with minibatch training loss = 1.6 and accuracy of 0.58\n",
      "Iteration 381: with minibatch training loss = 1.44 and accuracy of 0.62\n",
      "Iteration 382: with minibatch training loss = 1.68 and accuracy of 0.52\n",
      "Iteration 383: with minibatch training loss = 1.5 and accuracy of 0.59\n",
      "Iteration 384: with minibatch training loss = 1.41 and accuracy of 0.64\n",
      "Iteration 385: with minibatch training loss = 1.14 and accuracy of 0.75\n",
      "Iteration 386: with minibatch training loss = 1.48 and accuracy of 0.61\n",
      "Iteration 387: with minibatch training loss = 1.49 and accuracy of 0.61\n",
      "Iteration 388: with minibatch training loss = 1.1 and accuracy of 0.77\n",
      "Iteration 389: with minibatch training loss = 1.69 and accuracy of 0.59\n",
      "Iteration 390: with minibatch training loss = 1.32 and accuracy of 0.66\n",
      "Iteration 391: with minibatch training loss = 1.26 and accuracy of 0.72\n",
      "Iteration 392: with minibatch training loss = 1.22 and accuracy of 0.75\n",
      "Iteration 393: with minibatch training loss = 1.7 and accuracy of 0.56\n",
      "Iteration 394: with minibatch training loss = 1.54 and accuracy of 0.61\n",
      "Iteration 395: with minibatch training loss = 1.52 and accuracy of 0.64\n",
      "Iteration 396: with minibatch training loss = 1.33 and accuracy of 0.67\n",
      "Iteration 397: with minibatch training loss = 1.76 and accuracy of 0.55\n",
      "Iteration 398: with minibatch training loss = 1.21 and accuracy of 0.67\n",
      "Iteration 399: with minibatch training loss = 1.49 and accuracy of 0.59\n",
      "Iteration 400: with minibatch training loss = 1.52 and accuracy of 0.62\n",
      "Iteration 401: with minibatch training loss = 1.48 and accuracy of 0.69\n",
      "Iteration 402: with minibatch training loss = 1.15 and accuracy of 0.77\n",
      "Iteration 403: with minibatch training loss = 1.05 and accuracy of 0.75\n",
      "Iteration 404: with minibatch training loss = 1.13 and accuracy of 0.7\n",
      "Iteration 405: with minibatch training loss = 1.49 and accuracy of 0.59\n",
      "Iteration 406: with minibatch training loss = 1.3 and accuracy of 0.69\n",
      "Iteration 407: with minibatch training loss = 1.22 and accuracy of 0.75\n",
      "Iteration 408: with minibatch training loss = 1.7 and accuracy of 0.56\n",
      "Iteration 409: with minibatch training loss = 1.46 and accuracy of 0.66\n",
      "Iteration 410: with minibatch training loss = 1.5 and accuracy of 0.67\n",
      "Iteration 411: with minibatch training loss = 1.77 and accuracy of 0.56\n",
      "Iteration 412: with minibatch training loss = 1.3 and accuracy of 0.7\n",
      "Iteration 413: with minibatch training loss = 1.43 and accuracy of 0.64\n",
      "Iteration 414: with minibatch training loss = 1.36 and accuracy of 0.67\n",
      "Iteration 415: with minibatch training loss = 1.25 and accuracy of 0.64\n",
      "Iteration 416: with minibatch training loss = 1.11 and accuracy of 0.8\n",
      "Iteration 417: with minibatch training loss = 1.1 and accuracy of 0.77\n",
      "Iteration 418: with minibatch training loss = 1.06 and accuracy of 0.77\n",
      "Iteration 419: with minibatch training loss = 1.24 and accuracy of 0.69\n",
      "Iteration 420: with minibatch training loss = 1.44 and accuracy of 0.62\n",
      "Iteration 421: with minibatch training loss = 1.14 and accuracy of 0.73\n",
      "Iteration 422: with minibatch training loss = 1.42 and accuracy of 0.7\n",
      "Iteration 423: with minibatch training loss = 1.61 and accuracy of 0.61\n",
      "Iteration 424: with minibatch training loss = 1.14 and accuracy of 0.78\n",
      "Iteration 425: with minibatch training loss = 1.51 and accuracy of 0.64\n",
      "Iteration 426: with minibatch training loss = 1.31 and accuracy of 0.69\n",
      "Iteration 427: with minibatch training loss = 1.26 and accuracy of 0.69\n",
      "Iteration 428: with minibatch training loss = 1.42 and accuracy of 0.67\n",
      "Iteration 429: with minibatch training loss = 1.52 and accuracy of 0.62\n",
      "Iteration 430: with minibatch training loss = 1.38 and accuracy of 0.66\n",
      "Iteration 431: with minibatch training loss = 1.27 and accuracy of 0.7\n",
      "Iteration 432: with minibatch training loss = 1.35 and accuracy of 0.62\n",
      "Iteration 433: with minibatch training loss = 1.37 and accuracy of 0.69\n",
      "Iteration 434: with minibatch training loss = 1.3 and accuracy of 0.72\n",
      "Iteration 435: with minibatch training loss = 1.32 and accuracy of 0.66\n",
      "Iteration 436: with minibatch training loss = 1.46 and accuracy of 0.59\n",
      "Iteration 437: with minibatch training loss = 1.72 and accuracy of 0.55\n",
      "Iteration 438: with minibatch training loss = 1.42 and accuracy of 0.69\n",
      "Iteration 439: with minibatch training loss = 1.2 and accuracy of 0.72\n",
      "Iteration 440: with minibatch training loss = 1.48 and accuracy of 0.64\n",
      "Iteration 441: with minibatch training loss = 1.36 and accuracy of 0.64\n",
      "Iteration 442: with minibatch training loss = 1.24 and accuracy of 0.73\n",
      "Iteration 443: with minibatch training loss = 1.47 and accuracy of 0.66\n",
      "Iteration 444: with minibatch training loss = 1.35 and accuracy of 0.69\n",
      "Iteration 445: with minibatch training loss = 1.61 and accuracy of 0.59\n",
      "Iteration 446: with minibatch training loss = 1.22 and accuracy of 0.69\n",
      "Iteration 447: with minibatch training loss = 1.66 and accuracy of 0.56\n",
      "Iteration 448: with minibatch training loss = 1.41 and accuracy of 0.67\n",
      "Iteration 449: with minibatch training loss = 1.25 and accuracy of 0.7\n",
      "Iteration 450: with minibatch training loss = 1.5 and accuracy of 0.61\n",
      "Iteration 451: with minibatch training loss = 1.21 and accuracy of 0.72\n",
      "Iteration 452: with minibatch training loss = 1.46 and accuracy of 0.67\n",
      "Iteration 453: with minibatch training loss = 1.12 and accuracy of 0.73\n",
      "Iteration 454: with minibatch training loss = 1.35 and accuracy of 0.67\n",
      "Iteration 455: with minibatch training loss = 1.08 and accuracy of 0.77\n",
      "Iteration 456: with minibatch training loss = 1.11 and accuracy of 0.72\n",
      "Iteration 457: with minibatch training loss = 1.34 and accuracy of 0.7\n",
      "Iteration 458: with minibatch training loss = 1.46 and accuracy of 0.69\n",
      "Iteration 459: with minibatch training loss = 1.48 and accuracy of 0.62\n",
      "Iteration 460: with minibatch training loss = 1.35 and accuracy of 0.67\n",
      "Iteration 461: with minibatch training loss = 1.37 and accuracy of 0.66\n",
      "Iteration 462: with minibatch training loss = 1.64 and accuracy of 0.59\n",
      "Iteration 463: with minibatch training loss = 1.39 and accuracy of 0.69\n",
      "Iteration 464: with minibatch training loss = 1.38 and accuracy of 0.66\n",
      "Iteration 465: with minibatch training loss = 1.31 and accuracy of 0.67\n",
      "Iteration 466: with minibatch training loss = 1.08 and accuracy of 0.77\n",
      "Iteration 467: with minibatch training loss = 1.24 and accuracy of 0.73\n",
      "Iteration 468: with minibatch training loss = 1.12 and accuracy of 0.77\n",
      "Iteration 469: with minibatch training loss = 1.53 and accuracy of 0.66\n",
      "Iteration 470: with minibatch training loss = 1.22 and accuracy of 0.7\n",
      "Iteration 471: with minibatch training loss = 1.47 and accuracy of 0.67\n",
      "Iteration 472: with minibatch training loss = 1.2 and accuracy of 0.73\n",
      "Iteration 473: with minibatch training loss = 1.24 and accuracy of 0.69\n",
      "Iteration 474: with minibatch training loss = 1.56 and accuracy of 0.62\n",
      "Iteration 475: with minibatch training loss = 1.55 and accuracy of 0.61\n",
      "Iteration 476: with minibatch training loss = 1.28 and accuracy of 0.62\n",
      "Iteration 477: with minibatch training loss = 1.14 and accuracy of 0.75\n",
      "Iteration 478: with minibatch training loss = 1.09 and accuracy of 0.77\n",
      "Iteration 479: with minibatch training loss = 1.09 and accuracy of 0.78\n",
      "Iteration 480: with minibatch training loss = 1.25 and accuracy of 0.69\n",
      "Iteration 481: with minibatch training loss = 1.36 and accuracy of 0.67\n",
      "Iteration 482: with minibatch training loss = 1.34 and accuracy of 0.69\n",
      "Iteration 483: with minibatch training loss = 1.35 and accuracy of 0.67\n",
      "Iteration 484: with minibatch training loss = 1.46 and accuracy of 0.61\n",
      "Iteration 485: with minibatch training loss = 1.51 and accuracy of 0.64\n",
      "Iteration 486: with minibatch training loss = 1.31 and accuracy of 0.66\n",
      "Iteration 487: with minibatch training loss = 1.47 and accuracy of 0.64\n",
      "Iteration 488: with minibatch training loss = 1.16 and accuracy of 0.72\n",
      "Iteration 489: with minibatch training loss = 1.14 and accuracy of 0.72\n",
      "Iteration 490: with minibatch training loss = 1.61 and accuracy of 0.58\n",
      "Iteration 491: with minibatch training loss = 1.17 and accuracy of 0.73\n",
      "Iteration 492: with minibatch training loss = 1.13 and accuracy of 0.73\n",
      "Iteration 493: with minibatch training loss = 1.1 and accuracy of 0.72\n",
      "Iteration 494: with minibatch training loss = 1.04 and accuracy of 0.8\n",
      "Iteration 495: with minibatch training loss = 1.29 and accuracy of 0.7\n",
      "Iteration 496: with minibatch training loss = 1.25 and accuracy of 0.69\n",
      "Iteration 497: with minibatch training loss = 1.24 and accuracy of 0.67\n",
      "Iteration 498: with minibatch training loss = 1.18 and accuracy of 0.72\n",
      "Iteration 499: with minibatch training loss = 1.18 and accuracy of 0.7\n",
      "Iteration 500: with minibatch training loss = 1.34 and accuracy of 0.64\n",
      "Iteration 501: with minibatch training loss = 1.38 and accuracy of 0.69\n",
      "Iteration 502: with minibatch training loss = 1.31 and accuracy of 0.7\n",
      "Iteration 503: with minibatch training loss = 1.12 and accuracy of 0.7\n",
      "Iteration 504: with minibatch training loss = 1.2 and accuracy of 0.69\n",
      "Iteration 505: with minibatch training loss = 1.21 and accuracy of 0.72\n",
      "Iteration 506: with minibatch training loss = 1.41 and accuracy of 0.64\n",
      "Iteration 507: with minibatch training loss = 1.28 and accuracy of 0.67\n",
      "Iteration 508: with minibatch training loss = 1.24 and accuracy of 0.72\n",
      "Iteration 509: with minibatch training loss = 1.47 and accuracy of 0.62\n",
      "Iteration 510: with minibatch training loss = 1.1 and accuracy of 0.72\n",
      "Iteration 511: with minibatch training loss = 1.04 and accuracy of 0.77\n",
      "Iteration 512: with minibatch training loss = 1.41 and accuracy of 0.67\n",
      "Iteration 513: with minibatch training loss = 1.46 and accuracy of 0.67\n",
      "Iteration 514: with minibatch training loss = 1.39 and accuracy of 0.7\n",
      "Iteration 515: with minibatch training loss = 1.16 and accuracy of 0.75\n",
      "Iteration 516: with minibatch training loss = 1.31 and accuracy of 0.69\n",
      "Iteration 517: with minibatch training loss = 1.43 and accuracy of 0.64\n",
      "Iteration 518: with minibatch training loss = 1.26 and accuracy of 0.7\n",
      "Iteration 519: with minibatch training loss = 1.25 and accuracy of 0.72\n",
      "Iteration 520: with minibatch training loss = 1.12 and accuracy of 0.72\n",
      "Iteration 521: with minibatch training loss = 1.27 and accuracy of 0.69\n",
      "Iteration 522: with minibatch training loss = 1.31 and accuracy of 0.73\n",
      "Iteration 523: with minibatch training loss = 1.24 and accuracy of 0.7\n",
      "Iteration 524: with minibatch training loss = 1.43 and accuracy of 0.62\n",
      "Iteration 525: with minibatch training loss = 0.999 and accuracy of 0.77\n",
      "Iteration 526: with minibatch training loss = 1.34 and accuracy of 0.69\n",
      "Iteration 527: with minibatch training loss = 1 and accuracy of 0.72\n",
      "Iteration 528: with minibatch training loss = 1.46 and accuracy of 0.64\n",
      "Iteration 529: with minibatch training loss = 1.14 and accuracy of 0.78\n",
      "Iteration 530: with minibatch training loss = 1.62 and accuracy of 0.58\n",
      "Iteration 531: with minibatch training loss = 1.17 and accuracy of 0.75\n",
      "Iteration 532: with minibatch training loss = 1.2 and accuracy of 0.75\n",
      "Iteration 533: with minibatch training loss = 1.46 and accuracy of 0.62\n",
      "Iteration 534: with minibatch training loss = 1.45 and accuracy of 0.61\n",
      "Iteration 535: with minibatch training loss = 1.11 and accuracy of 0.73\n",
      "Iteration 536: with minibatch training loss = 1.35 and accuracy of 0.7\n",
      "Iteration 537: with minibatch training loss = 1.28 and accuracy of 0.67\n",
      "Iteration 538: with minibatch training loss = 1.21 and accuracy of 0.72\n",
      "Iteration 539: with minibatch training loss = 1.34 and accuracy of 0.64\n",
      "Iteration 540: with minibatch training loss = 1.25 and accuracy of 0.7\n",
      "Iteration 541: with minibatch training loss = 1.4 and accuracy of 0.67\n",
      "Iteration 542: with minibatch training loss = 1.19 and accuracy of 0.73\n",
      "Iteration 543: with minibatch training loss = 1.36 and accuracy of 0.66\n",
      "Iteration 544: with minibatch training loss = 1.26 and accuracy of 0.7\n",
      "Iteration 545: with minibatch training loss = 1.36 and accuracy of 0.66\n",
      "Iteration 546: with minibatch training loss = 1.34 and accuracy of 0.66\n",
      "Iteration 547: with minibatch training loss = 1.36 and accuracy of 0.69\n",
      "Iteration 548: with minibatch training loss = 1.19 and accuracy of 0.73\n",
      "Iteration 549: with minibatch training loss = 1.29 and accuracy of 0.72\n",
      "Iteration 550: with minibatch training loss = 1.32 and accuracy of 0.67\n",
      "Iteration 551: with minibatch training loss = 1.48 and accuracy of 0.62\n",
      "Iteration 552: with minibatch training loss = 1.3 and accuracy of 0.7\n",
      "Iteration 553: with minibatch training loss = 1.8 and accuracy of 0.52\n",
      "Iteration 554: with minibatch training loss = 1.3 and accuracy of 0.7\n",
      "Iteration 555: with minibatch training loss = 1.24 and accuracy of 0.7\n",
      "Iteration 556: with minibatch training loss = 1.31 and accuracy of 0.72\n",
      "Iteration 557: with minibatch training loss = 1.05 and accuracy of 0.75\n",
      "Iteration 558: with minibatch training loss = 1.41 and accuracy of 0.62\n",
      "Iteration 559: with minibatch training loss = 1.5 and accuracy of 0.61\n",
      "Iteration 560: with minibatch training loss = 1.15 and accuracy of 0.73\n",
      "Iteration 561: with minibatch training loss = 1.33 and accuracy of 0.7\n",
      "Iteration 562: with minibatch training loss = 1.32 and accuracy of 0.67\n",
      "Iteration 563: with minibatch training loss = 1.23 and accuracy of 0.67\n",
      "Iteration 564: with minibatch training loss = 1.55 and accuracy of 0.58\n",
      "Iteration 565: with minibatch training loss = 1.34 and accuracy of 0.7\n",
      "Iteration 566: with minibatch training loss = 1.28 and accuracy of 0.69\n",
      "Iteration 567: with minibatch training loss = 1.21 and accuracy of 0.7\n",
      "Iteration 568: with minibatch training loss = 1.04 and accuracy of 0.77\n",
      "Iteration 569: with minibatch training loss = 1.21 and accuracy of 0.72\n",
      "Iteration 570: with minibatch training loss = 1.32 and accuracy of 0.64\n",
      "Iteration 571: with minibatch training loss = 1.18 and accuracy of 0.7\n",
      "Iteration 572: with minibatch training loss = 1.39 and accuracy of 0.66\n",
      "Iteration 573: with minibatch training loss = 1.29 and accuracy of 0.67\n",
      "Iteration 574: with minibatch training loss = 1.29 and accuracy of 0.77\n",
      "Iteration 575: with minibatch training loss = 1.46 and accuracy of 0.66\n",
      "Iteration 576: with minibatch training loss = 1.28 and accuracy of 0.67\n",
      "Iteration 577: with minibatch training loss = 1.42 and accuracy of 0.62\n",
      "Iteration 578: with minibatch training loss = 1.19 and accuracy of 0.7\n",
      "Iteration 579: with minibatch training loss = 1.21 and accuracy of 0.7\n",
      "Iteration 580: with minibatch training loss = 1.33 and accuracy of 0.66\n",
      "Iteration 581: with minibatch training loss = 1.33 and accuracy of 0.69\n",
      "Iteration 582: with minibatch training loss = 1.05 and accuracy of 0.77\n",
      "Iteration 583: with minibatch training loss = 1.25 and accuracy of 0.72\n",
      "Iteration 584: with minibatch training loss = 1.2 and accuracy of 0.72\n",
      "Iteration 585: with minibatch training loss = 1.41 and accuracy of 0.66\n",
      "Iteration 586: with minibatch training loss = 1.13 and accuracy of 0.75\n",
      "Iteration 587: with minibatch training loss = 1.19 and accuracy of 0.73\n",
      "Iteration 588: with minibatch training loss = 1.49 and accuracy of 0.61\n",
      "Iteration 589: with minibatch training loss = 1.38 and accuracy of 0.64\n",
      "Iteration 590: with minibatch training loss = 1.36 and accuracy of 0.66\n",
      "Iteration 591: with minibatch training loss = 1.16 and accuracy of 0.78\n",
      "Iteration 592: with minibatch training loss = 1 and accuracy of 0.83\n",
      "Iteration 593: with minibatch training loss = 1.46 and accuracy of 0.64\n",
      "Iteration 594: with minibatch training loss = 1.39 and accuracy of 0.69\n",
      "Iteration 595: with minibatch training loss = 1.75 and accuracy of 0.53\n",
      "Iteration 596: with minibatch training loss = 1.6 and accuracy of 0.59\n",
      "Iteration 597: with minibatch training loss = 1.41 and accuracy of 0.64\n",
      "Iteration 598: with minibatch training loss = 1.28 and accuracy of 0.67\n",
      "Iteration 599: with minibatch training loss = 1.45 and accuracy of 0.61\n",
      "Iteration 600: with minibatch training loss = 1.56 and accuracy of 0.58\n",
      "Iteration 601: with minibatch training loss = 1.26 and accuracy of 0.69\n",
      "Iteration 602: with minibatch training loss = 1.11 and accuracy of 0.75\n",
      "Iteration 603: with minibatch training loss = 1.27 and accuracy of 0.72\n",
      "Iteration 604: with minibatch training loss = 1.31 and accuracy of 0.72\n",
      "Iteration 605: with minibatch training loss = 1.05 and accuracy of 0.8\n",
      "Iteration 606: with minibatch training loss = 1.05 and accuracy of 0.77\n",
      "Iteration 607: with minibatch training loss = 1.22 and accuracy of 0.7\n",
      "Iteration 608: with minibatch training loss = 1.18 and accuracy of 0.75\n",
      "Iteration 609: with minibatch training loss = 1.17 and accuracy of 0.73\n",
      "Iteration 610: with minibatch training loss = 1.32 and accuracy of 0.72\n",
      "Iteration 611: with minibatch training loss = 1.23 and accuracy of 0.72\n",
      "Iteration 612: with minibatch training loss = 1.47 and accuracy of 0.62\n",
      "Iteration 613: with minibatch training loss = 1.16 and accuracy of 0.7\n",
      "Iteration 614: with minibatch training loss = 1.39 and accuracy of 0.62\n",
      "Iteration 615: with minibatch training loss = 1.37 and accuracy of 0.66\n",
      "Iteration 616: with minibatch training loss = 1.26 and accuracy of 0.72\n",
      "Iteration 617: with minibatch training loss = 1.34 and accuracy of 0.66\n",
      "Iteration 618: with minibatch training loss = 1.36 and accuracy of 0.66\n",
      "Iteration 619: with minibatch training loss = 1.53 and accuracy of 0.62\n",
      "Iteration 620: with minibatch training loss = 1.14 and accuracy of 0.72\n",
      "Iteration 621: with minibatch training loss = 1.43 and accuracy of 0.67\n",
      "Iteration 622: with minibatch training loss = 1.35 and accuracy of 0.64\n",
      "Iteration 623: with minibatch training loss = 1.43 and accuracy of 0.64\n",
      "Iteration 624: with minibatch training loss = 1.35 and accuracy of 0.69\n",
      "Iteration 625: with minibatch training loss = 1.6 and accuracy of 0.59\n",
      "Iteration 626: with minibatch training loss = 1.76 and accuracy of 0.58\n",
      "Iteration 627: with minibatch training loss = 1.45 and accuracy of 0.67\n",
      "Iteration 628: with minibatch training loss = 1.01 and accuracy of 0.77\n",
      "Iteration 629: with minibatch training loss = 1.09 and accuracy of 0.77\n",
      "Iteration 630: with minibatch training loss = 1.32 and accuracy of 0.69\n",
      "Iteration 631: with minibatch training loss = 1.46 and accuracy of 0.62\n",
      "Iteration 632: with minibatch training loss = 1.23 and accuracy of 0.7\n",
      "Iteration 633: with minibatch training loss = 0.899 and accuracy of 0.83\n",
      "Iteration 634: with minibatch training loss = 1.14 and accuracy of 0.73\n",
      "Iteration 635: with minibatch training loss = 1.36 and accuracy of 0.67\n",
      "Iteration 636: with minibatch training loss = 1.32 and accuracy of 0.64\n",
      "Iteration 637: with minibatch training loss = 1.4 and accuracy of 0.61\n",
      "Iteration 638: with minibatch training loss = 1.25 and accuracy of 0.67\n",
      "Iteration 639: with minibatch training loss = 1.05 and accuracy of 0.73\n",
      "Iteration 640: with minibatch training loss = 1.42 and accuracy of 0.59\n",
      "Iteration 641: with minibatch training loss = 1.48 and accuracy of 0.66\n",
      "Iteration 642: with minibatch training loss = 1.07 and accuracy of 0.77\n",
      "Iteration 643: with minibatch training loss = 1.08 and accuracy of 0.73\n",
      "Iteration 644: with minibatch training loss = 1.42 and accuracy of 0.61\n",
      "Iteration 645: with minibatch training loss = 1.49 and accuracy of 0.64\n",
      "Iteration 646: with minibatch training loss = 1.17 and accuracy of 0.73\n",
      "Iteration 647: with minibatch training loss = 1.47 and accuracy of 0.61\n",
      "Iteration 648: with minibatch training loss = 1.2 and accuracy of 0.66\n",
      "Iteration 649: with minibatch training loss = 1.48 and accuracy of 0.59\n",
      "Iteration 650: with minibatch training loss = 1.25 and accuracy of 0.7\n",
      "Iteration 651: with minibatch training loss = 1.34 and accuracy of 0.69\n",
      "Iteration 652: with minibatch training loss = 1.23 and accuracy of 0.69\n",
      "Iteration 653: with minibatch training loss = 1.58 and accuracy of 0.61\n",
      "Iteration 654: with minibatch training loss = 1.45 and accuracy of 0.64\n",
      "Iteration 655: with minibatch training loss = 1.41 and accuracy of 0.58\n",
      "Iteration 656: with minibatch training loss = 1.47 and accuracy of 0.66\n",
      "Iteration 657: with minibatch training loss = 1.4 and accuracy of 0.62\n",
      "Iteration 658: with minibatch training loss = 1.75 and accuracy of 0.55\n",
      "Iteration 659: with minibatch training loss = 1.43 and accuracy of 0.62\n",
      "Iteration 660: with minibatch training loss = 1.52 and accuracy of 0.59\n",
      "Iteration 661: with minibatch training loss = 1.25 and accuracy of 0.7\n",
      "Iteration 662: with minibatch training loss = 1.4 and accuracy of 0.69\n",
      "Iteration 663: with minibatch training loss = 1.52 and accuracy of 0.67\n",
      "Iteration 664: with minibatch training loss = 1.3 and accuracy of 0.69\n",
      "Iteration 665: with minibatch training loss = 1.41 and accuracy of 0.66\n",
      "Iteration 666: with minibatch training loss = 1.36 and accuracy of 0.66\n",
      "Iteration 667: with minibatch training loss = 1.15 and accuracy of 0.67\n",
      "Iteration 668: with minibatch training loss = 1.71 and accuracy of 0.55\n",
      "Iteration 669: with minibatch training loss = 0.989 and accuracy of 0.8\n",
      "Iteration 670: with minibatch training loss = 1.27 and accuracy of 0.7\n",
      "Iteration 671: with minibatch training loss = 1.62 and accuracy of 0.59\n",
      "Iteration 672: with minibatch training loss = 1.34 and accuracy of 0.7\n",
      "Iteration 673: with minibatch training loss = 1.25 and accuracy of 0.66\n",
      "Iteration 674: with minibatch training loss = 1.11 and accuracy of 0.75\n",
      "Iteration 675: with minibatch training loss = 1.25 and accuracy of 0.7\n",
      "Iteration 676: with minibatch training loss = 1.34 and accuracy of 0.64\n",
      "Iteration 677: with minibatch training loss = 1.1 and accuracy of 0.78\n",
      "Iteration 678: with minibatch training loss = 1.25 and accuracy of 0.67\n",
      "Iteration 679: with minibatch training loss = 1.26 and accuracy of 0.7\n",
      "Iteration 680: with minibatch training loss = 1.22 and accuracy of 0.8\n",
      "Iteration 681: with minibatch training loss = 1.38 and accuracy of 0.66\n",
      "Iteration 682: with minibatch training loss = 1.18 and accuracy of 0.73\n",
      "Iteration 683: with minibatch training loss = 1.05 and accuracy of 0.8\n",
      "Iteration 684: with minibatch training loss = 1.47 and accuracy of 0.64\n",
      "Iteration 685: with minibatch training loss = 1.14 and accuracy of 0.73\n",
      "Iteration 686: with minibatch training loss = 1.2 and accuracy of 0.69\n",
      "Iteration 687: with minibatch training loss = 1.33 and accuracy of 0.66\n",
      "Iteration 688: with minibatch training loss = 1.15 and accuracy of 0.73\n",
      "Iteration 689: with minibatch training loss = 1.36 and accuracy of 0.67\n",
      "Iteration 690: with minibatch training loss = 1.22 and accuracy of 0.67\n",
      "Iteration 691: with minibatch training loss = 1.11 and accuracy of 0.77\n",
      "Iteration 692: with minibatch training loss = 1.4 and accuracy of 0.64\n",
      "Iteration 693: with minibatch training loss = 1.33 and accuracy of 0.66\n",
      "Iteration 694: with minibatch training loss = 1.72 and accuracy of 0.59\n",
      "Iteration 695: with minibatch training loss = 1.27 and accuracy of 0.67\n",
      "Iteration 696: with minibatch training loss = 1.26 and accuracy of 0.67\n",
      "Iteration 697: with minibatch training loss = 1.11 and accuracy of 0.73\n",
      "Iteration 698: with minibatch training loss = 1.16 and accuracy of 0.78\n",
      "Iteration 699: with minibatch training loss = 0.909 and accuracy of 0.75\n",
      "Iteration 700: with minibatch training loss = 1.33 and accuracy of 0.67\n",
      "Iteration 701: with minibatch training loss = 1.25 and accuracy of 0.7\n",
      "Iteration 702: with minibatch training loss = 1.1 and accuracy of 0.77\n",
      "Iteration 703: with minibatch training loss = 1.08 and accuracy of 0.8\n",
      "Iteration 704: with minibatch training loss = 0.993 and accuracy of 0.77\n",
      "Iteration 705: with minibatch training loss = 1.41 and accuracy of 0.64\n",
      "Iteration 706: with minibatch training loss = 1.22 and accuracy of 0.72\n",
      "Iteration 707: with minibatch training loss = 1.2 and accuracy of 0.72\n",
      "Iteration 708: with minibatch training loss = 1.32 and accuracy of 0.62\n",
      "Iteration 709: with minibatch training loss = 1.58 and accuracy of 0.61\n",
      "Iteration 710: with minibatch training loss = 1.45 and accuracy of 0.69\n",
      "Iteration 711: with minibatch training loss = 1.31 and accuracy of 0.7\n",
      "Iteration 712: with minibatch training loss = 1.2 and accuracy of 0.72\n",
      "Iteration 713: with minibatch training loss = 1.35 and accuracy of 0.66\n",
      "Iteration 714: with minibatch training loss = 1.29 and accuracy of 0.72\n",
      "Iteration 715: with minibatch training loss = 1.14 and accuracy of 0.73\n",
      "Iteration 716: with minibatch training loss = 1.08 and accuracy of 0.72\n",
      "Iteration 717: with minibatch training loss = 1.23 and accuracy of 0.69\n",
      "Iteration 718: with minibatch training loss = 1.28 and accuracy of 0.7\n",
      "Iteration 719: with minibatch training loss = 1.61 and accuracy of 0.59\n",
      "Iteration 720: with minibatch training loss = 1.23 and accuracy of 0.7\n",
      "Iteration 721: with minibatch training loss = 1.14 and accuracy of 0.73\n",
      "Iteration 722: with minibatch training loss = 1.57 and accuracy of 0.59\n",
      "Iteration 723: with minibatch training loss = 1.07 and accuracy of 0.77\n",
      "Iteration 724: with minibatch training loss = 1.33 and accuracy of 0.69\n",
      "Iteration 725: with minibatch training loss = 1.46 and accuracy of 0.62\n",
      "Iteration 726: with minibatch training loss = 1.16 and accuracy of 0.73\n",
      "Iteration 727: with minibatch training loss = 1.03 and accuracy of 0.75\n",
      "Iteration 728: with minibatch training loss = 1.14 and accuracy of 0.67\n",
      "Iteration 729: with minibatch training loss = 1.16 and accuracy of 0.69\n",
      "Iteration 730: with minibatch training loss = 1.03 and accuracy of 0.77\n",
      "Iteration 731: with minibatch training loss = 1.11 and accuracy of 0.75\n",
      "Iteration 732: with minibatch training loss = 1.42 and accuracy of 0.58\n",
      "Iteration 733: with minibatch training loss = 0.891 and accuracy of 0.77\n",
      "Iteration 734: with minibatch training loss = 1.02 and accuracy of 0.72\n",
      "Iteration 735: with minibatch training loss = 1.54 and accuracy of 0.59\n",
      "Iteration 736: with minibatch training loss = 1.15 and accuracy of 0.7\n",
      "Iteration 737: with minibatch training loss = 1.07 and accuracy of 0.73\n",
      "Iteration 738: with minibatch training loss = 0.929 and accuracy of 0.8\n",
      "Iteration 739: with minibatch training loss = 1.47 and accuracy of 0.62\n",
      "Iteration 740: with minibatch training loss = 1.21 and accuracy of 0.75\n",
      "Iteration 741: with minibatch training loss = 1.13 and accuracy of 0.73\n",
      "Iteration 742: with minibatch training loss = 1.07 and accuracy of 0.77\n",
      "Iteration 743: with minibatch training loss = 1.63 and accuracy of 0.59\n",
      "Iteration 744: with minibatch training loss = 1.18 and accuracy of 0.73\n",
      "Iteration 745: with minibatch training loss = 1.31 and accuracy of 0.72\n",
      "Iteration 746: with minibatch training loss = 1.09 and accuracy of 0.77\n",
      "Iteration 747: with minibatch training loss = 1.38 and accuracy of 0.66\n",
      "Iteration 748: with minibatch training loss = 1.6 and accuracy of 0.58\n",
      "Iteration 749: with minibatch training loss = 1.14 and accuracy of 0.78\n",
      "Iteration 750: with minibatch training loss = 1.37 and accuracy of 0.66\n",
      "Iteration 751: with minibatch training loss = 1.13 and accuracy of 0.75\n",
      "Iteration 752: with minibatch training loss = 1.28 and accuracy of 0.62\n",
      "Iteration 753: with minibatch training loss = 1.03 and accuracy of 0.78\n",
      "Iteration 754: with minibatch training loss = 1.09 and accuracy of 0.75\n",
      "Iteration 755: with minibatch training loss = 1.31 and accuracy of 0.72\n",
      "Iteration 756: with minibatch training loss = 1.36 and accuracy of 0.67\n",
      "Iteration 757: with minibatch training loss = 1.46 and accuracy of 0.58\n",
      "Iteration 758: with minibatch training loss = 1.2 and accuracy of 0.7\n",
      "Iteration 759: with minibatch training loss = 1.46 and accuracy of 0.62\n",
      "Iteration 760: with minibatch training loss = 1.48 and accuracy of 0.64\n",
      "Iteration 761: with minibatch training loss = 1.1 and accuracy of 0.75\n",
      "Iteration 762: with minibatch training loss = 1.45 and accuracy of 0.61\n",
      "Iteration 763: with minibatch training loss = 1.29 and accuracy of 0.69\n",
      "Iteration 764: with minibatch training loss = 1.36 and accuracy of 0.66\n",
      "Iteration 765: with minibatch training loss = 1.28 and accuracy of 0.67\n",
      "Iteration 766: with minibatch training loss = 1.54 and accuracy of 0.59\n",
      "Iteration 767: with minibatch training loss = 1.12 and accuracy of 0.77\n",
      "Iteration 768: with minibatch training loss = 1.14 and accuracy of 0.72\n",
      "Iteration 769: with minibatch training loss = 1.25 and accuracy of 0.72\n",
      "Iteration 770: with minibatch training loss = 0.826 and accuracy of 0.81\n",
      "Iteration 771: with minibatch training loss = 1.07 and accuracy of 0.75\n",
      "Iteration 772: with minibatch training loss = 1.22 and accuracy of 0.72\n",
      "Iteration 773: with minibatch training loss = 1.57 and accuracy of 0.61\n",
      "Iteration 774: with minibatch training loss = 1.47 and accuracy of 0.58\n",
      "Iteration 775: with minibatch training loss = 1.03 and accuracy of 0.78\n",
      "Iteration 776: with minibatch training loss = 1.15 and accuracy of 0.72\n",
      "Iteration 777: with minibatch training loss = 1.14 and accuracy of 0.72\n",
      "Iteration 778: with minibatch training loss = 1.39 and accuracy of 0.62\n",
      "Iteration 779: with minibatch training loss = 1.24 and accuracy of 0.69\n",
      "Iteration 780: with minibatch training loss = 1.02 and accuracy of 0.77\n",
      "Iteration 781: with minibatch training loss = 1.17 and accuracy of 0.7\n",
      "Iteration 782: with minibatch training loss = 1.28 and accuracy of 0.69\n",
      "Iteration 783: with minibatch training loss = 1.36 and accuracy of 0.64\n",
      "Iteration 784: with minibatch training loss = 1.61 and accuracy of 0.61\n",
      "Iteration 785: with minibatch training loss = 1.34 and accuracy of 0.69\n",
      "Iteration 786: with minibatch training loss = 1.16 and accuracy of 0.7\n",
      "Iteration 787: with minibatch training loss = 0.734 and accuracy of 0.86\n",
      "Iteration 788: with minibatch training loss = 1.22 and accuracy of 0.72\n",
      "Iteration 789: with minibatch training loss = 1.09 and accuracy of 0.73\n",
      "Iteration 790: with minibatch training loss = 1.2 and accuracy of 0.67\n",
      "Iteration 791: with minibatch training loss = 1.24 and accuracy of 0.7\n",
      "Iteration 792: with minibatch training loss = 0.968 and accuracy of 0.73\n",
      "Iteration 793: with minibatch training loss = 1.49 and accuracy of 0.62\n",
      "Iteration 794: with minibatch training loss = 1.12 and accuracy of 0.7\n",
      "Iteration 795: with minibatch training loss = 1.22 and accuracy of 0.67\n",
      "Iteration 796: with minibatch training loss = 1.38 and accuracy of 0.64\n",
      "Iteration 797: with minibatch training loss = 1.42 and accuracy of 0.62\n",
      "Iteration 798: with minibatch training loss = 1.26 and accuracy of 0.69\n",
      "Iteration 799: with minibatch training loss = 1.11 and accuracy of 0.7\n",
      "Iteration 800: with minibatch training loss = 1.13 and accuracy of 0.67\n",
      "Iteration 801: with minibatch training loss = 1.2 and accuracy of 0.73\n",
      "Iteration 802: with minibatch training loss = 1.33 and accuracy of 0.64\n",
      "Iteration 803: with minibatch training loss = 1.2 and accuracy of 0.72\n",
      "Iteration 804: with minibatch training loss = 1.2 and accuracy of 0.7\n",
      "Iteration 805: with minibatch training loss = 1.21 and accuracy of 0.72\n",
      "Iteration 806: with minibatch training loss = 1.26 and accuracy of 0.72\n",
      "Iteration 807: with minibatch training loss = 1.34 and accuracy of 0.66\n",
      "Iteration 808: with minibatch training loss = 1.13 and accuracy of 0.69\n",
      "Iteration 809: with minibatch training loss = 1.32 and accuracy of 0.64\n",
      "Iteration 810: with minibatch training loss = 1.16 and accuracy of 0.7\n",
      "Iteration 811: with minibatch training loss = 1.43 and accuracy of 0.61\n",
      "Iteration 812: with minibatch training loss = 1.21 and accuracy of 0.69\n",
      "Iteration 813: with minibatch training loss = 0.892 and accuracy of 0.81\n",
      "Iteration 814: with minibatch training loss = 0.751 and accuracy of 0.86\n",
      "Iteration 815: with minibatch training loss = 1.16 and accuracy of 0.72\n",
      "Iteration 816: with minibatch training loss = 1.32 and accuracy of 0.72\n",
      "Iteration 817: with minibatch training loss = 1.33 and accuracy of 0.69\n",
      "Iteration 818: with minibatch training loss = 0.862 and accuracy of 0.81\n",
      "Iteration 819: with minibatch training loss = 1.21 and accuracy of 0.73\n",
      "Iteration 820: with minibatch training loss = 1.29 and accuracy of 0.67\n",
      "Iteration 821: with minibatch training loss = 1.45 and accuracy of 0.64\n",
      "Iteration 822: with minibatch training loss = 1.21 and accuracy of 0.7\n",
      "Iteration 823: with minibatch training loss = 1.12 and accuracy of 0.69\n",
      "Iteration 824: with minibatch training loss = 1.15 and accuracy of 0.72\n",
      "Iteration 825: with minibatch training loss = 1.43 and accuracy of 0.64\n",
      "Iteration 826: with minibatch training loss = 1.75 and accuracy of 0.52\n",
      "Iteration 827: with minibatch training loss = 1.3 and accuracy of 0.64\n",
      "Iteration 828: with minibatch training loss = 1.1 and accuracy of 0.69\n",
      "Iteration 829: with minibatch training loss = 1.24 and accuracy of 0.69\n",
      "Iteration 830: with minibatch training loss = 1.02 and accuracy of 0.78\n",
      "Iteration 831: with minibatch training loss = 1.03 and accuracy of 0.77\n",
      "Iteration 832: with minibatch training loss = 1.28 and accuracy of 0.67\n",
      "Iteration 833: with minibatch training loss = 1.58 and accuracy of 0.59\n",
      "Iteration 834: with minibatch training loss = 1.23 and accuracy of 0.69\n",
      "Iteration 835: with minibatch training loss = 1.11 and accuracy of 0.72\n",
      "Iteration 836: with minibatch training loss = 1.22 and accuracy of 0.72\n",
      "Iteration 837: with minibatch training loss = 1.11 and accuracy of 0.72\n",
      "Iteration 838: with minibatch training loss = 1.42 and accuracy of 0.64\n",
      "Iteration 839: with minibatch training loss = 1.14 and accuracy of 0.7\n",
      "Iteration 840: with minibatch training loss = 1.09 and accuracy of 0.72\n",
      "Iteration 841: with minibatch training loss = 1.35 and accuracy of 0.66\n",
      "Iteration 842: with minibatch training loss = 1.19 and accuracy of 0.73\n",
      "Iteration 843: with minibatch training loss = 1.11 and accuracy of 0.72\n",
      "Iteration 844: with minibatch training loss = 1.17 and accuracy of 0.7\n",
      "Iteration 845: with minibatch training loss = 1.39 and accuracy of 0.61\n",
      "Iteration 846: with minibatch training loss = 1.4 and accuracy of 0.66\n",
      "Iteration 847: with minibatch training loss = 1.36 and accuracy of 0.66\n",
      "Iteration 848: with minibatch training loss = 1.54 and accuracy of 0.61\n",
      "Iteration 849: with minibatch training loss = 1.36 and accuracy of 0.66\n",
      "Iteration 850: with minibatch training loss = 1.26 and accuracy of 0.7\n",
      "Iteration 851: with minibatch training loss = 1.44 and accuracy of 0.66\n",
      "Iteration 852: with minibatch training loss = 1.24 and accuracy of 0.67\n",
      "Iteration 853: with minibatch training loss = 1.24 and accuracy of 0.67\n",
      "Iteration 854: with minibatch training loss = 1.18 and accuracy of 0.7\n",
      "Iteration 855: with minibatch training loss = 1.34 and accuracy of 0.67\n",
      "Iteration 856: with minibatch training loss = 1.12 and accuracy of 0.72\n",
      "Iteration 857: with minibatch training loss = 0.983 and accuracy of 0.81\n",
      "Iteration 858: with minibatch training loss = 0.978 and accuracy of 0.75\n",
      "Iteration 859: with minibatch training loss = 1.29 and accuracy of 0.67\n",
      "Iteration 860: with minibatch training loss = 1.18 and accuracy of 0.69\n",
      "Iteration 861: with minibatch training loss = 1.19 and accuracy of 0.67\n",
      "Iteration 862: with minibatch training loss = 0.982 and accuracy of 0.77\n",
      "Iteration 863: with minibatch training loss = 1.37 and accuracy of 0.64\n",
      "Iteration 864: with minibatch training loss = 1.23 and accuracy of 0.67\n",
      "Iteration 865: with minibatch training loss = 1.07 and accuracy of 0.73\n",
      "Iteration 866: with minibatch training loss = 1.35 and accuracy of 0.64\n",
      "Iteration 867: with minibatch training loss = 1.39 and accuracy of 0.62\n",
      "Iteration 868: with minibatch training loss = 1.19 and accuracy of 0.69\n",
      "Iteration 869: with minibatch training loss = 1.16 and accuracy of 0.69\n",
      "Iteration 870: with minibatch training loss = 0.87 and accuracy of 0.81\n",
      "Iteration 871: with minibatch training loss = 1.21 and accuracy of 0.7\n",
      "Iteration 872: with minibatch training loss = 0.983 and accuracy of 0.78\n",
      "Iteration 873: with minibatch training loss = 1.24 and accuracy of 0.72\n",
      "Iteration 874: with minibatch training loss = 1.14 and accuracy of 0.7\n",
      "Iteration 875: with minibatch training loss = 1.2 and accuracy of 0.69\n",
      "Iteration 876: with minibatch training loss = 1.15 and accuracy of 0.72\n",
      "Iteration 877: with minibatch training loss = 1.2 and accuracy of 0.69\n",
      "Iteration 878: with minibatch training loss = 1.02 and accuracy of 0.77\n",
      "Iteration 879: with minibatch training loss = 1.39 and accuracy of 0.66\n",
      "Iteration 880: with minibatch training loss = 1.47 and accuracy of 0.59\n",
      "Iteration 881: with minibatch training loss = 1.1 and accuracy of 0.75\n",
      "Iteration 882: with minibatch training loss = 1.18 and accuracy of 0.7\n",
      "Iteration 883: with minibatch training loss = 0.97 and accuracy of 0.73\n",
      "Iteration 884: with minibatch training loss = 1.05 and accuracy of 0.75\n",
      "Iteration 885: with minibatch training loss = 1.11 and accuracy of 0.72\n",
      "Iteration 886: with minibatch training loss = 1.13 and accuracy of 0.7\n",
      "Iteration 887: with minibatch training loss = 1.25 and accuracy of 0.69\n",
      "Iteration 888: with minibatch training loss = 1.07 and accuracy of 0.75\n",
      "Iteration 889: with minibatch training loss = 1.24 and accuracy of 0.72\n",
      "Iteration 890: with minibatch training loss = 0.942 and accuracy of 0.83\n",
      "Iteration 891: with minibatch training loss = 1.16 and accuracy of 0.72\n",
      "Iteration 892: with minibatch training loss = 1.15 and accuracy of 0.7\n",
      "Iteration 893: with minibatch training loss = 1.41 and accuracy of 0.61\n",
      "Iteration 894: with minibatch training loss = 1.32 and accuracy of 0.69\n",
      "Iteration 895: with minibatch training loss = 1.08 and accuracy of 0.75\n",
      "Iteration 896: with minibatch training loss = 1.48 and accuracy of 0.59\n",
      "Iteration 897: with minibatch training loss = 1.25 and accuracy of 0.67\n",
      "Iteration 898: with minibatch training loss = 1.34 and accuracy of 0.64\n",
      "Iteration 899: with minibatch training loss = 1.23 and accuracy of 0.72\n",
      "Iteration 900: with minibatch training loss = 1.02 and accuracy of 0.75\n",
      "Iteration 901: with minibatch training loss = 1.37 and accuracy of 0.66\n",
      "Iteration 902: with minibatch training loss = 1.58 and accuracy of 0.59\n",
      "Iteration 903: with minibatch training loss = 1.19 and accuracy of 0.7\n",
      "Iteration 904: with minibatch training loss = 1.41 and accuracy of 0.64\n",
      "Iteration 905: with minibatch training loss = 1.12 and accuracy of 0.75\n",
      "Iteration 906: with minibatch training loss = 0.979 and accuracy of 0.78\n",
      "Iteration 907: with minibatch training loss = 1.02 and accuracy of 0.73\n",
      "Iteration 908: with minibatch training loss = 1.31 and accuracy of 0.64\n",
      "Iteration 909: with minibatch training loss = 1.62 and accuracy of 0.59\n",
      "Iteration 910: with minibatch training loss = 1.31 and accuracy of 0.58\n",
      "Iteration 911: with minibatch training loss = 1.05 and accuracy of 0.78\n",
      "Iteration 912: with minibatch training loss = 1.29 and accuracy of 0.7\n",
      "Iteration 913: with minibatch training loss = 1.32 and accuracy of 0.7\n",
      "Iteration 914: with minibatch training loss = 1.36 and accuracy of 0.62\n",
      "Iteration 915: with minibatch training loss = 1.18 and accuracy of 0.7\n",
      "Iteration 916: with minibatch training loss = 1.02 and accuracy of 0.77\n",
      "Iteration 917: with minibatch training loss = 1.04 and accuracy of 0.72\n",
      "Iteration 918: with minibatch training loss = 0.853 and accuracy of 0.81\n",
      "Iteration 919: with minibatch training loss = 1.28 and accuracy of 0.69\n",
      "Iteration 920: with minibatch training loss = 1.26 and accuracy of 0.69\n",
      "Iteration 921: with minibatch training loss = 1.08 and accuracy of 0.73\n",
      "Iteration 922: with minibatch training loss = 1.09 and accuracy of 0.77\n",
      "Iteration 923: with minibatch training loss = 1 and accuracy of 0.8\n",
      "Iteration 924: with minibatch training loss = 1.32 and accuracy of 0.64\n",
      "Iteration 925: with minibatch training loss = 1.16 and accuracy of 0.72\n",
      "Iteration 926: with minibatch training loss = 1.09 and accuracy of 0.75\n",
      "Iteration 927: with minibatch training loss = 1.24 and accuracy of 0.7\n",
      "Iteration 928: with minibatch training loss = 1.34 and accuracy of 0.67\n",
      "Iteration 929: with minibatch training loss = 1.07 and accuracy of 0.77\n",
      "Iteration 930: with minibatch training loss = 1.25 and accuracy of 0.73\n",
      "Iteration 931: with minibatch training loss = 1.16 and accuracy of 0.72\n",
      "Iteration 932: with minibatch training loss = 1.17 and accuracy of 0.69\n",
      "Iteration 933: with minibatch training loss = 0.968 and accuracy of 0.78\n",
      "Iteration 934: with minibatch training loss = 1.06 and accuracy of 0.75\n",
      "Iteration 935: with minibatch training loss = 1.35 and accuracy of 0.7\n",
      "Iteration 936: with minibatch training loss = 1.36 and accuracy of 0.62\n",
      "Iteration 937: with minibatch training loss = 1.17 and accuracy of 0.72\n",
      "Iteration 938: with minibatch training loss = 1.29 and accuracy of 0.72\n",
      "Iteration 939: with minibatch training loss = 1.33 and accuracy of 0.64\n",
      "Iteration 940: with minibatch training loss = 1.13 and accuracy of 0.7\n",
      "Iteration 941: with minibatch training loss = 1.17 and accuracy of 0.69\n",
      "Iteration 942: with minibatch training loss = 1.22 and accuracy of 0.72\n",
      "Iteration 943: with minibatch training loss = 0.944 and accuracy of 0.78\n",
      "Iteration 944: with minibatch training loss = 1.25 and accuracy of 0.69\n",
      "Iteration 945: with minibatch training loss = 1.28 and accuracy of 0.67\n",
      "Iteration 946: with minibatch training loss = 1.35 and accuracy of 0.67\n",
      "Iteration 947: with minibatch training loss = 1.31 and accuracy of 0.72\n",
      "Iteration 948: with minibatch training loss = 1.2 and accuracy of 0.7\n",
      "Iteration 949: with minibatch training loss = 1.08 and accuracy of 0.7\n",
      "Iteration 950: with minibatch training loss = 1.42 and accuracy of 0.62\n",
      "Iteration 951: with minibatch training loss = 1.37 and accuracy of 0.66\n",
      "Iteration 952: with minibatch training loss = 1.11 and accuracy of 0.72\n",
      "Iteration 953: with minibatch training loss = 1.25 and accuracy of 0.7\n",
      "Iteration 954: with minibatch training loss = 1.2 and accuracy of 0.69\n",
      "Iteration 955: with minibatch training loss = 1.14 and accuracy of 0.7\n",
      "Iteration 956: with minibatch training loss = 0.872 and accuracy of 0.81\n",
      "Iteration 957: with minibatch training loss = 1.1 and accuracy of 0.73\n",
      "Iteration 958: with minibatch training loss = 1.24 and accuracy of 0.7\n",
      "Iteration 959: with minibatch training loss = 1.16 and accuracy of 0.69\n",
      "Iteration 960: with minibatch training loss = 1.17 and accuracy of 0.73\n",
      "Iteration 961: with minibatch training loss = 1.16 and accuracy of 0.69\n",
      "Iteration 962: with minibatch training loss = 1.54 and accuracy of 0.61\n",
      "Iteration 963: with minibatch training loss = 1.49 and accuracy of 0.59\n",
      "Iteration 964: with minibatch training loss = 1.42 and accuracy of 0.67\n",
      "Iteration 965: with minibatch training loss = 1.36 and accuracy of 0.69\n",
      "Iteration 966: with minibatch training loss = 1.19 and accuracy of 0.72\n",
      "Iteration 967: with minibatch training loss = 1 and accuracy of 0.77\n",
      "Iteration 968: with minibatch training loss = 1.13 and accuracy of 0.72\n",
      "Iteration 969: with minibatch training loss = 1.08 and accuracy of 0.77\n",
      "Iteration 970: with minibatch training loss = 1.2 and accuracy of 0.75\n",
      "Iteration 971: with minibatch training loss = 1.31 and accuracy of 0.69\n",
      "Iteration 972: with minibatch training loss = 0.957 and accuracy of 0.77\n",
      "Iteration 973: with minibatch training loss = 1.34 and accuracy of 0.69\n",
      "Iteration 974: with minibatch training loss = 1.22 and accuracy of 0.69\n",
      "Iteration 975: with minibatch training loss = 1.27 and accuracy of 0.67\n",
      "Iteration 976: with minibatch training loss = 0.788 and accuracy of 0.83\n",
      "Iteration 977: with minibatch training loss = 1.27 and accuracy of 0.7\n",
      "Iteration 978: with minibatch training loss = 1.14 and accuracy of 0.67\n",
      "Iteration 979: with minibatch training loss = 1.22 and accuracy of 0.69\n",
      "Iteration 980: with minibatch training loss = 1.42 and accuracy of 0.61\n",
      "Iteration 981: with minibatch training loss = 1.51 and accuracy of 0.59\n",
      "Iteration 982: with minibatch training loss = 1.01 and accuracy of 0.8\n",
      "Iteration 983: with minibatch training loss = 1.4 and accuracy of 0.59\n",
      "Iteration 984: with minibatch training loss = 1.37 and accuracy of 0.62\n",
      "Iteration 985: with minibatch training loss = 1.29 and accuracy of 0.66\n",
      "Iteration 986: with minibatch training loss = 1.04 and accuracy of 0.73\n",
      "Iteration 987: with minibatch training loss = 1.13 and accuracy of 0.73\n",
      "Iteration 988: with minibatch training loss = 1.24 and accuracy of 0.66\n",
      "Iteration 989: with minibatch training loss = 1.34 and accuracy of 0.67\n",
      "Iteration 990: with minibatch training loss = 1.35 and accuracy of 0.69\n",
      "Iteration 991: with minibatch training loss = 0.785 and accuracy of 0.83\n",
      "Iteration 992: with minibatch training loss = 1.21 and accuracy of 0.7\n",
      "Iteration 993: with minibatch training loss = 1.35 and accuracy of 0.66\n",
      "Iteration 994: with minibatch training loss = 1.09 and accuracy of 0.72\n",
      "Iteration 995: with minibatch training loss = 1.23 and accuracy of 0.7\n",
      "Iteration 996: with minibatch training loss = 0.988 and accuracy of 0.77\n",
      "Iteration 997: with minibatch training loss = 1.07 and accuracy of 0.73\n",
      "Iteration 998: with minibatch training loss = 1.07 and accuracy of 0.72\n",
      "Iteration 999: with minibatch training loss = 1.08 and accuracy of 0.73\n",
      "Iteration 1000: with minibatch training loss = 1.42 and accuracy of 0.64\n",
      "Iteration 1001: with minibatch training loss = 1.31 and accuracy of 0.67\n",
      "Iteration 1002: with minibatch training loss = 1.23 and accuracy of 0.69\n",
      "Iteration 1003: with minibatch training loss = 1.37 and accuracy of 0.67\n",
      "Iteration 1004: with minibatch training loss = 1.18 and accuracy of 0.72\n",
      "Iteration 1005: with minibatch training loss = 1.33 and accuracy of 0.66\n",
      "Iteration 1006: with minibatch training loss = 1.25 and accuracy of 0.7\n",
      "Iteration 1007: with minibatch training loss = 0.91 and accuracy of 0.8\n",
      "Iteration 1008: with minibatch training loss = 1.37 and accuracy of 0.67\n",
      "Iteration 1009: with minibatch training loss = 1.03 and accuracy of 0.73\n",
      "Iteration 1010: with minibatch training loss = 1.1 and accuracy of 0.75\n",
      "Iteration 1011: with minibatch training loss = 1.08 and accuracy of 0.75\n",
      "Iteration 1012: with minibatch training loss = 1.29 and accuracy of 0.67\n",
      "Iteration 1013: with minibatch training loss = 1.11 and accuracy of 0.73\n",
      "Iteration 1014: with minibatch training loss = 1.2 and accuracy of 0.75\n",
      "Iteration 1015: with minibatch training loss = 1.09 and accuracy of 0.75\n",
      "Iteration 1016: with minibatch training loss = 1.38 and accuracy of 0.66\n",
      "Iteration 1017: with minibatch training loss = 1.25 and accuracy of 0.7\n",
      "Iteration 1018: with minibatch training loss = 0.99 and accuracy of 0.77\n",
      "Iteration 1019: with minibatch training loss = 1.33 and accuracy of 0.64\n",
      "Iteration 1020: with minibatch training loss = 1.12 and accuracy of 0.73\n",
      "Iteration 1021: with minibatch training loss = 1.31 and accuracy of 0.69\n",
      "Iteration 1022: with minibatch training loss = 1.11 and accuracy of 0.75\n",
      "Iteration 1023: with minibatch training loss = 1.45 and accuracy of 0.59\n",
      "Iteration 1024: with minibatch training loss = 1.12 and accuracy of 0.77\n",
      "Iteration 1025: with minibatch training loss = 1.39 and accuracy of 0.66\n",
      "Iteration 1026: with minibatch training loss = 1.01 and accuracy of 0.77\n",
      "Iteration 1027: with minibatch training loss = 1.43 and accuracy of 0.66\n",
      "Iteration 1028: with minibatch training loss = 0.993 and accuracy of 0.77\n",
      "Iteration 1029: with minibatch training loss = 1.09 and accuracy of 0.75\n",
      "Iteration 1030: with minibatch training loss = 1.09 and accuracy of 0.72\n",
      "Iteration 1031: with minibatch training loss = 1.04 and accuracy of 0.78\n",
      "Iteration 1032: with minibatch training loss = 1.24 and accuracy of 0.66\n",
      "Iteration 1033: with minibatch training loss = 1.48 and accuracy of 0.58\n",
      "Iteration 1034: with minibatch training loss = 1.48 and accuracy of 0.59\n",
      "Iteration 1035: with minibatch training loss = 1.16 and accuracy of 0.7\n",
      "Iteration 1036: with minibatch training loss = 1.23 and accuracy of 0.67\n",
      "Iteration 1037: with minibatch training loss = 1.23 and accuracy of 0.69\n",
      "Iteration 1038: with minibatch training loss = 1.27 and accuracy of 0.67\n",
      "Iteration 1039: with minibatch training loss = 1.36 and accuracy of 0.69\n",
      "Iteration 1040: with minibatch training loss = 1.16 and accuracy of 0.7\n",
      "Iteration 1041: with minibatch training loss = 1.16 and accuracy of 0.7\n",
      "Iteration 1042: with minibatch training loss = 1.54 and accuracy of 0.58\n",
      "Iteration 1043: with minibatch training loss = 0.929 and accuracy of 0.77\n",
      "Iteration 1044: with minibatch training loss = 1.08 and accuracy of 0.72\n",
      "Iteration 1045: with minibatch training loss = 1.09 and accuracy of 0.75\n",
      "Iteration 1046: with minibatch training loss = 1.22 and accuracy of 0.73\n",
      "Iteration 1047: with minibatch training loss = 1.07 and accuracy of 0.7\n",
      "Iteration 1048: with minibatch training loss = 1.29 and accuracy of 0.66\n",
      "Iteration 1049: with minibatch training loss = 1.09 and accuracy of 0.7\n",
      "Iteration 1050: with minibatch training loss = 1.3 and accuracy of 0.67\n",
      "Iteration 1051: with minibatch training loss = 1.04 and accuracy of 0.8\n",
      "Iteration 1052: with minibatch training loss = 1.24 and accuracy of 0.69\n",
      "Iteration 1053: with minibatch training loss = 1.1 and accuracy of 0.73\n",
      "Iteration 1054: with minibatch training loss = 1.32 and accuracy of 0.66\n",
      "Iteration 1055: with minibatch training loss = 1 and accuracy of 0.8\n",
      "Iteration 1056: with minibatch training loss = 0.93 and accuracy of 0.77\n",
      "Iteration 1057: with minibatch training loss = 1.33 and accuracy of 0.64\n",
      "Iteration 1058: with minibatch training loss = 1.17 and accuracy of 0.67\n",
      "Iteration 1059: with minibatch training loss = 0.767 and accuracy of 0.83\n",
      "Iteration 1060: with minibatch training loss = 0.97 and accuracy of 0.73\n",
      "Iteration 1061: with minibatch training loss = 1.29 and accuracy of 0.67\n",
      "Iteration 1062: with minibatch training loss = 1.32 and accuracy of 0.69\n",
      "Iteration 1063: with minibatch training loss = 0.937 and accuracy of 0.75\n",
      "Iteration 1064: with minibatch training loss = 1.16 and accuracy of 0.67\n",
      "Iteration 1065: with minibatch training loss = 1.33 and accuracy of 0.66\n",
      "Iteration 1066: with minibatch training loss = 1.09 and accuracy of 0.75\n",
      "Iteration 1067: with minibatch training loss = 0.901 and accuracy of 0.75\n",
      "Iteration 1068: with minibatch training loss = 1.08 and accuracy of 0.72\n",
      "Iteration 1069: with minibatch training loss = 1.41 and accuracy of 0.64\n",
      "Iteration 1070: with minibatch training loss = 1.12 and accuracy of 0.7\n",
      "Iteration 1071: with minibatch training loss = 1.11 and accuracy of 0.77\n",
      "Iteration 1072: with minibatch training loss = 1.01 and accuracy of 0.77\n",
      "Iteration 1073: with minibatch training loss = 1.02 and accuracy of 0.75\n",
      "Iteration 1074: with minibatch training loss = 1.01 and accuracy of 0.73\n",
      "Iteration 1075: with minibatch training loss = 1.2 and accuracy of 0.7\n",
      "Iteration 1076: with minibatch training loss = 1.14 and accuracy of 0.69\n",
      "Iteration 1077: with minibatch training loss = 1 and accuracy of 0.75\n",
      "Iteration 1078: with minibatch training loss = 1.45 and accuracy of 0.58\n",
      "Iteration 1079: with minibatch training loss = 1.11 and accuracy of 0.72\n",
      "Iteration 1080: with minibatch training loss = 1.1 and accuracy of 0.72\n",
      "Iteration 1081: with minibatch training loss = 1.03 and accuracy of 0.75\n",
      "Iteration 1082: with minibatch training loss = 1.3 and accuracy of 0.7\n",
      "Iteration 1083: with minibatch training loss = 0.841 and accuracy of 0.78\n",
      "Iteration 1084: with minibatch training loss = 1.04 and accuracy of 0.7\n",
      "Iteration 1085: with minibatch training loss = 1.17 and accuracy of 0.67\n",
      "Iteration 1086: with minibatch training loss = 1.02 and accuracy of 0.75\n",
      "Iteration 1087: with minibatch training loss = 1.14 and accuracy of 0.73\n",
      "Iteration 1088: with minibatch training loss = 0.916 and accuracy of 0.75\n",
      "Iteration 1089: with minibatch training loss = 1.25 and accuracy of 0.69\n",
      "Iteration 1090: with minibatch training loss = 1.3 and accuracy of 0.67\n",
      "Iteration 1091: with minibatch training loss = 1.15 and accuracy of 0.72\n",
      "Iteration 1092: with minibatch training loss = 1.21 and accuracy of 0.67\n",
      "Iteration 1093: with minibatch training loss = 1.46 and accuracy of 0.64\n",
      "Iteration 1094: with minibatch training loss = 1.13 and accuracy of 0.73\n",
      "Iteration 1095: with minibatch training loss = 1.19 and accuracy of 0.66\n",
      "Iteration 1096: with minibatch training loss = 1.07 and accuracy of 0.73\n",
      "Iteration 1097: with minibatch training loss = 1.28 and accuracy of 0.67\n",
      "Iteration 1098: with minibatch training loss = 0.971 and accuracy of 0.78\n",
      "Iteration 1099: with minibatch training loss = 1.02 and accuracy of 0.75\n",
      "Iteration 1100: with minibatch training loss = 1.25 and accuracy of 0.67\n",
      "Iteration 1101: with minibatch training loss = 1.33 and accuracy of 0.69\n",
      "Iteration 1102: with minibatch training loss = 1.47 and accuracy of 0.64\n",
      "Iteration 1103: with minibatch training loss = 1.39 and accuracy of 0.64\n",
      "Iteration 1104: with minibatch training loss = 1 and accuracy of 0.75\n",
      "Iteration 1105: with minibatch training loss = 1.09 and accuracy of 0.75\n",
      "Iteration 1106: with minibatch training loss = 1.1 and accuracy of 0.7\n",
      "Iteration 1107: with minibatch training loss = 1.07 and accuracy of 0.77\n",
      "Iteration 1108: with minibatch training loss = 1.12 and accuracy of 0.73\n",
      "Iteration 1109: with minibatch training loss = 1.14 and accuracy of 0.69\n",
      "Iteration 1110: with minibatch training loss = 1.09 and accuracy of 0.72\n",
      "Iteration 1111: with minibatch training loss = 1.21 and accuracy of 0.69\n",
      "Iteration 1112: with minibatch training loss = 1.41 and accuracy of 0.59\n",
      "Iteration 1113: with minibatch training loss = 1.36 and accuracy of 0.62\n",
      "Iteration 1114: with minibatch training loss = 1.27 and accuracy of 0.66\n",
      "Iteration 1115: with minibatch training loss = 1.26 and accuracy of 0.64\n",
      "Iteration 1116: with minibatch training loss = 1.08 and accuracy of 0.77\n",
      "Iteration 1117: with minibatch training loss = 1.25 and accuracy of 0.66\n",
      "Iteration 1118: with minibatch training loss = 1.51 and accuracy of 0.62\n",
      "Iteration 1119: with minibatch training loss = 0.995 and accuracy of 0.77\n",
      "Iteration 1120: with minibatch training loss = 1.18 and accuracy of 0.7\n",
      "Iteration 1121: with minibatch training loss = 1.21 and accuracy of 0.67\n",
      "Iteration 1122: with minibatch training loss = 1.07 and accuracy of 0.75\n",
      "Iteration 1123: with minibatch training loss = 1.27 and accuracy of 0.69\n",
      "Iteration 1124: with minibatch training loss = 1.06 and accuracy of 0.75\n",
      "Iteration 1125: with minibatch training loss = 1.29 and accuracy of 0.67\n",
      "Iteration 1126: with minibatch training loss = 1.17 and accuracy of 0.69\n",
      "Iteration 1127: with minibatch training loss = 1.25 and accuracy of 0.69\n",
      "Iteration 1128: with minibatch training loss = 1.19 and accuracy of 0.67\n",
      "Iteration 1129: with minibatch training loss = 1.15 and accuracy of 0.72\n",
      "Iteration 1130: with minibatch training loss = 1.27 and accuracy of 0.69\n",
      "Iteration 1131: with minibatch training loss = 1.45 and accuracy of 0.59\n",
      "Iteration 1132: with minibatch training loss = 1.11 and accuracy of 0.73\n",
      "Iteration 1133: with minibatch training loss = 1.29 and accuracy of 0.64\n",
      "Iteration 1134: with minibatch training loss = 1.24 and accuracy of 0.7\n",
      "Iteration 1135: with minibatch training loss = 1.25 and accuracy of 0.66\n",
      "Iteration 1136: with minibatch training loss = 1.16 and accuracy of 0.72\n",
      "Iteration 1137: with minibatch training loss = 1.47 and accuracy of 0.64\n",
      "Iteration 1138: with minibatch training loss = 1.04 and accuracy of 0.75\n",
      "Iteration 1139: with minibatch training loss = 1.22 and accuracy of 0.64\n",
      "Iteration 1140: with minibatch training loss = 1.26 and accuracy of 0.7\n",
      "Iteration 1141: with minibatch training loss = 0.935 and accuracy of 0.78\n",
      "Iteration 1142: with minibatch training loss = 0.989 and accuracy of 0.75\n",
      "Iteration 1143: with minibatch training loss = 1.24 and accuracy of 0.67\n",
      "Iteration 1144: with minibatch training loss = 1.19 and accuracy of 0.7\n",
      "Iteration 1145: with minibatch training loss = 1.03 and accuracy of 0.77\n",
      "Iteration 1146: with minibatch training loss = 1.13 and accuracy of 0.77\n",
      "Iteration 1147: with minibatch training loss = 0.979 and accuracy of 0.77\n",
      "Iteration 1148: with minibatch training loss = 1.21 and accuracy of 0.67\n",
      "Iteration 1149: with minibatch training loss = 0.897 and accuracy of 0.78\n",
      "Iteration 1150: with minibatch training loss = 1.45 and accuracy of 0.61\n",
      "Iteration 1151: with minibatch training loss = 1.44 and accuracy of 0.61\n",
      "Iteration 1152: with minibatch training loss = 1.28 and accuracy of 0.72\n",
      "Iteration 1153: with minibatch training loss = 1.09 and accuracy of 0.73\n",
      "Iteration 1154: with minibatch training loss = 1.09 and accuracy of 0.73\n",
      "Iteration 1155: with minibatch training loss = 1.11 and accuracy of 0.73\n",
      "Iteration 1156: with minibatch training loss = 1.09 and accuracy of 0.7\n",
      "Iteration 1157: with minibatch training loss = 1.11 and accuracy of 0.72\n",
      "Iteration 1158: with minibatch training loss = 1.31 and accuracy of 0.64\n",
      "Iteration 1159: with minibatch training loss = 1.07 and accuracy of 0.73\n",
      "Iteration 1160: with minibatch training loss = 1.02 and accuracy of 0.73\n",
      "Iteration 1161: with minibatch training loss = 1.01 and accuracy of 0.72\n",
      "Iteration 1162: with minibatch training loss = 1.14 and accuracy of 0.73\n",
      "Iteration 1163: with minibatch training loss = 1.12 and accuracy of 0.73\n",
      "Iteration 1164: with minibatch training loss = 1.07 and accuracy of 0.73\n",
      "Iteration 1165: with minibatch training loss = 1.13 and accuracy of 0.7\n",
      "Iteration 1166: with minibatch training loss = 1.28 and accuracy of 0.67\n",
      "Iteration 1167: with minibatch training loss = 1.07 and accuracy of 0.73\n",
      "Iteration 1168: with minibatch training loss = 1.25 and accuracy of 0.69\n",
      "Iteration 1169: with minibatch training loss = 1.1 and accuracy of 0.7\n",
      "Iteration 1170: with minibatch training loss = 0.956 and accuracy of 0.78\n",
      "Iteration 1171: with minibatch training loss = 1.05 and accuracy of 0.72\n",
      "Iteration 1172: with minibatch training loss = 1.02 and accuracy of 0.77\n",
      "Iteration 1173: with minibatch training loss = 1.32 and accuracy of 0.62\n",
      "Iteration 1174: with minibatch training loss = 0.927 and accuracy of 0.77\n",
      "Iteration 1175: with minibatch training loss = 1.48 and accuracy of 0.59\n",
      "Iteration 1176: with minibatch training loss = 1.2 and accuracy of 0.69\n",
      "Iteration 1177: with minibatch training loss = 0.998 and accuracy of 0.75\n",
      "Iteration 1178: with minibatch training loss = 1.04 and accuracy of 0.77\n",
      "Iteration 1179: with minibatch training loss = 1.05 and accuracy of 0.73\n",
      "Iteration 1180: with minibatch training loss = 0.905 and accuracy of 0.77\n",
      "Iteration 1181: with minibatch training loss = 0.86 and accuracy of 0.81\n",
      "Iteration 1182: with minibatch training loss = 1.38 and accuracy of 0.56\n",
      "Iteration 1183: with minibatch training loss = 1.15 and accuracy of 0.69\n",
      "Iteration 1184: with minibatch training loss = 0.979 and accuracy of 0.72\n",
      "Iteration 1185: with minibatch training loss = 1.08 and accuracy of 0.77\n",
      "Iteration 1186: with minibatch training loss = 1.23 and accuracy of 0.67\n",
      "Iteration 1187: with minibatch training loss = 1.19 and accuracy of 0.69\n",
      "Iteration 1188: with minibatch training loss = 1.07 and accuracy of 0.78\n",
      "Iteration 1189: with minibatch training loss = 1.2 and accuracy of 0.7\n",
      "Iteration 1190: with minibatch training loss = 1.09 and accuracy of 0.77\n",
      "Iteration 1191: with minibatch training loss = 1.08 and accuracy of 0.73\n",
      "Iteration 1192: with minibatch training loss = 1.07 and accuracy of 0.7\n",
      "Iteration 1193: with minibatch training loss = 0.892 and accuracy of 0.8\n",
      "Iteration 1194: with minibatch training loss = 1.2 and accuracy of 0.66\n",
      "Iteration 1195: with minibatch training loss = 1.12 and accuracy of 0.72\n",
      "Iteration 1196: with minibatch training loss = 1.05 and accuracy of 0.72\n",
      "Iteration 1197: with minibatch training loss = 1.3 and accuracy of 0.67\n",
      "Iteration 1198: with minibatch training loss = 1.3 and accuracy of 0.69\n",
      "Iteration 1199: with minibatch training loss = 1.3 and accuracy of 0.69\n",
      "Iteration 1200: with minibatch training loss = 1.44 and accuracy of 0.64\n",
      "Iteration 1201: with minibatch training loss = 1.24 and accuracy of 0.69\n",
      "Iteration 1202: with minibatch training loss = 1.21 and accuracy of 0.72\n",
      "Iteration 1203: with minibatch training loss = 1.24 and accuracy of 0.69\n",
      "Iteration 1204: with minibatch training loss = 1.24 and accuracy of 0.69\n",
      "Iteration 1205: with minibatch training loss = 1.18 and accuracy of 0.66\n",
      "Iteration 1206: with minibatch training loss = 1.17 and accuracy of 0.72\n",
      "Iteration 1207: with minibatch training loss = 1.2 and accuracy of 0.69\n",
      "Iteration 1208: with minibatch training loss = 1.09 and accuracy of 0.7\n",
      "Iteration 1209: with minibatch training loss = 1.23 and accuracy of 0.66\n",
      "Iteration 1210: with minibatch training loss = 1.09 and accuracy of 0.73\n",
      "Iteration 1211: with minibatch training loss = 1.18 and accuracy of 0.7\n",
      "Iteration 1212: with minibatch training loss = 1.22 and accuracy of 0.7\n",
      "Iteration 1213: with minibatch training loss = 1.18 and accuracy of 0.67\n",
      "Iteration 1214: with minibatch training loss = 1.13 and accuracy of 0.72\n",
      "Iteration 1215: with minibatch training loss = 0.916 and accuracy of 0.78\n",
      "Iteration 1216: with minibatch training loss = 1.06 and accuracy of 0.72\n",
      "Iteration 1217: with minibatch training loss = 1.46 and accuracy of 0.59\n",
      "Iteration 1218: with minibatch training loss = 1.13 and accuracy of 0.72\n",
      "Iteration 1219: with minibatch training loss = 1.49 and accuracy of 0.59\n",
      "Iteration 1220: with minibatch training loss = 1.23 and accuracy of 0.66\n",
      "Iteration 1221: with minibatch training loss = 1.08 and accuracy of 0.73\n",
      "Iteration 1222: with minibatch training loss = 0.854 and accuracy of 0.84\n",
      "Iteration 1223: with minibatch training loss = 1.1 and accuracy of 0.72\n",
      "Iteration 1224: with minibatch training loss = 1.37 and accuracy of 0.64\n",
      "Iteration 1225: with minibatch training loss = 0.987 and accuracy of 0.75\n",
      "Iteration 1226: with minibatch training loss = 1.11 and accuracy of 0.72\n",
      "Iteration 1227: with minibatch training loss = 0.759 and accuracy of 0.84\n",
      "Iteration 1228: with minibatch training loss = 1.04 and accuracy of 0.78\n",
      "Iteration 1229: with minibatch training loss = 1.1 and accuracy of 0.69\n",
      "Iteration 1230: with minibatch training loss = 1.09 and accuracy of 0.72\n",
      "Iteration 1231: with minibatch training loss = 1.17 and accuracy of 0.72\n",
      "Iteration 1232: with minibatch training loss = 1.16 and accuracy of 0.69\n",
      "Iteration 1233: with minibatch training loss = 0.95 and accuracy of 0.78\n",
      "Iteration 1234: with minibatch training loss = 1.04 and accuracy of 0.75\n",
      "Iteration 1235: with minibatch training loss = 0.952 and accuracy of 0.75\n",
      "Iteration 1236: with minibatch training loss = 0.962 and accuracy of 0.75\n",
      "Iteration 1237: with minibatch training loss = 1.51 and accuracy of 0.53\n",
      "Iteration 1238: with minibatch training loss = 1.3 and accuracy of 0.64\n",
      "Iteration 1239: with minibatch training loss = 0.974 and accuracy of 0.77\n",
      "Iteration 1240: with minibatch training loss = 1.4 and accuracy of 0.62\n",
      "Iteration 1241: with minibatch training loss = 1.37 and accuracy of 0.64\n",
      "Iteration 1242: with minibatch training loss = 1.46 and accuracy of 0.62\n",
      "Iteration 1243: with minibatch training loss = 1.2 and accuracy of 0.67\n",
      "Iteration 1244: with minibatch training loss = 1.08 and accuracy of 0.72\n",
      "Iteration 1245: with minibatch training loss = 1.13 and accuracy of 0.72\n",
      "Iteration 1246: with minibatch training loss = 1.05 and accuracy of 0.73\n",
      "Iteration 1247: with minibatch training loss = 1.02 and accuracy of 0.73\n",
      "Iteration 1248: with minibatch training loss = 1.45 and accuracy of 0.59\n",
      "Iteration 1249: with minibatch training loss = 0.87 and accuracy of 0.8\n",
      "Iteration 1250: with minibatch training loss = 1.18 and accuracy of 0.67\n",
      "Iteration 1251: with minibatch training loss = 1.34 and accuracy of 0.69\n",
      "Iteration 1252: with minibatch training loss = 1.38 and accuracy of 0.62\n",
      "Iteration 1253: with minibatch training loss = 1.28 and accuracy of 0.66\n",
      "Iteration 1254: with minibatch training loss = 1.29 and accuracy of 0.69\n",
      "Iteration 1255: with minibatch training loss = 0.719 and accuracy of 0.84\n",
      "Iteration 1256: with minibatch training loss = 1.24 and accuracy of 0.72\n",
      "Iteration 1257: with minibatch training loss = 1.04 and accuracy of 0.72\n",
      "Iteration 1258: with minibatch training loss = 1.24 and accuracy of 0.66\n",
      "Iteration 1259: with minibatch training loss = 1.07 and accuracy of 0.75\n",
      "Iteration 1260: with minibatch training loss = 0.874 and accuracy of 0.8\n",
      "Iteration 1261: with minibatch training loss = 1.33 and accuracy of 0.64\n",
      "Iteration 1262: with minibatch training loss = 1.16 and accuracy of 0.7\n",
      "Iteration 1263: with minibatch training loss = 1.13 and accuracy of 0.75\n",
      "Iteration 1264: with minibatch training loss = 1.07 and accuracy of 0.73\n",
      "Iteration 1265: with minibatch training loss = 1.13 and accuracy of 0.72\n",
      "Iteration 1266: with minibatch training loss = 1.28 and accuracy of 0.66\n",
      "Iteration 1267: with minibatch training loss = 1.39 and accuracy of 0.62\n",
      "Iteration 1268: with minibatch training loss = 1.22 and accuracy of 0.69\n",
      "Iteration 1269: with minibatch training loss = 1.4 and accuracy of 0.64\n",
      "Iteration 1270: with minibatch training loss = 0.957 and accuracy of 0.8\n",
      "Iteration 1271: with minibatch training loss = 1.37 and accuracy of 0.66\n",
      "Iteration 1272: with minibatch training loss = 1.01 and accuracy of 0.78\n",
      "Iteration 1273: with minibatch training loss = 1.07 and accuracy of 0.73\n",
      "Iteration 1274: with minibatch training loss = 1.14 and accuracy of 0.67\n",
      "Iteration 1275: with minibatch training loss = 0.972 and accuracy of 0.77\n",
      "Iteration 1276: with minibatch training loss = 0.812 and accuracy of 0.77\n",
      "Iteration 1277: with minibatch training loss = 1.08 and accuracy of 0.75\n",
      "Iteration 1278: with minibatch training loss = 1.06 and accuracy of 0.75\n",
      "Iteration 1279: with minibatch training loss = 1.26 and accuracy of 0.67\n",
      "Iteration 1280: with minibatch training loss = 0.997 and accuracy of 0.73\n",
      "Iteration 1281: with minibatch training loss = 0.979 and accuracy of 0.72\n",
      "Iteration 1282: with minibatch training loss = 1.42 and accuracy of 0.61\n",
      "Iteration 1283: with minibatch training loss = 1.33 and accuracy of 0.64\n",
      "Iteration 1284: with minibatch training loss = 1.53 and accuracy of 0.59\n",
      "Iteration 1285: with minibatch training loss = 0.915 and accuracy of 0.8\n",
      "Iteration 1286: with minibatch training loss = 1.15 and accuracy of 0.73\n",
      "Iteration 1287: with minibatch training loss = 1.41 and accuracy of 0.66\n",
      "Iteration 1288: with minibatch training loss = 1.18 and accuracy of 0.69\n",
      "Iteration 1289: with minibatch training loss = 1.15 and accuracy of 0.73\n",
      "Iteration 1290: with minibatch training loss = 1.21 and accuracy of 0.69\n",
      "Iteration 1291: with minibatch training loss = 1.15 and accuracy of 0.67\n",
      "Iteration 1292: with minibatch training loss = 1.24 and accuracy of 0.72\n",
      "Iteration 1293: with minibatch training loss = 1.22 and accuracy of 0.69\n",
      "Iteration 1294: with minibatch training loss = 1.18 and accuracy of 0.69\n",
      "Iteration 1295: with minibatch training loss = 1.14 and accuracy of 0.69\n",
      "Iteration 1296: with minibatch training loss = 1.12 and accuracy of 0.72\n",
      "Iteration 1297: with minibatch training loss = 1.01 and accuracy of 0.77\n",
      "Iteration 1298: with minibatch training loss = 1.29 and accuracy of 0.73\n",
      "Iteration 1299: with minibatch training loss = 1.36 and accuracy of 0.64\n",
      "Iteration 1300: with minibatch training loss = 1.09 and accuracy of 0.69\n",
      "Iteration 1301: with minibatch training loss = 1.05 and accuracy of 0.7\n",
      "Iteration 1302: with minibatch training loss = 1.02 and accuracy of 0.77\n",
      "Iteration 1303: with minibatch training loss = 1.14 and accuracy of 0.69\n",
      "Iteration 1304: with minibatch training loss = 0.956 and accuracy of 0.78\n",
      "Iteration 1305: with minibatch training loss = 1.04 and accuracy of 0.8\n",
      "Iteration 1306: with minibatch training loss = 1.46 and accuracy of 0.62\n",
      "Iteration 1307: with minibatch training loss = 0.651 and accuracy of 0.86\n",
      "Iteration 1308: with minibatch training loss = 1.16 and accuracy of 0.7\n",
      "Iteration 1309: with minibatch training loss = 1.43 and accuracy of 0.59\n",
      "Iteration 1310: with minibatch training loss = 0.884 and accuracy of 0.77\n",
      "Iteration 1311: with minibatch training loss = 1.25 and accuracy of 0.7\n",
      "Iteration 1312: with minibatch training loss = 1.29 and accuracy of 0.67\n",
      "Iteration 1313: with minibatch training loss = 1.16 and accuracy of 0.73\n",
      "Iteration 1314: with minibatch training loss = 1.18 and accuracy of 0.69\n",
      "Iteration 1315: with minibatch training loss = 1.16 and accuracy of 0.73\n",
      "Iteration 1316: with minibatch training loss = 1.14 and accuracy of 0.72\n",
      "Iteration 1317: with minibatch training loss = 1.01 and accuracy of 0.73\n",
      "Iteration 1318: with minibatch training loss = 0.856 and accuracy of 0.75\n",
      "Iteration 1319: with minibatch training loss = 0.925 and accuracy of 0.78\n",
      "Iteration 1320: with minibatch training loss = 1.22 and accuracy of 0.69\n",
      "Iteration 1321: with minibatch training loss = 0.965 and accuracy of 0.75\n",
      "Iteration 1322: with minibatch training loss = 1 and accuracy of 0.75\n",
      "Iteration 1323: with minibatch training loss = 0.896 and accuracy of 0.8\n",
      "Iteration 1324: with minibatch training loss = 0.901 and accuracy of 0.8\n",
      "Iteration 1325: with minibatch training loss = 1.11 and accuracy of 0.7\n",
      "Iteration 1326: with minibatch training loss = 1.13 and accuracy of 0.67\n",
      "Iteration 1327: with minibatch training loss = 1.15 and accuracy of 0.69\n",
      "Iteration 1328: with minibatch training loss = 1.28 and accuracy of 0.7\n",
      "Iteration 1329: with minibatch training loss = 1.27 and accuracy of 0.67\n",
      "Iteration 1330: with minibatch training loss = 1.37 and accuracy of 0.61\n",
      "Iteration 1331: with minibatch training loss = 1.08 and accuracy of 0.75\n",
      "Iteration 1332: with minibatch training loss = 1.25 and accuracy of 0.69\n",
      "Iteration 1333: with minibatch training loss = 1.01 and accuracy of 0.78\n",
      "Iteration 1334: with minibatch training loss = 0.994 and accuracy of 0.75\n",
      "Iteration 1335: with minibatch training loss = 1.05 and accuracy of 0.7\n",
      "Iteration 1336: with minibatch training loss = 1.26 and accuracy of 0.66\n",
      "Iteration 1337: with minibatch training loss = 1.19 and accuracy of 0.7\n",
      "Iteration 1338: with minibatch training loss = 1.07 and accuracy of 0.73\n",
      "Iteration 1339: with minibatch training loss = 1.18 and accuracy of 0.67\n",
      "Iteration 1340: with minibatch training loss = 0.869 and accuracy of 0.8\n",
      "Iteration 1341: with minibatch training loss = 0.952 and accuracy of 0.75\n",
      "Iteration 1342: with minibatch training loss = 1.26 and accuracy of 0.67\n",
      "Iteration 1343: with minibatch training loss = 1.18 and accuracy of 0.7\n",
      "Iteration 1344: with minibatch training loss = 0.989 and accuracy of 0.78\n",
      "Iteration 1345: with minibatch training loss = 1.15 and accuracy of 0.7\n",
      "Iteration 1346: with minibatch training loss = 1.14 and accuracy of 0.7\n",
      "Iteration 1347: with minibatch training loss = 1.04 and accuracy of 0.69\n",
      "Iteration 1348: with minibatch training loss = 0.971 and accuracy of 0.77\n",
      "Iteration 1349: with minibatch training loss = 1.22 and accuracy of 0.69\n",
      "Iteration 1350: with minibatch training loss = 1.37 and accuracy of 0.67\n",
      "Iteration 1351: with minibatch training loss = 0.838 and accuracy of 0.83\n",
      "Iteration 1352: with minibatch training loss = 1.04 and accuracy of 0.73\n",
      "Iteration 1353: with minibatch training loss = 1.15 and accuracy of 0.7\n",
      "Iteration 1354: with minibatch training loss = 0.916 and accuracy of 0.77\n",
      "Iteration 1355: with minibatch training loss = 1.28 and accuracy of 0.66\n",
      "Iteration 1356: with minibatch training loss = 1.37 and accuracy of 0.62\n",
      "Iteration 1357: with minibatch training loss = 1.14 and accuracy of 0.62\n",
      "Iteration 1358: with minibatch training loss = 1.32 and accuracy of 0.64\n",
      "Iteration 1359: with minibatch training loss = 1.13 and accuracy of 0.75\n",
      "Iteration 1360: with minibatch training loss = 0.873 and accuracy of 0.73\n",
      "Iteration 1361: with minibatch training loss = 1.01 and accuracy of 0.78\n",
      "Iteration 1362: with minibatch training loss = 1.02 and accuracy of 0.73\n",
      "Iteration 1363: with minibatch training loss = 1.04 and accuracy of 0.77\n",
      "Iteration 1364: with minibatch training loss = 1.32 and accuracy of 0.67\n",
      "Iteration 1365: with minibatch training loss = 1.08 and accuracy of 0.75\n",
      "Iteration 1366: with minibatch training loss = 1.08 and accuracy of 0.75\n",
      "Iteration 1367: with minibatch training loss = 1.23 and accuracy of 0.64\n",
      "Iteration 1368: with minibatch training loss = 1.15 and accuracy of 0.73\n",
      "Iteration 1369: with minibatch training loss = 1.17 and accuracy of 0.67\n",
      "Iteration 1370: with minibatch training loss = 1.31 and accuracy of 0.69\n",
      "Iteration 1371: with minibatch training loss = 1.22 and accuracy of 0.69\n",
      "Iteration 1372: with minibatch training loss = 1.17 and accuracy of 0.67\n",
      "Iteration 1373: with minibatch training loss = 1.28 and accuracy of 0.69\n",
      "Iteration 1374: with minibatch training loss = 0.877 and accuracy of 0.8\n",
      "Iteration 1375: with minibatch training loss = 1.33 and accuracy of 0.7\n",
      "Iteration 1376: with minibatch training loss = 1.05 and accuracy of 0.7\n",
      "Iteration 1377: with minibatch training loss = 1.18 and accuracy of 0.7\n",
      "Iteration 1378: with minibatch training loss = 1.01 and accuracy of 0.73\n",
      "Iteration 1379: with minibatch training loss = 1.2 and accuracy of 0.64\n",
      "Iteration 1380: with minibatch training loss = 1.17 and accuracy of 0.69\n",
      "Iteration 1381: with minibatch training loss = 1.24 and accuracy of 0.66\n",
      "Iteration 1382: with minibatch training loss = 1.11 and accuracy of 0.77\n",
      "Iteration 1383: with minibatch training loss = 1.01 and accuracy of 0.72\n",
      "Iteration 1384: with minibatch training loss = 1.2 and accuracy of 0.66\n",
      "Iteration 1385: with minibatch training loss = 1.01 and accuracy of 0.77\n",
      "Iteration 1386: with minibatch training loss = 1.44 and accuracy of 0.61\n",
      "Validation loss: 0.5078713\n",
      "Model's weights saved at /Users/nhat/Documents/Projects/LetterClassifier/weights/model_se.ckpt\n",
      "Epoch 1, Overall loss = 1.38 and accuracy of 0.655\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyz1e7fD30QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x137f1dc88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1388: with minibatch training loss = 1.06 and accuracy of 0.75\n",
      "Iteration 1389: with minibatch training loss = 0.948 and accuracy of 0.77\n",
      "Iteration 1390: with minibatch training loss = 1.1 and accuracy of 0.73\n",
      "Iteration 1391: with minibatch training loss = 0.923 and accuracy of 0.78\n",
      "Iteration 1392: with minibatch training loss = 0.956 and accuracy of 0.77\n",
      "Iteration 1393: with minibatch training loss = 1.01 and accuracy of 0.75\n",
      "Iteration 1394: with minibatch training loss = 1.14 and accuracy of 0.73\n",
      "Iteration 1395: with minibatch training loss = 1.08 and accuracy of 0.75\n",
      "Iteration 1396: with minibatch training loss = 1.18 and accuracy of 0.7\n",
      "Iteration 1397: with minibatch training loss = 1.04 and accuracy of 0.77\n",
      "Iteration 1398: with minibatch training loss = 1.06 and accuracy of 0.73\n",
      "Iteration 1399: with minibatch training loss = 1.13 and accuracy of 0.73\n",
      "Iteration 1400: with minibatch training loss = 1.09 and accuracy of 0.73\n",
      "Iteration 1401: with minibatch training loss = 1.39 and accuracy of 0.66\n",
      "Iteration 1402: with minibatch training loss = 1.11 and accuracy of 0.75\n",
      "Iteration 1403: with minibatch training loss = 1.01 and accuracy of 0.77\n",
      "Iteration 1404: with minibatch training loss = 1.35 and accuracy of 0.61\n",
      "Iteration 1405: with minibatch training loss = 0.674 and accuracy of 0.86\n",
      "Iteration 1406: with minibatch training loss = 1.29 and accuracy of 0.69\n",
      "Iteration 1407: with minibatch training loss = 1.11 and accuracy of 0.72\n",
      "Iteration 1408: with minibatch training loss = 0.832 and accuracy of 0.78\n",
      "Iteration 1409: with minibatch training loss = 1.06 and accuracy of 0.7\n",
      "Iteration 1410: with minibatch training loss = 1.49 and accuracy of 0.59\n",
      "Iteration 1411: with minibatch training loss = 1.07 and accuracy of 0.73\n",
      "Iteration 1412: with minibatch training loss = 0.943 and accuracy of 0.77\n",
      "Iteration 1413: with minibatch training loss = 0.888 and accuracy of 0.78\n",
      "Iteration 1414: with minibatch training loss = 1.12 and accuracy of 0.75\n",
      "Iteration 1415: with minibatch training loss = 1.74 and accuracy of 0.5\n",
      "Iteration 1416: with minibatch training loss = 0.945 and accuracy of 0.78\n",
      "Iteration 1417: with minibatch training loss = 1.05 and accuracy of 0.7\n",
      "Iteration 1418: with minibatch training loss = 1.36 and accuracy of 0.69\n",
      "Iteration 1419: with minibatch training loss = 1.16 and accuracy of 0.69\n",
      "Iteration 1420: with minibatch training loss = 1.14 and accuracy of 0.73\n",
      "Iteration 1421: with minibatch training loss = 1 and accuracy of 0.75\n",
      "Iteration 1422: with minibatch training loss = 1.31 and accuracy of 0.66\n",
      "Iteration 1423: with minibatch training loss = 0.957 and accuracy of 0.75\n",
      "Iteration 1424: with minibatch training loss = 0.983 and accuracy of 0.73\n",
      "Iteration 1425: with minibatch training loss = 1.22 and accuracy of 0.7\n",
      "Iteration 1426: with minibatch training loss = 1 and accuracy of 0.77\n",
      "Iteration 1427: with minibatch training loss = 1.14 and accuracy of 0.7\n",
      "Iteration 1428: with minibatch training loss = 1.33 and accuracy of 0.7\n",
      "Iteration 1429: with minibatch training loss = 1.09 and accuracy of 0.72\n",
      "Iteration 1430: with minibatch training loss = 1.31 and accuracy of 0.7\n",
      "Iteration 1431: with minibatch training loss = 1.08 and accuracy of 0.7\n",
      "Iteration 1432: with minibatch training loss = 1.23 and accuracy of 0.69\n",
      "Iteration 1433: with minibatch training loss = 1.31 and accuracy of 0.64\n",
      "Iteration 1434: with minibatch training loss = 1.07 and accuracy of 0.72\n",
      "Iteration 1435: with minibatch training loss = 1.2 and accuracy of 0.72\n",
      "Iteration 1436: with minibatch training loss = 1.2 and accuracy of 0.7\n",
      "Iteration 1437: with minibatch training loss = 1.09 and accuracy of 0.72\n",
      "Iteration 1438: with minibatch training loss = 1.08 and accuracy of 0.7\n",
      "Iteration 1439: with minibatch training loss = 1.17 and accuracy of 0.66\n",
      "Iteration 1440: with minibatch training loss = 0.94 and accuracy of 0.77\n",
      "Iteration 1441: with minibatch training loss = 1.15 and accuracy of 0.7\n",
      "Iteration 1442: with minibatch training loss = 1.07 and accuracy of 0.7\n",
      "Iteration 1443: with minibatch training loss = 1.24 and accuracy of 0.66\n",
      "Iteration 1444: with minibatch training loss = 1.3 and accuracy of 0.66\n",
      "Iteration 1445: with minibatch training loss = 1.37 and accuracy of 0.66\n",
      "Iteration 1446: with minibatch training loss = 0.949 and accuracy of 0.77\n",
      "Iteration 1447: with minibatch training loss = 1.12 and accuracy of 0.75\n",
      "Iteration 1448: with minibatch training loss = 0.925 and accuracy of 0.77\n",
      "Iteration 1449: with minibatch training loss = 1.08 and accuracy of 0.72\n",
      "Iteration 1450: with minibatch training loss = 1.23 and accuracy of 0.7\n",
      "Iteration 1451: with minibatch training loss = 1.03 and accuracy of 0.72\n",
      "Iteration 1452: with minibatch training loss = 0.881 and accuracy of 0.8\n",
      "Iteration 1453: with minibatch training loss = 0.81 and accuracy of 0.8\n",
      "Iteration 1454: with minibatch training loss = 1.18 and accuracy of 0.73\n",
      "Iteration 1455: with minibatch training loss = 1.11 and accuracy of 0.72\n",
      "Iteration 1456: with minibatch training loss = 1.14 and accuracy of 0.66\n",
      "Iteration 1457: with minibatch training loss = 0.89 and accuracy of 0.77\n",
      "Iteration 1458: with minibatch training loss = 1.45 and accuracy of 0.61\n",
      "Iteration 1459: with minibatch training loss = 1.08 and accuracy of 0.72\n",
      "Iteration 1460: with minibatch training loss = 0.94 and accuracy of 0.73\n",
      "Iteration 1461: with minibatch training loss = 1.24 and accuracy of 0.66\n",
      "Iteration 1462: with minibatch training loss = 0.839 and accuracy of 0.8\n",
      "Iteration 1463: with minibatch training loss = 0.831 and accuracy of 0.84\n",
      "Iteration 1464: with minibatch training loss = 1.34 and accuracy of 0.59\n",
      "Iteration 1465: with minibatch training loss = 1.06 and accuracy of 0.77\n",
      "Iteration 1466: with minibatch training loss = 0.914 and accuracy of 0.77\n",
      "Iteration 1467: with minibatch training loss = 1.32 and accuracy of 0.62\n",
      "Iteration 1468: with minibatch training loss = 1.36 and accuracy of 0.66\n",
      "Iteration 1469: with minibatch training loss = 1.27 and accuracy of 0.67\n",
      "Iteration 1470: with minibatch training loss = 0.903 and accuracy of 0.75\n",
      "Iteration 1471: with minibatch training loss = 0.957 and accuracy of 0.77\n",
      "Iteration 1472: with minibatch training loss = 1.27 and accuracy of 0.62\n",
      "Iteration 1473: with minibatch training loss = 0.785 and accuracy of 0.84\n",
      "Iteration 1474: with minibatch training loss = 1.21 and accuracy of 0.66\n",
      "Iteration 1475: with minibatch training loss = 1.23 and accuracy of 0.69\n",
      "Iteration 1476: with minibatch training loss = 0.93 and accuracy of 0.77\n",
      "Iteration 1477: with minibatch training loss = 1.09 and accuracy of 0.75\n",
      "Iteration 1478: with minibatch training loss = 1.29 and accuracy of 0.67\n",
      "Iteration 1479: with minibatch training loss = 1.01 and accuracy of 0.77\n",
      "Iteration 1480: with minibatch training loss = 1.27 and accuracy of 0.66\n",
      "Iteration 1481: with minibatch training loss = 1.28 and accuracy of 0.61\n",
      "Iteration 1482: with minibatch training loss = 1.25 and accuracy of 0.66\n",
      "Iteration 1483: with minibatch training loss = 1.2 and accuracy of 0.7\n",
      "Iteration 1484: with minibatch training loss = 0.979 and accuracy of 0.78\n",
      "Iteration 1485: with minibatch training loss = 0.898 and accuracy of 0.73\n",
      "Iteration 1486: with minibatch training loss = 1.15 and accuracy of 0.73\n",
      "Iteration 1487: with minibatch training loss = 1.23 and accuracy of 0.7\n",
      "Iteration 1488: with minibatch training loss = 1.31 and accuracy of 0.66\n",
      "Iteration 1489: with minibatch training loss = 0.929 and accuracy of 0.78\n",
      "Iteration 1490: with minibatch training loss = 0.846 and accuracy of 0.81\n",
      "Iteration 1491: with minibatch training loss = 1.39 and accuracy of 0.61\n",
      "Iteration 1492: with minibatch training loss = 1.69 and accuracy of 0.48\n",
      "Iteration 1493: with minibatch training loss = 0.758 and accuracy of 0.83\n",
      "Iteration 1494: with minibatch training loss = 1.13 and accuracy of 0.69\n",
      "Iteration 1495: with minibatch training loss = 0.706 and accuracy of 0.84\n",
      "Iteration 1496: with minibatch training loss = 1.2 and accuracy of 0.72\n",
      "Iteration 1497: with minibatch training loss = 1.19 and accuracy of 0.69\n",
      "Iteration 1498: with minibatch training loss = 0.997 and accuracy of 0.75\n",
      "Iteration 1499: with minibatch training loss = 1.38 and accuracy of 0.64\n",
      "Iteration 1500: with minibatch training loss = 1.15 and accuracy of 0.73\n",
      "Iteration 1501: with minibatch training loss = 1.03 and accuracy of 0.73\n",
      "Iteration 1502: with minibatch training loss = 1.01 and accuracy of 0.75\n",
      "Iteration 1503: with minibatch training loss = 1.17 and accuracy of 0.62\n",
      "Iteration 1504: with minibatch training loss = 1.28 and accuracy of 0.66\n",
      "Iteration 1505: with minibatch training loss = 1.03 and accuracy of 0.75\n",
      "Iteration 1506: with minibatch training loss = 0.986 and accuracy of 0.75\n",
      "Iteration 1507: with minibatch training loss = 1.27 and accuracy of 0.7\n",
      "Iteration 1508: with minibatch training loss = 1.08 and accuracy of 0.73\n",
      "Iteration 1509: with minibatch training loss = 1.4 and accuracy of 0.62\n",
      "Iteration 1510: with minibatch training loss = 0.828 and accuracy of 0.81\n",
      "Iteration 1511: with minibatch training loss = 1.17 and accuracy of 0.7\n",
      "Iteration 1512: with minibatch training loss = 0.797 and accuracy of 0.81\n",
      "Iteration 1513: with minibatch training loss = 1.08 and accuracy of 0.75\n",
      "Iteration 1514: with minibatch training loss = 1.38 and accuracy of 0.59\n",
      "Iteration 1515: with minibatch training loss = 0.992 and accuracy of 0.73\n",
      "Iteration 1516: with minibatch training loss = 1.26 and accuracy of 0.69\n",
      "Iteration 1517: with minibatch training loss = 1.14 and accuracy of 0.69\n",
      "Iteration 1518: with minibatch training loss = 1.16 and accuracy of 0.69\n",
      "Iteration 1519: with minibatch training loss = 1.02 and accuracy of 0.78\n",
      "Iteration 1520: with minibatch training loss = 1.27 and accuracy of 0.69\n",
      "Iteration 1521: with minibatch training loss = 1.35 and accuracy of 0.67\n",
      "Iteration 1522: with minibatch training loss = 1.02 and accuracy of 0.73\n",
      "Iteration 1523: with minibatch training loss = 0.965 and accuracy of 0.77\n",
      "Iteration 1524: with minibatch training loss = 1.27 and accuracy of 0.62\n",
      "Iteration 1525: with minibatch training loss = 1.1 and accuracy of 0.73\n",
      "Iteration 1526: with minibatch training loss = 1.2 and accuracy of 0.7\n",
      "Iteration 1527: with minibatch training loss = 1.4 and accuracy of 0.61\n",
      "Iteration 1528: with minibatch training loss = 1.16 and accuracy of 0.72\n",
      "Iteration 1529: with minibatch training loss = 1.34 and accuracy of 0.62\n",
      "Iteration 1530: with minibatch training loss = 1.12 and accuracy of 0.7\n",
      "Iteration 1531: with minibatch training loss = 0.925 and accuracy of 0.81\n",
      "Iteration 1532: with minibatch training loss = 1.14 and accuracy of 0.73\n",
      "Iteration 1533: with minibatch training loss = 1.38 and accuracy of 0.61\n",
      "Iteration 1534: with minibatch training loss = 1.08 and accuracy of 0.73\n",
      "Iteration 1535: with minibatch training loss = 1.17 and accuracy of 0.7\n",
      "Iteration 1536: with minibatch training loss = 1.32 and accuracy of 0.67\n",
      "Iteration 1537: with minibatch training loss = 1.05 and accuracy of 0.73\n",
      "Iteration 1538: with minibatch training loss = 1.3 and accuracy of 0.64\n",
      "Iteration 1539: with minibatch training loss = 0.981 and accuracy of 0.73\n",
      "Iteration 1540: with minibatch training loss = 0.871 and accuracy of 0.8\n",
      "Iteration 1541: with minibatch training loss = 1.38 and accuracy of 0.62\n",
      "Iteration 1542: with minibatch training loss = 1.24 and accuracy of 0.7\n",
      "Iteration 1543: with minibatch training loss = 0.866 and accuracy of 0.8\n",
      "Iteration 1544: with minibatch training loss = 0.893 and accuracy of 0.78\n",
      "Iteration 1545: with minibatch training loss = 0.837 and accuracy of 0.8\n",
      "Iteration 1546: with minibatch training loss = 1.28 and accuracy of 0.64\n",
      "Iteration 1547: with minibatch training loss = 1.11 and accuracy of 0.69\n",
      "Iteration 1548: with minibatch training loss = 1.02 and accuracy of 0.75\n",
      "Iteration 1549: with minibatch training loss = 1.16 and accuracy of 0.69\n",
      "Iteration 1550: with minibatch training loss = 1.22 and accuracy of 0.67\n",
      "Iteration 1551: with minibatch training loss = 0.723 and accuracy of 0.84\n",
      "Iteration 1552: with minibatch training loss = 1.24 and accuracy of 0.64\n",
      "Iteration 1553: with minibatch training loss = 1.15 and accuracy of 0.69\n",
      "Iteration 1554: with minibatch training loss = 1.41 and accuracy of 0.61\n",
      "Iteration 1555: with minibatch training loss = 1.3 and accuracy of 0.66\n",
      "Iteration 1556: with minibatch training loss = 0.817 and accuracy of 0.81\n",
      "Iteration 1557: with minibatch training loss = 1.02 and accuracy of 0.75\n",
      "Iteration 1558: with minibatch training loss = 0.901 and accuracy of 0.81\n",
      "Iteration 1559: with minibatch training loss = 1.32 and accuracy of 0.67\n",
      "Iteration 1560: with minibatch training loss = 0.926 and accuracy of 0.78\n",
      "Iteration 1561: with minibatch training loss = 0.82 and accuracy of 0.83\n",
      "Iteration 1562: with minibatch training loss = 1.32 and accuracy of 0.62\n",
      "Iteration 1563: with minibatch training loss = 1.29 and accuracy of 0.67\n",
      "Iteration 1564: with minibatch training loss = 1.25 and accuracy of 0.67\n",
      "Iteration 1565: with minibatch training loss = 0.887 and accuracy of 0.8\n",
      "Iteration 1566: with minibatch training loss = 1.01 and accuracy of 0.75\n",
      "Iteration 1567: with minibatch training loss = 1.22 and accuracy of 0.72\n",
      "Iteration 1568: with minibatch training loss = 1.18 and accuracy of 0.69\n",
      "Iteration 1569: with minibatch training loss = 1.25 and accuracy of 0.67\n",
      "Iteration 1570: with minibatch training loss = 0.912 and accuracy of 0.75\n",
      "Iteration 1571: with minibatch training loss = 1.14 and accuracy of 0.69\n",
      "Iteration 1572: with minibatch training loss = 0.951 and accuracy of 0.78\n",
      "Iteration 1573: with minibatch training loss = 0.949 and accuracy of 0.73\n",
      "Iteration 1574: with minibatch training loss = 1.58 and accuracy of 0.62\n",
      "Iteration 1575: with minibatch training loss = 1.15 and accuracy of 0.7\n",
      "Iteration 1576: with minibatch training loss = 1.25 and accuracy of 0.66\n",
      "Iteration 1577: with minibatch training loss = 0.925 and accuracy of 0.77\n",
      "Iteration 1578: with minibatch training loss = 0.952 and accuracy of 0.8\n",
      "Iteration 1579: with minibatch training loss = 0.764 and accuracy of 0.81\n",
      "Iteration 1580: with minibatch training loss = 1.24 and accuracy of 0.66\n",
      "Iteration 1581: with minibatch training loss = 1.69 and accuracy of 0.56\n",
      "Iteration 1582: with minibatch training loss = 0.869 and accuracy of 0.81\n",
      "Iteration 1583: with minibatch training loss = 1.26 and accuracy of 0.64\n",
      "Iteration 1584: with minibatch training loss = 0.992 and accuracy of 0.78\n",
      "Iteration 1585: with minibatch training loss = 0.884 and accuracy of 0.77\n",
      "Iteration 1586: with minibatch training loss = 1.39 and accuracy of 0.64\n",
      "Iteration 1587: with minibatch training loss = 1.1 and accuracy of 0.72\n",
      "Iteration 1588: with minibatch training loss = 0.674 and accuracy of 0.84\n",
      "Iteration 1589: with minibatch training loss = 0.974 and accuracy of 0.7\n",
      "Iteration 1590: with minibatch training loss = 1.24 and accuracy of 0.66\n",
      "Iteration 1591: with minibatch training loss = 1.29 and accuracy of 0.67\n",
      "Iteration 1592: with minibatch training loss = 1.07 and accuracy of 0.73\n",
      "Iteration 1593: with minibatch training loss = 1.3 and accuracy of 0.62\n",
      "Iteration 1594: with minibatch training loss = 1.05 and accuracy of 0.73\n",
      "Iteration 1595: with minibatch training loss = 0.922 and accuracy of 0.78\n",
      "Iteration 1596: with minibatch training loss = 1.01 and accuracy of 0.77\n",
      "Iteration 1597: with minibatch training loss = 1.08 and accuracy of 0.73\n",
      "Iteration 1598: with minibatch training loss = 1.1 and accuracy of 0.7\n",
      "Iteration 1599: with minibatch training loss = 1.05 and accuracy of 0.7\n",
      "Iteration 1600: with minibatch training loss = 1.14 and accuracy of 0.67\n",
      "Iteration 1601: with minibatch training loss = 0.969 and accuracy of 0.78\n",
      "Iteration 1602: with minibatch training loss = 1.05 and accuracy of 0.73\n",
      "Iteration 1603: with minibatch training loss = 0.986 and accuracy of 0.77\n",
      "Iteration 1604: with minibatch training loss = 1.15 and accuracy of 0.7\n",
      "Iteration 1605: with minibatch training loss = 1.27 and accuracy of 0.64\n",
      "Iteration 1606: with minibatch training loss = 1.27 and accuracy of 0.67\n",
      "Iteration 1607: with minibatch training loss = 0.939 and accuracy of 0.75\n",
      "Iteration 1608: with minibatch training loss = 1.01 and accuracy of 0.75\n",
      "Iteration 1609: with minibatch training loss = 1.19 and accuracy of 0.67\n",
      "Iteration 1610: with minibatch training loss = 1.22 and accuracy of 0.69\n",
      "Iteration 1611: with minibatch training loss = 1.29 and accuracy of 0.66\n",
      "Iteration 1612: with minibatch training loss = 1.03 and accuracy of 0.77\n",
      "Iteration 1613: with minibatch training loss = 1.14 and accuracy of 0.7\n",
      "Iteration 1614: with minibatch training loss = 1 and accuracy of 0.75\n",
      "Iteration 1615: with minibatch training loss = 0.917 and accuracy of 0.83\n",
      "Iteration 1616: with minibatch training loss = 1.02 and accuracy of 0.69\n",
      "Iteration 1617: with minibatch training loss = 1.4 and accuracy of 0.66\n",
      "Iteration 1618: with minibatch training loss = 1.04 and accuracy of 0.73\n",
      "Iteration 1619: with minibatch training loss = 1.24 and accuracy of 0.69\n",
      "Iteration 1620: with minibatch training loss = 0.866 and accuracy of 0.86\n",
      "Iteration 1621: with minibatch training loss = 0.918 and accuracy of 0.78\n",
      "Iteration 1622: with minibatch training loss = 0.974 and accuracy of 0.73\n",
      "Iteration 1623: with minibatch training loss = 1.12 and accuracy of 0.7\n",
      "Iteration 1624: with minibatch training loss = 1.02 and accuracy of 0.75\n",
      "Iteration 1625: with minibatch training loss = 0.988 and accuracy of 0.73\n",
      "Iteration 1626: with minibatch training loss = 0.982 and accuracy of 0.73\n",
      "Iteration 1627: with minibatch training loss = 1.22 and accuracy of 0.67\n",
      "Iteration 1628: with minibatch training loss = 1.08 and accuracy of 0.72\n",
      "Iteration 1629: with minibatch training loss = 0.929 and accuracy of 0.77\n",
      "Iteration 1630: with minibatch training loss = 0.992 and accuracy of 0.77\n",
      "Iteration 1631: with minibatch training loss = 0.957 and accuracy of 0.78\n",
      "Iteration 1632: with minibatch training loss = 1.16 and accuracy of 0.72\n",
      "Iteration 1633: with minibatch training loss = 0.889 and accuracy of 0.78\n",
      "Iteration 1634: with minibatch training loss = 1.01 and accuracy of 0.72\n",
      "Iteration 1635: with minibatch training loss = 1.17 and accuracy of 0.7\n",
      "Iteration 1636: with minibatch training loss = 1.19 and accuracy of 0.69\n",
      "Iteration 1637: with minibatch training loss = 1.25 and accuracy of 0.69\n",
      "Iteration 1638: with minibatch training loss = 0.77 and accuracy of 0.81\n",
      "Iteration 1639: with minibatch training loss = 0.981 and accuracy of 0.72\n",
      "Iteration 1640: with minibatch training loss = 1.19 and accuracy of 0.66\n",
      "Iteration 1641: with minibatch training loss = 1.18 and accuracy of 0.7\n",
      "Iteration 1642: with minibatch training loss = 1.08 and accuracy of 0.72\n",
      "Iteration 1643: with minibatch training loss = 0.895 and accuracy of 0.73\n",
      "Iteration 1644: with minibatch training loss = 1.03 and accuracy of 0.73\n",
      "Iteration 1645: with minibatch training loss = 1.58 and accuracy of 0.56\n",
      "Iteration 1646: with minibatch training loss = 1.27 and accuracy of 0.64\n",
      "Iteration 1647: with minibatch training loss = 0.955 and accuracy of 0.78\n",
      "Iteration 1648: with minibatch training loss = 0.994 and accuracy of 0.72\n",
      "Iteration 1649: with minibatch training loss = 1.25 and accuracy of 0.66\n",
      "Iteration 1650: with minibatch training loss = 0.73 and accuracy of 0.8\n",
      "Iteration 1651: with minibatch training loss = 1.22 and accuracy of 0.69\n",
      "Iteration 1652: with minibatch training loss = 1.06 and accuracy of 0.75\n",
      "Iteration 1653: with minibatch training loss = 1.38 and accuracy of 0.62\n",
      "Iteration 1654: with minibatch training loss = 1.1 and accuracy of 0.7\n",
      "Iteration 1655: with minibatch training loss = 1.36 and accuracy of 0.62\n",
      "Iteration 1656: with minibatch training loss = 1.06 and accuracy of 0.72\n",
      "Iteration 1657: with minibatch training loss = 0.975 and accuracy of 0.8\n",
      "Iteration 1658: with minibatch training loss = 1.35 and accuracy of 0.66\n",
      "Iteration 1659: with minibatch training loss = 1.34 and accuracy of 0.61\n",
      "Iteration 1660: with minibatch training loss = 1.15 and accuracy of 0.7\n",
      "Iteration 1661: with minibatch training loss = 1.3 and accuracy of 0.67\n",
      "Iteration 1662: with minibatch training loss = 1.02 and accuracy of 0.72\n",
      "Iteration 1663: with minibatch training loss = 1.09 and accuracy of 0.72\n",
      "Iteration 1664: with minibatch training loss = 1.26 and accuracy of 0.69\n",
      "Iteration 1665: with minibatch training loss = 1.01 and accuracy of 0.77\n",
      "Iteration 1666: with minibatch training loss = 0.958 and accuracy of 0.77\n",
      "Iteration 1667: with minibatch training loss = 1.18 and accuracy of 0.67\n",
      "Iteration 1668: with minibatch training loss = 1.38 and accuracy of 0.64\n",
      "Iteration 1669: with minibatch training loss = 0.807 and accuracy of 0.81\n",
      "Iteration 1670: with minibatch training loss = 1.1 and accuracy of 0.69\n",
      "Iteration 1671: with minibatch training loss = 1.32 and accuracy of 0.64\n",
      "Iteration 1672: with minibatch training loss = 1.13 and accuracy of 0.72\n",
      "Iteration 1673: with minibatch training loss = 1.23 and accuracy of 0.69\n",
      "Iteration 1674: with minibatch training loss = 1.03 and accuracy of 0.73\n",
      "Iteration 1675: with minibatch training loss = 1.23 and accuracy of 0.72\n",
      "Iteration 1676: with minibatch training loss = 1.33 and accuracy of 0.64\n",
      "Iteration 1677: with minibatch training loss = 0.808 and accuracy of 0.83\n",
      "Iteration 1678: with minibatch training loss = 0.975 and accuracy of 0.75\n",
      "Iteration 1679: with minibatch training loss = 1.26 and accuracy of 0.58\n",
      "Iteration 1680: with minibatch training loss = 0.963 and accuracy of 0.72\n",
      "Iteration 1681: with minibatch training loss = 1.33 and accuracy of 0.69\n",
      "Iteration 1682: with minibatch training loss = 1.12 and accuracy of 0.72\n",
      "Iteration 1683: with minibatch training loss = 0.9 and accuracy of 0.78\n",
      "Iteration 1684: with minibatch training loss = 1.16 and accuracy of 0.73\n",
      "Iteration 1685: with minibatch training loss = 1.49 and accuracy of 0.59\n",
      "Iteration 1686: with minibatch training loss = 1.07 and accuracy of 0.7\n",
      "Iteration 1687: with minibatch training loss = 1.13 and accuracy of 0.77\n",
      "Iteration 1688: with minibatch training loss = 0.93 and accuracy of 0.73\n",
      "Iteration 1689: with minibatch training loss = 0.943 and accuracy of 0.77\n",
      "Iteration 1690: with minibatch training loss = 1.05 and accuracy of 0.75\n",
      "Iteration 1691: with minibatch training loss = 1.5 and accuracy of 0.62\n",
      "Iteration 1692: with minibatch training loss = 1.05 and accuracy of 0.75\n",
      "Iteration 1693: with minibatch training loss = 0.998 and accuracy of 0.73\n",
      "Iteration 1694: with minibatch training loss = 1.02 and accuracy of 0.72\n",
      "Iteration 1695: with minibatch training loss = 1 and accuracy of 0.72\n",
      "Iteration 1696: with minibatch training loss = 1.16 and accuracy of 0.72\n",
      "Iteration 1697: with minibatch training loss = 1.3 and accuracy of 0.64\n",
      "Iteration 1698: with minibatch training loss = 0.927 and accuracy of 0.77\n",
      "Iteration 1699: with minibatch training loss = 1.23 and accuracy of 0.66\n",
      "Iteration 1700: with minibatch training loss = 1.08 and accuracy of 0.73\n",
      "Iteration 1701: with minibatch training loss = 1.22 and accuracy of 0.67\n",
      "Iteration 1702: with minibatch training loss = 1.09 and accuracy of 0.73\n",
      "Iteration 1703: with minibatch training loss = 1.07 and accuracy of 0.73\n",
      "Iteration 1704: with minibatch training loss = 1.27 and accuracy of 0.62\n",
      "Iteration 1705: with minibatch training loss = 1.2 and accuracy of 0.69\n",
      "Iteration 1706: with minibatch training loss = 0.965 and accuracy of 0.77\n",
      "Iteration 1707: with minibatch training loss = 0.806 and accuracy of 0.8\n",
      "Iteration 1708: with minibatch training loss = 1.25 and accuracy of 0.67\n",
      "Iteration 1709: with minibatch training loss = 1.19 and accuracy of 0.73\n",
      "Iteration 1710: with minibatch training loss = 1.07 and accuracy of 0.72\n",
      "Iteration 1711: with minibatch training loss = 1.11 and accuracy of 0.72\n",
      "Iteration 1712: with minibatch training loss = 1.09 and accuracy of 0.72\n",
      "Iteration 1713: with minibatch training loss = 1.1 and accuracy of 0.72\n",
      "Iteration 1714: with minibatch training loss = 1.42 and accuracy of 0.59\n",
      "Iteration 1715: with minibatch training loss = 1.12 and accuracy of 0.69\n",
      "Iteration 1716: with minibatch training loss = 0.801 and accuracy of 0.8\n",
      "Iteration 1717: with minibatch training loss = 1.19 and accuracy of 0.7\n",
      "Iteration 1718: with minibatch training loss = 1.16 and accuracy of 0.72\n",
      "Iteration 1719: with minibatch training loss = 1.01 and accuracy of 0.72\n",
      "Iteration 1720: with minibatch training loss = 1.05 and accuracy of 0.72\n",
      "Iteration 1721: with minibatch training loss = 1.26 and accuracy of 0.64\n",
      "Iteration 1722: with minibatch training loss = 1.15 and accuracy of 0.7\n",
      "Iteration 1723: with minibatch training loss = 1.31 and accuracy of 0.67\n",
      "Iteration 1724: with minibatch training loss = 1.09 and accuracy of 0.73\n",
      "Iteration 1725: with minibatch training loss = 1.04 and accuracy of 0.78\n",
      "Iteration 1726: with minibatch training loss = 1.03 and accuracy of 0.72\n",
      "Iteration 1727: with minibatch training loss = 1.12 and accuracy of 0.73\n",
      "Iteration 1728: with minibatch training loss = 1.35 and accuracy of 0.64\n",
      "Iteration 1729: with minibatch training loss = 1.19 and accuracy of 0.64\n",
      "Iteration 1730: with minibatch training loss = 1.01 and accuracy of 0.77\n",
      "Iteration 1731: with minibatch training loss = 0.905 and accuracy of 0.75\n",
      "Iteration 1732: with minibatch training loss = 0.904 and accuracy of 0.77\n",
      "Iteration 1733: with minibatch training loss = 0.808 and accuracy of 0.84\n",
      "Iteration 1734: with minibatch training loss = 0.963 and accuracy of 0.75\n",
      "Iteration 1735: with minibatch training loss = 1 and accuracy of 0.73\n",
      "Iteration 1736: with minibatch training loss = 1.11 and accuracy of 0.73\n",
      "Iteration 1737: with minibatch training loss = 0.966 and accuracy of 0.78\n",
      "Iteration 1738: with minibatch training loss = 0.996 and accuracy of 0.72\n",
      "Iteration 1739: with minibatch training loss = 0.947 and accuracy of 0.75\n",
      "Iteration 1740: with minibatch training loss = 1.44 and accuracy of 0.64\n",
      "Iteration 1741: with minibatch training loss = 1.2 and accuracy of 0.69\n",
      "Iteration 1742: with minibatch training loss = 0.919 and accuracy of 0.75\n",
      "Iteration 1743: with minibatch training loss = 1.02 and accuracy of 0.73\n",
      "Iteration 1744: with minibatch training loss = 0.885 and accuracy of 0.8\n",
      "Iteration 1745: with minibatch training loss = 1.35 and accuracy of 0.59\n",
      "Iteration 1746: with minibatch training loss = 1.13 and accuracy of 0.69\n",
      "Iteration 1747: with minibatch training loss = 0.95 and accuracy of 0.78\n",
      "Iteration 1748: with minibatch training loss = 1.36 and accuracy of 0.67\n",
      "Iteration 1749: with minibatch training loss = 1.04 and accuracy of 0.72\n",
      "Iteration 1750: with minibatch training loss = 1.16 and accuracy of 0.7\n",
      "Iteration 1751: with minibatch training loss = 1.03 and accuracy of 0.77\n",
      "Iteration 1752: with minibatch training loss = 1.12 and accuracy of 0.72\n",
      "Iteration 1753: with minibatch training loss = 1.03 and accuracy of 0.75\n",
      "Iteration 1754: with minibatch training loss = 0.998 and accuracy of 0.78\n",
      "Iteration 1755: with minibatch training loss = 1.21 and accuracy of 0.66\n",
      "Iteration 1756: with minibatch training loss = 1.21 and accuracy of 0.69\n",
      "Iteration 1757: with minibatch training loss = 1.27 and accuracy of 0.7\n",
      "Iteration 1758: with minibatch training loss = 1.28 and accuracy of 0.67\n",
      "Iteration 1759: with minibatch training loss = 1.25 and accuracy of 0.66\n",
      "Iteration 1760: with minibatch training loss = 0.871 and accuracy of 0.77\n",
      "Iteration 1761: with minibatch training loss = 0.837 and accuracy of 0.78\n",
      "Iteration 1762: with minibatch training loss = 0.973 and accuracy of 0.78\n",
      "Iteration 1763: with minibatch training loss = 1.21 and accuracy of 0.67\n",
      "Iteration 1764: with minibatch training loss = 0.9 and accuracy of 0.78\n",
      "Iteration 1765: with minibatch training loss = 1.22 and accuracy of 0.7\n",
      "Iteration 1766: with minibatch training loss = 0.86 and accuracy of 0.78\n",
      "Iteration 1767: with minibatch training loss = 1.06 and accuracy of 0.72\n",
      "Iteration 1768: with minibatch training loss = 0.901 and accuracy of 0.77\n",
      "Iteration 1769: with minibatch training loss = 1.24 and accuracy of 0.64\n",
      "Iteration 1770: with minibatch training loss = 1.26 and accuracy of 0.69\n",
      "Iteration 1771: with minibatch training loss = 1.33 and accuracy of 0.62\n",
      "Iteration 1772: with minibatch training loss = 0.966 and accuracy of 0.72\n",
      "Iteration 1773: with minibatch training loss = 0.997 and accuracy of 0.75\n",
      "Iteration 1774: with minibatch training loss = 1.03 and accuracy of 0.75\n",
      "Iteration 1775: with minibatch training loss = 1.05 and accuracy of 0.73\n",
      "Iteration 1776: with minibatch training loss = 1.2 and accuracy of 0.66\n",
      "Iteration 1777: with minibatch training loss = 1.35 and accuracy of 0.64\n",
      "Iteration 1778: with minibatch training loss = 0.97 and accuracy of 0.75\n",
      "Iteration 1779: with minibatch training loss = 1 and accuracy of 0.73\n",
      "Iteration 1780: with minibatch training loss = 0.975 and accuracy of 0.78\n",
      "Iteration 1781: with minibatch training loss = 1.09 and accuracy of 0.73\n",
      "Iteration 1782: with minibatch training loss = 0.993 and accuracy of 0.77\n",
      "Iteration 1783: with minibatch training loss = 1.02 and accuracy of 0.77\n",
      "Iteration 1784: with minibatch training loss = 1.24 and accuracy of 0.67\n",
      "Iteration 1785: with minibatch training loss = 1.14 and accuracy of 0.7\n",
      "Iteration 1786: with minibatch training loss = 1.02 and accuracy of 0.77\n",
      "Iteration 1787: with minibatch training loss = 1.15 and accuracy of 0.7\n",
      "Iteration 1788: with minibatch training loss = 1.14 and accuracy of 0.67\n",
      "Iteration 1789: with minibatch training loss = 1.11 and accuracy of 0.72\n",
      "Iteration 1790: with minibatch training loss = 0.878 and accuracy of 0.8\n",
      "Iteration 1791: with minibatch training loss = 0.767 and accuracy of 0.77\n",
      "Iteration 1792: with minibatch training loss = 1.15 and accuracy of 0.69\n",
      "Iteration 1793: with minibatch training loss = 1.11 and accuracy of 0.67\n",
      "Iteration 1794: with minibatch training loss = 0.886 and accuracy of 0.8\n",
      "Iteration 1795: with minibatch training loss = 1.04 and accuracy of 0.75\n",
      "Iteration 1796: with minibatch training loss = 1.17 and accuracy of 0.67\n",
      "Iteration 1797: with minibatch training loss = 0.896 and accuracy of 0.77\n",
      "Iteration 1798: with minibatch training loss = 1.2 and accuracy of 0.7\n",
      "Iteration 1799: with minibatch training loss = 0.932 and accuracy of 0.73\n",
      "Iteration 1800: with minibatch training loss = 1.13 and accuracy of 0.72\n",
      "Iteration 1801: with minibatch training loss = 1.08 and accuracy of 0.72\n",
      "Iteration 1802: with minibatch training loss = 0.919 and accuracy of 0.81\n",
      "Iteration 1803: with minibatch training loss = 0.899 and accuracy of 0.78\n",
      "Iteration 1804: with minibatch training loss = 1.22 and accuracy of 0.67\n",
      "Iteration 1805: with minibatch training loss = 0.967 and accuracy of 0.77\n",
      "Iteration 1806: with minibatch training loss = 0.966 and accuracy of 0.78\n",
      "Iteration 1807: with minibatch training loss = 1.09 and accuracy of 0.77\n",
      "Iteration 1808: with minibatch training loss = 1.05 and accuracy of 0.7\n",
      "Iteration 1809: with minibatch training loss = 1.03 and accuracy of 0.72\n",
      "Iteration 1810: with minibatch training loss = 1.11 and accuracy of 0.72\n",
      "Iteration 1811: with minibatch training loss = 0.745 and accuracy of 0.78\n",
      "Iteration 1812: with minibatch training loss = 1.38 and accuracy of 0.67\n",
      "Iteration 1813: with minibatch training loss = 0.841 and accuracy of 0.8\n",
      "Iteration 1814: with minibatch training loss = 1.29 and accuracy of 0.67\n",
      "Iteration 1815: with minibatch training loss = 0.843 and accuracy of 0.8\n",
      "Iteration 1816: with minibatch training loss = 1.02 and accuracy of 0.78\n",
      "Iteration 1817: with minibatch training loss = 0.932 and accuracy of 0.75\n",
      "Iteration 1818: with minibatch training loss = 1.03 and accuracy of 0.73\n",
      "Iteration 1819: with minibatch training loss = 0.957 and accuracy of 0.77\n",
      "Iteration 1820: with minibatch training loss = 1.11 and accuracy of 0.7\n",
      "Iteration 1821: with minibatch training loss = 0.868 and accuracy of 0.8\n",
      "Iteration 1822: with minibatch training loss = 0.997 and accuracy of 0.78\n",
      "Iteration 1823: with minibatch training loss = 0.787 and accuracy of 0.83\n",
      "Iteration 1824: with minibatch training loss = 0.922 and accuracy of 0.75\n",
      "Iteration 1825: with minibatch training loss = 0.925 and accuracy of 0.78\n",
      "Iteration 1826: with minibatch training loss = 1.05 and accuracy of 0.73\n",
      "Iteration 1827: with minibatch training loss = 0.917 and accuracy of 0.77\n",
      "Iteration 1828: with minibatch training loss = 0.986 and accuracy of 0.75\n",
      "Iteration 1829: with minibatch training loss = 1.12 and accuracy of 0.72\n",
      "Iteration 1830: with minibatch training loss = 1.04 and accuracy of 0.75\n",
      "Iteration 1831: with minibatch training loss = 1.39 and accuracy of 0.62\n",
      "Iteration 1832: with minibatch training loss = 1.46 and accuracy of 0.58\n",
      "Iteration 1833: with minibatch training loss = 1.04 and accuracy of 0.73\n",
      "Iteration 1834: with minibatch training loss = 0.738 and accuracy of 0.83\n",
      "Iteration 1835: with minibatch training loss = 1.23 and accuracy of 0.67\n",
      "Iteration 1836: with minibatch training loss = 1.07 and accuracy of 0.72\n",
      "Iteration 1837: with minibatch training loss = 0.94 and accuracy of 0.77\n",
      "Iteration 1838: with minibatch training loss = 1 and accuracy of 0.75\n",
      "Iteration 1839: with minibatch training loss = 1.13 and accuracy of 0.69\n",
      "Iteration 1840: with minibatch training loss = 1.13 and accuracy of 0.72\n",
      "Iteration 1841: with minibatch training loss = 1.17 and accuracy of 0.69\n",
      "Iteration 1842: with minibatch training loss = 1.1 and accuracy of 0.73\n",
      "Iteration 1843: with minibatch training loss = 1.04 and accuracy of 0.73\n",
      "Iteration 1844: with minibatch training loss = 0.852 and accuracy of 0.78\n",
      "Iteration 1845: with minibatch training loss = 1.05 and accuracy of 0.7\n",
      "Iteration 1846: with minibatch training loss = 1.15 and accuracy of 0.72\n",
      "Iteration 1847: with minibatch training loss = 1.08 and accuracy of 0.7\n",
      "Iteration 1848: with minibatch training loss = 1.47 and accuracy of 0.61\n",
      "Iteration 1849: with minibatch training loss = 1.07 and accuracy of 0.73\n",
      "Iteration 1850: with minibatch training loss = 0.768 and accuracy of 0.8\n",
      "Iteration 1851: with minibatch training loss = 1.01 and accuracy of 0.77\n",
      "Iteration 1852: with minibatch training loss = 0.886 and accuracy of 0.78\n",
      "Iteration 1853: with minibatch training loss = 1.17 and accuracy of 0.72\n",
      "Iteration 1854: with minibatch training loss = 0.997 and accuracy of 0.73\n",
      "Iteration 1855: with minibatch training loss = 0.877 and accuracy of 0.8\n",
      "Iteration 1856: with minibatch training loss = 1.04 and accuracy of 0.77\n",
      "Iteration 1857: with minibatch training loss = 1.11 and accuracy of 0.69\n",
      "Iteration 1858: with minibatch training loss = 0.946 and accuracy of 0.78\n",
      "Iteration 1859: with minibatch training loss = 1.09 and accuracy of 0.7\n",
      "Iteration 1860: with minibatch training loss = 1.17 and accuracy of 0.69\n",
      "Iteration 1861: with minibatch training loss = 1.29 and accuracy of 0.67\n",
      "Iteration 1862: with minibatch training loss = 1.23 and accuracy of 0.67\n",
      "Iteration 1863: with minibatch training loss = 1.02 and accuracy of 0.77\n",
      "Iteration 1864: with minibatch training loss = 1.11 and accuracy of 0.67\n",
      "Iteration 1865: with minibatch training loss = 1.11 and accuracy of 0.73\n",
      "Iteration 1866: with minibatch training loss = 1.08 and accuracy of 0.7\n",
      "Iteration 1867: with minibatch training loss = 0.997 and accuracy of 0.73\n",
      "Iteration 1868: with minibatch training loss = 1.01 and accuracy of 0.77\n",
      "Iteration 1869: with minibatch training loss = 1.33 and accuracy of 0.66\n",
      "Iteration 1870: with minibatch training loss = 1.03 and accuracy of 0.73\n",
      "Iteration 1871: with minibatch training loss = 1.13 and accuracy of 0.7\n",
      "Iteration 1872: with minibatch training loss = 1.04 and accuracy of 0.73\n",
      "Iteration 1873: with minibatch training loss = 1.31 and accuracy of 0.67\n",
      "Iteration 1874: with minibatch training loss = 1.08 and accuracy of 0.67\n",
      "Iteration 1875: with minibatch training loss = 1.2 and accuracy of 0.64\n",
      "Iteration 1876: with minibatch training loss = 1.33 and accuracy of 0.64\n",
      "Iteration 1877: with minibatch training loss = 0.797 and accuracy of 0.78\n",
      "Iteration 1878: with minibatch training loss = 1.2 and accuracy of 0.66\n",
      "Iteration 1879: with minibatch training loss = 1.03 and accuracy of 0.7\n",
      "Iteration 1880: with minibatch training loss = 0.932 and accuracy of 0.75\n",
      "Iteration 1881: with minibatch training loss = 0.689 and accuracy of 0.83\n",
      "Iteration 1882: with minibatch training loss = 1.04 and accuracy of 0.72\n",
      "Iteration 1883: with minibatch training loss = 0.986 and accuracy of 0.75\n",
      "Iteration 1884: with minibatch training loss = 0.982 and accuracy of 0.73\n",
      "Iteration 1885: with minibatch training loss = 1.14 and accuracy of 0.7\n",
      "Iteration 1886: with minibatch training loss = 0.871 and accuracy of 0.75\n",
      "Iteration 1887: with minibatch training loss = 0.948 and accuracy of 0.75\n",
      "Iteration 1888: with minibatch training loss = 1.03 and accuracy of 0.72\n",
      "Iteration 1889: with minibatch training loss = 1 and accuracy of 0.75\n",
      "Iteration 1890: with minibatch training loss = 0.997 and accuracy of 0.77\n",
      "Iteration 1891: with minibatch training loss = 0.808 and accuracy of 0.8\n",
      "Iteration 1892: with minibatch training loss = 1.15 and accuracy of 0.7\n",
      "Iteration 1893: with minibatch training loss = 0.885 and accuracy of 0.81\n",
      "Iteration 1894: with minibatch training loss = 1.22 and accuracy of 0.64\n",
      "Iteration 1895: with minibatch training loss = 0.918 and accuracy of 0.78\n",
      "Iteration 1896: with minibatch training loss = 1.02 and accuracy of 0.72\n",
      "Iteration 1897: with minibatch training loss = 0.993 and accuracy of 0.73\n",
      "Iteration 1898: with minibatch training loss = 0.867 and accuracy of 0.8\n",
      "Iteration 1899: with minibatch training loss = 0.98 and accuracy of 0.77\n",
      "Iteration 1900: with minibatch training loss = 1.2 and accuracy of 0.69\n",
      "Iteration 1901: with minibatch training loss = 1.14 and accuracy of 0.69\n",
      "Iteration 1902: with minibatch training loss = 1.17 and accuracy of 0.67\n",
      "Iteration 1903: with minibatch training loss = 1.08 and accuracy of 0.72\n",
      "Iteration 1904: with minibatch training loss = 1.59 and accuracy of 0.59\n",
      "Iteration 1905: with minibatch training loss = 1.23 and accuracy of 0.69\n",
      "Iteration 1906: with minibatch training loss = 1.22 and accuracy of 0.64\n",
      "Iteration 1907: with minibatch training loss = 1.25 and accuracy of 0.67\n",
      "Iteration 1908: with minibatch training loss = 1.01 and accuracy of 0.75\n",
      "Iteration 1909: with minibatch training loss = 1.34 and accuracy of 0.64\n",
      "Iteration 1910: with minibatch training loss = 0.792 and accuracy of 0.83\n",
      "Iteration 1911: with minibatch training loss = 1.15 and accuracy of 0.7\n",
      "Iteration 1912: with minibatch training loss = 0.99 and accuracy of 0.75\n",
      "Iteration 1913: with minibatch training loss = 0.788 and accuracy of 0.8\n",
      "Iteration 1914: with minibatch training loss = 0.973 and accuracy of 0.75\n",
      "Iteration 1915: with minibatch training loss = 1.07 and accuracy of 0.73\n",
      "Iteration 1916: with minibatch training loss = 1.05 and accuracy of 0.72\n",
      "Iteration 1917: with minibatch training loss = 1.07 and accuracy of 0.72\n",
      "Iteration 1918: with minibatch training loss = 0.992 and accuracy of 0.75\n",
      "Iteration 1919: with minibatch training loss = 0.839 and accuracy of 0.81\n",
      "Iteration 1920: with minibatch training loss = 0.881 and accuracy of 0.81\n",
      "Iteration 1921: with minibatch training loss = 1.06 and accuracy of 0.7\n",
      "Iteration 1922: with minibatch training loss = 0.975 and accuracy of 0.75\n",
      "Iteration 1923: with minibatch training loss = 0.86 and accuracy of 0.83\n",
      "Iteration 1924: with minibatch training loss = 1.13 and accuracy of 0.73\n",
      "Iteration 1925: with minibatch training loss = 1.16 and accuracy of 0.67\n",
      "Iteration 1926: with minibatch training loss = 1.34 and accuracy of 0.64\n",
      "Iteration 1927: with minibatch training loss = 0.757 and accuracy of 0.78\n",
      "Iteration 1928: with minibatch training loss = 0.904 and accuracy of 0.77\n",
      "Iteration 1929: with minibatch training loss = 1.18 and accuracy of 0.72\n",
      "Iteration 1930: with minibatch training loss = 1.16 and accuracy of 0.7\n",
      "Iteration 1931: with minibatch training loss = 1.01 and accuracy of 0.77\n",
      "Iteration 1932: with minibatch training loss = 0.945 and accuracy of 0.77\n",
      "Iteration 1933: with minibatch training loss = 1.08 and accuracy of 0.7\n",
      "Iteration 1934: with minibatch training loss = 0.966 and accuracy of 0.73\n",
      "Iteration 1935: with minibatch training loss = 1.02 and accuracy of 0.72\n",
      "Iteration 1936: with minibatch training loss = 1.22 and accuracy of 0.64\n",
      "Iteration 1937: with minibatch training loss = 0.826 and accuracy of 0.81\n",
      "Iteration 1938: with minibatch training loss = 0.984 and accuracy of 0.73\n",
      "Iteration 1939: with minibatch training loss = 1.3 and accuracy of 0.7\n",
      "Iteration 1940: with minibatch training loss = 1.51 and accuracy of 0.58\n",
      "Iteration 1941: with minibatch training loss = 1.17 and accuracy of 0.72\n",
      "Iteration 1942: with minibatch training loss = 1.11 and accuracy of 0.73\n",
      "Iteration 1943: with minibatch training loss = 1.09 and accuracy of 0.7\n",
      "Iteration 1944: with minibatch training loss = 1.06 and accuracy of 0.73\n",
      "Iteration 1945: with minibatch training loss = 1.42 and accuracy of 0.62\n",
      "Iteration 1946: with minibatch training loss = 0.842 and accuracy of 0.78\n",
      "Iteration 1947: with minibatch training loss = 0.919 and accuracy of 0.78\n",
      "Iteration 1948: with minibatch training loss = 1.1 and accuracy of 0.72\n",
      "Iteration 1949: with minibatch training loss = 0.973 and accuracy of 0.78\n",
      "Iteration 1950: with minibatch training loss = 1.12 and accuracy of 0.69\n",
      "Iteration 1951: with minibatch training loss = 1.16 and accuracy of 0.64\n",
      "Iteration 1952: with minibatch training loss = 1.21 and accuracy of 0.67\n",
      "Iteration 1953: with minibatch training loss = 0.964 and accuracy of 0.75\n",
      "Iteration 1954: with minibatch training loss = 0.979 and accuracy of 0.7\n",
      "Iteration 1955: with minibatch training loss = 0.926 and accuracy of 0.78\n",
      "Iteration 1956: with minibatch training loss = 1.26 and accuracy of 0.66\n",
      "Iteration 1957: with minibatch training loss = 0.966 and accuracy of 0.75\n",
      "Iteration 1958: with minibatch training loss = 1.19 and accuracy of 0.62\n",
      "Iteration 1959: with minibatch training loss = 0.822 and accuracy of 0.8\n",
      "Iteration 1960: with minibatch training loss = 1.16 and accuracy of 0.73\n",
      "Iteration 1961: with minibatch training loss = 1.39 and accuracy of 0.61\n",
      "Iteration 1962: with minibatch training loss = 1.02 and accuracy of 0.73\n",
      "Iteration 1963: with minibatch training loss = 0.978 and accuracy of 0.73\n",
      "Iteration 1964: with minibatch training loss = 1.01 and accuracy of 0.7\n",
      "Iteration 1965: with minibatch training loss = 1.39 and accuracy of 0.64\n",
      "Iteration 1966: with minibatch training loss = 0.976 and accuracy of 0.73\n",
      "Iteration 1967: with minibatch training loss = 1.24 and accuracy of 0.69\n",
      "Iteration 1968: with minibatch training loss = 1.04 and accuracy of 0.72\n",
      "Iteration 1969: with minibatch training loss = 1.23 and accuracy of 0.72\n",
      "Iteration 1970: with minibatch training loss = 1.09 and accuracy of 0.7\n",
      "Iteration 1971: with minibatch training loss = 0.891 and accuracy of 0.78\n",
      "Iteration 1972: with minibatch training loss = 1.15 and accuracy of 0.66\n",
      "Iteration 1973: with minibatch training loss = 1.13 and accuracy of 0.73\n",
      "Iteration 1974: with minibatch training loss = 0.829 and accuracy of 0.78\n",
      "Iteration 1975: with minibatch training loss = 1.18 and accuracy of 0.73\n",
      "Iteration 1976: with minibatch training loss = 1.11 and accuracy of 0.7\n",
      "Iteration 1977: with minibatch training loss = 1.11 and accuracy of 0.72\n",
      "Iteration 1978: with minibatch training loss = 0.648 and accuracy of 0.86\n",
      "Iteration 1979: with minibatch training loss = 0.951 and accuracy of 0.8\n",
      "Iteration 1980: with minibatch training loss = 1.15 and accuracy of 0.67\n",
      "Iteration 1981: with minibatch training loss = 0.851 and accuracy of 0.78\n",
      "Iteration 1982: with minibatch training loss = 0.952 and accuracy of 0.73\n",
      "Iteration 1983: with minibatch training loss = 1.15 and accuracy of 0.7\n",
      "Iteration 1984: with minibatch training loss = 1.5 and accuracy of 0.58\n",
      "Iteration 1985: with minibatch training loss = 1.24 and accuracy of 0.64\n",
      "Iteration 1986: with minibatch training loss = 1.35 and accuracy of 0.59\n",
      "Iteration 1987: with minibatch training loss = 1.19 and accuracy of 0.67\n",
      "Iteration 1988: with minibatch training loss = 0.904 and accuracy of 0.77\n",
      "Iteration 1989: with minibatch training loss = 1.16 and accuracy of 0.7\n",
      "Iteration 1990: with minibatch training loss = 1.14 and accuracy of 0.67\n",
      "Iteration 1991: with minibatch training loss = 0.94 and accuracy of 0.77\n",
      "Iteration 1992: with minibatch training loss = 1.19 and accuracy of 0.72\n",
      "Iteration 1993: with minibatch training loss = 1.05 and accuracy of 0.72\n",
      "Iteration 1994: with minibatch training loss = 1.07 and accuracy of 0.77\n",
      "Iteration 1995: with minibatch training loss = 1.07 and accuracy of 0.72\n",
      "Iteration 1996: with minibatch training loss = 0.895 and accuracy of 0.75\n",
      "Iteration 1997: with minibatch training loss = 1.14 and accuracy of 0.7\n",
      "Iteration 1998: with minibatch training loss = 1.15 and accuracy of 0.66\n",
      "Iteration 1999: with minibatch training loss = 0.89 and accuracy of 0.8\n",
      "Iteration 2000: with minibatch training loss = 1.06 and accuracy of 0.7\n",
      "Iteration 2001: with minibatch training loss = 1.09 and accuracy of 0.72\n",
      "Iteration 2002: with minibatch training loss = 1.25 and accuracy of 0.67\n",
      "Iteration 2003: with minibatch training loss = 1.22 and accuracy of 0.67\n",
      "Iteration 2004: with minibatch training loss = 1.15 and accuracy of 0.7\n",
      "Iteration 2005: with minibatch training loss = 1.14 and accuracy of 0.73\n",
      "Iteration 2006: with minibatch training loss = 0.912 and accuracy of 0.77\n",
      "Iteration 2007: with minibatch training loss = 1.22 and accuracy of 0.66\n",
      "Iteration 2008: with minibatch training loss = 1.05 and accuracy of 0.73\n",
      "Iteration 2009: with minibatch training loss = 1.16 and accuracy of 0.72\n",
      "Iteration 2010: with minibatch training loss = 0.834 and accuracy of 0.8\n",
      "Iteration 2011: with minibatch training loss = 1.23 and accuracy of 0.66\n",
      "Iteration 2012: with minibatch training loss = 1.09 and accuracy of 0.72\n",
      "Iteration 2013: with minibatch training loss = 1.31 and accuracy of 0.66\n",
      "Iteration 2014: with minibatch training loss = 1.12 and accuracy of 0.72\n",
      "Iteration 2015: with minibatch training loss = 0.972 and accuracy of 0.72\n",
      "Iteration 2016: with minibatch training loss = 1.04 and accuracy of 0.73\n",
      "Iteration 2017: with minibatch training loss = 1.11 and accuracy of 0.7\n",
      "Iteration 2018: with minibatch training loss = 0.927 and accuracy of 0.8\n",
      "Iteration 2019: with minibatch training loss = 0.828 and accuracy of 0.73\n",
      "Iteration 2020: with minibatch training loss = 0.929 and accuracy of 0.78\n",
      "Iteration 2021: with minibatch training loss = 0.762 and accuracy of 0.83\n",
      "Iteration 2022: with minibatch training loss = 0.743 and accuracy of 0.86\n",
      "Iteration 2023: with minibatch training loss = 1.05 and accuracy of 0.7\n",
      "Iteration 2024: with minibatch training loss = 1.1 and accuracy of 0.7\n",
      "Iteration 2025: with minibatch training loss = 1.59 and accuracy of 0.58\n",
      "Iteration 2026: with minibatch training loss = 0.972 and accuracy of 0.75\n",
      "Iteration 2027: with minibatch training loss = 0.788 and accuracy of 0.8\n",
      "Iteration 2028: with minibatch training loss = 1.2 and accuracy of 0.66\n",
      "Iteration 2029: with minibatch training loss = 1.17 and accuracy of 0.69\n",
      "Iteration 2030: with minibatch training loss = 0.935 and accuracy of 0.75\n",
      "Iteration 2031: with minibatch training loss = 1.21 and accuracy of 0.61\n",
      "Iteration 2032: with minibatch training loss = 1.19 and accuracy of 0.66\n",
      "Iteration 2033: with minibatch training loss = 1.14 and accuracy of 0.73\n",
      "Iteration 2034: with minibatch training loss = 1.05 and accuracy of 0.72\n",
      "Iteration 2035: with minibatch training loss = 1.02 and accuracy of 0.77\n",
      "Iteration 2036: with minibatch training loss = 0.907 and accuracy of 0.78\n",
      "Iteration 2037: with minibatch training loss = 1.2 and accuracy of 0.72\n",
      "Iteration 2038: with minibatch training loss = 1.1 and accuracy of 0.72\n",
      "Iteration 2039: with minibatch training loss = 1.05 and accuracy of 0.72\n",
      "Iteration 2040: with minibatch training loss = 0.96 and accuracy of 0.75\n",
      "Iteration 2041: with minibatch training loss = 1.06 and accuracy of 0.7\n",
      "Iteration 2042: with minibatch training loss = 1.17 and accuracy of 0.75\n",
      "Iteration 2043: with minibatch training loss = 0.86 and accuracy of 0.77\n",
      "Iteration 2044: with minibatch training loss = 1.17 and accuracy of 0.72\n",
      "Iteration 2045: with minibatch training loss = 1.12 and accuracy of 0.69\n",
      "Iteration 2046: with minibatch training loss = 0.854 and accuracy of 0.78\n",
      "Iteration 2047: with minibatch training loss = 0.988 and accuracy of 0.73\n",
      "Iteration 2048: with minibatch training loss = 1.25 and accuracy of 0.61\n",
      "Iteration 2049: with minibatch training loss = 0.936 and accuracy of 0.77\n",
      "Iteration 2050: with minibatch training loss = 1.12 and accuracy of 0.69\n",
      "Iteration 2051: with minibatch training loss = 0.829 and accuracy of 0.81\n",
      "Iteration 2052: with minibatch training loss = 1.2 and accuracy of 0.69\n",
      "Iteration 2053: with minibatch training loss = 1.29 and accuracy of 0.69\n",
      "Iteration 2054: with minibatch training loss = 0.973 and accuracy of 0.77\n",
      "Iteration 2055: with minibatch training loss = 0.903 and accuracy of 0.8\n",
      "Iteration 2056: with minibatch training loss = 1.7 and accuracy of 0.59\n",
      "Iteration 2057: with minibatch training loss = 1.18 and accuracy of 0.7\n",
      "Iteration 2058: with minibatch training loss = 1.09 and accuracy of 0.72\n",
      "Iteration 2059: with minibatch training loss = 1.05 and accuracy of 0.69\n",
      "Iteration 2060: with minibatch training loss = 1.26 and accuracy of 0.66\n",
      "Iteration 2061: with minibatch training loss = 0.911 and accuracy of 0.77\n",
      "Iteration 2062: with minibatch training loss = 0.887 and accuracy of 0.78\n",
      "Iteration 2063: with minibatch training loss = 1.19 and accuracy of 0.69\n",
      "Iteration 2064: with minibatch training loss = 0.9 and accuracy of 0.8\n",
      "Iteration 2065: with minibatch training loss = 1.27 and accuracy of 0.66\n",
      "Iteration 2066: with minibatch training loss = 0.842 and accuracy of 0.8\n",
      "Iteration 2067: with minibatch training loss = 0.862 and accuracy of 0.8\n",
      "Iteration 2068: with minibatch training loss = 1.02 and accuracy of 0.7\n",
      "Iteration 2069: with minibatch training loss = 0.957 and accuracy of 0.77\n",
      "Iteration 2070: with minibatch training loss = 1.25 and accuracy of 0.69\n",
      "Iteration 2071: with minibatch training loss = 1.22 and accuracy of 0.7\n",
      "Iteration 2072: with minibatch training loss = 1.49 and accuracy of 0.59\n",
      "Iteration 2073: with minibatch training loss = 1.01 and accuracy of 0.75\n",
      "Iteration 2074: with minibatch training loss = 1.11 and accuracy of 0.67\n",
      "Iteration 2075: with minibatch training loss = 0.946 and accuracy of 0.75\n",
      "Iteration 2076: with minibatch training loss = 0.96 and accuracy of 0.77\n",
      "Iteration 2077: with minibatch training loss = 1.2 and accuracy of 0.67\n",
      "Iteration 2078: with minibatch training loss = 1.3 and accuracy of 0.64\n",
      "Iteration 2079: with minibatch training loss = 1.32 and accuracy of 0.7\n",
      "Iteration 2080: with minibatch training loss = 1.07 and accuracy of 0.7\n",
      "Iteration 2081: with minibatch training loss = 1.3 and accuracy of 0.67\n",
      "Iteration 2082: with minibatch training loss = 1.12 and accuracy of 0.7\n",
      "Iteration 2083: with minibatch training loss = 1.12 and accuracy of 0.7\n",
      "Iteration 2084: with minibatch training loss = 1.1 and accuracy of 0.77\n",
      "Iteration 2085: with minibatch training loss = 1.08 and accuracy of 0.72\n",
      "Iteration 2086: with minibatch training loss = 1.23 and accuracy of 0.69\n",
      "Iteration 2087: with minibatch training loss = 1.05 and accuracy of 0.72\n",
      "Iteration 2088: with minibatch training loss = 1.08 and accuracy of 0.72\n",
      "Iteration 2089: with minibatch training loss = 0.856 and accuracy of 0.77\n",
      "Iteration 2090: with minibatch training loss = 0.934 and accuracy of 0.77\n",
      "Iteration 2091: with minibatch training loss = 0.985 and accuracy of 0.75\n",
      "Iteration 2092: with minibatch training loss = 0.994 and accuracy of 0.7\n",
      "Iteration 2093: with minibatch training loss = 1.15 and accuracy of 0.75\n",
      "Iteration 2094: with minibatch training loss = 0.854 and accuracy of 0.78\n",
      "Iteration 2095: with minibatch training loss = 0.974 and accuracy of 0.75\n",
      "Iteration 2096: with minibatch training loss = 1.1 and accuracy of 0.72\n",
      "Iteration 2097: with minibatch training loss = 0.788 and accuracy of 0.81\n",
      "Iteration 2098: with minibatch training loss = 1.09 and accuracy of 0.69\n",
      "Iteration 2099: with minibatch training loss = 0.967 and accuracy of 0.78\n",
      "Iteration 2100: with minibatch training loss = 1.17 and accuracy of 0.7\n",
      "Iteration 2101: with minibatch training loss = 0.963 and accuracy of 0.77\n",
      "Iteration 2102: with minibatch training loss = 0.855 and accuracy of 0.78\n",
      "Iteration 2103: with minibatch training loss = 0.935 and accuracy of 0.78\n",
      "Iteration 2104: with minibatch training loss = 1.05 and accuracy of 0.72\n",
      "Iteration 2105: with minibatch training loss = 1.02 and accuracy of 0.77\n",
      "Iteration 2106: with minibatch training loss = 0.837 and accuracy of 0.83\n",
      "Iteration 2107: with minibatch training loss = 1.32 and accuracy of 0.66\n",
      "Iteration 2108: with minibatch training loss = 1.21 and accuracy of 0.67\n",
      "Iteration 2109: with minibatch training loss = 1.15 and accuracy of 0.72\n",
      "Iteration 2110: with minibatch training loss = 0.79 and accuracy of 0.83\n",
      "Iteration 2111: with minibatch training loss = 1.09 and accuracy of 0.75\n",
      "Iteration 2112: with minibatch training loss = 1.21 and accuracy of 0.7\n",
      "Iteration 2113: with minibatch training loss = 1.08 and accuracy of 0.73\n",
      "Iteration 2114: with minibatch training loss = 1.04 and accuracy of 0.77\n",
      "Iteration 2115: with minibatch training loss = 1.01 and accuracy of 0.72\n",
      "Iteration 2116: with minibatch training loss = 1.25 and accuracy of 0.62\n",
      "Iteration 2117: with minibatch training loss = 1.3 and accuracy of 0.69\n",
      "Iteration 2118: with minibatch training loss = 1.01 and accuracy of 0.77\n",
      "Iteration 2119: with minibatch training loss = 0.858 and accuracy of 0.78\n",
      "Iteration 2120: with minibatch training loss = 1.34 and accuracy of 0.62\n",
      "Iteration 2121: with minibatch training loss = 0.659 and accuracy of 0.84\n",
      "Iteration 2122: with minibatch training loss = 0.884 and accuracy of 0.78\n",
      "Iteration 2123: with minibatch training loss = 0.885 and accuracy of 0.8\n",
      "Iteration 2124: with minibatch training loss = 1.27 and accuracy of 0.67\n",
      "Iteration 2125: with minibatch training loss = 1.01 and accuracy of 0.72\n",
      "Iteration 2126: with minibatch training loss = 1.29 and accuracy of 0.67\n",
      "Iteration 2127: with minibatch training loss = 0.748 and accuracy of 0.83\n",
      "Iteration 2128: with minibatch training loss = 1.08 and accuracy of 0.73\n",
      "Iteration 2129: with minibatch training loss = 1.13 and accuracy of 0.7\n",
      "Iteration 2130: with minibatch training loss = 1.12 and accuracy of 0.72\n",
      "Iteration 2131: with minibatch training loss = 1.09 and accuracy of 0.75\n",
      "Iteration 2132: with minibatch training loss = 1.21 and accuracy of 0.69\n",
      "Iteration 2133: with minibatch training loss = 1.16 and accuracy of 0.72\n",
      "Iteration 2134: with minibatch training loss = 0.692 and accuracy of 0.84\n",
      "Iteration 2135: with minibatch training loss = 1.18 and accuracy of 0.69\n",
      "Iteration 2136: with minibatch training loss = 1.16 and accuracy of 0.7\n",
      "Iteration 2137: with minibatch training loss = 1.25 and accuracy of 0.66\n",
      "Iteration 2138: with minibatch training loss = 0.88 and accuracy of 0.8\n",
      "Iteration 2139: with minibatch training loss = 1.1 and accuracy of 0.69\n",
      "Iteration 2140: with minibatch training loss = 1.16 and accuracy of 0.67\n",
      "Iteration 2141: with minibatch training loss = 0.904 and accuracy of 0.77\n",
      "Iteration 2142: with minibatch training loss = 0.989 and accuracy of 0.75\n",
      "Iteration 2143: with minibatch training loss = 0.94 and accuracy of 0.8\n",
      "Iteration 2144: with minibatch training loss = 1.02 and accuracy of 0.73\n",
      "Iteration 2145: with minibatch training loss = 1.23 and accuracy of 0.66\n",
      "Iteration 2146: with minibatch training loss = 0.965 and accuracy of 0.77\n",
      "Iteration 2147: with minibatch training loss = 0.898 and accuracy of 0.78\n",
      "Iteration 2148: with minibatch training loss = 1.29 and accuracy of 0.64\n",
      "Iteration 2149: with minibatch training loss = 0.889 and accuracy of 0.77\n",
      "Iteration 2150: with minibatch training loss = 1.01 and accuracy of 0.77\n",
      "Iteration 2151: with minibatch training loss = 1.6 and accuracy of 0.58\n",
      "Iteration 2152: with minibatch training loss = 1.23 and accuracy of 0.7\n",
      "Iteration 2153: with minibatch training loss = 1.2 and accuracy of 0.67\n",
      "Iteration 2154: with minibatch training loss = 0.853 and accuracy of 0.8\n",
      "Iteration 2155: with minibatch training loss = 0.946 and accuracy of 0.77\n",
      "Iteration 2156: with minibatch training loss = 0.731 and accuracy of 0.86\n",
      "Iteration 2157: with minibatch training loss = 1.11 and accuracy of 0.73\n",
      "Iteration 2158: with minibatch training loss = 0.798 and accuracy of 0.8\n",
      "Iteration 2159: with minibatch training loss = 0.988 and accuracy of 0.77\n",
      "Iteration 2160: with minibatch training loss = 1.19 and accuracy of 0.66\n",
      "Iteration 2161: with minibatch training loss = 0.964 and accuracy of 0.77\n",
      "Iteration 2162: with minibatch training loss = 1.11 and accuracy of 0.69\n",
      "Iteration 2163: with minibatch training loss = 0.907 and accuracy of 0.78\n",
      "Iteration 2164: with minibatch training loss = 1.34 and accuracy of 0.59\n",
      "Iteration 2165: with minibatch training loss = 0.924 and accuracy of 0.78\n",
      "Iteration 2166: with minibatch training loss = 1.17 and accuracy of 0.72\n",
      "Iteration 2167: with minibatch training loss = 1.09 and accuracy of 0.69\n",
      "Iteration 2168: with minibatch training loss = 1.01 and accuracy of 0.72\n",
      "Iteration 2169: with minibatch training loss = 1.1 and accuracy of 0.72\n",
      "Iteration 2170: with minibatch training loss = 1.1 and accuracy of 0.7\n",
      "Iteration 2171: with minibatch training loss = 0.876 and accuracy of 0.8\n",
      "Iteration 2172: with minibatch training loss = 1.28 and accuracy of 0.69\n",
      "Iteration 2173: with minibatch training loss = 0.922 and accuracy of 0.77\n",
      "Iteration 2174: with minibatch training loss = 0.722 and accuracy of 0.84\n",
      "Iteration 2175: with minibatch training loss = 1.08 and accuracy of 0.7\n",
      "Iteration 2176: with minibatch training loss = 1.49 and accuracy of 0.53\n",
      "Iteration 2177: with minibatch training loss = 0.916 and accuracy of 0.77\n",
      "Iteration 2178: with minibatch training loss = 0.881 and accuracy of 0.75\n",
      "Iteration 2179: with minibatch training loss = 1.11 and accuracy of 0.73\n",
      "Iteration 2180: with minibatch training loss = 0.939 and accuracy of 0.77\n",
      "Iteration 2181: with minibatch training loss = 1.21 and accuracy of 0.7\n",
      "Iteration 2182: with minibatch training loss = 1.15 and accuracy of 0.66\n",
      "Iteration 2183: with minibatch training loss = 1.15 and accuracy of 0.7\n",
      "Iteration 2184: with minibatch training loss = 1.36 and accuracy of 0.66\n",
      "Iteration 2185: with minibatch training loss = 1.03 and accuracy of 0.77\n",
      "Iteration 2186: with minibatch training loss = 1.24 and accuracy of 0.64\n",
      "Iteration 2187: with minibatch training loss = 1.04 and accuracy of 0.7\n",
      "Iteration 2188: with minibatch training loss = 1.11 and accuracy of 0.67\n",
      "Iteration 2189: with minibatch training loss = 1.08 and accuracy of 0.75\n",
      "Iteration 2190: with minibatch training loss = 1.06 and accuracy of 0.67\n",
      "Iteration 2191: with minibatch training loss = 0.918 and accuracy of 0.77\n",
      "Iteration 2192: with minibatch training loss = 0.8 and accuracy of 0.81\n",
      "Iteration 2193: with minibatch training loss = 0.951 and accuracy of 0.75\n",
      "Iteration 2194: with minibatch training loss = 1.35 and accuracy of 0.62\n",
      "Iteration 2195: with minibatch training loss = 1.01 and accuracy of 0.69\n",
      "Iteration 2196: with minibatch training loss = 0.989 and accuracy of 0.72\n",
      "Iteration 2197: with minibatch training loss = 0.904 and accuracy of 0.78\n",
      "Iteration 2198: with minibatch training loss = 1.12 and accuracy of 0.7\n",
      "Iteration 2199: with minibatch training loss = 0.873 and accuracy of 0.78\n",
      "Iteration 2200: with minibatch training loss = 1.28 and accuracy of 0.66\n",
      "Iteration 2201: with minibatch training loss = 0.723 and accuracy of 0.83\n",
      "Iteration 2202: with minibatch training loss = 1.2 and accuracy of 0.67\n",
      "Iteration 2203: with minibatch training loss = 0.777 and accuracy of 0.83\n",
      "Iteration 2204: with minibatch training loss = 0.947 and accuracy of 0.73\n",
      "Iteration 2205: with minibatch training loss = 1.25 and accuracy of 0.66\n",
      "Iteration 2206: with minibatch training loss = 0.948 and accuracy of 0.78\n",
      "Iteration 2207: with minibatch training loss = 1.09 and accuracy of 0.73\n",
      "Iteration 2208: with minibatch training loss = 1.1 and accuracy of 0.72\n",
      "Iteration 2209: with minibatch training loss = 0.865 and accuracy of 0.77\n",
      "Iteration 2210: with minibatch training loss = 1.08 and accuracy of 0.69\n",
      "Iteration 2211: with minibatch training loss = 1.11 and accuracy of 0.73\n",
      "Iteration 2212: with minibatch training loss = 0.887 and accuracy of 0.8\n",
      "Iteration 2213: with minibatch training loss = 1.2 and accuracy of 0.67\n",
      "Iteration 2214: with minibatch training loss = 0.899 and accuracy of 0.78\n",
      "Iteration 2215: with minibatch training loss = 1.28 and accuracy of 0.64\n",
      "Iteration 2216: with minibatch training loss = 1.04 and accuracy of 0.73\n",
      "Iteration 2217: with minibatch training loss = 0.696 and accuracy of 0.8\n",
      "Iteration 2218: with minibatch training loss = 0.825 and accuracy of 0.81\n",
      "Iteration 2219: with minibatch training loss = 1.14 and accuracy of 0.67\n",
      "Iteration 2220: with minibatch training loss = 1.02 and accuracy of 0.72\n",
      "Iteration 2221: with minibatch training loss = 0.911 and accuracy of 0.8\n",
      "Iteration 2222: with minibatch training loss = 1.12 and accuracy of 0.72\n",
      "Iteration 2223: with minibatch training loss = 1.01 and accuracy of 0.73\n",
      "Iteration 2224: with minibatch training loss = 1.1 and accuracy of 0.73\n",
      "Iteration 2225: with minibatch training loss = 0.867 and accuracy of 0.81\n",
      "Iteration 2226: with minibatch training loss = 1.18 and accuracy of 0.66\n",
      "Iteration 2227: with minibatch training loss = 0.819 and accuracy of 0.77\n",
      "Iteration 2228: with minibatch training loss = 1.34 and accuracy of 0.61\n",
      "Iteration 2229: with minibatch training loss = 1.01 and accuracy of 0.72\n",
      "Iteration 2230: with minibatch training loss = 0.884 and accuracy of 0.8\n",
      "Iteration 2231: with minibatch training loss = 0.65 and accuracy of 0.88\n",
      "Iteration 2232: with minibatch training loss = 0.907 and accuracy of 0.75\n",
      "Iteration 2233: with minibatch training loss = 0.953 and accuracy of 0.73\n",
      "Iteration 2234: with minibatch training loss = 0.793 and accuracy of 0.81\n",
      "Iteration 2235: with minibatch training loss = 0.949 and accuracy of 0.8\n",
      "Iteration 2236: with minibatch training loss = 1.08 and accuracy of 0.72\n",
      "Iteration 2237: with minibatch training loss = 1.06 and accuracy of 0.72\n",
      "Iteration 2238: with minibatch training loss = 1.12 and accuracy of 0.7\n",
      "Iteration 2239: with minibatch training loss = 1.17 and accuracy of 0.69\n",
      "Iteration 2240: with minibatch training loss = 1.02 and accuracy of 0.72\n",
      "Iteration 2241: with minibatch training loss = 0.831 and accuracy of 0.78\n",
      "Iteration 2242: with minibatch training loss = 1.09 and accuracy of 0.73\n",
      "Iteration 2243: with minibatch training loss = 0.977 and accuracy of 0.73\n",
      "Iteration 2244: with minibatch training loss = 0.951 and accuracy of 0.73\n",
      "Iteration 2245: with minibatch training loss = 1.07 and accuracy of 0.73\n",
      "Iteration 2246: with minibatch training loss = 0.971 and accuracy of 0.72\n",
      "Iteration 2247: with minibatch training loss = 0.972 and accuracy of 0.73\n",
      "Iteration 2248: with minibatch training loss = 0.969 and accuracy of 0.78\n",
      "Iteration 2249: with minibatch training loss = 1.04 and accuracy of 0.72\n",
      "Iteration 2250: with minibatch training loss = 0.904 and accuracy of 0.78\n",
      "Iteration 2251: with minibatch training loss = 1.03 and accuracy of 0.7\n",
      "Iteration 2252: with minibatch training loss = 0.94 and accuracy of 0.77\n",
      "Iteration 2253: with minibatch training loss = 0.66 and accuracy of 0.86\n",
      "Iteration 2254: with minibatch training loss = 0.748 and accuracy of 0.84\n",
      "Iteration 2255: with minibatch training loss = 0.861 and accuracy of 0.73\n",
      "Iteration 2256: with minibatch training loss = 1.37 and accuracy of 0.58\n",
      "Iteration 2257: with minibatch training loss = 0.855 and accuracy of 0.8\n",
      "Iteration 2258: with minibatch training loss = 1.46 and accuracy of 0.59\n",
      "Iteration 2259: with minibatch training loss = 1.11 and accuracy of 0.67\n",
      "Iteration 2260: with minibatch training loss = 1.23 and accuracy of 0.66\n",
      "Iteration 2261: with minibatch training loss = 1.1 and accuracy of 0.77\n",
      "Iteration 2262: with minibatch training loss = 1.15 and accuracy of 0.62\n",
      "Iteration 2263: with minibatch training loss = 0.852 and accuracy of 0.77\n",
      "Iteration 2264: with minibatch training loss = 0.728 and accuracy of 0.78\n",
      "Iteration 2265: with minibatch training loss = 1.35 and accuracy of 0.64\n",
      "Iteration 2266: with minibatch training loss = 0.972 and accuracy of 0.77\n",
      "Iteration 2267: with minibatch training loss = 1.24 and accuracy of 0.66\n",
      "Iteration 2268: with minibatch training loss = 1.21 and accuracy of 0.64\n",
      "Iteration 2269: with minibatch training loss = 0.879 and accuracy of 0.8\n",
      "Iteration 2270: with minibatch training loss = 1.04 and accuracy of 0.72\n",
      "Iteration 2271: with minibatch training loss = 1.14 and accuracy of 0.75\n",
      "Iteration 2272: with minibatch training loss = 0.877 and accuracy of 0.78\n",
      "Iteration 2273: with minibatch training loss = 0.864 and accuracy of 0.8\n",
      "Iteration 2274: with minibatch training loss = 0.73 and accuracy of 0.84\n",
      "Iteration 2275: with minibatch training loss = 1.03 and accuracy of 0.77\n",
      "Iteration 2276: with minibatch training loss = 0.962 and accuracy of 0.75\n",
      "Iteration 2277: with minibatch training loss = 1.12 and accuracy of 0.69\n",
      "Iteration 2278: with minibatch training loss = 0.983 and accuracy of 0.77\n",
      "Iteration 2279: with minibatch training loss = 1.31 and accuracy of 0.64\n",
      "Iteration 2280: with minibatch training loss = 1.06 and accuracy of 0.75\n",
      "Iteration 2281: with minibatch training loss = 0.821 and accuracy of 0.84\n",
      "Iteration 2282: with minibatch training loss = 1.02 and accuracy of 0.73\n",
      "Iteration 2283: with minibatch training loss = 1.08 and accuracy of 0.73\n",
      "Iteration 2284: with minibatch training loss = 0.98 and accuracy of 0.72\n",
      "Iteration 2285: with minibatch training loss = 1.07 and accuracy of 0.69\n",
      "Iteration 2286: with minibatch training loss = 1.22 and accuracy of 0.67\n",
      "Iteration 2287: with minibatch training loss = 0.921 and accuracy of 0.77\n",
      "Iteration 2288: with minibatch training loss = 1.09 and accuracy of 0.7\n",
      "Iteration 2289: with minibatch training loss = 1.03 and accuracy of 0.78\n",
      "Iteration 2290: with minibatch training loss = 1.29 and accuracy of 0.61\n",
      "Iteration 2291: with minibatch training loss = 0.929 and accuracy of 0.73\n",
      "Iteration 2292: with minibatch training loss = 1.02 and accuracy of 0.7\n",
      "Iteration 2293: with minibatch training loss = 0.986 and accuracy of 0.73\n",
      "Iteration 2294: with minibatch training loss = 0.894 and accuracy of 0.8\n",
      "Iteration 2295: with minibatch training loss = 1.05 and accuracy of 0.69\n",
      "Iteration 2296: with minibatch training loss = 1.18 and accuracy of 0.7\n",
      "Iteration 2297: with minibatch training loss = 1.13 and accuracy of 0.7\n",
      "Iteration 2298: with minibatch training loss = 0.861 and accuracy of 0.77\n",
      "Iteration 2299: with minibatch training loss = 0.936 and accuracy of 0.77\n",
      "Iteration 2300: with minibatch training loss = 0.97 and accuracy of 0.75\n",
      "Iteration 2301: with minibatch training loss = 1.02 and accuracy of 0.77\n",
      "Iteration 2302: with minibatch training loss = 1.15 and accuracy of 0.69\n",
      "Iteration 2303: with minibatch training loss = 0.848 and accuracy of 0.8\n",
      "Iteration 2304: with minibatch training loss = 1.08 and accuracy of 0.73\n",
      "Iteration 2305: with minibatch training loss = 0.935 and accuracy of 0.75\n",
      "Iteration 2306: with minibatch training loss = 0.86 and accuracy of 0.75\n",
      "Iteration 2307: with minibatch training loss = 0.682 and accuracy of 0.8\n",
      "Iteration 2308: with minibatch training loss = 0.814 and accuracy of 0.8\n",
      "Iteration 2309: with minibatch training loss = 1.13 and accuracy of 0.7\n",
      "Iteration 2310: with minibatch training loss = 1.23 and accuracy of 0.72\n",
      "Iteration 2311: with minibatch training loss = 1.23 and accuracy of 0.69\n",
      "Iteration 2312: with minibatch training loss = 1.11 and accuracy of 0.7\n",
      "Iteration 2313: with minibatch training loss = 0.967 and accuracy of 0.77\n",
      "Iteration 2314: with minibatch training loss = 0.952 and accuracy of 0.77\n",
      "Iteration 2315: with minibatch training loss = 0.801 and accuracy of 0.8\n",
      "Iteration 2316: with minibatch training loss = 0.853 and accuracy of 0.78\n",
      "Iteration 2317: with minibatch training loss = 1.17 and accuracy of 0.67\n",
      "Iteration 2318: with minibatch training loss = 0.817 and accuracy of 0.78\n",
      "Iteration 2319: with minibatch training loss = 0.919 and accuracy of 0.77\n",
      "Iteration 2320: with minibatch training loss = 0.879 and accuracy of 0.77\n",
      "Iteration 2321: with minibatch training loss = 0.86 and accuracy of 0.77\n",
      "Iteration 2322: with minibatch training loss = 0.692 and accuracy of 0.83\n",
      "Iteration 2323: with minibatch training loss = 0.867 and accuracy of 0.77\n",
      "Iteration 2324: with minibatch training loss = 0.985 and accuracy of 0.75\n",
      "Iteration 2325: with minibatch training loss = 1.05 and accuracy of 0.72\n",
      "Iteration 2326: with minibatch training loss = 1.24 and accuracy of 0.7\n",
      "Iteration 2327: with minibatch training loss = 0.732 and accuracy of 0.83\n",
      "Iteration 2328: with minibatch training loss = 1.1 and accuracy of 0.69\n",
      "Iteration 2329: with minibatch training loss = 0.733 and accuracy of 0.83\n",
      "Iteration 2330: with minibatch training loss = 1.3 and accuracy of 0.67\n",
      "Iteration 2331: with minibatch training loss = 0.798 and accuracy of 0.78\n",
      "Iteration 2332: with minibatch training loss = 1.19 and accuracy of 0.69\n",
      "Iteration 2333: with minibatch training loss = 1.05 and accuracy of 0.72\n",
      "Iteration 2334: with minibatch training loss = 1.42 and accuracy of 0.62\n",
      "Iteration 2335: with minibatch training loss = 1.07 and accuracy of 0.7\n",
      "Iteration 2336: with minibatch training loss = 0.926 and accuracy of 0.78\n",
      "Iteration 2337: with minibatch training loss = 0.961 and accuracy of 0.75\n",
      "Iteration 2338: with minibatch training loss = 1.07 and accuracy of 0.75\n",
      "Iteration 2339: with minibatch training loss = 1.09 and accuracy of 0.69\n",
      "Iteration 2340: with minibatch training loss = 0.765 and accuracy of 0.77\n",
      "Iteration 2341: with minibatch training loss = 0.699 and accuracy of 0.83\n",
      "Iteration 2342: with minibatch training loss = 1.09 and accuracy of 0.73\n",
      "Iteration 2343: with minibatch training loss = 1.01 and accuracy of 0.77\n",
      "Iteration 2344: with minibatch training loss = 1.23 and accuracy of 0.69\n",
      "Iteration 2345: with minibatch training loss = 1.42 and accuracy of 0.61\n",
      "Iteration 2346: with minibatch training loss = 0.889 and accuracy of 0.8\n",
      "Iteration 2347: with minibatch training loss = 0.768 and accuracy of 0.8\n",
      "Iteration 2348: with minibatch training loss = 1.17 and accuracy of 0.67\n",
      "Iteration 2349: with minibatch training loss = 1.24 and accuracy of 0.67\n",
      "Iteration 2350: with minibatch training loss = 0.654 and accuracy of 0.84\n",
      "Iteration 2351: with minibatch training loss = 1.26 and accuracy of 0.62\n",
      "Iteration 2352: with minibatch training loss = 1.04 and accuracy of 0.75\n",
      "Iteration 2353: with minibatch training loss = 1.03 and accuracy of 0.73\n",
      "Iteration 2354: with minibatch training loss = 0.872 and accuracy of 0.78\n",
      "Iteration 2355: with minibatch training loss = 0.956 and accuracy of 0.78\n",
      "Iteration 2356: with minibatch training loss = 1.17 and accuracy of 0.7\n",
      "Iteration 2357: with minibatch training loss = 1.05 and accuracy of 0.72\n",
      "Iteration 2358: with minibatch training loss = 1.06 and accuracy of 0.75\n",
      "Iteration 2359: with minibatch training loss = 0.797 and accuracy of 0.8\n",
      "Iteration 2360: with minibatch training loss = 0.95 and accuracy of 0.73\n",
      "Iteration 2361: with minibatch training loss = 0.723 and accuracy of 0.83\n",
      "Iteration 2362: with minibatch training loss = 1.06 and accuracy of 0.72\n",
      "Iteration 2363: with minibatch training loss = 0.876 and accuracy of 0.78\n",
      "Iteration 2364: with minibatch training loss = 1 and accuracy of 0.75\n",
      "Iteration 2365: with minibatch training loss = 0.971 and accuracy of 0.75\n",
      "Iteration 2366: with minibatch training loss = 1.09 and accuracy of 0.7\n",
      "Iteration 2367: with minibatch training loss = 1.39 and accuracy of 0.61\n",
      "Iteration 2368: with minibatch training loss = 1.13 and accuracy of 0.7\n",
      "Iteration 2369: with minibatch training loss = 0.971 and accuracy of 0.7\n",
      "Iteration 2370: with minibatch training loss = 0.965 and accuracy of 0.73\n",
      "Iteration 2371: with minibatch training loss = 0.865 and accuracy of 0.81\n",
      "Iteration 2372: with minibatch training loss = 1.32 and accuracy of 0.62\n",
      "Iteration 2373: with minibatch training loss = 1.04 and accuracy of 0.73\n",
      "Iteration 2374: with minibatch training loss = 1.14 and accuracy of 0.69\n",
      "Iteration 2375: with minibatch training loss = 1.12 and accuracy of 0.67\n",
      "Iteration 2376: with minibatch training loss = 1.11 and accuracy of 0.72\n",
      "Iteration 2377: with minibatch training loss = 0.846 and accuracy of 0.8\n",
      "Iteration 2378: with minibatch training loss = 0.863 and accuracy of 0.8\n",
      "Iteration 2379: with minibatch training loss = 0.803 and accuracy of 0.81\n",
      "Iteration 2380: with minibatch training loss = 1.16 and accuracy of 0.69\n",
      "Iteration 2381: with minibatch training loss = 1.32 and accuracy of 0.64\n",
      "Iteration 2382: with minibatch training loss = 1.13 and accuracy of 0.7\n",
      "Iteration 2383: with minibatch training loss = 0.818 and accuracy of 0.77\n",
      "Iteration 2384: with minibatch training loss = 0.818 and accuracy of 0.78\n",
      "Iteration 2385: with minibatch training loss = 0.827 and accuracy of 0.78\n",
      "Iteration 2386: with minibatch training loss = 1.05 and accuracy of 0.72\n",
      "Iteration 2387: with minibatch training loss = 1.33 and accuracy of 0.64\n",
      "Iteration 2388: with minibatch training loss = 1.15 and accuracy of 0.72\n",
      "Iteration 2389: with minibatch training loss = 1.1 and accuracy of 0.73\n",
      "Iteration 2390: with minibatch training loss = 1.09 and accuracy of 0.69\n",
      "Iteration 2391: with minibatch training loss = 1.28 and accuracy of 0.66\n",
      "Iteration 2392: with minibatch training loss = 0.924 and accuracy of 0.73\n",
      "Iteration 2393: with minibatch training loss = 0.87 and accuracy of 0.77\n",
      "Iteration 2394: with minibatch training loss = 1.06 and accuracy of 0.69\n",
      "Iteration 2395: with minibatch training loss = 0.955 and accuracy of 0.73\n",
      "Iteration 2396: with minibatch training loss = 1.17 and accuracy of 0.69\n",
      "Iteration 2397: with minibatch training loss = 1.2 and accuracy of 0.67\n",
      "Iteration 2398: with minibatch training loss = 1.01 and accuracy of 0.73\n",
      "Iteration 2399: with minibatch training loss = 1 and accuracy of 0.73\n",
      "Iteration 2400: with minibatch training loss = 1.17 and accuracy of 0.69\n",
      "Iteration 2401: with minibatch training loss = 0.834 and accuracy of 0.78\n",
      "Iteration 2402: with minibatch training loss = 0.985 and accuracy of 0.75\n",
      "Iteration 2403: with minibatch training loss = 0.911 and accuracy of 0.8\n",
      "Iteration 2404: with minibatch training loss = 1.02 and accuracy of 0.7\n",
      "Iteration 2405: with minibatch training loss = 1.33 and accuracy of 0.64\n",
      "Iteration 2406: with minibatch training loss = 0.658 and accuracy of 0.84\n",
      "Iteration 2407: with minibatch training loss = 0.943 and accuracy of 0.78\n",
      "Iteration 2408: with minibatch training loss = 1.13 and accuracy of 0.69\n",
      "Iteration 2409: with minibatch training loss = 1.19 and accuracy of 0.7\n",
      "Iteration 2410: with minibatch training loss = 0.652 and accuracy of 0.84\n",
      "Iteration 2411: with minibatch training loss = 1.04 and accuracy of 0.75\n",
      "Iteration 2412: with minibatch training loss = 0.902 and accuracy of 0.8\n",
      "Iteration 2413: with minibatch training loss = 1.32 and accuracy of 0.67\n",
      "Iteration 2414: with minibatch training loss = 0.991 and accuracy of 0.77\n",
      "Iteration 2415: with minibatch training loss = 1.01 and accuracy of 0.72\n",
      "Iteration 2416: with minibatch training loss = 1.31 and accuracy of 0.67\n",
      "Iteration 2417: with minibatch training loss = 1.2 and accuracy of 0.69\n",
      "Iteration 2418: with minibatch training loss = 0.89 and accuracy of 0.78\n",
      "Iteration 2419: with minibatch training loss = 1.15 and accuracy of 0.69\n",
      "Iteration 2420: with minibatch training loss = 1.03 and accuracy of 0.75\n",
      "Iteration 2421: with minibatch training loss = 1.09 and accuracy of 0.73\n",
      "Iteration 2422: with minibatch training loss = 0.889 and accuracy of 0.78\n",
      "Iteration 2423: with minibatch training loss = 1.05 and accuracy of 0.69\n",
      "Iteration 2424: with minibatch training loss = 1.17 and accuracy of 0.67\n",
      "Iteration 2425: with minibatch training loss = 0.78 and accuracy of 0.78\n",
      "Iteration 2426: with minibatch training loss = 1.22 and accuracy of 0.69\n",
      "Iteration 2427: with minibatch training loss = 0.81 and accuracy of 0.83\n",
      "Iteration 2428: with minibatch training loss = 0.937 and accuracy of 0.78\n",
      "Iteration 2429: with minibatch training loss = 1.35 and accuracy of 0.62\n",
      "Iteration 2430: with minibatch training loss = 0.913 and accuracy of 0.75\n",
      "Iteration 2431: with minibatch training loss = 1.08 and accuracy of 0.7\n",
      "Iteration 2432: with minibatch training loss = 1.08 and accuracy of 0.67\n",
      "Iteration 2433: with minibatch training loss = 1.02 and accuracy of 0.73\n",
      "Iteration 2434: with minibatch training loss = 1.19 and accuracy of 0.69\n",
      "Iteration 2435: with minibatch training loss = 0.842 and accuracy of 0.8\n",
      "Iteration 2436: with minibatch training loss = 1.18 and accuracy of 0.64\n",
      "Iteration 2437: with minibatch training loss = 1.06 and accuracy of 0.78\n",
      "Iteration 2438: with minibatch training loss = 1.15 and accuracy of 0.69\n",
      "Iteration 2439: with minibatch training loss = 0.771 and accuracy of 0.81\n",
      "Iteration 2440: with minibatch training loss = 0.871 and accuracy of 0.78\n",
      "Iteration 2441: with minibatch training loss = 1.2 and accuracy of 0.67\n",
      "Iteration 2442: with minibatch training loss = 1.14 and accuracy of 0.7\n",
      "Iteration 2443: with minibatch training loss = 1.12 and accuracy of 0.73\n",
      "Iteration 2444: with minibatch training loss = 0.723 and accuracy of 0.83\n",
      "Iteration 2445: with minibatch training loss = 1.02 and accuracy of 0.7\n",
      "Iteration 2446: with minibatch training loss = 0.969 and accuracy of 0.77\n",
      "Iteration 2447: with minibatch training loss = 0.818 and accuracy of 0.8\n",
      "Iteration 2448: with minibatch training loss = 0.984 and accuracy of 0.77\n",
      "Iteration 2449: with minibatch training loss = 1.02 and accuracy of 0.72\n",
      "Iteration 2450: with minibatch training loss = 0.817 and accuracy of 0.78\n",
      "Iteration 2451: with minibatch training loss = 1.16 and accuracy of 0.69\n",
      "Iteration 2452: with minibatch training loss = 1.39 and accuracy of 0.64\n",
      "Iteration 2453: with minibatch training loss = 0.877 and accuracy of 0.77\n",
      "Iteration 2454: with minibatch training loss = 0.835 and accuracy of 0.78\n",
      "Iteration 2455: with minibatch training loss = 1.29 and accuracy of 0.66\n",
      "Iteration 2456: with minibatch training loss = 1.22 and accuracy of 0.66\n",
      "Iteration 2457: with minibatch training loss = 1.16 and accuracy of 0.64\n",
      "Iteration 2458: with minibatch training loss = 0.747 and accuracy of 0.81\n",
      "Iteration 2459: with minibatch training loss = 1.11 and accuracy of 0.69\n",
      "Iteration 2460: with minibatch training loss = 0.839 and accuracy of 0.81\n",
      "Iteration 2461: with minibatch training loss = 0.933 and accuracy of 0.78\n",
      "Iteration 2462: with minibatch training loss = 0.626 and accuracy of 0.88\n",
      "Iteration 2463: with minibatch training loss = 0.975 and accuracy of 0.75\n",
      "Iteration 2464: with minibatch training loss = 1.18 and accuracy of 0.67\n",
      "Iteration 2465: with minibatch training loss = 0.946 and accuracy of 0.75\n",
      "Iteration 2466: with minibatch training loss = 1.47 and accuracy of 0.59\n",
      "Iteration 2467: with minibatch training loss = 0.858 and accuracy of 0.8\n",
      "Iteration 2468: with minibatch training loss = 1.09 and accuracy of 0.69\n",
      "Iteration 2469: with minibatch training loss = 1.26 and accuracy of 0.62\n",
      "Iteration 2470: with minibatch training loss = 1.28 and accuracy of 0.64\n",
      "Iteration 2471: with minibatch training loss = 1.06 and accuracy of 0.7\n",
      "Iteration 2472: with minibatch training loss = 1.09 and accuracy of 0.67\n",
      "Iteration 2473: with minibatch training loss = 1.32 and accuracy of 0.64\n",
      "Iteration 2474: with minibatch training loss = 0.841 and accuracy of 0.8\n",
      "Iteration 2475: with minibatch training loss = 0.986 and accuracy of 0.78\n",
      "Iteration 2476: with minibatch training loss = 1.08 and accuracy of 0.72\n",
      "Iteration 2477: with minibatch training loss = 0.814 and accuracy of 0.8\n",
      "Iteration 2478: with minibatch training loss = 0.916 and accuracy of 0.78\n",
      "Iteration 2479: with minibatch training loss = 0.996 and accuracy of 0.73\n",
      "Iteration 2480: with minibatch training loss = 1.27 and accuracy of 0.66\n",
      "Iteration 2481: with minibatch training loss = 1.52 and accuracy of 0.56\n",
      "Iteration 2482: with minibatch training loss = 1.12 and accuracy of 0.73\n",
      "Iteration 2483: with minibatch training loss = 1.36 and accuracy of 0.61\n",
      "Iteration 2484: with minibatch training loss = 0.854 and accuracy of 0.77\n",
      "Iteration 2485: with minibatch training loss = 0.817 and accuracy of 0.77\n",
      "Iteration 2486: with minibatch training loss = 0.869 and accuracy of 0.81\n",
      "Iteration 2487: with minibatch training loss = 0.982 and accuracy of 0.73\n",
      "Iteration 2488: with minibatch training loss = 0.94 and accuracy of 0.72\n",
      "Iteration 2489: with minibatch training loss = 1.25 and accuracy of 0.66\n",
      "Iteration 2490: with minibatch training loss = 1.37 and accuracy of 0.62\n",
      "Iteration 2491: with minibatch training loss = 1.08 and accuracy of 0.7\n",
      "Iteration 2492: with minibatch training loss = 1.26 and accuracy of 0.7\n",
      "Iteration 2493: with minibatch training loss = 0.975 and accuracy of 0.73\n",
      "Iteration 2494: with minibatch training loss = 0.778 and accuracy of 0.81\n",
      "Iteration 2495: with minibatch training loss = 1.12 and accuracy of 0.72\n",
      "Iteration 2496: with minibatch training loss = 0.898 and accuracy of 0.77\n",
      "Iteration 2497: with minibatch training loss = 1.1 and accuracy of 0.7\n",
      "Iteration 2498: with minibatch training loss = 1.22 and accuracy of 0.7\n",
      "Iteration 2499: with minibatch training loss = 1.34 and accuracy of 0.62\n",
      "Iteration 2500: with minibatch training loss = 1.04 and accuracy of 0.77\n",
      "Iteration 2501: with minibatch training loss = 1.16 and accuracy of 0.69\n",
      "Iteration 2502: with minibatch training loss = 1.1 and accuracy of 0.69\n",
      "Iteration 2503: with minibatch training loss = 0.936 and accuracy of 0.72\n",
      "Iteration 2504: with minibatch training loss = 0.889 and accuracy of 0.8\n",
      "Iteration 2505: with minibatch training loss = 1.01 and accuracy of 0.73\n",
      "Iteration 2506: with minibatch training loss = 1.09 and accuracy of 0.7\n",
      "Iteration 2507: with minibatch training loss = 1.08 and accuracy of 0.73\n",
      "Iteration 2508: with minibatch training loss = 1.08 and accuracy of 0.7\n",
      "Iteration 2509: with minibatch training loss = 1.12 and accuracy of 0.7\n",
      "Iteration 2510: with minibatch training loss = 0.834 and accuracy of 0.77\n",
      "Iteration 2511: with minibatch training loss = 1.06 and accuracy of 0.72\n",
      "Iteration 2512: with minibatch training loss = 0.927 and accuracy of 0.77\n",
      "Iteration 2513: with minibatch training loss = 1.27 and accuracy of 0.64\n",
      "Iteration 2514: with minibatch training loss = 0.888 and accuracy of 0.81\n",
      "Iteration 2515: with minibatch training loss = 0.915 and accuracy of 0.75\n",
      "Iteration 2516: with minibatch training loss = 0.645 and accuracy of 0.83\n",
      "Iteration 2517: with minibatch training loss = 0.915 and accuracy of 0.77\n",
      "Iteration 2518: with minibatch training loss = 1.37 and accuracy of 0.64\n",
      "Iteration 2519: with minibatch training loss = 0.943 and accuracy of 0.8\n",
      "Iteration 2520: with minibatch training loss = 0.953 and accuracy of 0.77\n",
      "Iteration 2521: with minibatch training loss = 1.08 and accuracy of 0.7\n",
      "Iteration 2522: with minibatch training loss = 1.12 and accuracy of 0.72\n",
      "Iteration 2523: with minibatch training loss = 1.57 and accuracy of 0.59\n",
      "Iteration 2524: with minibatch training loss = 0.965 and accuracy of 0.72\n",
      "Iteration 2525: with minibatch training loss = 0.931 and accuracy of 0.8\n",
      "Iteration 2526: with minibatch training loss = 1.13 and accuracy of 0.67\n",
      "Iteration 2527: with minibatch training loss = 1.14 and accuracy of 0.67\n",
      "Iteration 2528: with minibatch training loss = 0.931 and accuracy of 0.78\n",
      "Iteration 2529: with minibatch training loss = 1.15 and accuracy of 0.69\n",
      "Iteration 2530: with minibatch training loss = 1.11 and accuracy of 0.7\n",
      "Iteration 2531: with minibatch training loss = 1.25 and accuracy of 0.67\n",
      "Iteration 2532: with minibatch training loss = 0.751 and accuracy of 0.8\n",
      "Iteration 2533: with minibatch training loss = 0.998 and accuracy of 0.75\n",
      "Iteration 2534: with minibatch training loss = 0.779 and accuracy of 0.8\n",
      "Iteration 2535: with minibatch training loss = 0.748 and accuracy of 0.81\n",
      "Iteration 2536: with minibatch training loss = 1.11 and accuracy of 0.69\n",
      "Iteration 2537: with minibatch training loss = 1.16 and accuracy of 0.69\n",
      "Iteration 2538: with minibatch training loss = 0.692 and accuracy of 0.83\n",
      "Iteration 2539: with minibatch training loss = 0.812 and accuracy of 0.81\n",
      "Iteration 2540: with minibatch training loss = 1.04 and accuracy of 0.72\n",
      "Iteration 2541: with minibatch training loss = 0.84 and accuracy of 0.83\n",
      "Iteration 2542: with minibatch training loss = 0.92 and accuracy of 0.8\n",
      "Iteration 2543: with minibatch training loss = 0.818 and accuracy of 0.8\n",
      "Iteration 2544: with minibatch training loss = 1.08 and accuracy of 0.72\n",
      "Iteration 2545: with minibatch training loss = 0.844 and accuracy of 0.81\n",
      "Iteration 2546: with minibatch training loss = 1.21 and accuracy of 0.62\n",
      "Iteration 2547: with minibatch training loss = 0.89 and accuracy of 0.77\n",
      "Iteration 2548: with minibatch training loss = 1.03 and accuracy of 0.72\n",
      "Iteration 2549: with minibatch training loss = 1.16 and accuracy of 0.66\n",
      "Iteration 2550: with minibatch training loss = 0.963 and accuracy of 0.77\n",
      "Iteration 2551: with minibatch training loss = 1.25 and accuracy of 0.67\n",
      "Iteration 2552: with minibatch training loss = 0.878 and accuracy of 0.75\n",
      "Iteration 2553: with minibatch training loss = 0.91 and accuracy of 0.8\n",
      "Iteration 2554: with minibatch training loss = 1.01 and accuracy of 0.72\n",
      "Iteration 2555: with minibatch training loss = 1.07 and accuracy of 0.72\n",
      "Iteration 2556: with minibatch training loss = 1.1 and accuracy of 0.7\n",
      "Iteration 2557: with minibatch training loss = 1.03 and accuracy of 0.7\n",
      "Iteration 2558: with minibatch training loss = 0.754 and accuracy of 0.84\n",
      "Iteration 2559: with minibatch training loss = 0.918 and accuracy of 0.75\n",
      "Iteration 2560: with minibatch training loss = 0.762 and accuracy of 0.81\n",
      "Iteration 2561: with minibatch training loss = 1.07 and accuracy of 0.69\n",
      "Iteration 2562: with minibatch training loss = 0.945 and accuracy of 0.77\n",
      "Iteration 2563: with minibatch training loss = 1.03 and accuracy of 0.75\n",
      "Iteration 2564: with minibatch training loss = 0.783 and accuracy of 0.78\n",
      "Iteration 2565: with minibatch training loss = 1.01 and accuracy of 0.72\n",
      "Iteration 2566: with minibatch training loss = 1.01 and accuracy of 0.75\n",
      "Iteration 2567: with minibatch training loss = 1.21 and accuracy of 0.66\n",
      "Iteration 2568: with minibatch training loss = 1.15 and accuracy of 0.66\n",
      "Iteration 2569: with minibatch training loss = 0.93 and accuracy of 0.77\n",
      "Iteration 2570: with minibatch training loss = 0.942 and accuracy of 0.77\n",
      "Iteration 2571: with minibatch training loss = 1.44 and accuracy of 0.59\n",
      "Iteration 2572: with minibatch training loss = 1.17 and accuracy of 0.69\n",
      "Iteration 2573: with minibatch training loss = 1.15 and accuracy of 0.72\n",
      "Iteration 2574: with minibatch training loss = 1.17 and accuracy of 0.7\n",
      "Iteration 2575: with minibatch training loss = 1.25 and accuracy of 0.67\n",
      "Iteration 2576: with minibatch training loss = 0.933 and accuracy of 0.78\n",
      "Iteration 2577: with minibatch training loss = 1.15 and accuracy of 0.72\n",
      "Iteration 2578: with minibatch training loss = 1.16 and accuracy of 0.69\n",
      "Iteration 2579: with minibatch training loss = 1.01 and accuracy of 0.73\n",
      "Iteration 2580: with minibatch training loss = 1.04 and accuracy of 0.72\n",
      "Iteration 2581: with minibatch training loss = 0.54 and accuracy of 0.91\n",
      "Iteration 2582: with minibatch training loss = 1.08 and accuracy of 0.7\n",
      "Iteration 2583: with minibatch training loss = 1.16 and accuracy of 0.7\n",
      "Iteration 2584: with minibatch training loss = 0.9 and accuracy of 0.72\n",
      "Iteration 2585: with minibatch training loss = 0.868 and accuracy of 0.8\n",
      "Iteration 2586: with minibatch training loss = 0.684 and accuracy of 0.84\n",
      "Iteration 2587: with minibatch training loss = 1.11 and accuracy of 0.69\n",
      "Iteration 2588: with minibatch training loss = 1.13 and accuracy of 0.69\n",
      "Iteration 2589: with minibatch training loss = 1.13 and accuracy of 0.7\n",
      "Iteration 2590: with minibatch training loss = 1.06 and accuracy of 0.77\n",
      "Iteration 2591: with minibatch training loss = 1.16 and accuracy of 0.67\n",
      "Iteration 2592: with minibatch training loss = 1.17 and accuracy of 0.69\n",
      "Iteration 2593: with minibatch training loss = 1.07 and accuracy of 0.67\n",
      "Iteration 2594: with minibatch training loss = 1.21 and accuracy of 0.66\n",
      "Iteration 2595: with minibatch training loss = 1.06 and accuracy of 0.73\n",
      "Iteration 2596: with minibatch training loss = 0.957 and accuracy of 0.72\n",
      "Iteration 2597: with minibatch training loss = 1.19 and accuracy of 0.72\n",
      "Iteration 2598: with minibatch training loss = 0.952 and accuracy of 0.73\n",
      "Iteration 2599: with minibatch training loss = 1.11 and accuracy of 0.72\n",
      "Iteration 2600: with minibatch training loss = 1.07 and accuracy of 0.73\n",
      "Iteration 2601: with minibatch training loss = 1.29 and accuracy of 0.64\n",
      "Iteration 2602: with minibatch training loss = 0.872 and accuracy of 0.8\n",
      "Iteration 2603: with minibatch training loss = 0.844 and accuracy of 0.8\n",
      "Iteration 2604: with minibatch training loss = 1.1 and accuracy of 0.72\n",
      "Iteration 2605: with minibatch training loss = 0.868 and accuracy of 0.8\n",
      "Iteration 2606: with minibatch training loss = 0.711 and accuracy of 0.81\n",
      "Iteration 2607: with minibatch training loss = 0.694 and accuracy of 0.83\n",
      "Iteration 2608: with minibatch training loss = 1.02 and accuracy of 0.72\n",
      "Iteration 2609: with minibatch training loss = 0.969 and accuracy of 0.75\n",
      "Iteration 2610: with minibatch training loss = 0.714 and accuracy of 0.83\n",
      "Iteration 2611: with minibatch training loss = 1.01 and accuracy of 0.78\n",
      "Iteration 2612: with minibatch training loss = 0.804 and accuracy of 0.77\n",
      "Iteration 2613: with minibatch training loss = 0.986 and accuracy of 0.78\n",
      "Iteration 2614: with minibatch training loss = 1.08 and accuracy of 0.73\n",
      "Iteration 2615: with minibatch training loss = 0.749 and accuracy of 0.81\n",
      "Iteration 2616: with minibatch training loss = 1.19 and accuracy of 0.69\n",
      "Iteration 2617: with minibatch training loss = 0.958 and accuracy of 0.72\n",
      "Iteration 2618: with minibatch training loss = 1.27 and accuracy of 0.64\n",
      "Iteration 2619: with minibatch training loss = 0.601 and accuracy of 0.89\n",
      "Iteration 2620: with minibatch training loss = 1.11 and accuracy of 0.7\n",
      "Iteration 2621: with minibatch training loss = 1.02 and accuracy of 0.73\n",
      "Iteration 2622: with minibatch training loss = 0.907 and accuracy of 0.75\n",
      "Iteration 2623: with minibatch training loss = 0.988 and accuracy of 0.73\n",
      "Iteration 2624: with minibatch training loss = 1.08 and accuracy of 0.7\n",
      "Iteration 2625: with minibatch training loss = 1.47 and accuracy of 0.64\n",
      "Iteration 2626: with minibatch training loss = 1.21 and accuracy of 0.69\n",
      "Iteration 2627: with minibatch training loss = 1.26 and accuracy of 0.69\n",
      "Iteration 2628: with minibatch training loss = 0.86 and accuracy of 0.78\n",
      "Iteration 2629: with minibatch training loss = 0.854 and accuracy of 0.81\n",
      "Iteration 2630: with minibatch training loss = 0.97 and accuracy of 0.73\n",
      "Iteration 2631: with minibatch training loss = 1.06 and accuracy of 0.73\n",
      "Iteration 2632: with minibatch training loss = 0.774 and accuracy of 0.81\n",
      "Iteration 2633: with minibatch training loss = 1.4 and accuracy of 0.62\n",
      "Iteration 2634: with minibatch training loss = 0.667 and accuracy of 0.84\n",
      "Iteration 2635: with minibatch training loss = 0.906 and accuracy of 0.75\n",
      "Iteration 2636: with minibatch training loss = 1.38 and accuracy of 0.62\n",
      "Iteration 2637: with minibatch training loss = 1.16 and accuracy of 0.67\n",
      "Iteration 2638: with minibatch training loss = 1.05 and accuracy of 0.73\n",
      "Iteration 2639: with minibatch training loss = 1.23 and accuracy of 0.7\n",
      "Iteration 2640: with minibatch training loss = 1.08 and accuracy of 0.7\n",
      "Iteration 2641: with minibatch training loss = 1.24 and accuracy of 0.62\n",
      "Iteration 2642: with minibatch training loss = 0.831 and accuracy of 0.8\n",
      "Iteration 2643: with minibatch training loss = 0.72 and accuracy of 0.83\n",
      "Iteration 2644: with minibatch training loss = 0.999 and accuracy of 0.77\n",
      "Iteration 2645: with minibatch training loss = 1.25 and accuracy of 0.69\n",
      "Iteration 2646: with minibatch training loss = 0.922 and accuracy of 0.75\n",
      "Iteration 2647: with minibatch training loss = 0.956 and accuracy of 0.73\n",
      "Iteration 2648: with minibatch training loss = 1.11 and accuracy of 0.7\n",
      "Iteration 2649: with minibatch training loss = 0.807 and accuracy of 0.78\n",
      "Iteration 2650: with minibatch training loss = 0.785 and accuracy of 0.78\n",
      "Iteration 2651: with minibatch training loss = 0.84 and accuracy of 0.81\n",
      "Iteration 2652: with minibatch training loss = 1.07 and accuracy of 0.69\n",
      "Iteration 2653: with minibatch training loss = 0.839 and accuracy of 0.77\n",
      "Iteration 2654: with minibatch training loss = 1.36 and accuracy of 0.66\n",
      "Iteration 2655: with minibatch training loss = 1.15 and accuracy of 0.69\n",
      "Iteration 2656: with minibatch training loss = 1.15 and accuracy of 0.69\n",
      "Iteration 2657: with minibatch training loss = 1.15 and accuracy of 0.72\n",
      "Iteration 2658: with minibatch training loss = 1.22 and accuracy of 0.69\n",
      "Iteration 2659: with minibatch training loss = 1.2 and accuracy of 0.69\n",
      "Iteration 2660: with minibatch training loss = 1.14 and accuracy of 0.67\n",
      "Iteration 2661: with minibatch training loss = 1.22 and accuracy of 0.66\n",
      "Iteration 2662: with minibatch training loss = 0.87 and accuracy of 0.75\n",
      "Iteration 2663: with minibatch training loss = 1.01 and accuracy of 0.69\n",
      "Iteration 2664: with minibatch training loss = 0.675 and accuracy of 0.81\n",
      "Iteration 2665: with minibatch training loss = 0.876 and accuracy of 0.78\n",
      "Iteration 2666: with minibatch training loss = 0.733 and accuracy of 0.84\n",
      "Iteration 2667: with minibatch training loss = 1.03 and accuracy of 0.77\n",
      "Iteration 2668: with minibatch training loss = 0.949 and accuracy of 0.7\n",
      "Iteration 2669: with minibatch training loss = 0.777 and accuracy of 0.75\n",
      "Iteration 2670: with minibatch training loss = 0.923 and accuracy of 0.75\n",
      "Iteration 2671: with minibatch training loss = 0.905 and accuracy of 0.8\n",
      "Iteration 2672: with minibatch training loss = 1.01 and accuracy of 0.75\n",
      "Iteration 2673: with minibatch training loss = 1.45 and accuracy of 0.59\n",
      "Iteration 2674: with minibatch training loss = 1.06 and accuracy of 0.73\n",
      "Iteration 2675: with minibatch training loss = 0.786 and accuracy of 0.78\n",
      "Iteration 2676: with minibatch training loss = 1.06 and accuracy of 0.77\n",
      "Iteration 2677: with minibatch training loss = 1.09 and accuracy of 0.72\n",
      "Iteration 2678: with minibatch training loss = 0.848 and accuracy of 0.78\n",
      "Iteration 2679: with minibatch training loss = 1.07 and accuracy of 0.67\n",
      "Iteration 2680: with minibatch training loss = 0.794 and accuracy of 0.8\n",
      "Iteration 2681: with minibatch training loss = 1.13 and accuracy of 0.72\n",
      "Iteration 2682: with minibatch training loss = 0.916 and accuracy of 0.72\n",
      "Iteration 2683: with minibatch training loss = 0.634 and accuracy of 0.86\n",
      "Iteration 2684: with minibatch training loss = 0.804 and accuracy of 0.8\n",
      "Iteration 2685: with minibatch training loss = 1.25 and accuracy of 0.66\n",
      "Iteration 2686: with minibatch training loss = 1.26 and accuracy of 0.67\n",
      "Iteration 2687: with minibatch training loss = 0.981 and accuracy of 0.72\n",
      "Iteration 2688: with minibatch training loss = 0.811 and accuracy of 0.77\n",
      "Iteration 2689: with minibatch training loss = 0.779 and accuracy of 0.78\n",
      "Iteration 2690: with minibatch training loss = 0.933 and accuracy of 0.78\n",
      "Iteration 2691: with minibatch training loss = 0.981 and accuracy of 0.72\n",
      "Iteration 2692: with minibatch training loss = 1.05 and accuracy of 0.73\n",
      "Iteration 2693: with minibatch training loss = 0.995 and accuracy of 0.77\n",
      "Iteration 2694: with minibatch training loss = 1.05 and accuracy of 0.73\n",
      "Iteration 2695: with minibatch training loss = 0.716 and accuracy of 0.84\n",
      "Iteration 2696: with minibatch training loss = 0.793 and accuracy of 0.78\n",
      "Iteration 2697: with minibatch training loss = 0.819 and accuracy of 0.78\n",
      "Iteration 2698: with minibatch training loss = 0.876 and accuracy of 0.75\n",
      "Iteration 2699: with minibatch training loss = 0.871 and accuracy of 0.78\n",
      "Iteration 2700: with minibatch training loss = 1.08 and accuracy of 0.7\n",
      "Iteration 2701: with minibatch training loss = 0.814 and accuracy of 0.81\n",
      "Iteration 2702: with minibatch training loss = 0.941 and accuracy of 0.73\n",
      "Iteration 2703: with minibatch training loss = 1.08 and accuracy of 0.69\n",
      "Iteration 2704: with minibatch training loss = 0.665 and accuracy of 0.84\n",
      "Iteration 2705: with minibatch training loss = 0.895 and accuracy of 0.78\n",
      "Iteration 2706: with minibatch training loss = 1.14 and accuracy of 0.7\n",
      "Iteration 2707: with minibatch training loss = 0.882 and accuracy of 0.77\n",
      "Iteration 2708: with minibatch training loss = 0.883 and accuracy of 0.72\n",
      "Iteration 2709: with minibatch training loss = 0.929 and accuracy of 0.77\n",
      "Iteration 2710: with minibatch training loss = 0.948 and accuracy of 0.73\n",
      "Iteration 2711: with minibatch training loss = 0.907 and accuracy of 0.77\n",
      "Iteration 2712: with minibatch training loss = 0.879 and accuracy of 0.8\n",
      "Iteration 2713: with minibatch training loss = 1.11 and accuracy of 0.67\n",
      "Iteration 2714: with minibatch training loss = 0.851 and accuracy of 0.77\n",
      "Iteration 2715: with minibatch training loss = 1.05 and accuracy of 0.69\n",
      "Iteration 2716: with minibatch training loss = 0.966 and accuracy of 0.77\n",
      "Iteration 2717: with minibatch training loss = 1.3 and accuracy of 0.67\n",
      "Iteration 2718: with minibatch training loss = 1.04 and accuracy of 0.7\n",
      "Iteration 2719: with minibatch training loss = 1.07 and accuracy of 0.7\n",
      "Iteration 2720: with minibatch training loss = 0.943 and accuracy of 0.72\n",
      "Iteration 2721: with minibatch training loss = 0.855 and accuracy of 0.78\n",
      "Iteration 2722: with minibatch training loss = 1.12 and accuracy of 0.73\n",
      "Iteration 2723: with minibatch training loss = 1.21 and accuracy of 0.67\n",
      "Iteration 2724: with minibatch training loss = 1.19 and accuracy of 0.7\n",
      "Iteration 2725: with minibatch training loss = 1.16 and accuracy of 0.67\n",
      "Iteration 2726: with minibatch training loss = 1.21 and accuracy of 0.66\n",
      "Iteration 2727: with minibatch training loss = 1.06 and accuracy of 0.7\n",
      "Iteration 2728: with minibatch training loss = 0.765 and accuracy of 0.81\n",
      "Iteration 2729: with minibatch training loss = 1.14 and accuracy of 0.67\n",
      "Iteration 2730: with minibatch training loss = 1.04 and accuracy of 0.72\n",
      "Iteration 2731: with minibatch training loss = 0.938 and accuracy of 0.72\n",
      "Iteration 2732: with minibatch training loss = 1.12 and accuracy of 0.69\n",
      "Iteration 2733: with minibatch training loss = 1.09 and accuracy of 0.72\n",
      "Iteration 2734: with minibatch training loss = 1.14 and accuracy of 0.69\n",
      "Iteration 2735: with minibatch training loss = 0.934 and accuracy of 0.73\n",
      "Iteration 2736: with minibatch training loss = 0.824 and accuracy of 0.77\n",
      "Iteration 2737: with minibatch training loss = 0.91 and accuracy of 0.77\n",
      "Iteration 2738: with minibatch training loss = 1.35 and accuracy of 0.64\n",
      "Iteration 2739: with minibatch training loss = 0.798 and accuracy of 0.8\n",
      "Iteration 2740: with minibatch training loss = 1.06 and accuracy of 0.72\n",
      "Iteration 2741: with minibatch training loss = 1.22 and accuracy of 0.67\n",
      "Iteration 2742: with minibatch training loss = 1.01 and accuracy of 0.73\n",
      "Iteration 2743: with minibatch training loss = 0.651 and accuracy of 0.83\n",
      "Iteration 2744: with minibatch training loss = 1.33 and accuracy of 0.61\n",
      "Iteration 2745: with minibatch training loss = 1.01 and accuracy of 0.72\n",
      "Iteration 2746: with minibatch training loss = 1.14 and accuracy of 0.73\n",
      "Iteration 2747: with minibatch training loss = 1.05 and accuracy of 0.73\n",
      "Iteration 2748: with minibatch training loss = 0.611 and accuracy of 0.83\n",
      "Iteration 2749: with minibatch training loss = 0.785 and accuracy of 0.83\n",
      "Iteration 2750: with minibatch training loss = 1.33 and accuracy of 0.64\n",
      "Iteration 2751: with minibatch training loss = 0.764 and accuracy of 0.81\n",
      "Iteration 2752: with minibatch training loss = 0.886 and accuracy of 0.78\n",
      "Iteration 2753: with minibatch training loss = 1.24 and accuracy of 0.67\n",
      "Iteration 2754: with minibatch training loss = 1.35 and accuracy of 0.62\n",
      "Iteration 2755: with minibatch training loss = 0.947 and accuracy of 0.77\n",
      "Iteration 2756: with minibatch training loss = 1.28 and accuracy of 0.67\n",
      "Iteration 2757: with minibatch training loss = 1.12 and accuracy of 0.7\n",
      "Iteration 2758: with minibatch training loss = 0.891 and accuracy of 0.78\n",
      "Iteration 2759: with minibatch training loss = 0.804 and accuracy of 0.8\n",
      "Iteration 2760: with minibatch training loss = 1.01 and accuracy of 0.72\n",
      "Iteration 2761: with minibatch training loss = 1.31 and accuracy of 0.62\n",
      "Iteration 2762: with minibatch training loss = 0.846 and accuracy of 0.78\n",
      "Iteration 2763: with minibatch training loss = 1.33 and accuracy of 0.64\n",
      "Iteration 2764: with minibatch training loss = 0.832 and accuracy of 0.73\n",
      "Iteration 2765: with minibatch training loss = 1.05 and accuracy of 0.7\n",
      "Iteration 2766: with minibatch training loss = 1.14 and accuracy of 0.67\n",
      "Iteration 2767: with minibatch training loss = 0.88 and accuracy of 0.75\n",
      "Iteration 2768: with minibatch training loss = 0.958 and accuracy of 0.73\n",
      "Iteration 2769: with minibatch training loss = 1.15 and accuracy of 0.64\n",
      "Iteration 2770: with minibatch training loss = 0.95 and accuracy of 0.77\n",
      "Iteration 2771: with minibatch training loss = 1.29 and accuracy of 0.64\n",
      "Iteration 2772: with minibatch training loss = 1.14 and accuracy of 0.69\n",
      "Iteration 2773: with minibatch training loss = 0.903 and accuracy of 0.78\n",
      "Iteration 2774: with minibatch training loss = 1.12 and accuracy of 0.67\n",
      "Validation loss: 0.40666682\n",
      "Model's weights saved at /Users/nhat/Documents/Projects/LetterClassifier/weights/model_se.ckpt\n",
      "Epoch 2, Overall loss = 1.06 and accuracy of 0.725\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n46ETZFQOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1206be160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2776: with minibatch training loss = 1.19 and accuracy of 0.66\n",
      "Iteration 2777: with minibatch training loss = 0.891 and accuracy of 0.75\n",
      "Iteration 2778: with minibatch training loss = 0.979 and accuracy of 0.73\n",
      "Iteration 2779: with minibatch training loss = 1.15 and accuracy of 0.69\n",
      "Iteration 2780: with minibatch training loss = 1.15 and accuracy of 0.7\n",
      "Iteration 2781: with minibatch training loss = 0.874 and accuracy of 0.77\n",
      "Iteration 2782: with minibatch training loss = 0.81 and accuracy of 0.8\n",
      "Iteration 2783: with minibatch training loss = 0.716 and accuracy of 0.8\n",
      "Iteration 2784: with minibatch training loss = 1.15 and accuracy of 0.64\n",
      "Iteration 2785: with minibatch training loss = 1.11 and accuracy of 0.7\n",
      "Iteration 2786: with minibatch training loss = 1.08 and accuracy of 0.72\n",
      "Iteration 2787: with minibatch training loss = 1.08 and accuracy of 0.7\n",
      "Iteration 2788: with minibatch training loss = 0.802 and accuracy of 0.8\n",
      "Iteration 2789: with minibatch training loss = 0.954 and accuracy of 0.73\n",
      "Iteration 2790: with minibatch training loss = 1.18 and accuracy of 0.67\n",
      "Iteration 2791: with minibatch training loss = 0.954 and accuracy of 0.75\n",
      "Iteration 2792: with minibatch training loss = 1.15 and accuracy of 0.67\n",
      "Iteration 2793: with minibatch training loss = 0.856 and accuracy of 0.8\n",
      "Iteration 2794: with minibatch training loss = 0.89 and accuracy of 0.78\n",
      "Iteration 2795: with minibatch training loss = 1 and accuracy of 0.73\n",
      "Iteration 2796: with minibatch training loss = 0.797 and accuracy of 0.83\n",
      "Iteration 2797: with minibatch training loss = 0.737 and accuracy of 0.8\n",
      "Iteration 2798: with minibatch training loss = 1.15 and accuracy of 0.7\n",
      "Iteration 2799: with minibatch training loss = 1.21 and accuracy of 0.69\n",
      "Iteration 2800: with minibatch training loss = 1.02 and accuracy of 0.72\n",
      "Iteration 2801: with minibatch training loss = 1.05 and accuracy of 0.67\n",
      "Iteration 2802: with minibatch training loss = 1.14 and accuracy of 0.7\n",
      "Iteration 2803: with minibatch training loss = 0.799 and accuracy of 0.8\n",
      "Iteration 2804: with minibatch training loss = 0.816 and accuracy of 0.81\n",
      "Iteration 2805: with minibatch training loss = 0.929 and accuracy of 0.78\n",
      "Iteration 2806: with minibatch training loss = 0.99 and accuracy of 0.78\n",
      "Iteration 2807: with minibatch training loss = 1.01 and accuracy of 0.73\n",
      "Iteration 2808: with minibatch training loss = 0.774 and accuracy of 0.83\n",
      "Iteration 2809: with minibatch training loss = 0.966 and accuracy of 0.7\n",
      "Iteration 2810: with minibatch training loss = 0.768 and accuracy of 0.83\n",
      "Iteration 2811: with minibatch training loss = 1.03 and accuracy of 0.69\n",
      "Iteration 2812: with minibatch training loss = 0.823 and accuracy of 0.78\n",
      "Iteration 2813: with minibatch training loss = 0.841 and accuracy of 0.78\n",
      "Iteration 2814: with minibatch training loss = 1.5 and accuracy of 0.59\n",
      "Iteration 2815: with minibatch training loss = 1.02 and accuracy of 0.75\n",
      "Iteration 2816: with minibatch training loss = 0.945 and accuracy of 0.75\n",
      "Iteration 2817: with minibatch training loss = 0.611 and accuracy of 0.84\n",
      "Iteration 2818: with minibatch training loss = 0.847 and accuracy of 0.8\n",
      "Iteration 2819: with minibatch training loss = 1.12 and accuracy of 0.72\n",
      "Iteration 2820: with minibatch training loss = 1.1 and accuracy of 0.69\n",
      "Iteration 2821: with minibatch training loss = 1.04 and accuracy of 0.72\n",
      "Iteration 2822: with minibatch training loss = 0.774 and accuracy of 0.78\n",
      "Iteration 2823: with minibatch training loss = 1.03 and accuracy of 0.73\n",
      "Iteration 2824: with minibatch training loss = 1.24 and accuracy of 0.67\n",
      "Iteration 2825: with minibatch training loss = 0.756 and accuracy of 0.78\n",
      "Iteration 2826: with minibatch training loss = 0.847 and accuracy of 0.78\n",
      "Iteration 2827: with minibatch training loss = 0.853 and accuracy of 0.77\n",
      "Iteration 2828: with minibatch training loss = 0.716 and accuracy of 0.83\n",
      "Iteration 2829: with minibatch training loss = 1.17 and accuracy of 0.7\n",
      "Iteration 2830: with minibatch training loss = 0.968 and accuracy of 0.7\n",
      "Iteration 2831: with minibatch training loss = 0.832 and accuracy of 0.78\n",
      "Iteration 2832: with minibatch training loss = 0.826 and accuracy of 0.78\n",
      "Iteration 2833: with minibatch training loss = 1.16 and accuracy of 0.67\n",
      "Iteration 2834: with minibatch training loss = 1.19 and accuracy of 0.64\n",
      "Iteration 2835: with minibatch training loss = 1.53 and accuracy of 0.59\n",
      "Iteration 2836: with minibatch training loss = 1.02 and accuracy of 0.77\n",
      "Iteration 2837: with minibatch training loss = 1.16 and accuracy of 0.67\n",
      "Iteration 2838: with minibatch training loss = 1.14 and accuracy of 0.7\n",
      "Iteration 2839: with minibatch training loss = 0.78 and accuracy of 0.78\n",
      "Iteration 2840: with minibatch training loss = 0.719 and accuracy of 0.84\n",
      "Iteration 2841: with minibatch training loss = 0.983 and accuracy of 0.75\n",
      "Iteration 2842: with minibatch training loss = 1 and accuracy of 0.77\n",
      "Iteration 2843: with minibatch training loss = 1.12 and accuracy of 0.72\n",
      "Iteration 2844: with minibatch training loss = 1.21 and accuracy of 0.7\n",
      "Iteration 2845: with minibatch training loss = 1.23 and accuracy of 0.64\n",
      "Iteration 2846: with minibatch training loss = 1.31 and accuracy of 0.64\n",
      "Iteration 2847: with minibatch training loss = 0.903 and accuracy of 0.78\n",
      "Iteration 2848: with minibatch training loss = 1.17 and accuracy of 0.7\n",
      "Iteration 2849: with minibatch training loss = 0.948 and accuracy of 0.78\n",
      "Iteration 2850: with minibatch training loss = 0.814 and accuracy of 0.8\n",
      "Iteration 2851: with minibatch training loss = 0.84 and accuracy of 0.77\n",
      "Iteration 2852: with minibatch training loss = 1.1 and accuracy of 0.72\n",
      "Iteration 2853: with minibatch training loss = 1.29 and accuracy of 0.69\n",
      "Iteration 2854: with minibatch training loss = 1.01 and accuracy of 0.73\n",
      "Iteration 2855: with minibatch training loss = 1.04 and accuracy of 0.69\n",
      "Iteration 2856: with minibatch training loss = 1.1 and accuracy of 0.67\n",
      "Iteration 2857: with minibatch training loss = 0.892 and accuracy of 0.73\n",
      "Iteration 2858: with minibatch training loss = 0.736 and accuracy of 0.81\n",
      "Iteration 2859: with minibatch training loss = 0.933 and accuracy of 0.78\n",
      "Iteration 2860: with minibatch training loss = 0.82 and accuracy of 0.77\n",
      "Iteration 2861: with minibatch training loss = 0.676 and accuracy of 0.81\n",
      "Iteration 2862: with minibatch training loss = 0.851 and accuracy of 0.78\n",
      "Iteration 2863: with minibatch training loss = 0.82 and accuracy of 0.81\n",
      "Iteration 2864: with minibatch training loss = 0.959 and accuracy of 0.75\n",
      "Iteration 2865: with minibatch training loss = 0.92 and accuracy of 0.73\n",
      "Iteration 2866: with minibatch training loss = 1.38 and accuracy of 0.61\n",
      "Iteration 2867: with minibatch training loss = 1.28 and accuracy of 0.62\n",
      "Iteration 2868: with minibatch training loss = 0.691 and accuracy of 0.78\n",
      "Iteration 2869: with minibatch training loss = 1 and accuracy of 0.7\n",
      "Iteration 2870: with minibatch training loss = 0.944 and accuracy of 0.77\n",
      "Iteration 2871: with minibatch training loss = 1.04 and accuracy of 0.7\n",
      "Iteration 2872: with minibatch training loss = 1.02 and accuracy of 0.77\n",
      "Iteration 2873: with minibatch training loss = 1.02 and accuracy of 0.72\n",
      "Iteration 2874: with minibatch training loss = 1.07 and accuracy of 0.7\n",
      "Iteration 2875: with minibatch training loss = 0.972 and accuracy of 0.72\n",
      "Iteration 2876: with minibatch training loss = 1.28 and accuracy of 0.64\n",
      "Iteration 2877: with minibatch training loss = 0.961 and accuracy of 0.73\n",
      "Iteration 2878: with minibatch training loss = 0.979 and accuracy of 0.75\n",
      "Iteration 2879: with minibatch training loss = 1.15 and accuracy of 0.7\n",
      "Iteration 2880: with minibatch training loss = 0.825 and accuracy of 0.8\n",
      "Iteration 2881: with minibatch training loss = 0.965 and accuracy of 0.73\n",
      "Iteration 2882: with minibatch training loss = 0.854 and accuracy of 0.78\n",
      "Iteration 2883: with minibatch training loss = 0.996 and accuracy of 0.73\n",
      "Iteration 2884: with minibatch training loss = 0.944 and accuracy of 0.75\n",
      "Iteration 2885: with minibatch training loss = 0.791 and accuracy of 0.8\n",
      "Iteration 2886: with minibatch training loss = 1.06 and accuracy of 0.69\n",
      "Iteration 2887: with minibatch training loss = 1.11 and accuracy of 0.72\n",
      "Iteration 2888: with minibatch training loss = 0.953 and accuracy of 0.75\n",
      "Iteration 2889: with minibatch training loss = 1.06 and accuracy of 0.72\n",
      "Iteration 2890: with minibatch training loss = 0.994 and accuracy of 0.75\n",
      "Iteration 2891: with minibatch training loss = 0.88 and accuracy of 0.77\n",
      "Iteration 2892: with minibatch training loss = 0.944 and accuracy of 0.75\n",
      "Iteration 2893: with minibatch training loss = 0.844 and accuracy of 0.78\n",
      "Iteration 2894: with minibatch training loss = 1.27 and accuracy of 0.66\n",
      "Iteration 2895: with minibatch training loss = 1.1 and accuracy of 0.73\n",
      "Iteration 2896: with minibatch training loss = 0.915 and accuracy of 0.75\n",
      "Iteration 2897: with minibatch training loss = 1.25 and accuracy of 0.64\n",
      "Iteration 2898: with minibatch training loss = 1.06 and accuracy of 0.72\n",
      "Iteration 2899: with minibatch training loss = 0.898 and accuracy of 0.78\n",
      "Iteration 2900: with minibatch training loss = 1.34 and accuracy of 0.59\n",
      "Iteration 2901: with minibatch training loss = 0.907 and accuracy of 0.78\n",
      "Iteration 2902: with minibatch training loss = 1.11 and accuracy of 0.67\n",
      "Iteration 2903: with minibatch training loss = 1.2 and accuracy of 0.69\n",
      "Iteration 2904: with minibatch training loss = 0.948 and accuracy of 0.72\n",
      "Iteration 2905: with minibatch training loss = 0.878 and accuracy of 0.8\n",
      "Iteration 2906: with minibatch training loss = 0.983 and accuracy of 0.72\n",
      "Iteration 2907: with minibatch training loss = 1.16 and accuracy of 0.67\n",
      "Iteration 2908: with minibatch training loss = 0.852 and accuracy of 0.75\n",
      "Iteration 2909: with minibatch training loss = 1.2 and accuracy of 0.7\n",
      "Iteration 2910: with minibatch training loss = 0.984 and accuracy of 0.75\n",
      "Iteration 2911: with minibatch training loss = 1.24 and accuracy of 0.66\n",
      "Iteration 2912: with minibatch training loss = 1.27 and accuracy of 0.64\n",
      "Iteration 2913: with minibatch training loss = 1.16 and accuracy of 0.69\n",
      "Iteration 2914: with minibatch training loss = 0.922 and accuracy of 0.73\n",
      "Iteration 2915: with minibatch training loss = 1.18 and accuracy of 0.64\n",
      "Iteration 2916: with minibatch training loss = 1.13 and accuracy of 0.7\n",
      "Iteration 2917: with minibatch training loss = 0.949 and accuracy of 0.77\n",
      "Iteration 2918: with minibatch training loss = 0.772 and accuracy of 0.83\n",
      "Iteration 2919: with minibatch training loss = 1 and accuracy of 0.73\n",
      "Iteration 2920: with minibatch training loss = 1.34 and accuracy of 0.64\n",
      "Iteration 2921: with minibatch training loss = 0.941 and accuracy of 0.75\n",
      "Iteration 2922: with minibatch training loss = 0.968 and accuracy of 0.77\n",
      "Iteration 2923: with minibatch training loss = 0.99 and accuracy of 0.77\n",
      "Iteration 2924: with minibatch training loss = 0.97 and accuracy of 0.77\n",
      "Iteration 2925: with minibatch training loss = 0.811 and accuracy of 0.78\n",
      "Iteration 2926: with minibatch training loss = 1.15 and accuracy of 0.69\n",
      "Iteration 2927: with minibatch training loss = 0.761 and accuracy of 0.83\n",
      "Iteration 2928: with minibatch training loss = 1.02 and accuracy of 0.7\n",
      "Iteration 2929: with minibatch training loss = 0.984 and accuracy of 0.75\n",
      "Iteration 2930: with minibatch training loss = 0.745 and accuracy of 0.83\n",
      "Iteration 2931: with minibatch training loss = 1.22 and accuracy of 0.67\n",
      "Iteration 2932: with minibatch training loss = 0.788 and accuracy of 0.81\n",
      "Iteration 2933: with minibatch training loss = 1.11 and accuracy of 0.7\n",
      "Iteration 2934: with minibatch training loss = 1.21 and accuracy of 0.7\n",
      "Iteration 2935: with minibatch training loss = 0.911 and accuracy of 0.75\n",
      "Iteration 2936: with minibatch training loss = 0.998 and accuracy of 0.72\n",
      "Iteration 2937: with minibatch training loss = 1.1 and accuracy of 0.7\n",
      "Iteration 2938: with minibatch training loss = 0.987 and accuracy of 0.72\n",
      "Iteration 2939: with minibatch training loss = 0.83 and accuracy of 0.78\n",
      "Iteration 2940: with minibatch training loss = 0.702 and accuracy of 0.83\n",
      "Iteration 2941: with minibatch training loss = 1.35 and accuracy of 0.62\n",
      "Iteration 2942: with minibatch training loss = 1.15 and accuracy of 0.72\n",
      "Iteration 2943: with minibatch training loss = 1.15 and accuracy of 0.69\n",
      "Iteration 2944: with minibatch training loss = 0.891 and accuracy of 0.77\n",
      "Iteration 2945: with minibatch training loss = 1.17 and accuracy of 0.69\n",
      "Iteration 2946: with minibatch training loss = 1.09 and accuracy of 0.73\n",
      "Iteration 2947: with minibatch training loss = 0.938 and accuracy of 0.77\n",
      "Iteration 2948: with minibatch training loss = 0.928 and accuracy of 0.75\n",
      "Iteration 2949: with minibatch training loss = 0.898 and accuracy of 0.78\n",
      "Iteration 2950: with minibatch training loss = 1.23 and accuracy of 0.62\n",
      "Iteration 2951: with minibatch training loss = 0.955 and accuracy of 0.77\n",
      "Iteration 2952: with minibatch training loss = 1.4 and accuracy of 0.56\n",
      "Iteration 2953: with minibatch training loss = 0.676 and accuracy of 0.83\n",
      "Iteration 2954: with minibatch training loss = 1.04 and accuracy of 0.75\n",
      "Iteration 2955: with minibatch training loss = 1.3 and accuracy of 0.64\n",
      "Iteration 2956: with minibatch training loss = 0.993 and accuracy of 0.77\n",
      "Iteration 2957: with minibatch training loss = 0.926 and accuracy of 0.77\n",
      "Iteration 2958: with minibatch training loss = 0.715 and accuracy of 0.78\n",
      "Iteration 2959: with minibatch training loss = 1.06 and accuracy of 0.7\n",
      "Iteration 2960: with minibatch training loss = 1.04 and accuracy of 0.72\n",
      "Iteration 2961: with minibatch training loss = 0.862 and accuracy of 0.8\n",
      "Iteration 2962: with minibatch training loss = 1.12 and accuracy of 0.75\n",
      "Iteration 2963: with minibatch training loss = 0.915 and accuracy of 0.75\n",
      "Iteration 2964: with minibatch training loss = 0.923 and accuracy of 0.73\n",
      "Iteration 2965: with minibatch training loss = 0.946 and accuracy of 0.8\n",
      "Iteration 2966: with minibatch training loss = 0.807 and accuracy of 0.81\n",
      "Iteration 2967: with minibatch training loss = 0.802 and accuracy of 0.81\n",
      "Iteration 2968: with minibatch training loss = 1.05 and accuracy of 0.73\n",
      "Iteration 2969: with minibatch training loss = 1.08 and accuracy of 0.69\n",
      "Iteration 2970: with minibatch training loss = 0.875 and accuracy of 0.78\n",
      "Iteration 2971: with minibatch training loss = 1.19 and accuracy of 0.67\n",
      "Iteration 2972: with minibatch training loss = 0.993 and accuracy of 0.73\n",
      "Iteration 2973: with minibatch training loss = 0.939 and accuracy of 0.75\n",
      "Iteration 2974: with minibatch training loss = 1.06 and accuracy of 0.7\n",
      "Iteration 2975: with minibatch training loss = 0.89 and accuracy of 0.73\n",
      "Iteration 2976: with minibatch training loss = 0.894 and accuracy of 0.72\n",
      "Iteration 2977: with minibatch training loss = 0.886 and accuracy of 0.72\n",
      "Iteration 2978: with minibatch training loss = 0.955 and accuracy of 0.73\n",
      "Iteration 2979: with minibatch training loss = 1.24 and accuracy of 0.64\n",
      "Iteration 2980: with minibatch training loss = 0.931 and accuracy of 0.73\n",
      "Iteration 2981: with minibatch training loss = 1.3 and accuracy of 0.62\n",
      "Iteration 2982: with minibatch training loss = 0.845 and accuracy of 0.73\n",
      "Iteration 2983: with minibatch training loss = 0.98 and accuracy of 0.75\n",
      "Iteration 2984: with minibatch training loss = 0.949 and accuracy of 0.77\n",
      "Iteration 2985: with minibatch training loss = 1.14 and accuracy of 0.69\n",
      "Iteration 2986: with minibatch training loss = 0.846 and accuracy of 0.77\n",
      "Iteration 2987: with minibatch training loss = 0.904 and accuracy of 0.77\n",
      "Iteration 2988: with minibatch training loss = 1.01 and accuracy of 0.75\n",
      "Iteration 2989: with minibatch training loss = 0.926 and accuracy of 0.77\n",
      "Iteration 2990: with minibatch training loss = 0.843 and accuracy of 0.75\n",
      "Iteration 2991: with minibatch training loss = 0.941 and accuracy of 0.78\n",
      "Iteration 2992: with minibatch training loss = 0.879 and accuracy of 0.78\n",
      "Iteration 2993: with minibatch training loss = 0.747 and accuracy of 0.8\n",
      "Iteration 2994: with minibatch training loss = 0.92 and accuracy of 0.75\n",
      "Iteration 2995: with minibatch training loss = 0.944 and accuracy of 0.73\n",
      "Iteration 2996: with minibatch training loss = 1.23 and accuracy of 0.73\n",
      "Iteration 2997: with minibatch training loss = 1.09 and accuracy of 0.7\n",
      "Iteration 2998: with minibatch training loss = 0.801 and accuracy of 0.78\n",
      "Iteration 2999: with minibatch training loss = 1.18 and accuracy of 0.69\n",
      "Iteration 3000: with minibatch training loss = 0.859 and accuracy of 0.75\n",
      "Iteration 3001: with minibatch training loss = 1.11 and accuracy of 0.67\n",
      "Iteration 3002: with minibatch training loss = 0.985 and accuracy of 0.75\n",
      "Iteration 3003: with minibatch training loss = 1.1 and accuracy of 0.7\n",
      "Iteration 3004: with minibatch training loss = 1.2 and accuracy of 0.61\n",
      "Iteration 3005: with minibatch training loss = 1.01 and accuracy of 0.72\n",
      "Iteration 3006: with minibatch training loss = 0.874 and accuracy of 0.77\n",
      "Iteration 3007: with minibatch training loss = 1.06 and accuracy of 0.72\n",
      "Iteration 3008: with minibatch training loss = 1.08 and accuracy of 0.72\n",
      "Iteration 3009: with minibatch training loss = 0.791 and accuracy of 0.78\n",
      "Iteration 3010: with minibatch training loss = 0.945 and accuracy of 0.75\n",
      "Iteration 3011: with minibatch training loss = 0.805 and accuracy of 0.81\n",
      "Iteration 3012: with minibatch training loss = 1.11 and accuracy of 0.7\n",
      "Iteration 3013: with minibatch training loss = 1.15 and accuracy of 0.7\n",
      "Iteration 3014: with minibatch training loss = 0.972 and accuracy of 0.72\n",
      "Iteration 3015: with minibatch training loss = 1.03 and accuracy of 0.73\n",
      "Iteration 3016: with minibatch training loss = 0.933 and accuracy of 0.73\n",
      "Iteration 3017: with minibatch training loss = 0.961 and accuracy of 0.73\n",
      "Iteration 3018: with minibatch training loss = 0.924 and accuracy of 0.75\n",
      "Iteration 3019: with minibatch training loss = 0.877 and accuracy of 0.77\n",
      "Iteration 3020: with minibatch training loss = 0.971 and accuracy of 0.75\n",
      "Iteration 3021: with minibatch training loss = 0.837 and accuracy of 0.8\n",
      "Iteration 3022: with minibatch training loss = 1.35 and accuracy of 0.64\n",
      "Iteration 3023: with minibatch training loss = 0.803 and accuracy of 0.81\n",
      "Iteration 3024: with minibatch training loss = 1.14 and accuracy of 0.67\n",
      "Iteration 3025: with minibatch training loss = 1.09 and accuracy of 0.69\n",
      "Iteration 3026: with minibatch training loss = 0.788 and accuracy of 0.8\n",
      "Iteration 3027: with minibatch training loss = 0.901 and accuracy of 0.77\n",
      "Iteration 3028: with minibatch training loss = 0.83 and accuracy of 0.78\n",
      "Iteration 3029: with minibatch training loss = 0.998 and accuracy of 0.75\n",
      "Iteration 3030: with minibatch training loss = 0.992 and accuracy of 0.72\n",
      "Iteration 3031: with minibatch training loss = 0.971 and accuracy of 0.75\n",
      "Iteration 3032: with minibatch training loss = 0.929 and accuracy of 0.72\n",
      "Iteration 3033: with minibatch training loss = 0.834 and accuracy of 0.77\n",
      "Iteration 3034: with minibatch training loss = 1.1 and accuracy of 0.69\n",
      "Iteration 3035: with minibatch training loss = 0.798 and accuracy of 0.77\n",
      "Iteration 3036: with minibatch training loss = 0.967 and accuracy of 0.77\n",
      "Iteration 3037: with minibatch training loss = 1.23 and accuracy of 0.64\n",
      "Iteration 3038: with minibatch training loss = 0.85 and accuracy of 0.75\n",
      "Iteration 3039: with minibatch training loss = 0.812 and accuracy of 0.77\n",
      "Iteration 3040: with minibatch training loss = 0.961 and accuracy of 0.73\n",
      "Iteration 3041: with minibatch training loss = 0.789 and accuracy of 0.77\n",
      "Iteration 3042: with minibatch training loss = 1 and accuracy of 0.77\n",
      "Iteration 3043: with minibatch training loss = 1.23 and accuracy of 0.66\n",
      "Iteration 3044: with minibatch training loss = 1.23 and accuracy of 0.67\n",
      "Iteration 3045: with minibatch training loss = 0.903 and accuracy of 0.73\n",
      "Iteration 3046: with minibatch training loss = 1.13 and accuracy of 0.73\n",
      "Iteration 3047: with minibatch training loss = 0.946 and accuracy of 0.72\n",
      "Iteration 3048: with minibatch training loss = 0.811 and accuracy of 0.78\n",
      "Iteration 3049: with minibatch training loss = 1.29 and accuracy of 0.62\n",
      "Iteration 3050: with minibatch training loss = 0.954 and accuracy of 0.75\n",
      "Iteration 3051: with minibatch training loss = 0.873 and accuracy of 0.78\n",
      "Iteration 3052: with minibatch training loss = 0.901 and accuracy of 0.77\n",
      "Iteration 3053: with minibatch training loss = 0.891 and accuracy of 0.75\n",
      "Iteration 3054: with minibatch training loss = 0.863 and accuracy of 0.77\n",
      "Iteration 3055: with minibatch training loss = 1.15 and accuracy of 0.69\n",
      "Iteration 3056: with minibatch training loss = 0.979 and accuracy of 0.72\n",
      "Iteration 3057: with minibatch training loss = 0.931 and accuracy of 0.73\n",
      "Iteration 3058: with minibatch training loss = 1.12 and accuracy of 0.7\n",
      "Iteration 3059: with minibatch training loss = 1.04 and accuracy of 0.7\n",
      "Iteration 3060: with minibatch training loss = 0.671 and accuracy of 0.83\n",
      "Iteration 3061: with minibatch training loss = 1.11 and accuracy of 0.69\n",
      "Iteration 3062: with minibatch training loss = 0.742 and accuracy of 0.81\n",
      "Iteration 3063: with minibatch training loss = 0.987 and accuracy of 0.73\n",
      "Iteration 3064: with minibatch training loss = 0.754 and accuracy of 0.83\n",
      "Iteration 3065: with minibatch training loss = 0.969 and accuracy of 0.77\n",
      "Iteration 3066: with minibatch training loss = 0.815 and accuracy of 0.77\n",
      "Iteration 3067: with minibatch training loss = 1.23 and accuracy of 0.64\n",
      "Iteration 3068: with minibatch training loss = 1.09 and accuracy of 0.69\n",
      "Iteration 3069: with minibatch training loss = 1.01 and accuracy of 0.77\n",
      "Iteration 3070: with minibatch training loss = 1.17 and accuracy of 0.66\n",
      "Iteration 3071: with minibatch training loss = 1.02 and accuracy of 0.75\n",
      "Iteration 3072: with minibatch training loss = 0.901 and accuracy of 0.8\n",
      "Iteration 3073: with minibatch training loss = 1.25 and accuracy of 0.67\n",
      "Iteration 3074: with minibatch training loss = 1.07 and accuracy of 0.72\n",
      "Iteration 3075: with minibatch training loss = 1.22 and accuracy of 0.69\n",
      "Iteration 3076: with minibatch training loss = 0.802 and accuracy of 0.8\n",
      "Iteration 3077: with minibatch training loss = 0.986 and accuracy of 0.75\n",
      "Iteration 3078: with minibatch training loss = 1.07 and accuracy of 0.69\n",
      "Iteration 3079: with minibatch training loss = 0.812 and accuracy of 0.8\n",
      "Iteration 3080: with minibatch training loss = 0.915 and accuracy of 0.78\n",
      "Iteration 3081: with minibatch training loss = 0.995 and accuracy of 0.72\n",
      "Iteration 3082: with minibatch training loss = 0.976 and accuracy of 0.72\n",
      "Iteration 3083: with minibatch training loss = 0.993 and accuracy of 0.69\n",
      "Iteration 3084: with minibatch training loss = 1.16 and accuracy of 0.67\n",
      "Iteration 3085: with minibatch training loss = 0.767 and accuracy of 0.77\n",
      "Iteration 3086: with minibatch training loss = 1.03 and accuracy of 0.72\n",
      "Iteration 3087: with minibatch training loss = 0.861 and accuracy of 0.77\n",
      "Iteration 3088: with minibatch training loss = 0.732 and accuracy of 0.84\n",
      "Iteration 3089: with minibatch training loss = 1.03 and accuracy of 0.72\n",
      "Iteration 3090: with minibatch training loss = 1.37 and accuracy of 0.62\n",
      "Iteration 3091: with minibatch training loss = 1.25 and accuracy of 0.67\n",
      "Iteration 3092: with minibatch training loss = 0.823 and accuracy of 0.75\n",
      "Iteration 3093: with minibatch training loss = 0.899 and accuracy of 0.78\n",
      "Iteration 3094: with minibatch training loss = 0.898 and accuracy of 0.8\n",
      "Iteration 3095: with minibatch training loss = 0.862 and accuracy of 0.77\n",
      "Iteration 3096: with minibatch training loss = 1.16 and accuracy of 0.7\n",
      "Iteration 3097: with minibatch training loss = 0.984 and accuracy of 0.75\n",
      "Iteration 3098: with minibatch training loss = 0.83 and accuracy of 0.8\n",
      "Iteration 3099: with minibatch training loss = 0.805 and accuracy of 0.78\n",
      "Iteration 3100: with minibatch training loss = 0.961 and accuracy of 0.7\n",
      "Iteration 3101: with minibatch training loss = 1.11 and accuracy of 0.72\n",
      "Iteration 3102: with minibatch training loss = 1.07 and accuracy of 0.7\n",
      "Iteration 3103: with minibatch training loss = 1.1 and accuracy of 0.72\n",
      "Iteration 3104: with minibatch training loss = 0.959 and accuracy of 0.73\n",
      "Iteration 3105: with minibatch training loss = 0.79 and accuracy of 0.78\n",
      "Iteration 3106: with minibatch training loss = 1.13 and accuracy of 0.69\n",
      "Iteration 3107: with minibatch training loss = 0.865 and accuracy of 0.77\n",
      "Iteration 3108: with minibatch training loss = 1.01 and accuracy of 0.69\n",
      "Iteration 3109: with minibatch training loss = 0.918 and accuracy of 0.72\n",
      "Iteration 3110: with minibatch training loss = 1.23 and accuracy of 0.69\n",
      "Iteration 3111: with minibatch training loss = 1.13 and accuracy of 0.73\n",
      "Iteration 3112: with minibatch training loss = 0.89 and accuracy of 0.8\n",
      "Iteration 3113: with minibatch training loss = 0.722 and accuracy of 0.81\n",
      "Iteration 3114: with minibatch training loss = 1.02 and accuracy of 0.72\n",
      "Iteration 3115: with minibatch training loss = 0.795 and accuracy of 0.83\n",
      "Iteration 3116: with minibatch training loss = 1.35 and accuracy of 0.64\n",
      "Iteration 3117: with minibatch training loss = 1.03 and accuracy of 0.69\n",
      "Iteration 3118: with minibatch training loss = 1.26 and accuracy of 0.67\n",
      "Iteration 3119: with minibatch training loss = 0.849 and accuracy of 0.8\n",
      "Iteration 3120: with minibatch training loss = 0.916 and accuracy of 0.77\n",
      "Iteration 3121: with minibatch training loss = 1 and accuracy of 0.72\n",
      "Iteration 3122: with minibatch training loss = 0.888 and accuracy of 0.77\n",
      "Iteration 3123: with minibatch training loss = 0.614 and accuracy of 0.86\n",
      "Iteration 3124: with minibatch training loss = 0.99 and accuracy of 0.75\n",
      "Iteration 3125: with minibatch training loss = 1.05 and accuracy of 0.73\n",
      "Iteration 3126: with minibatch training loss = 1.03 and accuracy of 0.73\n",
      "Iteration 3127: with minibatch training loss = 0.863 and accuracy of 0.77\n",
      "Iteration 3128: with minibatch training loss = 0.99 and accuracy of 0.75\n",
      "Iteration 3129: with minibatch training loss = 1 and accuracy of 0.72\n",
      "Iteration 3130: with minibatch training loss = 1.1 and accuracy of 0.7\n",
      "Iteration 3131: with minibatch training loss = 0.937 and accuracy of 0.75\n",
      "Iteration 3132: with minibatch training loss = 1.07 and accuracy of 0.73\n",
      "Iteration 3133: with minibatch training loss = 1.16 and accuracy of 0.69\n",
      "Iteration 3134: with minibatch training loss = 0.693 and accuracy of 0.86\n",
      "Iteration 3135: with minibatch training loss = 1.02 and accuracy of 0.72\n",
      "Iteration 3136: with minibatch training loss = 0.968 and accuracy of 0.78\n",
      "Iteration 3137: with minibatch training loss = 1.34 and accuracy of 0.61\n",
      "Iteration 3138: with minibatch training loss = 0.934 and accuracy of 0.72\n",
      "Iteration 3139: with minibatch training loss = 0.882 and accuracy of 0.8\n",
      "Iteration 3140: with minibatch training loss = 0.94 and accuracy of 0.75\n",
      "Iteration 3141: with minibatch training loss = 0.965 and accuracy of 0.75\n",
      "Iteration 3142: with minibatch training loss = 0.897 and accuracy of 0.77\n",
      "Iteration 3143: with minibatch training loss = 1.09 and accuracy of 0.73\n",
      "Iteration 3144: with minibatch training loss = 1.18 and accuracy of 0.69\n",
      "Iteration 3145: with minibatch training loss = 0.833 and accuracy of 0.78\n",
      "Iteration 3146: with minibatch training loss = 0.932 and accuracy of 0.75\n",
      "Iteration 3147: with minibatch training loss = 0.986 and accuracy of 0.75\n",
      "Iteration 3148: with minibatch training loss = 0.98 and accuracy of 0.73\n",
      "Iteration 3149: with minibatch training loss = 0.767 and accuracy of 0.8\n",
      "Iteration 3150: with minibatch training loss = 1.01 and accuracy of 0.72\n",
      "Iteration 3151: with minibatch training loss = 0.862 and accuracy of 0.77\n",
      "Iteration 3152: with minibatch training loss = 0.807 and accuracy of 0.78\n",
      "Iteration 3153: with minibatch training loss = 1.07 and accuracy of 0.7\n",
      "Iteration 3154: with minibatch training loss = 0.87 and accuracy of 0.75\n",
      "Iteration 3155: with minibatch training loss = 0.741 and accuracy of 0.81\n",
      "Iteration 3156: with minibatch training loss = 0.828 and accuracy of 0.8\n",
      "Iteration 3157: with minibatch training loss = 0.893 and accuracy of 0.77\n",
      "Iteration 3158: with minibatch training loss = 1.18 and accuracy of 0.7\n",
      "Iteration 3159: with minibatch training loss = 0.961 and accuracy of 0.77\n",
      "Iteration 3160: with minibatch training loss = 0.871 and accuracy of 0.77\n",
      "Iteration 3161: with minibatch training loss = 1.27 and accuracy of 0.67\n",
      "Iteration 3162: with minibatch training loss = 1.14 and accuracy of 0.7\n",
      "Iteration 3163: with minibatch training loss = 1.05 and accuracy of 0.7\n",
      "Iteration 3164: with minibatch training loss = 1.04 and accuracy of 0.69\n",
      "Iteration 3165: with minibatch training loss = 0.936 and accuracy of 0.73\n",
      "Iteration 3166: with minibatch training loss = 0.966 and accuracy of 0.78\n",
      "Iteration 3167: with minibatch training loss = 1.18 and accuracy of 0.67\n",
      "Iteration 3168: with minibatch training loss = 0.822 and accuracy of 0.78\n",
      "Iteration 3169: with minibatch training loss = 1.14 and accuracy of 0.7\n",
      "Iteration 3170: with minibatch training loss = 1.09 and accuracy of 0.69\n",
      "Iteration 3171: with minibatch training loss = 1.04 and accuracy of 0.73\n",
      "Iteration 3172: with minibatch training loss = 0.767 and accuracy of 0.8\n",
      "Iteration 3173: with minibatch training loss = 1.01 and accuracy of 0.73\n",
      "Iteration 3174: with minibatch training loss = 0.865 and accuracy of 0.81\n",
      "Iteration 3175: with minibatch training loss = 1.02 and accuracy of 0.75\n",
      "Iteration 3176: with minibatch training loss = 0.916 and accuracy of 0.8\n",
      "Iteration 3177: with minibatch training loss = 1.05 and accuracy of 0.73\n",
      "Iteration 3178: with minibatch training loss = 0.944 and accuracy of 0.77\n",
      "Iteration 3179: with minibatch training loss = 0.786 and accuracy of 0.8\n",
      "Iteration 3180: with minibatch training loss = 1.17 and accuracy of 0.69\n",
      "Iteration 3181: with minibatch training loss = 0.926 and accuracy of 0.72\n",
      "Iteration 3182: with minibatch training loss = 0.645 and accuracy of 0.83\n",
      "Iteration 3183: with minibatch training loss = 1.02 and accuracy of 0.73\n",
      "Iteration 3184: with minibatch training loss = 0.788 and accuracy of 0.8\n",
      "Iteration 3185: with minibatch training loss = 1.16 and accuracy of 0.69\n",
      "Iteration 3186: with minibatch training loss = 1.07 and accuracy of 0.72\n",
      "Iteration 3187: with minibatch training loss = 0.859 and accuracy of 0.75\n",
      "Iteration 3188: with minibatch training loss = 0.653 and accuracy of 0.86\n",
      "Iteration 3189: with minibatch training loss = 0.932 and accuracy of 0.75\n",
      "Iteration 3190: with minibatch training loss = 0.774 and accuracy of 0.84\n",
      "Iteration 3191: with minibatch training loss = 1.13 and accuracy of 0.72\n",
      "Iteration 3192: with minibatch training loss = 0.915 and accuracy of 0.78\n",
      "Iteration 3193: with minibatch training loss = 0.929 and accuracy of 0.8\n",
      "Iteration 3194: with minibatch training loss = 0.92 and accuracy of 0.73\n",
      "Iteration 3195: with minibatch training loss = 0.906 and accuracy of 0.77\n",
      "Iteration 3196: with minibatch training loss = 0.917 and accuracy of 0.73\n",
      "Iteration 3197: with minibatch training loss = 0.956 and accuracy of 0.73\n",
      "Iteration 3198: with minibatch training loss = 0.958 and accuracy of 0.78\n",
      "Iteration 3199: with minibatch training loss = 1.13 and accuracy of 0.67\n",
      "Iteration 3200: with minibatch training loss = 1.06 and accuracy of 0.72\n",
      "Iteration 3201: with minibatch training loss = 1.24 and accuracy of 0.62\n",
      "Iteration 3202: with minibatch training loss = 1.31 and accuracy of 0.66\n",
      "Iteration 3203: with minibatch training loss = 1.05 and accuracy of 0.69\n",
      "Iteration 3204: with minibatch training loss = 0.754 and accuracy of 0.86\n",
      "Iteration 3205: with minibatch training loss = 1.08 and accuracy of 0.67\n",
      "Iteration 3206: with minibatch training loss = 0.814 and accuracy of 0.78\n",
      "Iteration 3207: with minibatch training loss = 0.734 and accuracy of 0.8\n",
      "Iteration 3208: with minibatch training loss = 1.27 and accuracy of 0.67\n",
      "Iteration 3209: with minibatch training loss = 0.883 and accuracy of 0.75\n",
      "Iteration 3210: with minibatch training loss = 0.716 and accuracy of 0.83\n",
      "Iteration 3211: with minibatch training loss = 1.13 and accuracy of 0.7\n",
      "Iteration 3212: with minibatch training loss = 1.1 and accuracy of 0.66\n",
      "Iteration 3213: with minibatch training loss = 1.12 and accuracy of 0.72\n",
      "Iteration 3214: with minibatch training loss = 1.06 and accuracy of 0.73\n",
      "Iteration 3215: with minibatch training loss = 1.2 and accuracy of 0.62\n",
      "Iteration 3216: with minibatch training loss = 0.693 and accuracy of 0.81\n",
      "Iteration 3217: with minibatch training loss = 0.959 and accuracy of 0.75\n",
      "Iteration 3218: with minibatch training loss = 0.791 and accuracy of 0.78\n",
      "Iteration 3219: with minibatch training loss = 0.737 and accuracy of 0.8\n",
      "Iteration 3220: with minibatch training loss = 0.791 and accuracy of 0.8\n",
      "Iteration 3221: with minibatch training loss = 1.16 and accuracy of 0.7\n",
      "Iteration 3222: with minibatch training loss = 0.85 and accuracy of 0.75\n",
      "Iteration 3223: with minibatch training loss = 0.911 and accuracy of 0.75\n",
      "Iteration 3224: with minibatch training loss = 1.13 and accuracy of 0.67\n",
      "Iteration 3225: with minibatch training loss = 0.902 and accuracy of 0.78\n",
      "Iteration 3226: with minibatch training loss = 1.1 and accuracy of 0.67\n",
      "Iteration 3227: with minibatch training loss = 1.06 and accuracy of 0.67\n",
      "Iteration 3228: with minibatch training loss = 1.08 and accuracy of 0.7\n",
      "Iteration 3229: with minibatch training loss = 0.618 and accuracy of 0.88\n",
      "Iteration 3230: with minibatch training loss = 1.3 and accuracy of 0.64\n",
      "Iteration 3231: with minibatch training loss = 1.11 and accuracy of 0.72\n",
      "Iteration 3232: with minibatch training loss = 0.999 and accuracy of 0.73\n",
      "Iteration 3233: with minibatch training loss = 1.16 and accuracy of 0.67\n",
      "Iteration 3234: with minibatch training loss = 1.03 and accuracy of 0.72\n",
      "Iteration 3235: with minibatch training loss = 0.965 and accuracy of 0.72\n",
      "Iteration 3236: with minibatch training loss = 0.905 and accuracy of 0.75\n",
      "Iteration 3237: with minibatch training loss = 1.06 and accuracy of 0.73\n",
      "Iteration 3238: with minibatch training loss = 0.865 and accuracy of 0.8\n",
      "Iteration 3239: with minibatch training loss = 1.24 and accuracy of 0.67\n",
      "Iteration 3240: with minibatch training loss = 1.15 and accuracy of 0.67\n",
      "Iteration 3241: with minibatch training loss = 0.869 and accuracy of 0.8\n",
      "Iteration 3242: with minibatch training loss = 0.866 and accuracy of 0.77\n",
      "Iteration 3243: with minibatch training loss = 1.05 and accuracy of 0.7\n",
      "Iteration 3244: with minibatch training loss = 1.14 and accuracy of 0.66\n",
      "Iteration 3245: with minibatch training loss = 1.1 and accuracy of 0.7\n",
      "Iteration 3246: with minibatch training loss = 1.13 and accuracy of 0.7\n",
      "Iteration 3247: with minibatch training loss = 1.13 and accuracy of 0.67\n",
      "Iteration 3248: with minibatch training loss = 0.745 and accuracy of 0.8\n",
      "Iteration 3249: with minibatch training loss = 0.798 and accuracy of 0.8\n",
      "Iteration 3250: with minibatch training loss = 0.805 and accuracy of 0.78\n",
      "Iteration 3251: with minibatch training loss = 1.13 and accuracy of 0.67\n",
      "Iteration 3252: with minibatch training loss = 1.04 and accuracy of 0.72\n",
      "Iteration 3253: with minibatch training loss = 1.15 and accuracy of 0.69\n",
      "Iteration 3254: with minibatch training loss = 0.932 and accuracy of 0.73\n",
      "Iteration 3255: with minibatch training loss = 1.26 and accuracy of 0.67\n",
      "Iteration 3256: with minibatch training loss = 0.933 and accuracy of 0.75\n",
      "Iteration 3257: with minibatch training loss = 0.873 and accuracy of 0.77\n",
      "Iteration 3258: with minibatch training loss = 0.962 and accuracy of 0.72\n",
      "Iteration 3259: with minibatch training loss = 1.04 and accuracy of 0.72\n",
      "Iteration 3260: with minibatch training loss = 0.945 and accuracy of 0.78\n",
      "Iteration 3261: with minibatch training loss = 1.04 and accuracy of 0.72\n",
      "Iteration 3262: with minibatch training loss = 1.03 and accuracy of 0.72\n",
      "Iteration 3263: with minibatch training loss = 0.82 and accuracy of 0.8\n",
      "Iteration 3264: with minibatch training loss = 0.797 and accuracy of 0.78\n",
      "Iteration 3265: with minibatch training loss = 0.849 and accuracy of 0.8\n",
      "Iteration 3266: with minibatch training loss = 1.03 and accuracy of 0.72\n",
      "Iteration 3267: with minibatch training loss = 1.07 and accuracy of 0.72\n",
      "Iteration 3268: with minibatch training loss = 0.535 and accuracy of 0.88\n",
      "Iteration 3269: with minibatch training loss = 0.931 and accuracy of 0.75\n",
      "Iteration 3270: with minibatch training loss = 0.602 and accuracy of 0.86\n",
      "Iteration 3271: with minibatch training loss = 0.995 and accuracy of 0.73\n",
      "Iteration 3272: with minibatch training loss = 1.26 and accuracy of 0.67\n",
      "Iteration 3273: with minibatch training loss = 1.05 and accuracy of 0.7\n",
      "Iteration 3274: with minibatch training loss = 0.794 and accuracy of 0.78\n",
      "Iteration 3275: with minibatch training loss = 0.962 and accuracy of 0.73\n",
      "Iteration 3276: with minibatch training loss = 1.14 and accuracy of 0.7\n",
      "Iteration 3277: with minibatch training loss = 0.812 and accuracy of 0.81\n",
      "Iteration 3278: with minibatch training loss = 0.911 and accuracy of 0.77\n",
      "Iteration 3279: with minibatch training loss = 0.678 and accuracy of 0.83\n",
      "Iteration 3280: with minibatch training loss = 1.02 and accuracy of 0.69\n",
      "Iteration 3281: with minibatch training loss = 0.871 and accuracy of 0.78\n",
      "Iteration 3282: with minibatch training loss = 1.33 and accuracy of 0.64\n",
      "Iteration 3283: with minibatch training loss = 1.05 and accuracy of 0.73\n",
      "Iteration 3284: with minibatch training loss = 0.781 and accuracy of 0.8\n",
      "Iteration 3285: with minibatch training loss = 0.896 and accuracy of 0.8\n",
      "Iteration 3286: with minibatch training loss = 0.685 and accuracy of 0.83\n",
      "Iteration 3287: with minibatch training loss = 0.925 and accuracy of 0.73\n",
      "Iteration 3288: with minibatch training loss = 0.644 and accuracy of 0.83\n",
      "Iteration 3289: with minibatch training loss = 1.1 and accuracy of 0.7\n",
      "Iteration 3290: with minibatch training loss = 1.1 and accuracy of 0.67\n",
      "Iteration 3291: with minibatch training loss = 0.855 and accuracy of 0.75\n",
      "Iteration 3292: with minibatch training loss = 0.952 and accuracy of 0.77\n",
      "Iteration 3293: with minibatch training loss = 1.39 and accuracy of 0.59\n",
      "Iteration 3294: with minibatch training loss = 1.12 and accuracy of 0.66\n",
      "Iteration 3295: with minibatch training loss = 1.09 and accuracy of 0.77\n",
      "Iteration 3296: with minibatch training loss = 0.901 and accuracy of 0.73\n",
      "Iteration 3297: with minibatch training loss = 1.2 and accuracy of 0.67\n",
      "Iteration 3298: with minibatch training loss = 0.844 and accuracy of 0.83\n",
      "Iteration 3299: with minibatch training loss = 0.619 and accuracy of 0.84\n",
      "Iteration 3300: with minibatch training loss = 1.07 and accuracy of 0.69\n",
      "Iteration 3301: with minibatch training loss = 0.83 and accuracy of 0.77\n",
      "Iteration 3302: with minibatch training loss = 1.11 and accuracy of 0.7\n",
      "Iteration 3303: with minibatch training loss = 0.942 and accuracy of 0.73\n",
      "Iteration 3304: with minibatch training loss = 0.816 and accuracy of 0.8\n",
      "Iteration 3305: with minibatch training loss = 0.876 and accuracy of 0.73\n",
      "Iteration 3306: with minibatch training loss = 0.919 and accuracy of 0.8\n",
      "Iteration 3307: with minibatch training loss = 1.03 and accuracy of 0.75\n",
      "Iteration 3308: with minibatch training loss = 0.654 and accuracy of 0.84\n",
      "Iteration 3309: with minibatch training loss = 0.746 and accuracy of 0.81\n",
      "Iteration 3310: with minibatch training loss = 0.893 and accuracy of 0.77\n",
      "Iteration 3311: with minibatch training loss = 1.06 and accuracy of 0.73\n",
      "Iteration 3312: with minibatch training loss = 0.954 and accuracy of 0.77\n",
      "Iteration 3313: with minibatch training loss = 0.495 and accuracy of 0.89\n",
      "Iteration 3314: with minibatch training loss = 0.866 and accuracy of 0.8\n",
      "Iteration 3315: with minibatch training loss = 0.973 and accuracy of 0.73\n",
      "Iteration 3316: with minibatch training loss = 1.06 and accuracy of 0.69\n",
      "Iteration 3317: with minibatch training loss = 1.16 and accuracy of 0.69\n",
      "Iteration 3318: with minibatch training loss = 0.869 and accuracy of 0.8\n",
      "Iteration 3319: with minibatch training loss = 0.795 and accuracy of 0.8\n",
      "Iteration 3320: with minibatch training loss = 1.23 and accuracy of 0.66\n",
      "Iteration 3321: with minibatch training loss = 0.622 and accuracy of 0.81\n",
      "Iteration 3322: with minibatch training loss = 1.11 and accuracy of 0.7\n",
      "Iteration 3323: with minibatch training loss = 1.07 and accuracy of 0.75\n",
      "Iteration 3324: with minibatch training loss = 1.23 and accuracy of 0.69\n",
      "Iteration 3325: with minibatch training loss = 0.66 and accuracy of 0.86\n",
      "Iteration 3326: with minibatch training loss = 0.946 and accuracy of 0.77\n",
      "Iteration 3327: with minibatch training loss = 1.49 and accuracy of 0.64\n",
      "Iteration 3328: with minibatch training loss = 1.11 and accuracy of 0.66\n",
      "Iteration 3329: with minibatch training loss = 1.24 and accuracy of 0.69\n",
      "Iteration 3330: with minibatch training loss = 1.02 and accuracy of 0.73\n",
      "Iteration 3331: with minibatch training loss = 0.952 and accuracy of 0.77\n",
      "Iteration 3332: with minibatch training loss = 0.75 and accuracy of 0.81\n",
      "Iteration 3333: with minibatch training loss = 1.26 and accuracy of 0.67\n",
      "Iteration 3334: with minibatch training loss = 0.912 and accuracy of 0.78\n",
      "Iteration 3335: with minibatch training loss = 0.835 and accuracy of 0.83\n",
      "Iteration 3336: with minibatch training loss = 0.903 and accuracy of 0.77\n",
      "Iteration 3337: with minibatch training loss = 0.741 and accuracy of 0.86\n",
      "Iteration 3338: with minibatch training loss = 0.916 and accuracy of 0.78\n",
      "Iteration 3339: with minibatch training loss = 1.38 and accuracy of 0.61\n",
      "Iteration 3340: with minibatch training loss = 1.02 and accuracy of 0.73\n",
      "Iteration 3341: with minibatch training loss = 0.973 and accuracy of 0.73\n",
      "Iteration 3342: with minibatch training loss = 0.89 and accuracy of 0.75\n",
      "Iteration 3343: with minibatch training loss = 1.14 and accuracy of 0.72\n",
      "Iteration 3344: with minibatch training loss = 0.773 and accuracy of 0.78\n",
      "Iteration 3345: with minibatch training loss = 0.924 and accuracy of 0.73\n",
      "Iteration 3346: with minibatch training loss = 1.05 and accuracy of 0.69\n",
      "Iteration 3347: with minibatch training loss = 0.721 and accuracy of 0.81\n",
      "Iteration 3348: with minibatch training loss = 1.1 and accuracy of 0.69\n",
      "Iteration 3349: with minibatch training loss = 1.12 and accuracy of 0.7\n",
      "Iteration 3350: with minibatch training loss = 0.875 and accuracy of 0.77\n",
      "Iteration 3351: with minibatch training loss = 1.25 and accuracy of 0.64\n",
      "Iteration 3352: with minibatch training loss = 1.02 and accuracy of 0.72\n",
      "Iteration 3353: with minibatch training loss = 1.04 and accuracy of 0.75\n",
      "Iteration 3354: with minibatch training loss = 0.747 and accuracy of 0.8\n",
      "Iteration 3355: with minibatch training loss = 1.35 and accuracy of 0.61\n",
      "Iteration 3356: with minibatch training loss = 1.05 and accuracy of 0.69\n",
      "Iteration 3357: with minibatch training loss = 0.908 and accuracy of 0.83\n",
      "Iteration 3358: with minibatch training loss = 1.02 and accuracy of 0.75\n",
      "Iteration 3359: with minibatch training loss = 1.05 and accuracy of 0.7\n",
      "Iteration 3360: with minibatch training loss = 1.07 and accuracy of 0.69\n",
      "Iteration 3361: with minibatch training loss = 1.09 and accuracy of 0.72\n",
      "Iteration 3362: with minibatch training loss = 0.773 and accuracy of 0.78\n",
      "Iteration 3363: with minibatch training loss = 1.4 and accuracy of 0.59\n",
      "Iteration 3364: with minibatch training loss = 1.39 and accuracy of 0.58\n",
      "Iteration 3365: with minibatch training loss = 0.967 and accuracy of 0.72\n",
      "Iteration 3366: with minibatch training loss = 1.14 and accuracy of 0.67\n",
      "Iteration 3367: with minibatch training loss = 1.05 and accuracy of 0.73\n",
      "Iteration 3368: with minibatch training loss = 1.05 and accuracy of 0.73\n",
      "Iteration 3369: with minibatch training loss = 1.37 and accuracy of 0.62\n",
      "Iteration 3370: with minibatch training loss = 1 and accuracy of 0.72\n",
      "Iteration 3371: with minibatch training loss = 0.965 and accuracy of 0.73\n",
      "Iteration 3372: with minibatch training loss = 0.87 and accuracy of 0.83\n",
      "Iteration 3373: with minibatch training loss = 0.849 and accuracy of 0.73\n",
      "Iteration 3374: with minibatch training loss = 1.28 and accuracy of 0.62\n",
      "Iteration 3375: with minibatch training loss = 1.17 and accuracy of 0.66\n",
      "Iteration 3376: with minibatch training loss = 0.658 and accuracy of 0.81\n",
      "Iteration 3377: with minibatch training loss = 0.966 and accuracy of 0.72\n",
      "Iteration 3378: with minibatch training loss = 0.95 and accuracy of 0.73\n",
      "Iteration 3379: with minibatch training loss = 1.04 and accuracy of 0.72\n",
      "Iteration 3380: with minibatch training loss = 0.682 and accuracy of 0.84\n",
      "Iteration 3381: with minibatch training loss = 1.14 and accuracy of 0.72\n",
      "Iteration 3382: with minibatch training loss = 1.15 and accuracy of 0.67\n",
      "Iteration 3383: with minibatch training loss = 0.862 and accuracy of 0.78\n",
      "Iteration 3384: with minibatch training loss = 0.829 and accuracy of 0.75\n",
      "Iteration 3385: with minibatch training loss = 0.854 and accuracy of 0.73\n",
      "Iteration 3386: with minibatch training loss = 1.17 and accuracy of 0.69\n",
      "Iteration 3387: with minibatch training loss = 1.04 and accuracy of 0.73\n",
      "Iteration 3388: with minibatch training loss = 0.921 and accuracy of 0.78\n",
      "Iteration 3389: with minibatch training loss = 0.798 and accuracy of 0.78\n",
      "Iteration 3390: with minibatch training loss = 1.22 and accuracy of 0.64\n",
      "Iteration 3391: with minibatch training loss = 0.988 and accuracy of 0.72\n",
      "Iteration 3392: with minibatch training loss = 0.878 and accuracy of 0.75\n",
      "Iteration 3393: with minibatch training loss = 0.989 and accuracy of 0.73\n",
      "Iteration 3394: with minibatch training loss = 0.824 and accuracy of 0.75\n",
      "Iteration 3395: with minibatch training loss = 1.43 and accuracy of 0.62\n",
      "Iteration 3396: with minibatch training loss = 0.971 and accuracy of 0.75\n",
      "Iteration 3397: with minibatch training loss = 1.1 and accuracy of 0.72\n",
      "Iteration 3398: with minibatch training loss = 1.07 and accuracy of 0.67\n",
      "Iteration 3399: with minibatch training loss = 1.01 and accuracy of 0.7\n",
      "Iteration 3400: with minibatch training loss = 0.83 and accuracy of 0.78\n",
      "Iteration 3401: with minibatch training loss = 1.13 and accuracy of 0.72\n",
      "Iteration 3402: with minibatch training loss = 0.546 and accuracy of 0.88\n",
      "Iteration 3403: with minibatch training loss = 1.16 and accuracy of 0.69\n",
      "Iteration 3404: with minibatch training loss = 0.97 and accuracy of 0.75\n",
      "Iteration 3405: with minibatch training loss = 1.01 and accuracy of 0.72\n",
      "Iteration 3406: with minibatch training loss = 0.975 and accuracy of 0.77\n",
      "Iteration 3407: with minibatch training loss = 0.797 and accuracy of 0.78\n",
      "Iteration 3408: with minibatch training loss = 0.756 and accuracy of 0.78\n",
      "Iteration 3409: with minibatch training loss = 0.749 and accuracy of 0.81\n",
      "Iteration 3410: with minibatch training loss = 0.87 and accuracy of 0.83\n",
      "Iteration 3411: with minibatch training loss = 0.798 and accuracy of 0.81\n",
      "Iteration 3412: with minibatch training loss = 0.663 and accuracy of 0.84\n",
      "Iteration 3413: with minibatch training loss = 0.772 and accuracy of 0.81\n",
      "Iteration 3414: with minibatch training loss = 1.11 and accuracy of 0.7\n",
      "Iteration 3415: with minibatch training loss = 0.901 and accuracy of 0.77\n",
      "Iteration 3416: with minibatch training loss = 0.986 and accuracy of 0.77\n",
      "Iteration 3417: with minibatch training loss = 1.24 and accuracy of 0.66\n",
      "Iteration 3418: with minibatch training loss = 0.503 and accuracy of 0.88\n",
      "Iteration 3419: with minibatch training loss = 1.09 and accuracy of 0.69\n",
      "Iteration 3420: with minibatch training loss = 1.03 and accuracy of 0.73\n",
      "Iteration 3421: with minibatch training loss = 1.19 and accuracy of 0.69\n",
      "Iteration 3422: with minibatch training loss = 1.05 and accuracy of 0.73\n",
      "Iteration 3423: with minibatch training loss = 1.04 and accuracy of 0.72\n",
      "Iteration 3424: with minibatch training loss = 0.609 and accuracy of 0.86\n",
      "Iteration 3425: with minibatch training loss = 1.52 and accuracy of 0.58\n",
      "Iteration 3426: with minibatch training loss = 1.04 and accuracy of 0.72\n",
      "Iteration 3427: with minibatch training loss = 1.1 and accuracy of 0.69\n",
      "Iteration 3428: with minibatch training loss = 0.998 and accuracy of 0.72\n",
      "Iteration 3429: with minibatch training loss = 1.05 and accuracy of 0.7\n",
      "Iteration 3430: with minibatch training loss = 0.736 and accuracy of 0.84\n",
      "Iteration 3431: with minibatch training loss = 0.849 and accuracy of 0.75\n",
      "Iteration 3432: with minibatch training loss = 1.13 and accuracy of 0.77\n",
      "Iteration 3433: with minibatch training loss = 0.768 and accuracy of 0.81\n",
      "Iteration 3434: with minibatch training loss = 0.839 and accuracy of 0.77\n",
      "Iteration 3435: with minibatch training loss = 0.949 and accuracy of 0.73\n",
      "Iteration 3436: with minibatch training loss = 1.49 and accuracy of 0.56\n",
      "Iteration 3437: with minibatch training loss = 0.997 and accuracy of 0.72\n",
      "Iteration 3438: with minibatch training loss = 1.02 and accuracy of 0.75\n",
      "Iteration 3439: with minibatch training loss = 1.33 and accuracy of 0.64\n",
      "Iteration 3440: with minibatch training loss = 0.974 and accuracy of 0.73\n",
      "Iteration 3441: with minibatch training loss = 0.99 and accuracy of 0.72\n",
      "Iteration 3442: with minibatch training loss = 1 and accuracy of 0.72\n",
      "Iteration 3443: with minibatch training loss = 1.09 and accuracy of 0.7\n",
      "Iteration 3444: with minibatch training loss = 1.49 and accuracy of 0.61\n",
      "Iteration 3445: with minibatch training loss = 0.902 and accuracy of 0.75\n",
      "Iteration 3446: with minibatch training loss = 0.837 and accuracy of 0.75\n",
      "Iteration 3447: with minibatch training loss = 1.28 and accuracy of 0.62\n",
      "Iteration 3448: with minibatch training loss = 0.77 and accuracy of 0.8\n",
      "Iteration 3449: with minibatch training loss = 0.861 and accuracy of 0.78\n",
      "Iteration 3450: with minibatch training loss = 0.774 and accuracy of 0.78\n",
      "Iteration 3451: with minibatch training loss = 0.879 and accuracy of 0.77\n",
      "Iteration 3452: with minibatch training loss = 1.28 and accuracy of 0.66\n",
      "Iteration 3453: with minibatch training loss = 1.03 and accuracy of 0.73\n",
      "Iteration 3454: with minibatch training loss = 1.36 and accuracy of 0.61\n",
      "Iteration 3455: with minibatch training loss = 0.862 and accuracy of 0.8\n",
      "Iteration 3456: with minibatch training loss = 0.707 and accuracy of 0.84\n",
      "Iteration 3457: with minibatch training loss = 0.982 and accuracy of 0.75\n",
      "Iteration 3458: with minibatch training loss = 0.612 and accuracy of 0.86\n",
      "Iteration 3459: with minibatch training loss = 1.12 and accuracy of 0.73\n",
      "Iteration 3460: with minibatch training loss = 1.26 and accuracy of 0.66\n",
      "Iteration 3461: with minibatch training loss = 0.816 and accuracy of 0.77\n",
      "Iteration 3462: with minibatch training loss = 0.922 and accuracy of 0.72\n",
      "Iteration 3463: with minibatch training loss = 0.747 and accuracy of 0.8\n",
      "Iteration 3464: with minibatch training loss = 0.654 and accuracy of 0.83\n",
      "Iteration 3465: with minibatch training loss = 1.29 and accuracy of 0.67\n",
      "Iteration 3466: with minibatch training loss = 1.17 and accuracy of 0.69\n",
      "Iteration 3467: with minibatch training loss = 0.909 and accuracy of 0.77\n",
      "Iteration 3468: with minibatch training loss = 0.621 and accuracy of 0.86\n",
      "Iteration 3469: with minibatch training loss = 1.11 and accuracy of 0.72\n",
      "Iteration 3470: with minibatch training loss = 0.965 and accuracy of 0.72\n",
      "Iteration 3471: with minibatch training loss = 1.31 and accuracy of 0.64\n",
      "Iteration 3472: with minibatch training loss = 1.04 and accuracy of 0.72\n",
      "Iteration 3473: with minibatch training loss = 1.15 and accuracy of 0.67\n",
      "Iteration 3474: with minibatch training loss = 1.01 and accuracy of 0.73\n",
      "Iteration 3475: with minibatch training loss = 1.13 and accuracy of 0.7\n",
      "Iteration 3476: with minibatch training loss = 0.784 and accuracy of 0.81\n",
      "Iteration 3477: with minibatch training loss = 0.829 and accuracy of 0.78\n",
      "Iteration 3478: with minibatch training loss = 0.708 and accuracy of 0.83\n",
      "Iteration 3479: with minibatch training loss = 0.627 and accuracy of 0.86\n",
      "Iteration 3480: with minibatch training loss = 0.882 and accuracy of 0.77\n",
      "Iteration 3481: with minibatch training loss = 0.836 and accuracy of 0.8\n",
      "Iteration 3482: with minibatch training loss = 1.01 and accuracy of 0.73\n",
      "Iteration 3483: with minibatch training loss = 0.651 and accuracy of 0.83\n",
      "Iteration 3484: with minibatch training loss = 1.16 and accuracy of 0.72\n",
      "Iteration 3485: with minibatch training loss = 1.35 and accuracy of 0.62\n",
      "Iteration 3486: with minibatch training loss = 0.762 and accuracy of 0.8\n",
      "Iteration 3487: with minibatch training loss = 1.27 and accuracy of 0.66\n",
      "Iteration 3488: with minibatch training loss = 1.09 and accuracy of 0.7\n",
      "Iteration 3489: with minibatch training loss = 1 and accuracy of 0.7\n",
      "Iteration 3490: with minibatch training loss = 1.15 and accuracy of 0.7\n",
      "Iteration 3491: with minibatch training loss = 1.3 and accuracy of 0.62\n",
      "Iteration 3492: with minibatch training loss = 0.845 and accuracy of 0.78\n",
      "Iteration 3493: with minibatch training loss = 0.898 and accuracy of 0.78\n",
      "Iteration 3494: with minibatch training loss = 0.838 and accuracy of 0.78\n",
      "Iteration 3495: with minibatch training loss = 0.894 and accuracy of 0.75\n",
      "Iteration 3496: with minibatch training loss = 0.867 and accuracy of 0.78\n",
      "Iteration 3497: with minibatch training loss = 0.966 and accuracy of 0.73\n",
      "Iteration 3498: with minibatch training loss = 1.04 and accuracy of 0.75\n",
      "Iteration 3499: with minibatch training loss = 0.873 and accuracy of 0.8\n",
      "Iteration 3500: with minibatch training loss = 1.09 and accuracy of 0.72\n",
      "Iteration 3501: with minibatch training loss = 1.16 and accuracy of 0.7\n",
      "Iteration 3502: with minibatch training loss = 0.778 and accuracy of 0.83\n",
      "Iteration 3503: with minibatch training loss = 0.854 and accuracy of 0.77\n",
      "Iteration 3504: with minibatch training loss = 0.688 and accuracy of 0.83\n",
      "Iteration 3505: with minibatch training loss = 1.11 and accuracy of 0.72\n",
      "Iteration 3506: with minibatch training loss = 0.995 and accuracy of 0.73\n",
      "Iteration 3507: with minibatch training loss = 1.2 and accuracy of 0.69\n",
      "Iteration 3508: with minibatch training loss = 1.03 and accuracy of 0.73\n",
      "Iteration 3509: with minibatch training loss = 0.672 and accuracy of 0.81\n",
      "Iteration 3510: with minibatch training loss = 0.771 and accuracy of 0.78\n",
      "Iteration 3511: with minibatch training loss = 1.15 and accuracy of 0.67\n",
      "Iteration 3512: with minibatch training loss = 1.14 and accuracy of 0.69\n",
      "Iteration 3513: with minibatch training loss = 1.04 and accuracy of 0.7\n",
      "Iteration 3514: with minibatch training loss = 1.03 and accuracy of 0.7\n",
      "Iteration 3515: with minibatch training loss = 0.735 and accuracy of 0.8\n",
      "Iteration 3516: with minibatch training loss = 1.02 and accuracy of 0.72\n",
      "Iteration 3517: with minibatch training loss = 0.723 and accuracy of 0.78\n",
      "Iteration 3518: with minibatch training loss = 1.04 and accuracy of 0.7\n",
      "Iteration 3519: with minibatch training loss = 1.23 and accuracy of 0.67\n",
      "Iteration 3520: with minibatch training loss = 0.758 and accuracy of 0.83\n",
      "Iteration 3521: with minibatch training loss = 0.989 and accuracy of 0.73\n",
      "Iteration 3522: with minibatch training loss = 0.77 and accuracy of 0.81\n",
      "Iteration 3523: with minibatch training loss = 1.03 and accuracy of 0.73\n",
      "Iteration 3524: with minibatch training loss = 1.01 and accuracy of 0.75\n",
      "Iteration 3525: with minibatch training loss = 0.953 and accuracy of 0.78\n",
      "Iteration 3526: with minibatch training loss = 0.988 and accuracy of 0.73\n",
      "Iteration 3527: with minibatch training loss = 0.664 and accuracy of 0.84\n",
      "Iteration 3528: with minibatch training loss = 0.938 and accuracy of 0.72\n",
      "Iteration 3529: with minibatch training loss = 1.06 and accuracy of 0.69\n",
      "Iteration 3530: with minibatch training loss = 0.974 and accuracy of 0.7\n",
      "Iteration 3531: with minibatch training loss = 0.68 and accuracy of 0.81\n",
      "Iteration 3532: with minibatch training loss = 1.17 and accuracy of 0.69\n",
      "Iteration 3533: with minibatch training loss = 1.11 and accuracy of 0.72\n",
      "Iteration 3534: with minibatch training loss = 0.755 and accuracy of 0.8\n",
      "Iteration 3535: with minibatch training loss = 1.21 and accuracy of 0.66\n",
      "Iteration 3536: with minibatch training loss = 1.1 and accuracy of 0.73\n",
      "Iteration 3537: with minibatch training loss = 0.927 and accuracy of 0.75\n",
      "Iteration 3538: with minibatch training loss = 0.914 and accuracy of 0.83\n",
      "Iteration 3539: with minibatch training loss = 1 and accuracy of 0.7\n",
      "Iteration 3540: with minibatch training loss = 1.16 and accuracy of 0.66\n",
      "Iteration 3541: with minibatch training loss = 1.09 and accuracy of 0.7\n",
      "Iteration 3542: with minibatch training loss = 0.936 and accuracy of 0.72\n",
      "Iteration 3543: with minibatch training loss = 0.912 and accuracy of 0.73\n",
      "Iteration 3544: with minibatch training loss = 1.29 and accuracy of 0.64\n",
      "Iteration 3545: with minibatch training loss = 0.854 and accuracy of 0.78\n",
      "Iteration 3546: with minibatch training loss = 0.689 and accuracy of 0.83\n",
      "Iteration 3547: with minibatch training loss = 1.01 and accuracy of 0.77\n",
      "Iteration 3548: with minibatch training loss = 0.78 and accuracy of 0.8\n",
      "Iteration 3549: with minibatch training loss = 1.14 and accuracy of 0.67\n",
      "Iteration 3550: with minibatch training loss = 1.09 and accuracy of 0.7\n",
      "Iteration 3551: with minibatch training loss = 1.01 and accuracy of 0.73\n",
      "Iteration 3552: with minibatch training loss = 0.83 and accuracy of 0.77\n",
      "Iteration 3553: with minibatch training loss = 0.831 and accuracy of 0.78\n",
      "Iteration 3554: with minibatch training loss = 1.22 and accuracy of 0.7\n",
      "Iteration 3555: with minibatch training loss = 1.16 and accuracy of 0.66\n",
      "Iteration 3556: with minibatch training loss = 1.02 and accuracy of 0.72\n",
      "Iteration 3557: with minibatch training loss = 0.843 and accuracy of 0.78\n",
      "Iteration 3558: with minibatch training loss = 1.08 and accuracy of 0.7\n",
      "Iteration 3559: with minibatch training loss = 0.967 and accuracy of 0.72\n",
      "Iteration 3560: with minibatch training loss = 0.855 and accuracy of 0.77\n",
      "Iteration 3561: with minibatch training loss = 0.875 and accuracy of 0.8\n",
      "Iteration 3562: with minibatch training loss = 0.728 and accuracy of 0.83\n",
      "Iteration 3563: with minibatch training loss = 1.01 and accuracy of 0.7\n",
      "Iteration 3564: with minibatch training loss = 1.2 and accuracy of 0.67\n",
      "Iteration 3565: with minibatch training loss = 0.694 and accuracy of 0.8\n",
      "Iteration 3566: with minibatch training loss = 0.972 and accuracy of 0.73\n",
      "Iteration 3567: with minibatch training loss = 1.07 and accuracy of 0.7\n",
      "Iteration 3568: with minibatch training loss = 1.36 and accuracy of 0.62\n",
      "Iteration 3569: with minibatch training loss = 1.09 and accuracy of 0.7\n",
      "Iteration 3570: with minibatch training loss = 0.908 and accuracy of 0.77\n",
      "Iteration 3571: with minibatch training loss = 0.985 and accuracy of 0.77\n",
      "Iteration 3572: with minibatch training loss = 0.951 and accuracy of 0.77\n",
      "Iteration 3573: with minibatch training loss = 1.12 and accuracy of 0.73\n",
      "Iteration 3574: with minibatch training loss = 1.18 and accuracy of 0.7\n",
      "Iteration 3575: with minibatch training loss = 0.94 and accuracy of 0.72\n",
      "Iteration 3576: with minibatch training loss = 0.864 and accuracy of 0.78\n",
      "Iteration 3577: with minibatch training loss = 1.09 and accuracy of 0.73\n",
      "Iteration 3578: with minibatch training loss = 0.916 and accuracy of 0.73\n",
      "Iteration 3579: with minibatch training loss = 0.907 and accuracy of 0.77\n",
      "Iteration 3580: with minibatch training loss = 1.02 and accuracy of 0.7\n",
      "Iteration 3581: with minibatch training loss = 0.739 and accuracy of 0.8\n",
      "Iteration 3582: with minibatch training loss = 0.851 and accuracy of 0.75\n",
      "Iteration 3583: with minibatch training loss = 0.832 and accuracy of 0.73\n",
      "Iteration 3584: with minibatch training loss = 1.14 and accuracy of 0.66\n",
      "Iteration 3585: with minibatch training loss = 1.1 and accuracy of 0.72\n",
      "Iteration 3586: with minibatch training loss = 0.994 and accuracy of 0.73\n",
      "Iteration 3587: with minibatch training loss = 0.656 and accuracy of 0.81\n",
      "Iteration 3588: with minibatch training loss = 0.953 and accuracy of 0.75\n",
      "Iteration 3589: with minibatch training loss = 0.652 and accuracy of 0.81\n",
      "Iteration 3590: with minibatch training loss = 1.09 and accuracy of 0.69\n",
      "Iteration 3591: with minibatch training loss = 1.01 and accuracy of 0.73\n",
      "Iteration 3592: with minibatch training loss = 0.969 and accuracy of 0.73\n",
      "Iteration 3593: with minibatch training loss = 0.944 and accuracy of 0.77\n",
      "Iteration 3594: with minibatch training loss = 1.15 and accuracy of 0.69\n",
      "Iteration 3595: with minibatch training loss = 0.67 and accuracy of 0.83\n",
      "Iteration 3596: with minibatch training loss = 0.874 and accuracy of 0.77\n",
      "Iteration 3597: with minibatch training loss = 1.11 and accuracy of 0.72\n",
      "Iteration 3598: with minibatch training loss = 0.784 and accuracy of 0.78\n",
      "Iteration 3599: with minibatch training loss = 0.876 and accuracy of 0.75\n",
      "Iteration 3600: with minibatch training loss = 1.19 and accuracy of 0.69\n",
      "Iteration 3601: with minibatch training loss = 1.01 and accuracy of 0.73\n",
      "Iteration 3602: with minibatch training loss = 0.753 and accuracy of 0.81\n",
      "Iteration 3603: with minibatch training loss = 1 and accuracy of 0.72\n",
      "Iteration 3604: with minibatch training loss = 0.929 and accuracy of 0.77\n",
      "Iteration 3605: with minibatch training loss = 0.695 and accuracy of 0.78\n",
      "Iteration 3606: with minibatch training loss = 0.803 and accuracy of 0.8\n",
      "Iteration 3607: with minibatch training loss = 0.99 and accuracy of 0.73\n",
      "Iteration 3608: with minibatch training loss = 0.922 and accuracy of 0.77\n",
      "Iteration 3609: with minibatch training loss = 0.922 and accuracy of 0.8\n",
      "Iteration 3610: with minibatch training loss = 1.16 and accuracy of 0.7\n",
      "Iteration 3611: with minibatch training loss = 0.995 and accuracy of 0.73\n",
      "Iteration 3612: with minibatch training loss = 1.52 and accuracy of 0.55\n",
      "Iteration 3613: with minibatch training loss = 0.738 and accuracy of 0.81\n",
      "Iteration 3614: with minibatch training loss = 1.32 and accuracy of 0.64\n",
      "Iteration 3615: with minibatch training loss = 1.21 and accuracy of 0.67\n",
      "Iteration 3616: with minibatch training loss = 0.696 and accuracy of 0.78\n",
      "Iteration 3617: with minibatch training loss = 0.742 and accuracy of 0.8\n",
      "Iteration 3618: with minibatch training loss = 0.64 and accuracy of 0.84\n",
      "Iteration 3619: with minibatch training loss = 0.803 and accuracy of 0.77\n",
      "Iteration 3620: with minibatch training loss = 0.886 and accuracy of 0.77\n",
      "Iteration 3621: with minibatch training loss = 1.04 and accuracy of 0.7\n",
      "Iteration 3622: with minibatch training loss = 0.767 and accuracy of 0.81\n",
      "Iteration 3623: with minibatch training loss = 1.13 and accuracy of 0.72\n",
      "Iteration 3624: with minibatch training loss = 1.31 and accuracy of 0.62\n",
      "Iteration 3625: with minibatch training loss = 1.01 and accuracy of 0.75\n",
      "Iteration 3626: with minibatch training loss = 0.828 and accuracy of 0.8\n",
      "Iteration 3627: with minibatch training loss = 0.665 and accuracy of 0.86\n",
      "Iteration 3628: with minibatch training loss = 1.22 and accuracy of 0.67\n",
      "Iteration 3629: with minibatch training loss = 0.654 and accuracy of 0.84\n",
      "Iteration 3630: with minibatch training loss = 1.09 and accuracy of 0.69\n",
      "Iteration 3631: with minibatch training loss = 0.704 and accuracy of 0.83\n",
      "Iteration 3632: with minibatch training loss = 0.919 and accuracy of 0.75\n",
      "Iteration 3633: with minibatch training loss = 0.895 and accuracy of 0.77\n",
      "Iteration 3634: with minibatch training loss = 1.29 and accuracy of 0.62\n",
      "Iteration 3635: with minibatch training loss = 1.11 and accuracy of 0.7\n",
      "Iteration 3636: with minibatch training loss = 1.14 and accuracy of 0.73\n",
      "Iteration 3637: with minibatch training loss = 0.779 and accuracy of 0.8\n",
      "Iteration 3638: with minibatch training loss = 1.01 and accuracy of 0.75\n",
      "Iteration 3639: with minibatch training loss = 1.32 and accuracy of 0.62\n",
      "Iteration 3640: with minibatch training loss = 1.01 and accuracy of 0.75\n",
      "Iteration 3641: with minibatch training loss = 0.849 and accuracy of 0.77\n",
      "Iteration 3642: with minibatch training loss = 0.918 and accuracy of 0.77\n",
      "Iteration 3643: with minibatch training loss = 1.16 and accuracy of 0.66\n",
      "Iteration 3644: with minibatch training loss = 0.979 and accuracy of 0.73\n",
      "Iteration 3645: with minibatch training loss = 0.62 and accuracy of 0.84\n",
      "Iteration 3646: with minibatch training loss = 0.985 and accuracy of 0.75\n",
      "Iteration 3647: with minibatch training loss = 1.08 and accuracy of 0.7\n",
      "Iteration 3648: with minibatch training loss = 0.828 and accuracy of 0.75\n",
      "Iteration 3649: with minibatch training loss = 1.27 and accuracy of 0.69\n",
      "Iteration 3650: with minibatch training loss = 1.2 and accuracy of 0.64\n",
      "Iteration 3651: with minibatch training loss = 0.734 and accuracy of 0.81\n",
      "Iteration 3652: with minibatch training loss = 0.992 and accuracy of 0.73\n",
      "Iteration 3653: with minibatch training loss = 1.19 and accuracy of 0.66\n",
      "Iteration 3654: with minibatch training loss = 1.24 and accuracy of 0.67\n",
      "Iteration 3655: with minibatch training loss = 0.772 and accuracy of 0.83\n",
      "Iteration 3656: with minibatch training loss = 1.04 and accuracy of 0.67\n",
      "Iteration 3657: with minibatch training loss = 0.97 and accuracy of 0.73\n",
      "Iteration 3658: with minibatch training loss = 0.806 and accuracy of 0.8\n",
      "Iteration 3659: with minibatch training loss = 1.05 and accuracy of 0.72\n",
      "Iteration 3660: with minibatch training loss = 0.944 and accuracy of 0.72\n",
      "Iteration 3661: with minibatch training loss = 0.775 and accuracy of 0.8\n",
      "Iteration 3662: with minibatch training loss = 0.753 and accuracy of 0.81\n",
      "Iteration 3663: with minibatch training loss = 0.802 and accuracy of 0.8\n",
      "Iteration 3664: with minibatch training loss = 0.821 and accuracy of 0.78\n",
      "Iteration 3665: with minibatch training loss = 0.969 and accuracy of 0.75\n",
      "Iteration 3666: with minibatch training loss = 1 and accuracy of 0.75\n",
      "Iteration 3667: with minibatch training loss = 0.9 and accuracy of 0.77\n",
      "Iteration 3668: with minibatch training loss = 0.528 and accuracy of 0.88\n",
      "Iteration 3669: with minibatch training loss = 0.738 and accuracy of 0.84\n",
      "Iteration 3670: with minibatch training loss = 1.29 and accuracy of 0.64\n",
      "Iteration 3671: with minibatch training loss = 0.986 and accuracy of 0.73\n",
      "Iteration 3672: with minibatch training loss = 0.966 and accuracy of 0.7\n",
      "Iteration 3673: with minibatch training loss = 1.11 and accuracy of 0.72\n",
      "Iteration 3674: with minibatch training loss = 1.07 and accuracy of 0.67\n",
      "Iteration 3675: with minibatch training loss = 0.542 and accuracy of 0.89\n",
      "Iteration 3676: with minibatch training loss = 0.886 and accuracy of 0.77\n",
      "Iteration 3677: with minibatch training loss = 1.24 and accuracy of 0.69\n",
      "Iteration 3678: with minibatch training loss = 1.13 and accuracy of 0.67\n",
      "Iteration 3679: with minibatch training loss = 1.11 and accuracy of 0.72\n",
      "Iteration 3680: with minibatch training loss = 0.855 and accuracy of 0.77\n",
      "Iteration 3681: with minibatch training loss = 0.936 and accuracy of 0.77\n",
      "Iteration 3682: with minibatch training loss = 1.03 and accuracy of 0.72\n",
      "Iteration 3683: with minibatch training loss = 0.612 and accuracy of 0.84\n",
      "Iteration 3684: with minibatch training loss = 0.823 and accuracy of 0.78\n",
      "Iteration 3685: with minibatch training loss = 1.02 and accuracy of 0.7\n",
      "Iteration 3686: with minibatch training loss = 0.695 and accuracy of 0.78\n",
      "Iteration 3687: with minibatch training loss = 0.949 and accuracy of 0.72\n",
      "Iteration 3688: with minibatch training loss = 0.922 and accuracy of 0.73\n",
      "Iteration 3689: with minibatch training loss = 0.745 and accuracy of 0.84\n",
      "Iteration 3690: with minibatch training loss = 0.844 and accuracy of 0.77\n",
      "Iteration 3691: with minibatch training loss = 0.969 and accuracy of 0.77\n",
      "Iteration 3692: with minibatch training loss = 0.929 and accuracy of 0.75\n",
      "Iteration 3693: with minibatch training loss = 0.842 and accuracy of 0.75\n",
      "Iteration 3694: with minibatch training loss = 0.99 and accuracy of 0.7\n",
      "Iteration 3695: with minibatch training loss = 1.02 and accuracy of 0.69\n",
      "Iteration 3696: with minibatch training loss = 1.23 and accuracy of 0.64\n",
      "Iteration 3697: with minibatch training loss = 0.782 and accuracy of 0.81\n",
      "Iteration 3698: with minibatch training loss = 1.16 and accuracy of 0.67\n",
      "Iteration 3699: with minibatch training loss = 0.821 and accuracy of 0.8\n",
      "Iteration 3700: with minibatch training loss = 0.853 and accuracy of 0.78\n",
      "Iteration 3701: with minibatch training loss = 0.952 and accuracy of 0.75\n",
      "Iteration 3702: with minibatch training loss = 0.908 and accuracy of 0.75\n",
      "Iteration 3703: with minibatch training loss = 0.996 and accuracy of 0.75\n",
      "Iteration 3704: with minibatch training loss = 0.964 and accuracy of 0.75\n",
      "Iteration 3705: with minibatch training loss = 0.906 and accuracy of 0.73\n",
      "Iteration 3706: with minibatch training loss = 0.814 and accuracy of 0.78\n",
      "Iteration 3707: with minibatch training loss = 0.63 and accuracy of 0.86\n",
      "Iteration 3708: with minibatch training loss = 0.812 and accuracy of 0.77\n",
      "Iteration 3709: with minibatch training loss = 0.958 and accuracy of 0.73\n",
      "Iteration 3710: with minibatch training loss = 1.02 and accuracy of 0.75\n",
      "Iteration 3711: with minibatch training loss = 0.693 and accuracy of 0.81\n",
      "Iteration 3712: with minibatch training loss = 1.15 and accuracy of 0.69\n",
      "Iteration 3713: with minibatch training loss = 0.78 and accuracy of 0.78\n",
      "Iteration 3714: with minibatch training loss = 1.23 and accuracy of 0.7\n",
      "Iteration 3715: with minibatch training loss = 0.849 and accuracy of 0.8\n",
      "Iteration 3716: with minibatch training loss = 0.97 and accuracy of 0.72\n",
      "Iteration 3717: with minibatch training loss = 0.919 and accuracy of 0.75\n",
      "Iteration 3718: with minibatch training loss = 1.39 and accuracy of 0.62\n",
      "Iteration 3719: with minibatch training loss = 1.12 and accuracy of 0.7\n",
      "Iteration 3720: with minibatch training loss = 1.11 and accuracy of 0.7\n",
      "Iteration 3721: with minibatch training loss = 1.05 and accuracy of 0.72\n",
      "Iteration 3722: with minibatch training loss = 1.12 and accuracy of 0.72\n",
      "Iteration 3723: with minibatch training loss = 0.885 and accuracy of 0.8\n",
      "Iteration 3724: with minibatch training loss = 0.635 and accuracy of 0.84\n",
      "Iteration 3725: with minibatch training loss = 0.879 and accuracy of 0.75\n",
      "Iteration 3726: with minibatch training loss = 1.29 and accuracy of 0.66\n",
      "Iteration 3727: with minibatch training loss = 0.994 and accuracy of 0.69\n",
      "Iteration 3728: with minibatch training loss = 0.85 and accuracy of 0.8\n",
      "Iteration 3729: with minibatch training loss = 0.764 and accuracy of 0.83\n",
      "Iteration 3730: with minibatch training loss = 0.929 and accuracy of 0.77\n",
      "Iteration 3731: with minibatch training loss = 0.83 and accuracy of 0.78\n",
      "Iteration 3732: with minibatch training loss = 1.08 and accuracy of 0.7\n",
      "Iteration 3733: with minibatch training loss = 0.782 and accuracy of 0.83\n",
      "Iteration 3734: with minibatch training loss = 1.05 and accuracy of 0.72\n",
      "Iteration 3735: with minibatch training loss = 0.996 and accuracy of 0.72\n",
      "Iteration 3736: with minibatch training loss = 1.14 and accuracy of 0.67\n",
      "Iteration 3737: with minibatch training loss = 0.915 and accuracy of 0.73\n",
      "Iteration 3738: with minibatch training loss = 1.04 and accuracy of 0.73\n",
      "Iteration 3739: with minibatch training loss = 1 and accuracy of 0.77\n",
      "Iteration 3740: with minibatch training loss = 1.06 and accuracy of 0.72\n",
      "Iteration 3741: with minibatch training loss = 1.22 and accuracy of 0.66\n",
      "Iteration 3742: with minibatch training loss = 1.01 and accuracy of 0.72\n",
      "Iteration 3743: with minibatch training loss = 0.862 and accuracy of 0.75\n",
      "Iteration 3744: with minibatch training loss = 1.23 and accuracy of 0.64\n",
      "Iteration 3745: with minibatch training loss = 1.28 and accuracy of 0.67\n",
      "Iteration 3746: with minibatch training loss = 0.749 and accuracy of 0.81\n",
      "Iteration 3747: with minibatch training loss = 0.997 and accuracy of 0.73\n",
      "Iteration 3748: with minibatch training loss = 0.867 and accuracy of 0.78\n",
      "Iteration 3749: with minibatch training loss = 0.978 and accuracy of 0.75\n",
      "Iteration 3750: with minibatch training loss = 0.979 and accuracy of 0.73\n",
      "Iteration 3751: with minibatch training loss = 0.998 and accuracy of 0.73\n",
      "Iteration 3752: with minibatch training loss = 0.847 and accuracy of 0.8\n",
      "Iteration 3753: with minibatch training loss = 0.929 and accuracy of 0.73\n",
      "Iteration 3754: with minibatch training loss = 0.804 and accuracy of 0.78\n",
      "Iteration 3755: with minibatch training loss = 1.08 and accuracy of 0.66\n",
      "Iteration 3756: with minibatch training loss = 0.966 and accuracy of 0.78\n",
      "Iteration 3757: with minibatch training loss = 1.08 and accuracy of 0.69\n",
      "Iteration 3758: with minibatch training loss = 0.583 and accuracy of 0.84\n",
      "Iteration 3759: with minibatch training loss = 0.974 and accuracy of 0.77\n",
      "Iteration 3760: with minibatch training loss = 1.05 and accuracy of 0.75\n",
      "Iteration 3761: with minibatch training loss = 1.32 and accuracy of 0.62\n",
      "Iteration 3762: with minibatch training loss = 1.17 and accuracy of 0.69\n",
      "Iteration 3763: with minibatch training loss = 1.2 and accuracy of 0.64\n",
      "Iteration 3764: with minibatch training loss = 1.04 and accuracy of 0.73\n",
      "Iteration 3765: with minibatch training loss = 0.981 and accuracy of 0.78\n",
      "Iteration 3766: with minibatch training loss = 1.08 and accuracy of 0.7\n",
      "Iteration 3767: with minibatch training loss = 0.801 and accuracy of 0.8\n",
      "Iteration 3768: with minibatch training loss = 0.876 and accuracy of 0.75\n",
      "Iteration 3769: with minibatch training loss = 0.744 and accuracy of 0.8\n",
      "Iteration 3770: with minibatch training loss = 1.04 and accuracy of 0.72\n",
      "Iteration 3771: with minibatch training loss = 0.969 and accuracy of 0.73\n",
      "Iteration 3772: with minibatch training loss = 1.12 and accuracy of 0.67\n",
      "Iteration 3773: with minibatch training loss = 1.01 and accuracy of 0.72\n",
      "Iteration 3774: with minibatch training loss = 1.04 and accuracy of 0.72\n",
      "Iteration 3775: with minibatch training loss = 0.923 and accuracy of 0.75\n",
      "Iteration 3776: with minibatch training loss = 0.948 and accuracy of 0.78\n",
      "Iteration 3777: with minibatch training loss = 1.25 and accuracy of 0.64\n",
      "Iteration 3778: with minibatch training loss = 1.09 and accuracy of 0.72\n",
      "Iteration 3779: with minibatch training loss = 0.791 and accuracy of 0.78\n",
      "Iteration 3780: with minibatch training loss = 0.981 and accuracy of 0.73\n",
      "Iteration 3781: with minibatch training loss = 0.963 and accuracy of 0.75\n",
      "Iteration 3782: with minibatch training loss = 0.948 and accuracy of 0.78\n",
      "Iteration 3783: with minibatch training loss = 0.788 and accuracy of 0.77\n",
      "Iteration 3784: with minibatch training loss = 0.932 and accuracy of 0.75\n",
      "Iteration 3785: with minibatch training loss = 0.939 and accuracy of 0.73\n",
      "Iteration 3786: with minibatch training loss = 0.946 and accuracy of 0.75\n",
      "Iteration 3787: with minibatch training loss = 0.653 and accuracy of 0.84\n",
      "Iteration 3788: with minibatch training loss = 1.1 and accuracy of 0.69\n",
      "Iteration 3789: with minibatch training loss = 1.06 and accuracy of 0.69\n",
      "Iteration 3790: with minibatch training loss = 0.965 and accuracy of 0.75\n",
      "Iteration 3791: with minibatch training loss = 1.06 and accuracy of 0.72\n",
      "Iteration 3792: with minibatch training loss = 0.805 and accuracy of 0.78\n",
      "Iteration 3793: with minibatch training loss = 1.02 and accuracy of 0.72\n",
      "Iteration 3794: with minibatch training loss = 0.87 and accuracy of 0.75\n",
      "Iteration 3795: with minibatch training loss = 0.905 and accuracy of 0.75\n",
      "Iteration 3796: with minibatch training loss = 0.988 and accuracy of 0.73\n",
      "Iteration 3797: with minibatch training loss = 1.31 and accuracy of 0.64\n",
      "Iteration 3798: with minibatch training loss = 0.758 and accuracy of 0.81\n",
      "Iteration 3799: with minibatch training loss = 0.837 and accuracy of 0.75\n",
      "Iteration 3800: with minibatch training loss = 0.729 and accuracy of 0.84\n",
      "Iteration 3801: with minibatch training loss = 0.851 and accuracy of 0.81\n",
      "Iteration 3802: with minibatch training loss = 0.976 and accuracy of 0.78\n",
      "Iteration 3803: with minibatch training loss = 0.798 and accuracy of 0.8\n",
      "Iteration 3804: with minibatch training loss = 0.999 and accuracy of 0.77\n",
      "Iteration 3805: with minibatch training loss = 0.672 and accuracy of 0.83\n",
      "Iteration 3806: with minibatch training loss = 0.661 and accuracy of 0.84\n",
      "Iteration 3807: with minibatch training loss = 1.05 and accuracy of 0.72\n",
      "Iteration 3808: with minibatch training loss = 1.19 and accuracy of 0.69\n",
      "Iteration 3809: with minibatch training loss = 0.676 and accuracy of 0.84\n",
      "Iteration 3810: with minibatch training loss = 0.577 and accuracy of 0.86\n",
      "Iteration 3811: with minibatch training loss = 0.842 and accuracy of 0.73\n",
      "Iteration 3812: with minibatch training loss = 0.868 and accuracy of 0.8\n",
      "Iteration 3813: with minibatch training loss = 0.98 and accuracy of 0.72\n",
      "Iteration 3814: with minibatch training loss = 1.28 and accuracy of 0.64\n",
      "Iteration 3815: with minibatch training loss = 0.982 and accuracy of 0.77\n",
      "Iteration 3816: with minibatch training loss = 0.866 and accuracy of 0.78\n",
      "Iteration 3817: with minibatch training loss = 1.16 and accuracy of 0.66\n",
      "Iteration 3818: with minibatch training loss = 1.02 and accuracy of 0.73\n",
      "Iteration 3819: with minibatch training loss = 0.91 and accuracy of 0.73\n",
      "Iteration 3820: with minibatch training loss = 0.748 and accuracy of 0.8\n",
      "Iteration 3821: with minibatch training loss = 0.726 and accuracy of 0.84\n",
      "Iteration 3822: with minibatch training loss = 1.02 and accuracy of 0.77\n",
      "Iteration 3823: with minibatch training loss = 0.798 and accuracy of 0.75\n",
      "Iteration 3824: with minibatch training loss = 1.22 and accuracy of 0.66\n",
      "Iteration 3825: with minibatch training loss = 1.16 and accuracy of 0.7\n",
      "Iteration 3826: with minibatch training loss = 1.05 and accuracy of 0.7\n",
      "Iteration 3827: with minibatch training loss = 1.1 and accuracy of 0.72\n",
      "Iteration 3828: with minibatch training loss = 0.679 and accuracy of 0.8\n",
      "Iteration 3829: with minibatch training loss = 0.838 and accuracy of 0.78\n",
      "Iteration 3830: with minibatch training loss = 1.18 and accuracy of 0.69\n",
      "Iteration 3831: with minibatch training loss = 0.823 and accuracy of 0.78\n",
      "Iteration 3832: with minibatch training loss = 1 and accuracy of 0.75\n",
      "Iteration 3833: with minibatch training loss = 0.85 and accuracy of 0.77\n",
      "Iteration 3834: with minibatch training loss = 0.859 and accuracy of 0.75\n",
      "Iteration 3835: with minibatch training loss = 0.612 and accuracy of 0.84\n",
      "Iteration 3836: with minibatch training loss = 0.811 and accuracy of 0.78\n",
      "Iteration 3837: with minibatch training loss = 0.866 and accuracy of 0.75\n",
      "Iteration 3838: with minibatch training loss = 0.903 and accuracy of 0.77\n",
      "Iteration 3839: with minibatch training loss = 0.884 and accuracy of 0.78\n",
      "Iteration 3840: with minibatch training loss = 1.04 and accuracy of 0.77\n",
      "Iteration 3841: with minibatch training loss = 0.767 and accuracy of 0.81\n",
      "Iteration 3842: with minibatch training loss = 0.83 and accuracy of 0.78\n",
      "Iteration 3843: with minibatch training loss = 1.24 and accuracy of 0.67\n",
      "Iteration 3844: with minibatch training loss = 1.03 and accuracy of 0.69\n",
      "Iteration 3845: with minibatch training loss = 1.09 and accuracy of 0.7\n",
      "Iteration 3846: with minibatch training loss = 0.83 and accuracy of 0.75\n",
      "Iteration 3847: with minibatch training loss = 1.24 and accuracy of 0.64\n",
      "Iteration 3848: with minibatch training loss = 0.846 and accuracy of 0.75\n",
      "Iteration 3849: with minibatch training loss = 0.822 and accuracy of 0.81\n",
      "Iteration 3850: with minibatch training loss = 0.736 and accuracy of 0.78\n",
      "Iteration 3851: with minibatch training loss = 1.17 and accuracy of 0.67\n",
      "Iteration 3852: with minibatch training loss = 0.868 and accuracy of 0.77\n",
      "Iteration 3853: with minibatch training loss = 0.777 and accuracy of 0.8\n",
      "Iteration 3854: with minibatch training loss = 0.994 and accuracy of 0.73\n",
      "Iteration 3855: with minibatch training loss = 0.904 and accuracy of 0.73\n",
      "Iteration 3856: with minibatch training loss = 0.936 and accuracy of 0.77\n",
      "Iteration 3857: with minibatch training loss = 0.802 and accuracy of 0.78\n",
      "Iteration 3858: with minibatch training loss = 0.878 and accuracy of 0.78\n",
      "Iteration 3859: with minibatch training loss = 0.933 and accuracy of 0.72\n",
      "Iteration 3860: with minibatch training loss = 1.04 and accuracy of 0.7\n",
      "Iteration 3861: with minibatch training loss = 0.825 and accuracy of 0.78\n",
      "Iteration 3862: with minibatch training loss = 0.971 and accuracy of 0.75\n",
      "Iteration 3863: with minibatch training loss = 1.21 and accuracy of 0.66\n",
      "Iteration 3864: with minibatch training loss = 0.885 and accuracy of 0.77\n",
      "Iteration 3865: with minibatch training loss = 0.868 and accuracy of 0.78\n",
      "Iteration 3866: with minibatch training loss = 1.21 and accuracy of 0.66\n",
      "Iteration 3867: with minibatch training loss = 0.87 and accuracy of 0.75\n",
      "Iteration 3868: with minibatch training loss = 1.04 and accuracy of 0.7\n",
      "Iteration 3869: with minibatch training loss = 0.55 and accuracy of 0.86\n",
      "Iteration 3870: with minibatch training loss = 0.877 and accuracy of 0.78\n",
      "Iteration 3871: with minibatch training loss = 1.08 and accuracy of 0.69\n",
      "Iteration 3872: with minibatch training loss = 1.33 and accuracy of 0.62\n",
      "Iteration 3873: with minibatch training loss = 0.982 and accuracy of 0.73\n",
      "Iteration 3874: with minibatch training loss = 0.859 and accuracy of 0.77\n",
      "Iteration 3875: with minibatch training loss = 0.574 and accuracy of 0.88\n",
      "Iteration 3876: with minibatch training loss = 0.921 and accuracy of 0.73\n",
      "Iteration 3877: with minibatch training loss = 0.874 and accuracy of 0.77\n",
      "Iteration 3878: with minibatch training loss = 1.06 and accuracy of 0.75\n",
      "Iteration 3879: with minibatch training loss = 0.969 and accuracy of 0.72\n",
      "Iteration 3880: with minibatch training loss = 0.974 and accuracy of 0.73\n",
      "Iteration 3881: with minibatch training loss = 0.861 and accuracy of 0.78\n",
      "Iteration 3882: with minibatch training loss = 0.938 and accuracy of 0.77\n",
      "Iteration 3883: with minibatch training loss = 0.858 and accuracy of 0.78\n",
      "Iteration 3884: with minibatch training loss = 0.903 and accuracy of 0.77\n",
      "Iteration 3885: with minibatch training loss = 1.05 and accuracy of 0.73\n",
      "Iteration 3886: with minibatch training loss = 1.1 and accuracy of 0.69\n",
      "Iteration 3887: with minibatch training loss = 0.844 and accuracy of 0.75\n",
      "Iteration 3888: with minibatch training loss = 0.936 and accuracy of 0.77\n",
      "Iteration 3889: with minibatch training loss = 1.05 and accuracy of 0.73\n",
      "Iteration 3890: with minibatch training loss = 0.837 and accuracy of 0.75\n",
      "Iteration 3891: with minibatch training loss = 1.25 and accuracy of 0.69\n",
      "Iteration 3892: with minibatch training loss = 0.824 and accuracy of 0.81\n",
      "Iteration 3893: with minibatch training loss = 1.19 and accuracy of 0.66\n",
      "Iteration 3894: with minibatch training loss = 0.87 and accuracy of 0.77\n",
      "Iteration 3895: with minibatch training loss = 0.96 and accuracy of 0.75\n",
      "Iteration 3896: with minibatch training loss = 0.849 and accuracy of 0.73\n",
      "Iteration 3897: with minibatch training loss = 1.19 and accuracy of 0.69\n",
      "Iteration 3898: with minibatch training loss = 0.917 and accuracy of 0.75\n",
      "Iteration 3899: with minibatch training loss = 0.47 and accuracy of 0.89\n",
      "Iteration 3900: with minibatch training loss = 0.961 and accuracy of 0.77\n",
      "Iteration 3901: with minibatch training loss = 1.06 and accuracy of 0.7\n",
      "Iteration 3902: with minibatch training loss = 0.804 and accuracy of 0.8\n",
      "Iteration 3903: with minibatch training loss = 1.06 and accuracy of 0.73\n",
      "Iteration 3904: with minibatch training loss = 0.727 and accuracy of 0.8\n",
      "Iteration 3905: with minibatch training loss = 0.921 and accuracy of 0.75\n",
      "Iteration 3906: with minibatch training loss = 0.919 and accuracy of 0.75\n",
      "Iteration 3907: with minibatch training loss = 0.618 and accuracy of 0.83\n",
      "Iteration 3908: with minibatch training loss = 0.889 and accuracy of 0.8\n",
      "Iteration 3909: with minibatch training loss = 0.956 and accuracy of 0.73\n",
      "Iteration 3910: with minibatch training loss = 0.919 and accuracy of 0.77\n",
      "Iteration 3911: with minibatch training loss = 0.783 and accuracy of 0.8\n",
      "Iteration 3912: with minibatch training loss = 0.95 and accuracy of 0.73\n",
      "Iteration 3913: with minibatch training loss = 0.843 and accuracy of 0.8\n",
      "Iteration 3914: with minibatch training loss = 0.787 and accuracy of 0.81\n",
      "Iteration 3915: with minibatch training loss = 0.947 and accuracy of 0.75\n",
      "Iteration 3916: with minibatch training loss = 1.23 and accuracy of 0.67\n",
      "Iteration 3917: with minibatch training loss = 0.802 and accuracy of 0.78\n",
      "Iteration 3918: with minibatch training loss = 0.995 and accuracy of 0.72\n",
      "Iteration 3919: with minibatch training loss = 0.918 and accuracy of 0.73\n",
      "Iteration 3920: with minibatch training loss = 0.905 and accuracy of 0.75\n",
      "Iteration 3921: with minibatch training loss = 1.04 and accuracy of 0.7\n",
      "Iteration 3922: with minibatch training loss = 1.02 and accuracy of 0.7\n",
      "Iteration 3923: with minibatch training loss = 1.01 and accuracy of 0.69\n",
      "Iteration 3924: with minibatch training loss = 1.31 and accuracy of 0.69\n",
      "Iteration 3925: with minibatch training loss = 1.05 and accuracy of 0.7\n",
      "Iteration 3926: with minibatch training loss = 1.29 and accuracy of 0.64\n",
      "Iteration 3927: with minibatch training loss = 0.802 and accuracy of 0.78\n",
      "Iteration 3928: with minibatch training loss = 0.848 and accuracy of 0.8\n",
      "Iteration 3929: with minibatch training loss = 0.849 and accuracy of 0.73\n",
      "Iteration 3930: with minibatch training loss = 0.941 and accuracy of 0.77\n",
      "Iteration 3931: with minibatch training loss = 0.913 and accuracy of 0.77\n",
      "Iteration 3932: with minibatch training loss = 1 and accuracy of 0.72\n",
      "Iteration 3933: with minibatch training loss = 1.24 and accuracy of 0.64\n",
      "Iteration 3934: with minibatch training loss = 0.831 and accuracy of 0.73\n",
      "Iteration 3935: with minibatch training loss = 0.846 and accuracy of 0.75\n",
      "Iteration 3936: with minibatch training loss = 0.821 and accuracy of 0.78\n",
      "Iteration 3937: with minibatch training loss = 0.825 and accuracy of 0.81\n",
      "Iteration 3938: with minibatch training loss = 0.96 and accuracy of 0.75\n",
      "Iteration 3939: with minibatch training loss = 0.989 and accuracy of 0.77\n",
      "Iteration 3940: with minibatch training loss = 0.98 and accuracy of 0.73\n",
      "Iteration 3941: with minibatch training loss = 0.79 and accuracy of 0.77\n",
      "Iteration 3942: with minibatch training loss = 0.753 and accuracy of 0.77\n",
      "Iteration 3943: with minibatch training loss = 0.941 and accuracy of 0.75\n",
      "Iteration 3944: with minibatch training loss = 0.797 and accuracy of 0.78\n",
      "Iteration 3945: with minibatch training loss = 1 and accuracy of 0.7\n",
      "Iteration 3946: with minibatch training loss = 0.569 and accuracy of 0.88\n",
      "Iteration 3947: with minibatch training loss = 1.07 and accuracy of 0.66\n",
      "Iteration 3948: with minibatch training loss = 0.881 and accuracy of 0.73\n",
      "Iteration 3949: with minibatch training loss = 0.894 and accuracy of 0.73\n",
      "Iteration 3950: with minibatch training loss = 1.06 and accuracy of 0.7\n",
      "Iteration 3951: with minibatch training loss = 0.579 and accuracy of 0.84\n",
      "Iteration 3952: with minibatch training loss = 1.09 and accuracy of 0.7\n",
      "Iteration 3953: with minibatch training loss = 1.07 and accuracy of 0.69\n",
      "Iteration 3954: with minibatch training loss = 1.05 and accuracy of 0.69\n",
      "Iteration 3955: with minibatch training loss = 0.796 and accuracy of 0.8\n",
      "Iteration 3956: with minibatch training loss = 1 and accuracy of 0.73\n",
      "Iteration 3957: with minibatch training loss = 0.998 and accuracy of 0.73\n",
      "Iteration 3958: with minibatch training loss = 1.04 and accuracy of 0.75\n",
      "Iteration 3959: with minibatch training loss = 1.05 and accuracy of 0.7\n",
      "Iteration 3960: with minibatch training loss = 0.816 and accuracy of 0.78\n",
      "Iteration 3961: with minibatch training loss = 1.03 and accuracy of 0.72\n",
      "Iteration 3962: with minibatch training loss = 1.03 and accuracy of 0.72\n",
      "Iteration 3963: with minibatch training loss = 0.989 and accuracy of 0.73\n",
      "Iteration 3964: with minibatch training loss = 0.8 and accuracy of 0.8\n",
      "Iteration 3965: with minibatch training loss = 0.842 and accuracy of 0.8\n",
      "Iteration 3966: with minibatch training loss = 1.22 and accuracy of 0.69\n",
      "Iteration 3967: with minibatch training loss = 0.907 and accuracy of 0.77\n",
      "Iteration 3968: with minibatch training loss = 0.851 and accuracy of 0.77\n",
      "Iteration 3969: with minibatch training loss = 0.93 and accuracy of 0.75\n",
      "Iteration 3970: with minibatch training loss = 1.11 and accuracy of 0.69\n",
      "Iteration 3971: with minibatch training loss = 0.843 and accuracy of 0.73\n",
      "Iteration 3972: with minibatch training loss = 0.909 and accuracy of 0.75\n",
      "Iteration 3973: with minibatch training loss = 0.807 and accuracy of 0.78\n",
      "Iteration 3974: with minibatch training loss = 0.896 and accuracy of 0.75\n",
      "Iteration 3975: with minibatch training loss = 0.911 and accuracy of 0.77\n",
      "Iteration 3976: with minibatch training loss = 1.25 and accuracy of 0.66\n",
      "Iteration 3977: with minibatch training loss = 1.01 and accuracy of 0.7\n",
      "Iteration 3978: with minibatch training loss = 0.912 and accuracy of 0.75\n",
      "Iteration 3979: with minibatch training loss = 1.11 and accuracy of 0.7\n",
      "Iteration 3980: with minibatch training loss = 0.778 and accuracy of 0.8\n",
      "Iteration 3981: with minibatch training loss = 0.944 and accuracy of 0.75\n",
      "Iteration 3982: with minibatch training loss = 1.14 and accuracy of 0.7\n",
      "Iteration 3983: with minibatch training loss = 0.818 and accuracy of 0.81\n",
      "Iteration 3984: with minibatch training loss = 0.813 and accuracy of 0.77\n",
      "Iteration 3985: with minibatch training loss = 0.528 and accuracy of 0.91\n",
      "Iteration 3986: with minibatch training loss = 0.872 and accuracy of 0.75\n",
      "Iteration 3987: with minibatch training loss = 0.729 and accuracy of 0.84\n",
      "Iteration 3988: with minibatch training loss = 1.05 and accuracy of 0.73\n",
      "Iteration 3989: with minibatch training loss = 0.892 and accuracy of 0.73\n",
      "Iteration 3990: with minibatch training loss = 0.863 and accuracy of 0.78\n",
      "Iteration 3991: with minibatch training loss = 0.799 and accuracy of 0.8\n",
      "Iteration 3992: with minibatch training loss = 0.923 and accuracy of 0.75\n",
      "Iteration 3993: with minibatch training loss = 0.717 and accuracy of 0.81\n",
      "Iteration 3994: with minibatch training loss = 0.619 and accuracy of 0.86\n",
      "Iteration 3995: with minibatch training loss = 0.724 and accuracy of 0.84\n",
      "Iteration 3996: with minibatch training loss = 1.02 and accuracy of 0.73\n",
      "Iteration 3997: with minibatch training loss = 0.867 and accuracy of 0.75\n",
      "Iteration 3998: with minibatch training loss = 0.716 and accuracy of 0.83\n",
      "Iteration 3999: with minibatch training loss = 0.97 and accuracy of 0.73\n",
      "Iteration 4000: with minibatch training loss = 1.01 and accuracy of 0.72\n",
      "Iteration 4001: with minibatch training loss = 1.09 and accuracy of 0.72\n",
      "Iteration 4002: with minibatch training loss = 1.14 and accuracy of 0.69\n",
      "Iteration 4003: with minibatch training loss = 0.95 and accuracy of 0.77\n",
      "Iteration 4004: with minibatch training loss = 0.749 and accuracy of 0.78\n",
      "Iteration 4005: with minibatch training loss = 1.14 and accuracy of 0.66\n",
      "Iteration 4006: with minibatch training loss = 0.819 and accuracy of 0.8\n",
      "Iteration 4007: with minibatch training loss = 0.946 and accuracy of 0.75\n",
      "Iteration 4008: with minibatch training loss = 0.761 and accuracy of 0.8\n",
      "Iteration 4009: with minibatch training loss = 1.02 and accuracy of 0.73\n",
      "Iteration 4010: with minibatch training loss = 0.799 and accuracy of 0.77\n",
      "Iteration 4011: with minibatch training loss = 0.864 and accuracy of 0.75\n",
      "Iteration 4012: with minibatch training loss = 0.96 and accuracy of 0.7\n",
      "Iteration 4013: with minibatch training loss = 1.28 and accuracy of 0.66\n",
      "Iteration 4014: with minibatch training loss = 1.22 and accuracy of 0.62\n",
      "Iteration 4015: with minibatch training loss = 0.708 and accuracy of 0.83\n",
      "Iteration 4016: with minibatch training loss = 1.04 and accuracy of 0.7\n",
      "Iteration 4017: with minibatch training loss = 0.746 and accuracy of 0.83\n",
      "Iteration 4018: with minibatch training loss = 1.06 and accuracy of 0.67\n",
      "Iteration 4019: with minibatch training loss = 0.917 and accuracy of 0.7\n",
      "Iteration 4020: with minibatch training loss = 1.03 and accuracy of 0.7\n",
      "Iteration 4021: with minibatch training loss = 1.05 and accuracy of 0.72\n",
      "Iteration 4022: with minibatch training loss = 0.769 and accuracy of 0.8\n",
      "Iteration 4023: with minibatch training loss = 0.903 and accuracy of 0.75\n",
      "Iteration 4024: with minibatch training loss = 0.998 and accuracy of 0.72\n",
      "Iteration 4025: with minibatch training loss = 0.823 and accuracy of 0.77\n",
      "Iteration 4026: with minibatch training loss = 0.96 and accuracy of 0.73\n",
      "Iteration 4027: with minibatch training loss = 1.26 and accuracy of 0.7\n",
      "Iteration 4028: with minibatch training loss = 0.995 and accuracy of 0.7\n",
      "Iteration 4029: with minibatch training loss = 1.01 and accuracy of 0.72\n",
      "Iteration 4030: with minibatch training loss = 1.16 and accuracy of 0.67\n",
      "Iteration 4031: with minibatch training loss = 0.915 and accuracy of 0.73\n",
      "Iteration 4032: with minibatch training loss = 1 and accuracy of 0.72\n",
      "Iteration 4033: with minibatch training loss = 0.952 and accuracy of 0.73\n",
      "Iteration 4034: with minibatch training loss = 0.939 and accuracy of 0.72\n",
      "Iteration 4035: with minibatch training loss = 0.905 and accuracy of 0.78\n",
      "Iteration 4036: with minibatch training loss = 0.779 and accuracy of 0.78\n",
      "Iteration 4037: with minibatch training loss = 1.12 and accuracy of 0.73\n",
      "Iteration 4038: with minibatch training loss = 1.28 and accuracy of 0.66\n",
      "Iteration 4039: with minibatch training loss = 0.906 and accuracy of 0.78\n",
      "Iteration 4040: with minibatch training loss = 0.994 and accuracy of 0.73\n",
      "Iteration 4041: with minibatch training loss = 0.779 and accuracy of 0.77\n",
      "Iteration 4042: with minibatch training loss = 0.874 and accuracy of 0.81\n",
      "Iteration 4043: with minibatch training loss = 1.06 and accuracy of 0.69\n",
      "Iteration 4044: with minibatch training loss = 1.09 and accuracy of 0.69\n",
      "Iteration 4045: with minibatch training loss = 1.12 and accuracy of 0.72\n",
      "Iteration 4046: with minibatch training loss = 1.19 and accuracy of 0.66\n",
      "Iteration 4047: with minibatch training loss = 0.925 and accuracy of 0.72\n",
      "Iteration 4048: with minibatch training loss = 0.941 and accuracy of 0.75\n",
      "Iteration 4049: with minibatch training loss = 0.904 and accuracy of 0.75\n",
      "Iteration 4050: with minibatch training loss = 1.17 and accuracy of 0.67\n",
      "Iteration 4051: with minibatch training loss = 0.855 and accuracy of 0.77\n",
      "Iteration 4052: with minibatch training loss = 0.994 and accuracy of 0.73\n",
      "Iteration 4053: with minibatch training loss = 0.806 and accuracy of 0.77\n",
      "Iteration 4054: with minibatch training loss = 0.964 and accuracy of 0.77\n",
      "Iteration 4055: with minibatch training loss = 0.982 and accuracy of 0.77\n",
      "Iteration 4056: with minibatch training loss = 0.637 and accuracy of 0.84\n",
      "Iteration 4057: with minibatch training loss = 0.818 and accuracy of 0.77\n",
      "Iteration 4058: with minibatch training loss = 0.9 and accuracy of 0.75\n",
      "Iteration 4059: with minibatch training loss = 0.727 and accuracy of 0.81\n",
      "Iteration 4060: with minibatch training loss = 1.07 and accuracy of 0.72\n",
      "Iteration 4061: with minibatch training loss = 1.14 and accuracy of 0.7\n",
      "Iteration 4062: with minibatch training loss = 1.04 and accuracy of 0.73\n",
      "Iteration 4063: with minibatch training loss = 1.06 and accuracy of 0.72\n",
      "Iteration 4064: with minibatch training loss = 0.951 and accuracy of 0.75\n",
      "Iteration 4065: with minibatch training loss = 0.637 and accuracy of 0.86\n",
      "Iteration 4066: with minibatch training loss = 1.01 and accuracy of 0.73\n",
      "Iteration 4067: with minibatch training loss = 0.972 and accuracy of 0.77\n",
      "Iteration 4068: with minibatch training loss = 0.717 and accuracy of 0.81\n",
      "Iteration 4069: with minibatch training loss = 0.97 and accuracy of 0.72\n",
      "Iteration 4070: with minibatch training loss = 0.775 and accuracy of 0.81\n",
      "Iteration 4071: with minibatch training loss = 0.999 and accuracy of 0.72\n",
      "Iteration 4072: with minibatch training loss = 1.02 and accuracy of 0.7\n",
      "Iteration 4073: with minibatch training loss = 0.776 and accuracy of 0.81\n",
      "Iteration 4074: with minibatch training loss = 0.834 and accuracy of 0.81\n",
      "Iteration 4075: with minibatch training loss = 1.03 and accuracy of 0.67\n",
      "Iteration 4076: with minibatch training loss = 0.807 and accuracy of 0.75\n",
      "Iteration 4077: with minibatch training loss = 0.693 and accuracy of 0.81\n",
      "Iteration 4078: with minibatch training loss = 0.767 and accuracy of 0.8\n",
      "Iteration 4079: with minibatch training loss = 0.928 and accuracy of 0.73\n",
      "Iteration 4080: with minibatch training loss = 0.888 and accuracy of 0.75\n",
      "Iteration 4081: with minibatch training loss = 1.06 and accuracy of 0.75\n",
      "Iteration 4082: with minibatch training loss = 1.04 and accuracy of 0.72\n",
      "Iteration 4083: with minibatch training loss = 0.944 and accuracy of 0.73\n",
      "Iteration 4084: with minibatch training loss = 0.882 and accuracy of 0.81\n",
      "Iteration 4085: with minibatch training loss = 1.09 and accuracy of 0.69\n",
      "Iteration 4086: with minibatch training loss = 1.05 and accuracy of 0.72\n",
      "Iteration 4087: with minibatch training loss = 1.1 and accuracy of 0.67\n",
      "Iteration 4088: with minibatch training loss = 0.753 and accuracy of 0.78\n",
      "Iteration 4089: with minibatch training loss = 0.802 and accuracy of 0.77\n",
      "Iteration 4090: with minibatch training loss = 0.91 and accuracy of 0.75\n",
      "Iteration 4091: with minibatch training loss = 0.722 and accuracy of 0.83\n",
      "Iteration 4092: with minibatch training loss = 1.06 and accuracy of 0.7\n",
      "Iteration 4093: with minibatch training loss = 0.873 and accuracy of 0.78\n",
      "Iteration 4094: with minibatch training loss = 0.872 and accuracy of 0.77\n",
      "Iteration 4095: with minibatch training loss = 1.14 and accuracy of 0.66\n",
      "Iteration 4096: with minibatch training loss = 1 and accuracy of 0.69\n",
      "Iteration 4097: with minibatch training loss = 0.832 and accuracy of 0.77\n",
      "Iteration 4098: with minibatch training loss = 1.09 and accuracy of 0.72\n",
      "Iteration 4099: with minibatch training loss = 0.987 and accuracy of 0.73\n",
      "Iteration 4100: with minibatch training loss = 0.896 and accuracy of 0.8\n",
      "Iteration 4101: with minibatch training loss = 1.16 and accuracy of 0.69\n",
      "Iteration 4102: with minibatch training loss = 1.01 and accuracy of 0.7\n",
      "Iteration 4103: with minibatch training loss = 0.993 and accuracy of 0.75\n",
      "Iteration 4104: with minibatch training loss = 0.704 and accuracy of 0.81\n",
      "Iteration 4105: with minibatch training loss = 0.9 and accuracy of 0.77\n",
      "Iteration 4106: with minibatch training loss = 0.834 and accuracy of 0.81\n",
      "Iteration 4107: with minibatch training loss = 1.11 and accuracy of 0.7\n",
      "Iteration 4108: with minibatch training loss = 0.954 and accuracy of 0.75\n",
      "Iteration 4109: with minibatch training loss = 0.699 and accuracy of 0.83\n",
      "Iteration 4110: with minibatch training loss = 0.954 and accuracy of 0.78\n",
      "Iteration 4111: with minibatch training loss = 1.19 and accuracy of 0.64\n",
      "Iteration 4112: with minibatch training loss = 0.945 and accuracy of 0.73\n",
      "Iteration 4113: with minibatch training loss = 0.743 and accuracy of 0.8\n",
      "Iteration 4114: with minibatch training loss = 0.897 and accuracy of 0.78\n",
      "Iteration 4115: with minibatch training loss = 0.87 and accuracy of 0.7\n",
      "Iteration 4116: with minibatch training loss = 0.973 and accuracy of 0.72\n",
      "Iteration 4117: with minibatch training loss = 0.91 and accuracy of 0.77\n",
      "Iteration 4118: with minibatch training loss = 1.01 and accuracy of 0.73\n",
      "Iteration 4119: with minibatch training loss = 1.01 and accuracy of 0.73\n",
      "Iteration 4120: with minibatch training loss = 0.866 and accuracy of 0.77\n",
      "Iteration 4121: with minibatch training loss = 0.808 and accuracy of 0.8\n",
      "Iteration 4122: with minibatch training loss = 0.72 and accuracy of 0.81\n",
      "Iteration 4123: with minibatch training loss = 0.687 and accuracy of 0.8\n",
      "Iteration 4124: with minibatch training loss = 1.03 and accuracy of 0.7\n",
      "Iteration 4125: with minibatch training loss = 1.1 and accuracy of 0.7\n",
      "Iteration 4126: with minibatch training loss = 1.12 and accuracy of 0.7\n",
      "Iteration 4127: with minibatch training loss = 0.841 and accuracy of 0.77\n",
      "Iteration 4128: with minibatch training loss = 0.83 and accuracy of 0.77\n",
      "Iteration 4129: with minibatch training loss = 0.87 and accuracy of 0.75\n",
      "Iteration 4130: with minibatch training loss = 0.678 and accuracy of 0.83\n",
      "Iteration 4131: with minibatch training loss = 0.903 and accuracy of 0.73\n",
      "Iteration 4132: with minibatch training loss = 1.18 and accuracy of 0.61\n",
      "Iteration 4133: with minibatch training loss = 1.33 and accuracy of 0.58\n",
      "Iteration 4134: with minibatch training loss = 1.26 and accuracy of 0.61\n",
      "Iteration 4135: with minibatch training loss = 0.996 and accuracy of 0.75\n",
      "Iteration 4136: with minibatch training loss = 0.758 and accuracy of 0.78\n",
      "Iteration 4137: with minibatch training loss = 0.896 and accuracy of 0.73\n",
      "Iteration 4138: with minibatch training loss = 0.95 and accuracy of 0.75\n",
      "Iteration 4139: with minibatch training loss = 0.841 and accuracy of 0.78\n",
      "Iteration 4140: with minibatch training loss = 0.874 and accuracy of 0.77\n",
      "Iteration 4141: with minibatch training loss = 0.868 and accuracy of 0.75\n",
      "Iteration 4142: with minibatch training loss = 1.04 and accuracy of 0.7\n",
      "Iteration 4143: with minibatch training loss = 1.15 and accuracy of 0.69\n",
      "Iteration 4144: with minibatch training loss = 0.926 and accuracy of 0.77\n",
      "Iteration 4145: with minibatch training loss = 0.809 and accuracy of 0.8\n",
      "Iteration 4146: with minibatch training loss = 0.867 and accuracy of 0.81\n",
      "Iteration 4147: with minibatch training loss = 0.876 and accuracy of 0.75\n",
      "Iteration 4148: with minibatch training loss = 1.05 and accuracy of 0.75\n",
      "Iteration 4149: with minibatch training loss = 0.723 and accuracy of 0.81\n",
      "Iteration 4150: with minibatch training loss = 0.725 and accuracy of 0.8\n",
      "Iteration 4151: with minibatch training loss = 0.916 and accuracy of 0.78\n",
      "Iteration 4152: with minibatch training loss = 1.01 and accuracy of 0.72\n",
      "Iteration 4153: with minibatch training loss = 0.796 and accuracy of 0.8\n",
      "Iteration 4154: with minibatch training loss = 0.837 and accuracy of 0.78\n",
      "Iteration 4155: with minibatch training loss = 1.19 and accuracy of 0.7\n",
      "Iteration 4156: with minibatch training loss = 0.907 and accuracy of 0.77\n",
      "Iteration 4157: with minibatch training loss = 0.905 and accuracy of 0.73\n",
      "Iteration 4158: with minibatch training loss = 0.911 and accuracy of 0.77\n",
      "Iteration 4159: with minibatch training loss = 0.934 and accuracy of 0.75\n",
      "Iteration 4160: with minibatch training loss = 0.735 and accuracy of 0.83\n",
      "Iteration 4161: with minibatch training loss = 1.06 and accuracy of 0.7\n",
      "Iteration 4162: with minibatch training loss = 1.09 and accuracy of 0.69\n",
      "Validation loss: 0.33264855\n",
      "Model's weights saved at /Users/nhat/Documents/Projects/LetterClassifier/weights/model_se.ckpt\n",
      "Epoch 3, Overall loss = 0.966 and accuracy of 0.741\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEJQSASIIgiAo5V8vJ72VAAVRcAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x121066518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4164: with minibatch training loss = 0.895 and accuracy of 0.73\n",
      "Iteration 4165: with minibatch training loss = 1.15 and accuracy of 0.69\n",
      "Iteration 4166: with minibatch training loss = 0.746 and accuracy of 0.81\n",
      "Iteration 4167: with minibatch training loss = 0.982 and accuracy of 0.72\n",
      "Iteration 4168: with minibatch training loss = 0.74 and accuracy of 0.83\n",
      "Iteration 4169: with minibatch training loss = 0.981 and accuracy of 0.75\n",
      "Iteration 4170: with minibatch training loss = 1.02 and accuracy of 0.73\n",
      "Iteration 4171: with minibatch training loss = 0.794 and accuracy of 0.78\n",
      "Iteration 4172: with minibatch training loss = 1.04 and accuracy of 0.7\n",
      "Iteration 4173: with minibatch training loss = 0.884 and accuracy of 0.77\n",
      "Iteration 4174: with minibatch training loss = 1.08 and accuracy of 0.69\n",
      "Iteration 4175: with minibatch training loss = 0.761 and accuracy of 0.83\n",
      "Iteration 4176: with minibatch training loss = 0.915 and accuracy of 0.72\n",
      "Iteration 4177: with minibatch training loss = 0.956 and accuracy of 0.75\n",
      "Iteration 4178: with minibatch training loss = 0.978 and accuracy of 0.73\n",
      "Iteration 4179: with minibatch training loss = 0.754 and accuracy of 0.83\n",
      "Iteration 4180: with minibatch training loss = 1.02 and accuracy of 0.69\n",
      "Iteration 4181: with minibatch training loss = 0.774 and accuracy of 0.8\n",
      "Iteration 4182: with minibatch training loss = 0.688 and accuracy of 0.81\n",
      "Iteration 4183: with minibatch training loss = 0.872 and accuracy of 0.75\n",
      "Iteration 4184: with minibatch training loss = 0.642 and accuracy of 0.86\n",
      "Iteration 4185: with minibatch training loss = 0.668 and accuracy of 0.83\n",
      "Iteration 4186: with minibatch training loss = 1.39 and accuracy of 0.62\n",
      "Iteration 4187: with minibatch training loss = 0.892 and accuracy of 0.78\n",
      "Iteration 4188: with minibatch training loss = 0.903 and accuracy of 0.73\n",
      "Iteration 4189: with minibatch training loss = 0.906 and accuracy of 0.73\n",
      "Iteration 4190: with minibatch training loss = 1.09 and accuracy of 0.7\n",
      "Iteration 4191: with minibatch training loss = 0.92 and accuracy of 0.77\n",
      "Iteration 4192: with minibatch training loss = 0.895 and accuracy of 0.78\n",
      "Iteration 4193: with minibatch training loss = 0.62 and accuracy of 0.86\n",
      "Iteration 4194: with minibatch training loss = 0.967 and accuracy of 0.77\n",
      "Iteration 4195: with minibatch training loss = 0.967 and accuracy of 0.73\n",
      "Iteration 4196: with minibatch training loss = 0.804 and accuracy of 0.8\n",
      "Iteration 4197: with minibatch training loss = 1.01 and accuracy of 0.69\n",
      "Iteration 4198: with minibatch training loss = 0.997 and accuracy of 0.7\n",
      "Iteration 4199: with minibatch training loss = 0.939 and accuracy of 0.72\n",
      "Iteration 4200: with minibatch training loss = 0.802 and accuracy of 0.77\n",
      "Iteration 4201: with minibatch training loss = 1.14 and accuracy of 0.67\n",
      "Iteration 4202: with minibatch training loss = 1.02 and accuracy of 0.72\n",
      "Iteration 4203: with minibatch training loss = 0.732 and accuracy of 0.84\n",
      "Iteration 4204: with minibatch training loss = 0.867 and accuracy of 0.81\n",
      "Iteration 4205: with minibatch training loss = 1.08 and accuracy of 0.67\n",
      "Iteration 4206: with minibatch training loss = 1.07 and accuracy of 0.72\n",
      "Iteration 4207: with minibatch training loss = 0.937 and accuracy of 0.77\n",
      "Iteration 4208: with minibatch training loss = 0.977 and accuracy of 0.72\n",
      "Iteration 4209: with minibatch training loss = 0.832 and accuracy of 0.73\n",
      "Iteration 4210: with minibatch training loss = 1.08 and accuracy of 0.7\n",
      "Iteration 4211: with minibatch training loss = 0.944 and accuracy of 0.75\n",
      "Iteration 4212: with minibatch training loss = 1.24 and accuracy of 0.66\n",
      "Iteration 4213: with minibatch training loss = 0.532 and accuracy of 0.88\n",
      "Iteration 4214: with minibatch training loss = 0.748 and accuracy of 0.83\n",
      "Iteration 4215: with minibatch training loss = 0.838 and accuracy of 0.75\n",
      "Iteration 4216: with minibatch training loss = 1.03 and accuracy of 0.7\n",
      "Iteration 4217: with minibatch training loss = 0.905 and accuracy of 0.75\n",
      "Iteration 4218: with minibatch training loss = 1.02 and accuracy of 0.72\n",
      "Iteration 4219: with minibatch training loss = 1.02 and accuracy of 0.73\n",
      "Iteration 4220: with minibatch training loss = 0.791 and accuracy of 0.8\n",
      "Iteration 4221: with minibatch training loss = 1.15 and accuracy of 0.7\n",
      "Iteration 4222: with minibatch training loss = 0.914 and accuracy of 0.75\n",
      "Iteration 4223: with minibatch training loss = 1.02 and accuracy of 0.77\n",
      "Iteration 4224: with minibatch training loss = 0.866 and accuracy of 0.73\n",
      "Iteration 4225: with minibatch training loss = 0.811 and accuracy of 0.75\n",
      "Iteration 4226: with minibatch training loss = 0.915 and accuracy of 0.75\n",
      "Iteration 4227: with minibatch training loss = 0.616 and accuracy of 0.84\n",
      "Iteration 4228: with minibatch training loss = 0.822 and accuracy of 0.78\n",
      "Iteration 4229: with minibatch training loss = 1.09 and accuracy of 0.7\n",
      "Iteration 4230: with minibatch training loss = 0.872 and accuracy of 0.77\n",
      "Iteration 4231: with minibatch training loss = 1.01 and accuracy of 0.69\n",
      "Iteration 4232: with minibatch training loss = 0.845 and accuracy of 0.75\n",
      "Iteration 4233: with minibatch training loss = 1.11 and accuracy of 0.72\n",
      "Iteration 4234: with minibatch training loss = 0.906 and accuracy of 0.77\n",
      "Iteration 4235: with minibatch training loss = 0.869 and accuracy of 0.77\n",
      "Iteration 4236: with minibatch training loss = 0.926 and accuracy of 0.8\n",
      "Iteration 4237: with minibatch training loss = 0.975 and accuracy of 0.75\n",
      "Iteration 4238: with minibatch training loss = 1.06 and accuracy of 0.67\n",
      "Iteration 4239: with minibatch training loss = 0.822 and accuracy of 0.78\n",
      "Iteration 4240: with minibatch training loss = 0.916 and accuracy of 0.73\n",
      "Iteration 4241: with minibatch training loss = 0.881 and accuracy of 0.78\n",
      "Iteration 4242: with minibatch training loss = 1.17 and accuracy of 0.69\n",
      "Iteration 4243: with minibatch training loss = 0.917 and accuracy of 0.73\n",
      "Iteration 4244: with minibatch training loss = 0.851 and accuracy of 0.77\n",
      "Iteration 4245: with minibatch training loss = 0.909 and accuracy of 0.77\n",
      "Iteration 4246: with minibatch training loss = 0.826 and accuracy of 0.77\n",
      "Iteration 4247: with minibatch training loss = 1.02 and accuracy of 0.73\n",
      "Iteration 4248: with minibatch training loss = 0.554 and accuracy of 0.88\n",
      "Iteration 4249: with minibatch training loss = 1.01 and accuracy of 0.72\n",
      "Iteration 4250: with minibatch training loss = 0.74 and accuracy of 0.81\n",
      "Iteration 4251: with minibatch training loss = 0.714 and accuracy of 0.83\n",
      "Iteration 4252: with minibatch training loss = 0.732 and accuracy of 0.83\n",
      "Iteration 4253: with minibatch training loss = 1 and accuracy of 0.73\n",
      "Iteration 4254: with minibatch training loss = 1.19 and accuracy of 0.67\n",
      "Iteration 4255: with minibatch training loss = 0.854 and accuracy of 0.77\n",
      "Iteration 4256: with minibatch training loss = 0.882 and accuracy of 0.77\n",
      "Iteration 4257: with minibatch training loss = 0.861 and accuracy of 0.8\n",
      "Iteration 4258: with minibatch training loss = 0.781 and accuracy of 0.81\n",
      "Iteration 4259: with minibatch training loss = 0.721 and accuracy of 0.81\n",
      "Iteration 4260: with minibatch training loss = 0.965 and accuracy of 0.75\n",
      "Iteration 4261: with minibatch training loss = 0.959 and accuracy of 0.73\n",
      "Iteration 4262: with minibatch training loss = 1.05 and accuracy of 0.69\n",
      "Iteration 4263: with minibatch training loss = 0.777 and accuracy of 0.78\n",
      "Iteration 4264: with minibatch training loss = 1.29 and accuracy of 0.62\n",
      "Iteration 4265: with minibatch training loss = 0.765 and accuracy of 0.8\n",
      "Iteration 4266: with minibatch training loss = 0.889 and accuracy of 0.78\n",
      "Iteration 4267: with minibatch training loss = 1.11 and accuracy of 0.7\n",
      "Iteration 4268: with minibatch training loss = 1.07 and accuracy of 0.7\n",
      "Iteration 4269: with minibatch training loss = 0.711 and accuracy of 0.83\n",
      "Iteration 4270: with minibatch training loss = 0.524 and accuracy of 0.86\n",
      "Iteration 4271: with minibatch training loss = 0.89 and accuracy of 0.75\n",
      "Iteration 4272: with minibatch training loss = 1.2 and accuracy of 0.64\n",
      "Iteration 4273: with minibatch training loss = 1.07 and accuracy of 0.67\n",
      "Iteration 4274: with minibatch training loss = 0.923 and accuracy of 0.77\n",
      "Iteration 4275: with minibatch training loss = 0.992 and accuracy of 0.73\n",
      "Iteration 4276: with minibatch training loss = 1.13 and accuracy of 0.72\n",
      "Iteration 4277: with minibatch training loss = 0.632 and accuracy of 0.83\n",
      "Iteration 4278: with minibatch training loss = 1.04 and accuracy of 0.7\n",
      "Iteration 4279: with minibatch training loss = 0.744 and accuracy of 0.77\n",
      "Iteration 4280: with minibatch training loss = 0.963 and accuracy of 0.75\n",
      "Iteration 4281: with minibatch training loss = 0.852 and accuracy of 0.78\n",
      "Iteration 4282: with minibatch training loss = 0.936 and accuracy of 0.72\n",
      "Iteration 4283: with minibatch training loss = 0.827 and accuracy of 0.8\n",
      "Iteration 4284: with minibatch training loss = 0.628 and accuracy of 0.84\n",
      "Iteration 4285: with minibatch training loss = 1.29 and accuracy of 0.62\n",
      "Iteration 4286: with minibatch training loss = 0.81 and accuracy of 0.8\n",
      "Iteration 4287: with minibatch training loss = 0.941 and accuracy of 0.73\n",
      "Iteration 4288: with minibatch training loss = 0.772 and accuracy of 0.81\n",
      "Iteration 4289: with minibatch training loss = 0.847 and accuracy of 0.78\n",
      "Iteration 4290: with minibatch training loss = 1.05 and accuracy of 0.73\n",
      "Iteration 4291: with minibatch training loss = 1.04 and accuracy of 0.69\n",
      "Iteration 4292: with minibatch training loss = 0.766 and accuracy of 0.8\n",
      "Iteration 4293: with minibatch training loss = 0.965 and accuracy of 0.75\n",
      "Iteration 4294: with minibatch training loss = 1.08 and accuracy of 0.69\n",
      "Iteration 4295: with minibatch training loss = 0.978 and accuracy of 0.73\n",
      "Iteration 4296: with minibatch training loss = 0.649 and accuracy of 0.83\n",
      "Iteration 4297: with minibatch training loss = 1.11 and accuracy of 0.69\n",
      "Iteration 4298: with minibatch training loss = 0.965 and accuracy of 0.73\n",
      "Iteration 4299: with minibatch training loss = 0.704 and accuracy of 0.83\n",
      "Iteration 4300: with minibatch training loss = 0.992 and accuracy of 0.73\n",
      "Iteration 4301: with minibatch training loss = 0.897 and accuracy of 0.78\n",
      "Iteration 4302: with minibatch training loss = 0.944 and accuracy of 0.75\n",
      "Iteration 4303: with minibatch training loss = 1.23 and accuracy of 0.66\n",
      "Iteration 4304: with minibatch training loss = 1.05 and accuracy of 0.7\n",
      "Iteration 4305: with minibatch training loss = 0.861 and accuracy of 0.8\n",
      "Iteration 4306: with minibatch training loss = 1.15 and accuracy of 0.72\n",
      "Iteration 4307: with minibatch training loss = 1.22 and accuracy of 0.64\n",
      "Iteration 4308: with minibatch training loss = 0.673 and accuracy of 0.84\n",
      "Iteration 4309: with minibatch training loss = 0.855 and accuracy of 0.75\n",
      "Iteration 4310: with minibatch training loss = 0.896 and accuracy of 0.78\n",
      "Iteration 4311: with minibatch training loss = 0.735 and accuracy of 0.83\n",
      "Iteration 4312: with minibatch training loss = 1.03 and accuracy of 0.7\n",
      "Iteration 4313: with minibatch training loss = 0.883 and accuracy of 0.73\n",
      "Iteration 4314: with minibatch training loss = 1.08 and accuracy of 0.73\n",
      "Iteration 4315: with minibatch training loss = 0.644 and accuracy of 0.83\n",
      "Iteration 4316: with minibatch training loss = 0.848 and accuracy of 0.75\n",
      "Iteration 4317: with minibatch training loss = 0.776 and accuracy of 0.8\n",
      "Iteration 4318: with minibatch training loss = 0.857 and accuracy of 0.77\n",
      "Iteration 4319: with minibatch training loss = 0.798 and accuracy of 0.8\n",
      "Iteration 4320: with minibatch training loss = 0.708 and accuracy of 0.81\n",
      "Iteration 4321: with minibatch training loss = 0.548 and accuracy of 0.86\n",
      "Iteration 4322: with minibatch training loss = 1.18 and accuracy of 0.69\n",
      "Iteration 4323: with minibatch training loss = 0.875 and accuracy of 0.75\n",
      "Iteration 4324: with minibatch training loss = 0.64 and accuracy of 0.84\n",
      "Iteration 4325: with minibatch training loss = 0.823 and accuracy of 0.8\n",
      "Iteration 4326: with minibatch training loss = 0.957 and accuracy of 0.73\n",
      "Iteration 4327: with minibatch training loss = 0.952 and accuracy of 0.73\n",
      "Iteration 4328: with minibatch training loss = 0.833 and accuracy of 0.78\n",
      "Iteration 4329: with minibatch training loss = 0.977 and accuracy of 0.73\n",
      "Iteration 4330: with minibatch training loss = 1.16 and accuracy of 0.66\n",
      "Iteration 4331: with minibatch training loss = 0.795 and accuracy of 0.77\n",
      "Iteration 4332: with minibatch training loss = 0.813 and accuracy of 0.8\n",
      "Iteration 4333: with minibatch training loss = 1.03 and accuracy of 0.7\n",
      "Iteration 4334: with minibatch training loss = 0.916 and accuracy of 0.78\n",
      "Iteration 4335: with minibatch training loss = 0.746 and accuracy of 0.8\n",
      "Iteration 4336: with minibatch training loss = 0.961 and accuracy of 0.75\n",
      "Iteration 4337: with minibatch training loss = 1.02 and accuracy of 0.72\n",
      "Iteration 4338: with minibatch training loss = 0.759 and accuracy of 0.78\n",
      "Iteration 4339: with minibatch training loss = 0.696 and accuracy of 0.83\n",
      "Iteration 4340: with minibatch training loss = 0.727 and accuracy of 0.83\n",
      "Iteration 4341: with minibatch training loss = 0.91 and accuracy of 0.77\n",
      "Iteration 4342: with minibatch training loss = 0.962 and accuracy of 0.75\n",
      "Iteration 4343: with minibatch training loss = 0.93 and accuracy of 0.75\n",
      "Iteration 4344: with minibatch training loss = 0.937 and accuracy of 0.73\n",
      "Iteration 4345: with minibatch training loss = 0.777 and accuracy of 0.8\n",
      "Iteration 4346: with minibatch training loss = 0.97 and accuracy of 0.75\n",
      "Iteration 4347: with minibatch training loss = 1.18 and accuracy of 0.66\n",
      "Iteration 4348: with minibatch training loss = 1.06 and accuracy of 0.69\n",
      "Iteration 4349: with minibatch training loss = 0.923 and accuracy of 0.78\n",
      "Iteration 4350: with minibatch training loss = 1.37 and accuracy of 0.64\n",
      "Iteration 4351: with minibatch training loss = 0.937 and accuracy of 0.75\n",
      "Iteration 4352: with minibatch training loss = 0.765 and accuracy of 0.83\n",
      "Iteration 4353: with minibatch training loss = 1.09 and accuracy of 0.72\n",
      "Iteration 4354: with minibatch training loss = 1.11 and accuracy of 0.7\n",
      "Iteration 4355: with minibatch training loss = 0.892 and accuracy of 0.73\n",
      "Iteration 4356: with minibatch training loss = 1.16 and accuracy of 0.67\n",
      "Iteration 4357: with minibatch training loss = 0.738 and accuracy of 0.84\n",
      "Iteration 4358: with minibatch training loss = 0.733 and accuracy of 0.81\n",
      "Iteration 4359: with minibatch training loss = 1.3 and accuracy of 0.61\n",
      "Iteration 4360: with minibatch training loss = 0.741 and accuracy of 0.83\n",
      "Iteration 4361: with minibatch training loss = 0.789 and accuracy of 0.78\n",
      "Iteration 4362: with minibatch training loss = 0.788 and accuracy of 0.8\n",
      "Iteration 4363: with minibatch training loss = 0.943 and accuracy of 0.73\n",
      "Iteration 4364: with minibatch training loss = 0.932 and accuracy of 0.72\n",
      "Iteration 4365: with minibatch training loss = 0.949 and accuracy of 0.73\n",
      "Iteration 4366: with minibatch training loss = 0.933 and accuracy of 0.73\n",
      "Iteration 4367: with minibatch training loss = 0.976 and accuracy of 0.73\n",
      "Iteration 4368: with minibatch training loss = 0.931 and accuracy of 0.73\n",
      "Iteration 4369: with minibatch training loss = 0.737 and accuracy of 0.8\n",
      "Iteration 4370: with minibatch training loss = 0.935 and accuracy of 0.72\n",
      "Iteration 4371: with minibatch training loss = 0.766 and accuracy of 0.8\n",
      "Iteration 4372: with minibatch training loss = 1.14 and accuracy of 0.73\n",
      "Iteration 4373: with minibatch training loss = 1.11 and accuracy of 0.7\n",
      "Iteration 4374: with minibatch training loss = 0.683 and accuracy of 0.8\n",
      "Iteration 4375: with minibatch training loss = 1.01 and accuracy of 0.72\n",
      "Iteration 4376: with minibatch training loss = 1.04 and accuracy of 0.69\n",
      "Iteration 4377: with minibatch training loss = 0.646 and accuracy of 0.81\n",
      "Iteration 4378: with minibatch training loss = 0.603 and accuracy of 0.88\n",
      "Iteration 4379: with minibatch training loss = 0.741 and accuracy of 0.8\n",
      "Iteration 4380: with minibatch training loss = 1.19 and accuracy of 0.67\n",
      "Iteration 4381: with minibatch training loss = 1.04 and accuracy of 0.72\n",
      "Iteration 4382: with minibatch training loss = 1.01 and accuracy of 0.73\n",
      "Iteration 4383: with minibatch training loss = 0.822 and accuracy of 0.78\n",
      "Iteration 4384: with minibatch training loss = 0.971 and accuracy of 0.75\n",
      "Iteration 4385: with minibatch training loss = 1.08 and accuracy of 0.69\n",
      "Iteration 4386: with minibatch training loss = 0.707 and accuracy of 0.78\n",
      "Iteration 4387: with minibatch training loss = 1.06 and accuracy of 0.7\n",
      "Iteration 4388: with minibatch training loss = 0.986 and accuracy of 0.7\n",
      "Iteration 4389: with minibatch training loss = 0.802 and accuracy of 0.75\n",
      "Iteration 4390: with minibatch training loss = 0.772 and accuracy of 0.8\n",
      "Iteration 4391: with minibatch training loss = 1.1 and accuracy of 0.7\n",
      "Iteration 4392: with minibatch training loss = 0.798 and accuracy of 0.77\n",
      "Iteration 4393: with minibatch training loss = 0.993 and accuracy of 0.73\n",
      "Iteration 4394: with minibatch training loss = 0.753 and accuracy of 0.81\n",
      "Iteration 4395: with minibatch training loss = 0.816 and accuracy of 0.75\n",
      "Iteration 4396: with minibatch training loss = 1.07 and accuracy of 0.67\n",
      "Iteration 4397: with minibatch training loss = 0.87 and accuracy of 0.72\n",
      "Iteration 4398: with minibatch training loss = 1.25 and accuracy of 0.66\n",
      "Iteration 4399: with minibatch training loss = 0.718 and accuracy of 0.78\n",
      "Iteration 4400: with minibatch training loss = 0.936 and accuracy of 0.77\n",
      "Iteration 4401: with minibatch training loss = 1.11 and accuracy of 0.69\n",
      "Iteration 4402: with minibatch training loss = 0.775 and accuracy of 0.77\n",
      "Iteration 4403: with minibatch training loss = 0.724 and accuracy of 0.8\n",
      "Iteration 4404: with minibatch training loss = 0.917 and accuracy of 0.73\n",
      "Iteration 4405: with minibatch training loss = 0.699 and accuracy of 0.81\n",
      "Iteration 4406: with minibatch training loss = 1.08 and accuracy of 0.72\n",
      "Iteration 4407: with minibatch training loss = 0.809 and accuracy of 0.8\n",
      "Iteration 4408: with minibatch training loss = 0.828 and accuracy of 0.78\n",
      "Iteration 4409: with minibatch training loss = 0.722 and accuracy of 0.81\n",
      "Iteration 4410: with minibatch training loss = 0.909 and accuracy of 0.78\n",
      "Iteration 4411: with minibatch training loss = 0.893 and accuracy of 0.77\n",
      "Iteration 4412: with minibatch training loss = 0.912 and accuracy of 0.75\n",
      "Iteration 4413: with minibatch training loss = 0.784 and accuracy of 0.78\n",
      "Iteration 4414: with minibatch training loss = 0.733 and accuracy of 0.83\n",
      "Iteration 4415: with minibatch training loss = 0.915 and accuracy of 0.73\n",
      "Iteration 4416: with minibatch training loss = 0.935 and accuracy of 0.72\n",
      "Iteration 4417: with minibatch training loss = 0.818 and accuracy of 0.8\n",
      "Iteration 4418: with minibatch training loss = 0.918 and accuracy of 0.72\n",
      "Iteration 4419: with minibatch training loss = 0.95 and accuracy of 0.75\n",
      "Iteration 4420: with minibatch training loss = 1.04 and accuracy of 0.7\n",
      "Iteration 4421: with minibatch training loss = 0.906 and accuracy of 0.78\n",
      "Iteration 4422: with minibatch training loss = 0.985 and accuracy of 0.73\n",
      "Iteration 4423: with minibatch training loss = 0.837 and accuracy of 0.8\n",
      "Iteration 4424: with minibatch training loss = 0.919 and accuracy of 0.75\n",
      "Iteration 4425: with minibatch training loss = 0.877 and accuracy of 0.75\n",
      "Iteration 4426: with minibatch training loss = 0.89 and accuracy of 0.75\n",
      "Iteration 4427: with minibatch training loss = 0.932 and accuracy of 0.73\n",
      "Iteration 4428: with minibatch training loss = 0.944 and accuracy of 0.75\n",
      "Iteration 4429: with minibatch training loss = 1.16 and accuracy of 0.62\n",
      "Iteration 4430: with minibatch training loss = 0.842 and accuracy of 0.77\n",
      "Iteration 4431: with minibatch training loss = 1.23 and accuracy of 0.67\n",
      "Iteration 4432: with minibatch training loss = 0.955 and accuracy of 0.73\n",
      "Iteration 4433: with minibatch training loss = 0.868 and accuracy of 0.78\n",
      "Iteration 4434: with minibatch training loss = 1.17 and accuracy of 0.67\n",
      "Iteration 4435: with minibatch training loss = 0.772 and accuracy of 0.8\n",
      "Iteration 4436: with minibatch training loss = 0.723 and accuracy of 0.81\n",
      "Iteration 4437: with minibatch training loss = 1 and accuracy of 0.7\n",
      "Iteration 4438: with minibatch training loss = 1.03 and accuracy of 0.72\n",
      "Iteration 4439: with minibatch training loss = 0.846 and accuracy of 0.75\n",
      "Iteration 4440: with minibatch training loss = 1.22 and accuracy of 0.66\n",
      "Iteration 4441: with minibatch training loss = 0.705 and accuracy of 0.81\n",
      "Iteration 4442: with minibatch training loss = 0.947 and accuracy of 0.72\n",
      "Iteration 4443: with minibatch training loss = 0.605 and accuracy of 0.84\n",
      "Iteration 4444: with minibatch training loss = 0.609 and accuracy of 0.84\n",
      "Iteration 4445: with minibatch training loss = 0.575 and accuracy of 0.84\n",
      "Iteration 4446: with minibatch training loss = 0.934 and accuracy of 0.75\n",
      "Iteration 4447: with minibatch training loss = 0.999 and accuracy of 0.72\n",
      "Iteration 4448: with minibatch training loss = 0.787 and accuracy of 0.78\n",
      "Iteration 4449: with minibatch training loss = 0.837 and accuracy of 0.77\n",
      "Iteration 4450: with minibatch training loss = 0.817 and accuracy of 0.77\n",
      "Iteration 4451: with minibatch training loss = 0.653 and accuracy of 0.84\n",
      "Iteration 4452: with minibatch training loss = 0.868 and accuracy of 0.77\n",
      "Iteration 4453: with minibatch training loss = 0.741 and accuracy of 0.84\n",
      "Iteration 4454: with minibatch training loss = 1.03 and accuracy of 0.72\n",
      "Iteration 4455: with minibatch training loss = 1.12 and accuracy of 0.64\n",
      "Iteration 4456: with minibatch training loss = 1.01 and accuracy of 0.69\n",
      "Iteration 4457: with minibatch training loss = 1.05 and accuracy of 0.72\n",
      "Iteration 4458: with minibatch training loss = 1.14 and accuracy of 0.67\n",
      "Iteration 4459: with minibatch training loss = 0.835 and accuracy of 0.77\n",
      "Iteration 4460: with minibatch training loss = 1.08 and accuracy of 0.73\n",
      "Iteration 4461: with minibatch training loss = 0.841 and accuracy of 0.75\n",
      "Iteration 4462: with minibatch training loss = 0.949 and accuracy of 0.77\n",
      "Iteration 4463: with minibatch training loss = 0.583 and accuracy of 0.89\n",
      "Iteration 4464: with minibatch training loss = 0.627 and accuracy of 0.83\n",
      "Iteration 4465: with minibatch training loss = 0.941 and accuracy of 0.73\n",
      "Iteration 4466: with minibatch training loss = 0.775 and accuracy of 0.8\n",
      "Iteration 4467: with minibatch training loss = 1.27 and accuracy of 0.67\n",
      "Iteration 4468: with minibatch training loss = 0.839 and accuracy of 0.78\n",
      "Iteration 4469: with minibatch training loss = 0.646 and accuracy of 0.83\n",
      "Iteration 4470: with minibatch training loss = 0.995 and accuracy of 0.72\n",
      "Iteration 4471: with minibatch training loss = 1.25 and accuracy of 0.64\n",
      "Iteration 4472: with minibatch training loss = 0.966 and accuracy of 0.73\n",
      "Iteration 4473: with minibatch training loss = 0.717 and accuracy of 0.78\n",
      "Iteration 4474: with minibatch training loss = 1.02 and accuracy of 0.75\n",
      "Iteration 4475: with minibatch training loss = 0.966 and accuracy of 0.73\n",
      "Iteration 4476: with minibatch training loss = 0.843 and accuracy of 0.77\n",
      "Iteration 4477: with minibatch training loss = 1.06 and accuracy of 0.7\n",
      "Iteration 4478: with minibatch training loss = 0.953 and accuracy of 0.75\n",
      "Iteration 4479: with minibatch training loss = 0.798 and accuracy of 0.78\n",
      "Iteration 4480: with minibatch training loss = 0.887 and accuracy of 0.72\n",
      "Iteration 4481: with minibatch training loss = 1.26 and accuracy of 0.66\n",
      "Iteration 4482: with minibatch training loss = 0.842 and accuracy of 0.77\n",
      "Iteration 4483: with minibatch training loss = 1.1 and accuracy of 0.64\n",
      "Iteration 4484: with minibatch training loss = 0.898 and accuracy of 0.73\n",
      "Iteration 4485: with minibatch training loss = 0.699 and accuracy of 0.84\n",
      "Iteration 4486: with minibatch training loss = 0.785 and accuracy of 0.73\n",
      "Iteration 4487: with minibatch training loss = 1.12 and accuracy of 0.72\n",
      "Iteration 4488: with minibatch training loss = 0.882 and accuracy of 0.73\n",
      "Iteration 4489: with minibatch training loss = 0.965 and accuracy of 0.77\n",
      "Iteration 4490: with minibatch training loss = 0.76 and accuracy of 0.8\n",
      "Iteration 4491: with minibatch training loss = 0.947 and accuracy of 0.7\n",
      "Iteration 4492: with minibatch training loss = 0.647 and accuracy of 0.84\n",
      "Iteration 4493: with minibatch training loss = 0.724 and accuracy of 0.81\n",
      "Iteration 4494: with minibatch training loss = 1.13 and accuracy of 0.72\n",
      "Iteration 4495: with minibatch training loss = 0.686 and accuracy of 0.83\n",
      "Iteration 4496: with minibatch training loss = 0.797 and accuracy of 0.8\n",
      "Iteration 4497: with minibatch training loss = 1.27 and accuracy of 0.62\n",
      "Iteration 4498: with minibatch training loss = 1.04 and accuracy of 0.72\n",
      "Iteration 4499: with minibatch training loss = 0.857 and accuracy of 0.78\n",
      "Iteration 4500: with minibatch training loss = 0.952 and accuracy of 0.77\n",
      "Iteration 4501: with minibatch training loss = 0.727 and accuracy of 0.81\n",
      "Iteration 4502: with minibatch training loss = 1.03 and accuracy of 0.72\n",
      "Iteration 4503: with minibatch training loss = 0.875 and accuracy of 0.77\n",
      "Iteration 4504: with minibatch training loss = 1.12 and accuracy of 0.69\n",
      "Iteration 4505: with minibatch training loss = 0.799 and accuracy of 0.78\n",
      "Iteration 4506: with minibatch training loss = 0.823 and accuracy of 0.78\n",
      "Iteration 4507: with minibatch training loss = 0.87 and accuracy of 0.75\n",
      "Iteration 4508: with minibatch training loss = 1.17 and accuracy of 0.64\n",
      "Iteration 4509: with minibatch training loss = 0.879 and accuracy of 0.77\n",
      "Iteration 4510: with minibatch training loss = 0.725 and accuracy of 0.81\n",
      "Iteration 4511: with minibatch training loss = 0.925 and accuracy of 0.72\n",
      "Iteration 4512: with minibatch training loss = 0.996 and accuracy of 0.75\n",
      "Iteration 4513: with minibatch training loss = 0.758 and accuracy of 0.83\n",
      "Iteration 4514: with minibatch training loss = 0.996 and accuracy of 0.73\n",
      "Iteration 4515: with minibatch training loss = 0.944 and accuracy of 0.73\n",
      "Iteration 4516: with minibatch training loss = 0.766 and accuracy of 0.83\n",
      "Iteration 4517: with minibatch training loss = 0.59 and accuracy of 0.84\n",
      "Iteration 4518: with minibatch training loss = 0.999 and accuracy of 0.72\n",
      "Iteration 4519: with minibatch training loss = 1.05 and accuracy of 0.7\n",
      "Iteration 4520: with minibatch training loss = 0.734 and accuracy of 0.81\n",
      "Iteration 4521: with minibatch training loss = 0.831 and accuracy of 0.75\n",
      "Iteration 4522: with minibatch training loss = 0.963 and accuracy of 0.73\n",
      "Iteration 4523: with minibatch training loss = 0.899 and accuracy of 0.77\n",
      "Iteration 4524: with minibatch training loss = 0.887 and accuracy of 0.75\n",
      "Iteration 4525: with minibatch training loss = 1.12 and accuracy of 0.7\n",
      "Iteration 4526: with minibatch training loss = 1 and accuracy of 0.7\n",
      "Iteration 4527: with minibatch training loss = 0.908 and accuracy of 0.75\n",
      "Iteration 4528: with minibatch training loss = 0.721 and accuracy of 0.81\n",
      "Iteration 4529: with minibatch training loss = 1.01 and accuracy of 0.73\n",
      "Iteration 4530: with minibatch training loss = 1.07 and accuracy of 0.66\n",
      "Iteration 4531: with minibatch training loss = 0.866 and accuracy of 0.75\n",
      "Iteration 4532: with minibatch training loss = 0.914 and accuracy of 0.75\n",
      "Iteration 4533: with minibatch training loss = 0.89 and accuracy of 0.75\n",
      "Iteration 4534: with minibatch training loss = 0.857 and accuracy of 0.78\n",
      "Iteration 4535: with minibatch training loss = 0.927 and accuracy of 0.75\n",
      "Iteration 4536: with minibatch training loss = 1.17 and accuracy of 0.69\n",
      "Iteration 4537: with minibatch training loss = 0.85 and accuracy of 0.75\n",
      "Iteration 4538: with minibatch training loss = 0.885 and accuracy of 0.73\n",
      "Iteration 4539: with minibatch training loss = 0.776 and accuracy of 0.8\n",
      "Iteration 4540: with minibatch training loss = 0.79 and accuracy of 0.77\n",
      "Iteration 4541: with minibatch training loss = 1.12 and accuracy of 0.69\n",
      "Iteration 4542: with minibatch training loss = 1.21 and accuracy of 0.64\n",
      "Iteration 4543: with minibatch training loss = 0.917 and accuracy of 0.72\n",
      "Iteration 4544: with minibatch training loss = 1.11 and accuracy of 0.67\n",
      "Iteration 4545: with minibatch training loss = 0.971 and accuracy of 0.69\n",
      "Iteration 4546: with minibatch training loss = 0.992 and accuracy of 0.67\n",
      "Iteration 4547: with minibatch training loss = 1.41 and accuracy of 0.61\n",
      "Iteration 4548: with minibatch training loss = 0.792 and accuracy of 0.77\n",
      "Iteration 4549: with minibatch training loss = 1.03 and accuracy of 0.73\n",
      "Iteration 4550: with minibatch training loss = 1.03 and accuracy of 0.73\n",
      "Iteration 4551: with minibatch training loss = 0.804 and accuracy of 0.77\n",
      "Iteration 4552: with minibatch training loss = 0.639 and accuracy of 0.84\n",
      "Iteration 4553: with minibatch training loss = 0.918 and accuracy of 0.73\n",
      "Iteration 4554: with minibatch training loss = 0.712 and accuracy of 0.81\n",
      "Iteration 4555: with minibatch training loss = 0.762 and accuracy of 0.81\n",
      "Iteration 4556: with minibatch training loss = 0.668 and accuracy of 0.84\n",
      "Iteration 4557: with minibatch training loss = 1.12 and accuracy of 0.69\n",
      "Iteration 4558: with minibatch training loss = 0.901 and accuracy of 0.75\n",
      "Iteration 4559: with minibatch training loss = 0.972 and accuracy of 0.77\n",
      "Iteration 4560: with minibatch training loss = 1.15 and accuracy of 0.66\n",
      "Iteration 4561: with minibatch training loss = 1.29 and accuracy of 0.67\n",
      "Iteration 4562: with minibatch training loss = 0.706 and accuracy of 0.81\n",
      "Iteration 4563: with minibatch training loss = 0.974 and accuracy of 0.75\n",
      "Iteration 4564: with minibatch training loss = 0.893 and accuracy of 0.78\n",
      "Iteration 4565: with minibatch training loss = 0.868 and accuracy of 0.75\n",
      "Iteration 4566: with minibatch training loss = 0.863 and accuracy of 0.78\n",
      "Iteration 4567: with minibatch training loss = 0.663 and accuracy of 0.81\n",
      "Iteration 4568: with minibatch training loss = 0.899 and accuracy of 0.75\n",
      "Iteration 4569: with minibatch training loss = 1.01 and accuracy of 0.73\n",
      "Iteration 4570: with minibatch training loss = 0.669 and accuracy of 0.84\n",
      "Iteration 4571: with minibatch training loss = 1.02 and accuracy of 0.73\n",
      "Iteration 4572: with minibatch training loss = 1.09 and accuracy of 0.72\n",
      "Iteration 4573: with minibatch training loss = 1.05 and accuracy of 0.72\n",
      "Iteration 4574: with minibatch training loss = 1.16 and accuracy of 0.67\n",
      "Iteration 4575: with minibatch training loss = 0.919 and accuracy of 0.75\n",
      "Iteration 4576: with minibatch training loss = 1.16 and accuracy of 0.66\n",
      "Iteration 4577: with minibatch training loss = 1.16 and accuracy of 0.69\n",
      "Iteration 4578: with minibatch training loss = 1.06 and accuracy of 0.72\n",
      "Iteration 4579: with minibatch training loss = 0.732 and accuracy of 0.81\n",
      "Iteration 4580: with minibatch training loss = 1.15 and accuracy of 0.69\n",
      "Iteration 4581: with minibatch training loss = 1.04 and accuracy of 0.73\n",
      "Iteration 4582: with minibatch training loss = 0.807 and accuracy of 0.81\n",
      "Iteration 4583: with minibatch training loss = 1.06 and accuracy of 0.72\n",
      "Iteration 4584: with minibatch training loss = 1.05 and accuracy of 0.73\n",
      "Iteration 4585: with minibatch training loss = 0.977 and accuracy of 0.7\n",
      "Iteration 4586: with minibatch training loss = 1.09 and accuracy of 0.69\n",
      "Iteration 4587: with minibatch training loss = 0.973 and accuracy of 0.73\n",
      "Iteration 4588: with minibatch training loss = 0.904 and accuracy of 0.78\n",
      "Iteration 4589: with minibatch training loss = 0.892 and accuracy of 0.75\n",
      "Iteration 4590: with minibatch training loss = 0.979 and accuracy of 0.7\n",
      "Iteration 4591: with minibatch training loss = 1.09 and accuracy of 0.66\n",
      "Iteration 4592: with minibatch training loss = 0.672 and accuracy of 0.83\n",
      "Iteration 4593: with minibatch training loss = 0.935 and accuracy of 0.73\n",
      "Iteration 4594: with minibatch training loss = 0.887 and accuracy of 0.75\n",
      "Iteration 4595: with minibatch training loss = 0.837 and accuracy of 0.78\n",
      "Iteration 4596: with minibatch training loss = 1.38 and accuracy of 0.64\n",
      "Iteration 4597: with minibatch training loss = 0.678 and accuracy of 0.81\n",
      "Iteration 4598: with minibatch training loss = 0.862 and accuracy of 0.8\n",
      "Iteration 4599: with minibatch training loss = 1.26 and accuracy of 0.64\n",
      "Iteration 4600: with minibatch training loss = 0.951 and accuracy of 0.73\n",
      "Iteration 4601: with minibatch training loss = 1 and accuracy of 0.77\n",
      "Iteration 4602: with minibatch training loss = 1.08 and accuracy of 0.67\n",
      "Iteration 4603: with minibatch training loss = 0.822 and accuracy of 0.77\n",
      "Iteration 4604: with minibatch training loss = 0.702 and accuracy of 0.83\n",
      "Iteration 4605: with minibatch training loss = 0.83 and accuracy of 0.77\n",
      "Iteration 4606: with minibatch training loss = 0.818 and accuracy of 0.78\n",
      "Iteration 4607: with minibatch training loss = 1.07 and accuracy of 0.7\n",
      "Iteration 4608: with minibatch training loss = 0.864 and accuracy of 0.78\n",
      "Iteration 4609: with minibatch training loss = 1.2 and accuracy of 0.61\n",
      "Iteration 4610: with minibatch training loss = 0.658 and accuracy of 0.81\n",
      "Iteration 4611: with minibatch training loss = 0.892 and accuracy of 0.75\n",
      "Iteration 4612: with minibatch training loss = 0.999 and accuracy of 0.75\n",
      "Iteration 4613: with minibatch training loss = 1.04 and accuracy of 0.69\n",
      "Iteration 4614: with minibatch training loss = 1.07 and accuracy of 0.7\n",
      "Iteration 4615: with minibatch training loss = 0.78 and accuracy of 0.8\n",
      "Iteration 4616: with minibatch training loss = 1.01 and accuracy of 0.73\n",
      "Iteration 4617: with minibatch training loss = 0.812 and accuracy of 0.8\n",
      "Iteration 4618: with minibatch training loss = 0.97 and accuracy of 0.77\n",
      "Iteration 4619: with minibatch training loss = 0.882 and accuracy of 0.77\n",
      "Iteration 4620: with minibatch training loss = 0.781 and accuracy of 0.77\n",
      "Iteration 4621: with minibatch training loss = 0.814 and accuracy of 0.78\n",
      "Iteration 4622: with minibatch training loss = 0.969 and accuracy of 0.73\n",
      "Iteration 4623: with minibatch training loss = 0.999 and accuracy of 0.73\n",
      "Iteration 4624: with minibatch training loss = 0.941 and accuracy of 0.73\n",
      "Iteration 4625: with minibatch training loss = 0.728 and accuracy of 0.84\n",
      "Iteration 4626: with minibatch training loss = 0.984 and accuracy of 0.75\n",
      "Iteration 4627: with minibatch training loss = 0.712 and accuracy of 0.81\n",
      "Iteration 4628: with minibatch training loss = 0.793 and accuracy of 0.77\n",
      "Iteration 4629: with minibatch training loss = 0.837 and accuracy of 0.8\n",
      "Iteration 4630: with minibatch training loss = 0.989 and accuracy of 0.73\n",
      "Iteration 4631: with minibatch training loss = 0.766 and accuracy of 0.8\n",
      "Iteration 4632: with minibatch training loss = 0.907 and accuracy of 0.72\n",
      "Iteration 4633: with minibatch training loss = 0.861 and accuracy of 0.77\n",
      "Iteration 4634: with minibatch training loss = 1.07 and accuracy of 0.7\n",
      "Iteration 4635: with minibatch training loss = 0.851 and accuracy of 0.75\n",
      "Iteration 4636: with minibatch training loss = 0.603 and accuracy of 0.86\n",
      "Iteration 4637: with minibatch training loss = 1.08 and accuracy of 0.72\n",
      "Iteration 4638: with minibatch training loss = 0.904 and accuracy of 0.72\n",
      "Iteration 4639: with minibatch training loss = 0.696 and accuracy of 0.83\n",
      "Iteration 4640: with minibatch training loss = 1 and accuracy of 0.7\n",
      "Iteration 4641: with minibatch training loss = 0.822 and accuracy of 0.75\n",
      "Iteration 4642: with minibatch training loss = 0.725 and accuracy of 0.81\n",
      "Iteration 4643: with minibatch training loss = 0.948 and accuracy of 0.78\n",
      "Iteration 4644: with minibatch training loss = 0.963 and accuracy of 0.72\n",
      "Iteration 4645: with minibatch training loss = 1.15 and accuracy of 0.69\n",
      "Iteration 4646: with minibatch training loss = 0.845 and accuracy of 0.8\n",
      "Iteration 4647: with minibatch training loss = 1.19 and accuracy of 0.67\n",
      "Iteration 4648: with minibatch training loss = 1.15 and accuracy of 0.69\n",
      "Iteration 4649: with minibatch training loss = 1.03 and accuracy of 0.7\n",
      "Iteration 4650: with minibatch training loss = 0.749 and accuracy of 0.78\n",
      "Iteration 4651: with minibatch training loss = 0.951 and accuracy of 0.73\n",
      "Iteration 4652: with minibatch training loss = 1.05 and accuracy of 0.73\n",
      "Iteration 4653: with minibatch training loss = 0.994 and accuracy of 0.75\n",
      "Iteration 4654: with minibatch training loss = 0.818 and accuracy of 0.78\n",
      "Iteration 4655: with minibatch training loss = 0.796 and accuracy of 0.8\n",
      "Iteration 4656: with minibatch training loss = 0.676 and accuracy of 0.83\n",
      "Iteration 4657: with minibatch training loss = 0.85 and accuracy of 0.75\n",
      "Iteration 4658: with minibatch training loss = 0.866 and accuracy of 0.75\n",
      "Iteration 4659: with minibatch training loss = 0.804 and accuracy of 0.77\n",
      "Iteration 4660: with minibatch training loss = 0.776 and accuracy of 0.78\n",
      "Iteration 4661: with minibatch training loss = 0.765 and accuracy of 0.81\n",
      "Iteration 4662: with minibatch training loss = 0.724 and accuracy of 0.8\n",
      "Iteration 4663: with minibatch training loss = 0.854 and accuracy of 0.77\n",
      "Iteration 4664: with minibatch training loss = 0.885 and accuracy of 0.77\n",
      "Iteration 4665: with minibatch training loss = 0.744 and accuracy of 0.8\n",
      "Iteration 4666: with minibatch training loss = 0.954 and accuracy of 0.75\n",
      "Iteration 4667: with minibatch training loss = 0.701 and accuracy of 0.81\n",
      "Iteration 4668: with minibatch training loss = 1.26 and accuracy of 0.67\n",
      "Iteration 4669: with minibatch training loss = 0.832 and accuracy of 0.77\n",
      "Iteration 4670: with minibatch training loss = 1.16 and accuracy of 0.66\n",
      "Iteration 4671: with minibatch training loss = 1.04 and accuracy of 0.69\n",
      "Iteration 4672: with minibatch training loss = 0.833 and accuracy of 0.77\n",
      "Iteration 4673: with minibatch training loss = 0.651 and accuracy of 0.86\n",
      "Iteration 4674: with minibatch training loss = 0.718 and accuracy of 0.81\n",
      "Iteration 4675: with minibatch training loss = 1.07 and accuracy of 0.69\n",
      "Iteration 4676: with minibatch training loss = 0.898 and accuracy of 0.73\n",
      "Iteration 4677: with minibatch training loss = 1.02 and accuracy of 0.73\n",
      "Iteration 4678: with minibatch training loss = 1 and accuracy of 0.69\n",
      "Iteration 4679: with minibatch training loss = 0.822 and accuracy of 0.77\n",
      "Iteration 4680: with minibatch training loss = 1.05 and accuracy of 0.75\n",
      "Iteration 4681: with minibatch training loss = 1.07 and accuracy of 0.7\n",
      "Iteration 4682: with minibatch training loss = 1.22 and accuracy of 0.66\n",
      "Iteration 4683: with minibatch training loss = 1.07 and accuracy of 0.69\n",
      "Iteration 4684: with minibatch training loss = 0.953 and accuracy of 0.73\n",
      "Iteration 4685: with minibatch training loss = 0.975 and accuracy of 0.73\n",
      "Iteration 4686: with minibatch training loss = 0.945 and accuracy of 0.75\n",
      "Iteration 4687: with minibatch training loss = 0.818 and accuracy of 0.78\n",
      "Iteration 4688: with minibatch training loss = 0.789 and accuracy of 0.78\n",
      "Iteration 4689: with minibatch training loss = 0.797 and accuracy of 0.75\n",
      "Iteration 4690: with minibatch training loss = 0.773 and accuracy of 0.77\n",
      "Iteration 4691: with minibatch training loss = 0.899 and accuracy of 0.77\n",
      "Iteration 4692: with minibatch training loss = 0.691 and accuracy of 0.81\n",
      "Iteration 4693: with minibatch training loss = 0.937 and accuracy of 0.73\n",
      "Iteration 4694: with minibatch training loss = 0.946 and accuracy of 0.75\n",
      "Iteration 4695: with minibatch training loss = 0.771 and accuracy of 0.81\n",
      "Iteration 4696: with minibatch training loss = 0.549 and accuracy of 0.86\n",
      "Iteration 4697: with minibatch training loss = 0.998 and accuracy of 0.72\n",
      "Iteration 4698: with minibatch training loss = 0.788 and accuracy of 0.81\n",
      "Iteration 4699: with minibatch training loss = 0.946 and accuracy of 0.75\n",
      "Iteration 4700: with minibatch training loss = 0.997 and accuracy of 0.75\n",
      "Iteration 4701: with minibatch training loss = 0.886 and accuracy of 0.75\n",
      "Iteration 4702: with minibatch training loss = 1 and accuracy of 0.72\n",
      "Iteration 4703: with minibatch training loss = 0.964 and accuracy of 0.69\n",
      "Iteration 4704: with minibatch training loss = 0.926 and accuracy of 0.7\n",
      "Iteration 4705: with minibatch training loss = 0.706 and accuracy of 0.84\n",
      "Iteration 4706: with minibatch training loss = 0.888 and accuracy of 0.77\n",
      "Iteration 4707: with minibatch training loss = 1.24 and accuracy of 0.66\n",
      "Iteration 4708: with minibatch training loss = 0.761 and accuracy of 0.75\n",
      "Iteration 4709: with minibatch training loss = 0.891 and accuracy of 0.75\n",
      "Iteration 4710: with minibatch training loss = 0.849 and accuracy of 0.73\n",
      "Iteration 4711: with minibatch training loss = 0.954 and accuracy of 0.73\n",
      "Iteration 4712: with minibatch training loss = 1.23 and accuracy of 0.66\n",
      "Iteration 4713: with minibatch training loss = 0.695 and accuracy of 0.81\n",
      "Iteration 4714: with minibatch training loss = 0.966 and accuracy of 0.73\n",
      "Iteration 4715: with minibatch training loss = 0.986 and accuracy of 0.73\n",
      "Iteration 4716: with minibatch training loss = 1.02 and accuracy of 0.7\n",
      "Iteration 4717: with minibatch training loss = 0.974 and accuracy of 0.72\n",
      "Iteration 4718: with minibatch training loss = 0.91 and accuracy of 0.77\n",
      "Iteration 4719: with minibatch training loss = 1.01 and accuracy of 0.7\n",
      "Iteration 4720: with minibatch training loss = 0.91 and accuracy of 0.75\n",
      "Iteration 4721: with minibatch training loss = 1.33 and accuracy of 0.62\n",
      "Iteration 4722: with minibatch training loss = 0.791 and accuracy of 0.83\n",
      "Iteration 4723: with minibatch training loss = 0.749 and accuracy of 0.81\n",
      "Iteration 4724: with minibatch training loss = 0.922 and accuracy of 0.77\n",
      "Iteration 4725: with minibatch training loss = 0.828 and accuracy of 0.78\n",
      "Iteration 4726: with minibatch training loss = 1.02 and accuracy of 0.75\n",
      "Iteration 4727: with minibatch training loss = 0.882 and accuracy of 0.78\n",
      "Iteration 4728: with minibatch training loss = 0.783 and accuracy of 0.75\n",
      "Iteration 4729: with minibatch training loss = 0.987 and accuracy of 0.72\n",
      "Iteration 4730: with minibatch training loss = 0.763 and accuracy of 0.8\n",
      "Iteration 4731: with minibatch training loss = 0.65 and accuracy of 0.83\n",
      "Iteration 4732: with minibatch training loss = 0.933 and accuracy of 0.72\n",
      "Iteration 4733: with minibatch training loss = 0.653 and accuracy of 0.83\n",
      "Iteration 4734: with minibatch training loss = 0.898 and accuracy of 0.73\n",
      "Iteration 4735: with minibatch training loss = 0.781 and accuracy of 0.77\n",
      "Iteration 4736: with minibatch training loss = 0.767 and accuracy of 0.78\n",
      "Iteration 4737: with minibatch training loss = 0.895 and accuracy of 0.77\n",
      "Iteration 4738: with minibatch training loss = 0.806 and accuracy of 0.8\n",
      "Iteration 4739: with minibatch training loss = 0.677 and accuracy of 0.86\n",
      "Iteration 4740: with minibatch training loss = 1.05 and accuracy of 0.69\n",
      "Iteration 4741: with minibatch training loss = 1.13 and accuracy of 0.7\n",
      "Iteration 4742: with minibatch training loss = 0.864 and accuracy of 0.77\n",
      "Iteration 4743: with minibatch training loss = 0.88 and accuracy of 0.75\n",
      "Iteration 4744: with minibatch training loss = 0.802 and accuracy of 0.77\n",
      "Iteration 4745: with minibatch training loss = 0.931 and accuracy of 0.77\n",
      "Iteration 4746: with minibatch training loss = 1.05 and accuracy of 0.69\n",
      "Iteration 4747: with minibatch training loss = 0.984 and accuracy of 0.7\n",
      "Iteration 4748: with minibatch training loss = 0.99 and accuracy of 0.72\n",
      "Iteration 4749: with minibatch training loss = 1.03 and accuracy of 0.75\n",
      "Iteration 4750: with minibatch training loss = 1.15 and accuracy of 0.67\n",
      "Iteration 4751: with minibatch training loss = 0.987 and accuracy of 0.77\n",
      "Iteration 4752: with minibatch training loss = 0.889 and accuracy of 0.72\n",
      "Iteration 4753: with minibatch training loss = 0.851 and accuracy of 0.77\n",
      "Iteration 4754: with minibatch training loss = 0.88 and accuracy of 0.75\n",
      "Iteration 4755: with minibatch training loss = 0.774 and accuracy of 0.81\n",
      "Iteration 4756: with minibatch training loss = 1.14 and accuracy of 0.67\n",
      "Iteration 4757: with minibatch training loss = 0.981 and accuracy of 0.7\n",
      "Iteration 4758: with minibatch training loss = 0.997 and accuracy of 0.7\n",
      "Iteration 4759: with minibatch training loss = 0.985 and accuracy of 0.72\n",
      "Iteration 4760: with minibatch training loss = 1.04 and accuracy of 0.75\n",
      "Iteration 4761: with minibatch training loss = 0.937 and accuracy of 0.73\n",
      "Iteration 4762: with minibatch training loss = 1.03 and accuracy of 0.7\n",
      "Iteration 4763: with minibatch training loss = 0.894 and accuracy of 0.77\n",
      "Iteration 4764: with minibatch training loss = 1.06 and accuracy of 0.73\n",
      "Iteration 4765: with minibatch training loss = 0.708 and accuracy of 0.81\n",
      "Iteration 4766: with minibatch training loss = 1.48 and accuracy of 0.58\n",
      "Iteration 4767: with minibatch training loss = 1.1 and accuracy of 0.67\n",
      "Iteration 4768: with minibatch training loss = 0.85 and accuracy of 0.77\n",
      "Iteration 4769: with minibatch training loss = 1.01 and accuracy of 0.75\n",
      "Iteration 4770: with minibatch training loss = 1.12 and accuracy of 0.72\n",
      "Iteration 4771: with minibatch training loss = 0.791 and accuracy of 0.81\n",
      "Iteration 4772: with minibatch training loss = 0.956 and accuracy of 0.72\n",
      "Iteration 4773: with minibatch training loss = 0.792 and accuracy of 0.8\n",
      "Iteration 4774: with minibatch training loss = 1.44 and accuracy of 0.59\n",
      "Iteration 4775: with minibatch training loss = 0.942 and accuracy of 0.7\n",
      "Iteration 4776: with minibatch training loss = 0.958 and accuracy of 0.75\n",
      "Iteration 4777: with minibatch training loss = 0.701 and accuracy of 0.81\n",
      "Iteration 4778: with minibatch training loss = 0.977 and accuracy of 0.73\n",
      "Iteration 4779: with minibatch training loss = 0.823 and accuracy of 0.78\n",
      "Iteration 4780: with minibatch training loss = 0.832 and accuracy of 0.75\n",
      "Iteration 4781: with minibatch training loss = 1.15 and accuracy of 0.69\n",
      "Iteration 4782: with minibatch training loss = 1.24 and accuracy of 0.62\n",
      "Iteration 4783: with minibatch training loss = 0.922 and accuracy of 0.78\n",
      "Iteration 4784: with minibatch training loss = 1.13 and accuracy of 0.67\n",
      "Iteration 4785: with minibatch training loss = 0.929 and accuracy of 0.77\n",
      "Iteration 4786: with minibatch training loss = 0.68 and accuracy of 0.83\n",
      "Iteration 4787: with minibatch training loss = 0.747 and accuracy of 0.78\n",
      "Iteration 4788: with minibatch training loss = 0.897 and accuracy of 0.75\n",
      "Iteration 4789: with minibatch training loss = 1.09 and accuracy of 0.72\n",
      "Iteration 4790: with minibatch training loss = 0.849 and accuracy of 0.73\n",
      "Iteration 4791: with minibatch training loss = 0.874 and accuracy of 0.78\n",
      "Iteration 4792: with minibatch training loss = 0.94 and accuracy of 0.75\n",
      "Iteration 4793: with minibatch training loss = 0.75 and accuracy of 0.78\n",
      "Iteration 4794: with minibatch training loss = 1.06 and accuracy of 0.72\n",
      "Iteration 4795: with minibatch training loss = 0.959 and accuracy of 0.7\n",
      "Iteration 4796: with minibatch training loss = 0.7 and accuracy of 0.81\n",
      "Iteration 4797: with minibatch training loss = 0.579 and accuracy of 0.84\n",
      "Iteration 4798: with minibatch training loss = 0.853 and accuracy of 0.78\n",
      "Iteration 4799: with minibatch training loss = 1.22 and accuracy of 0.67\n",
      "Iteration 4800: with minibatch training loss = 0.832 and accuracy of 0.78\n",
      "Iteration 4801: with minibatch training loss = 1.05 and accuracy of 0.72\n",
      "Iteration 4802: with minibatch training loss = 0.823 and accuracy of 0.77\n",
      "Iteration 4803: with minibatch training loss = 1.02 and accuracy of 0.73\n",
      "Iteration 4804: with minibatch training loss = 0.924 and accuracy of 0.75\n",
      "Iteration 4805: with minibatch training loss = 1.03 and accuracy of 0.69\n",
      "Iteration 4806: with minibatch training loss = 0.89 and accuracy of 0.75\n",
      "Iteration 4807: with minibatch training loss = 0.957 and accuracy of 0.75\n",
      "Iteration 4808: with minibatch training loss = 1.18 and accuracy of 0.66\n",
      "Iteration 4809: with minibatch training loss = 0.835 and accuracy of 0.75\n",
      "Iteration 4810: with minibatch training loss = 0.886 and accuracy of 0.75\n",
      "Iteration 4811: with minibatch training loss = 0.834 and accuracy of 0.77\n",
      "Iteration 4812: with minibatch training loss = 0.922 and accuracy of 0.77\n",
      "Iteration 4813: with minibatch training loss = 1.15 and accuracy of 0.67\n",
      "Iteration 4814: with minibatch training loss = 0.947 and accuracy of 0.77\n",
      "Iteration 4815: with minibatch training loss = 0.91 and accuracy of 0.78\n",
      "Iteration 4816: with minibatch training loss = 1.02 and accuracy of 0.72\n",
      "Iteration 4817: with minibatch training loss = 0.933 and accuracy of 0.77\n",
      "Iteration 4818: with minibatch training loss = 0.933 and accuracy of 0.75\n",
      "Iteration 4819: with minibatch training loss = 0.788 and accuracy of 0.75\n",
      "Iteration 4820: with minibatch training loss = 1.24 and accuracy of 0.7\n",
      "Iteration 4821: with minibatch training loss = 1.06 and accuracy of 0.75\n",
      "Iteration 4822: with minibatch training loss = 0.743 and accuracy of 0.78\n",
      "Iteration 4823: with minibatch training loss = 0.844 and accuracy of 0.75\n",
      "Iteration 4824: with minibatch training loss = 0.894 and accuracy of 0.77\n",
      "Iteration 4825: with minibatch training loss = 0.893 and accuracy of 0.75\n",
      "Iteration 4826: with minibatch training loss = 1.1 and accuracy of 0.69\n",
      "Iteration 4827: with minibatch training loss = 0.834 and accuracy of 0.8\n",
      "Iteration 4828: with minibatch training loss = 0.884 and accuracy of 0.77\n",
      "Iteration 4829: with minibatch training loss = 1.04 and accuracy of 0.75\n",
      "Iteration 4830: with minibatch training loss = 0.637 and accuracy of 0.84\n",
      "Iteration 4831: with minibatch training loss = 0.896 and accuracy of 0.77\n",
      "Iteration 4832: with minibatch training loss = 1.19 and accuracy of 0.7\n",
      "Iteration 4833: with minibatch training loss = 0.792 and accuracy of 0.78\n",
      "Iteration 4834: with minibatch training loss = 0.975 and accuracy of 0.75\n",
      "Iteration 4835: with minibatch training loss = 0.791 and accuracy of 0.8\n",
      "Iteration 4836: with minibatch training loss = 0.763 and accuracy of 0.8\n",
      "Iteration 4837: with minibatch training loss = 0.916 and accuracy of 0.73\n",
      "Iteration 4838: with minibatch training loss = 0.78 and accuracy of 0.77\n",
      "Iteration 4839: with minibatch training loss = 0.652 and accuracy of 0.84\n",
      "Iteration 4840: with minibatch training loss = 1.05 and accuracy of 0.69\n",
      "Iteration 4841: with minibatch training loss = 0.831 and accuracy of 0.77\n",
      "Iteration 4842: with minibatch training loss = 0.934 and accuracy of 0.73\n",
      "Iteration 4843: with minibatch training loss = 1.1 and accuracy of 0.69\n",
      "Iteration 4844: with minibatch training loss = 1.2 and accuracy of 0.66\n",
      "Iteration 4845: with minibatch training loss = 0.885 and accuracy of 0.72\n",
      "Iteration 4846: with minibatch training loss = 0.923 and accuracy of 0.75\n",
      "Iteration 4847: with minibatch training loss = 1.13 and accuracy of 0.72\n",
      "Iteration 4848: with minibatch training loss = 0.812 and accuracy of 0.78\n",
      "Iteration 4849: with minibatch training loss = 0.922 and accuracy of 0.73\n",
      "Iteration 4850: with minibatch training loss = 1.08 and accuracy of 0.72\n",
      "Iteration 4851: with minibatch training loss = 0.73 and accuracy of 0.78\n",
      "Iteration 4852: with minibatch training loss = 1.02 and accuracy of 0.72\n",
      "Iteration 4853: with minibatch training loss = 0.939 and accuracy of 0.77\n",
      "Iteration 4854: with minibatch training loss = 0.849 and accuracy of 0.75\n",
      "Iteration 4855: with minibatch training loss = 1.2 and accuracy of 0.69\n",
      "Iteration 4856: with minibatch training loss = 0.951 and accuracy of 0.73\n",
      "Iteration 4857: with minibatch training loss = 1.12 and accuracy of 0.7\n",
      "Iteration 4858: with minibatch training loss = 1.25 and accuracy of 0.66\n",
      "Iteration 4859: with minibatch training loss = 0.994 and accuracy of 0.78\n",
      "Iteration 4860: with minibatch training loss = 1.13 and accuracy of 0.7\n",
      "Iteration 4861: with minibatch training loss = 1.04 and accuracy of 0.73\n",
      "Iteration 4862: with minibatch training loss = 0.771 and accuracy of 0.81\n",
      "Iteration 4863: with minibatch training loss = 1.07 and accuracy of 0.7\n",
      "Iteration 4864: with minibatch training loss = 0.997 and accuracy of 0.72\n",
      "Iteration 4865: with minibatch training loss = 0.787 and accuracy of 0.78\n",
      "Iteration 4866: with minibatch training loss = 0.701 and accuracy of 0.83\n",
      "Iteration 4867: with minibatch training loss = 0.815 and accuracy of 0.8\n",
      "Iteration 4868: with minibatch training loss = 0.782 and accuracy of 0.77\n",
      "Iteration 4869: with minibatch training loss = 1.13 and accuracy of 0.69\n",
      "Iteration 4870: with minibatch training loss = 1.04 and accuracy of 0.73\n",
      "Iteration 4871: with minibatch training loss = 0.817 and accuracy of 0.77\n",
      "Iteration 4872: with minibatch training loss = 1.13 and accuracy of 0.69\n",
      "Iteration 4873: with minibatch training loss = 0.951 and accuracy of 0.72\n",
      "Iteration 4874: with minibatch training loss = 1.01 and accuracy of 0.7\n",
      "Iteration 4875: with minibatch training loss = 0.733 and accuracy of 0.8\n",
      "Iteration 4876: with minibatch training loss = 0.77 and accuracy of 0.8\n",
      "Iteration 4877: with minibatch training loss = 0.871 and accuracy of 0.75\n",
      "Iteration 4878: with minibatch training loss = 1.03 and accuracy of 0.72\n",
      "Iteration 4879: with minibatch training loss = 0.818 and accuracy of 0.78\n",
      "Iteration 4880: with minibatch training loss = 0.849 and accuracy of 0.78\n",
      "Iteration 4881: with minibatch training loss = 0.811 and accuracy of 0.84\n",
      "Iteration 4882: with minibatch training loss = 1.08 and accuracy of 0.69\n",
      "Iteration 4883: with minibatch training loss = 0.955 and accuracy of 0.78\n",
      "Iteration 4884: with minibatch training loss = 0.723 and accuracy of 0.8\n",
      "Iteration 4885: with minibatch training loss = 0.936 and accuracy of 0.75\n",
      "Iteration 4886: with minibatch training loss = 1.06 and accuracy of 0.69\n",
      "Iteration 4887: with minibatch training loss = 0.823 and accuracy of 0.81\n",
      "Iteration 4888: with minibatch training loss = 0.82 and accuracy of 0.8\n",
      "Iteration 4889: with minibatch training loss = 0.614 and accuracy of 0.86\n",
      "Iteration 4890: with minibatch training loss = 0.858 and accuracy of 0.81\n",
      "Iteration 4891: with minibatch training loss = 0.906 and accuracy of 0.77\n",
      "Iteration 4892: with minibatch training loss = 1.21 and accuracy of 0.66\n",
      "Iteration 4893: with minibatch training loss = 0.764 and accuracy of 0.78\n",
      "Iteration 4894: with minibatch training loss = 1.03 and accuracy of 0.73\n",
      "Iteration 4895: with minibatch training loss = 0.918 and accuracy of 0.75\n",
      "Iteration 4896: with minibatch training loss = 1.4 and accuracy of 0.64\n",
      "Iteration 4897: with minibatch training loss = 0.66 and accuracy of 0.8\n",
      "Iteration 4898: with minibatch training loss = 0.674 and accuracy of 0.84\n",
      "Iteration 4899: with minibatch training loss = 0.926 and accuracy of 0.73\n",
      "Iteration 4900: with minibatch training loss = 0.898 and accuracy of 0.78\n",
      "Iteration 4901: with minibatch training loss = 0.935 and accuracy of 0.73\n",
      "Iteration 4902: with minibatch training loss = 0.908 and accuracy of 0.77\n",
      "Iteration 4903: with minibatch training loss = 0.701 and accuracy of 0.81\n",
      "Iteration 4904: with minibatch training loss = 0.827 and accuracy of 0.8\n",
      "Iteration 4905: with minibatch training loss = 1.37 and accuracy of 0.61\n",
      "Iteration 4906: with minibatch training loss = 0.63 and accuracy of 0.84\n",
      "Iteration 4907: with minibatch training loss = 0.827 and accuracy of 0.78\n",
      "Iteration 4908: with minibatch training loss = 0.806 and accuracy of 0.78\n",
      "Iteration 4909: with minibatch training loss = 1.25 and accuracy of 0.62\n",
      "Iteration 4910: with minibatch training loss = 1.09 and accuracy of 0.69\n",
      "Iteration 4911: with minibatch training loss = 0.839 and accuracy of 0.77\n",
      "Iteration 4912: with minibatch training loss = 1.06 and accuracy of 0.69\n",
      "Iteration 4913: with minibatch training loss = 0.795 and accuracy of 0.81\n",
      "Iteration 4914: with minibatch training loss = 1.15 and accuracy of 0.69\n",
      "Iteration 4915: with minibatch training loss = 1.07 and accuracy of 0.7\n",
      "Iteration 4916: with minibatch training loss = 0.772 and accuracy of 0.78\n",
      "Iteration 4917: with minibatch training loss = 0.542 and accuracy of 0.84\n",
      "Iteration 4918: with minibatch training loss = 0.657 and accuracy of 0.83\n",
      "Iteration 4919: with minibatch training loss = 0.946 and accuracy of 0.75\n",
      "Iteration 4920: with minibatch training loss = 0.979 and accuracy of 0.72\n",
      "Iteration 4921: with minibatch training loss = 0.945 and accuracy of 0.73\n",
      "Iteration 4922: with minibatch training loss = 0.677 and accuracy of 0.84\n",
      "Iteration 4923: with minibatch training loss = 1.1 and accuracy of 0.72\n",
      "Iteration 4924: with minibatch training loss = 1.12 and accuracy of 0.67\n",
      "Iteration 4925: with minibatch training loss = 0.782 and accuracy of 0.77\n",
      "Iteration 4926: with minibatch training loss = 0.963 and accuracy of 0.77\n",
      "Iteration 4927: with minibatch training loss = 1.09 and accuracy of 0.7\n",
      "Iteration 4928: with minibatch training loss = 0.864 and accuracy of 0.75\n",
      "Iteration 4929: with minibatch training loss = 0.87 and accuracy of 0.8\n",
      "Iteration 4930: with minibatch training loss = 0.569 and accuracy of 0.88\n",
      "Iteration 4931: with minibatch training loss = 0.919 and accuracy of 0.75\n",
      "Iteration 4932: with minibatch training loss = 1.12 and accuracy of 0.7\n",
      "Iteration 4933: with minibatch training loss = 1.07 and accuracy of 0.72\n",
      "Iteration 4934: with minibatch training loss = 0.718 and accuracy of 0.81\n",
      "Iteration 4935: with minibatch training loss = 0.685 and accuracy of 0.8\n",
      "Iteration 4936: with minibatch training loss = 0.619 and accuracy of 0.84\n",
      "Iteration 4937: with minibatch training loss = 0.685 and accuracy of 0.81\n",
      "Iteration 4938: with minibatch training loss = 1.03 and accuracy of 0.72\n",
      "Iteration 4939: with minibatch training loss = 0.697 and accuracy of 0.84\n",
      "Iteration 4940: with minibatch training loss = 1.2 and accuracy of 0.69\n",
      "Iteration 4941: with minibatch training loss = 0.848 and accuracy of 0.75\n",
      "Iteration 4942: with minibatch training loss = 0.831 and accuracy of 0.78\n",
      "Iteration 4943: with minibatch training loss = 0.668 and accuracy of 0.8\n",
      "Iteration 4944: with minibatch training loss = 1.02 and accuracy of 0.7\n",
      "Iteration 4945: with minibatch training loss = 0.864 and accuracy of 0.72\n",
      "Iteration 4946: with minibatch training loss = 0.714 and accuracy of 0.81\n",
      "Iteration 4947: with minibatch training loss = 0.832 and accuracy of 0.77\n",
      "Iteration 4948: with minibatch training loss = 1.03 and accuracy of 0.75\n",
      "Iteration 4949: with minibatch training loss = 0.809 and accuracy of 0.8\n",
      "Iteration 4950: with minibatch training loss = 0.8 and accuracy of 0.8\n",
      "Iteration 4951: with minibatch training loss = 0.781 and accuracy of 0.8\n",
      "Iteration 4952: with minibatch training loss = 0.89 and accuracy of 0.75\n",
      "Iteration 4953: with minibatch training loss = 0.924 and accuracy of 0.77\n",
      "Iteration 4954: with minibatch training loss = 0.91 and accuracy of 0.77\n",
      "Iteration 4955: with minibatch training loss = 0.916 and accuracy of 0.75\n",
      "Iteration 4956: with minibatch training loss = 1.06 and accuracy of 0.69\n",
      "Iteration 4957: with minibatch training loss = 0.526 and accuracy of 0.89\n",
      "Iteration 4958: with minibatch training loss = 0.789 and accuracy of 0.77\n",
      "Iteration 4959: with minibatch training loss = 0.841 and accuracy of 0.78\n",
      "Iteration 4960: with minibatch training loss = 1.27 and accuracy of 0.64\n",
      "Iteration 4961: with minibatch training loss = 0.964 and accuracy of 0.77\n",
      "Iteration 4962: with minibatch training loss = 0.998 and accuracy of 0.7\n",
      "Iteration 4963: with minibatch training loss = 1.1 and accuracy of 0.7\n",
      "Iteration 4964: with minibatch training loss = 1.1 and accuracy of 0.75\n",
      "Iteration 4965: with minibatch training loss = 1.1 and accuracy of 0.72\n",
      "Iteration 4966: with minibatch training loss = 0.773 and accuracy of 0.77\n",
      "Iteration 4967: with minibatch training loss = 0.912 and accuracy of 0.77\n",
      "Iteration 4968: with minibatch training loss = 0.89 and accuracy of 0.77\n",
      "Iteration 4969: with minibatch training loss = 0.758 and accuracy of 0.77\n",
      "Iteration 4970: with minibatch training loss = 1.08 and accuracy of 0.66\n",
      "Iteration 4971: with minibatch training loss = 0.966 and accuracy of 0.69\n",
      "Iteration 4972: with minibatch training loss = 1.02 and accuracy of 0.72\n",
      "Iteration 4973: with minibatch training loss = 0.727 and accuracy of 0.83\n",
      "Iteration 4974: with minibatch training loss = 0.923 and accuracy of 0.73\n",
      "Iteration 4975: with minibatch training loss = 0.899 and accuracy of 0.73\n",
      "Iteration 4976: with minibatch training loss = 0.795 and accuracy of 0.77\n",
      "Iteration 4977: with minibatch training loss = 0.716 and accuracy of 0.83\n",
      "Iteration 4978: with minibatch training loss = 0.909 and accuracy of 0.78\n",
      "Iteration 4979: with minibatch training loss = 0.908 and accuracy of 0.75\n",
      "Iteration 4980: with minibatch training loss = 1.16 and accuracy of 0.67\n",
      "Iteration 4981: with minibatch training loss = 0.659 and accuracy of 0.83\n",
      "Iteration 4982: with minibatch training loss = 1.32 and accuracy of 0.64\n",
      "Iteration 4983: with minibatch training loss = 0.81 and accuracy of 0.8\n",
      "Iteration 4984: with minibatch training loss = 1.22 and accuracy of 0.67\n",
      "Iteration 4985: with minibatch training loss = 1.05 and accuracy of 0.72\n",
      "Iteration 4986: with minibatch training loss = 0.863 and accuracy of 0.73\n",
      "Iteration 4987: with minibatch training loss = 0.885 and accuracy of 0.75\n",
      "Iteration 4988: with minibatch training loss = 0.974 and accuracy of 0.75\n",
      "Iteration 4989: with minibatch training loss = 1.02 and accuracy of 0.69\n",
      "Iteration 4990: with minibatch training loss = 0.951 and accuracy of 0.73\n",
      "Iteration 4991: with minibatch training loss = 0.693 and accuracy of 0.84\n",
      "Iteration 4992: with minibatch training loss = 0.804 and accuracy of 0.77\n",
      "Iteration 4993: with minibatch training loss = 0.821 and accuracy of 0.72\n",
      "Iteration 4994: with minibatch training loss = 0.754 and accuracy of 0.8\n",
      "Iteration 4995: with minibatch training loss = 0.899 and accuracy of 0.75\n",
      "Iteration 4996: with minibatch training loss = 0.848 and accuracy of 0.75\n",
      "Iteration 4997: with minibatch training loss = 0.84 and accuracy of 0.84\n",
      "Iteration 4998: with minibatch training loss = 1.14 and accuracy of 0.64\n",
      "Iteration 4999: with minibatch training loss = 0.962 and accuracy of 0.72\n",
      "Iteration 5000: with minibatch training loss = 1.08 and accuracy of 0.69\n",
      "Iteration 5001: with minibatch training loss = 1.15 and accuracy of 0.69\n",
      "Iteration 5002: with minibatch training loss = 0.962 and accuracy of 0.75\n",
      "Iteration 5003: with minibatch training loss = 1.12 and accuracy of 0.72\n",
      "Iteration 5004: with minibatch training loss = 0.931 and accuracy of 0.73\n",
      "Iteration 5005: with minibatch training loss = 0.548 and accuracy of 0.88\n",
      "Iteration 5006: with minibatch training loss = 0.918 and accuracy of 0.73\n",
      "Iteration 5007: with minibatch training loss = 0.812 and accuracy of 0.77\n",
      "Iteration 5008: with minibatch training loss = 0.96 and accuracy of 0.72\n",
      "Iteration 5009: with minibatch training loss = 1.14 and accuracy of 0.69\n",
      "Iteration 5010: with minibatch training loss = 0.762 and accuracy of 0.81\n",
      "Iteration 5011: with minibatch training loss = 0.712 and accuracy of 0.81\n",
      "Iteration 5012: with minibatch training loss = 1.02 and accuracy of 0.69\n",
      "Iteration 5013: with minibatch training loss = 0.642 and accuracy of 0.84\n",
      "Iteration 5014: with minibatch training loss = 0.829 and accuracy of 0.78\n",
      "Iteration 5015: with minibatch training loss = 0.781 and accuracy of 0.8\n",
      "Iteration 5016: with minibatch training loss = 0.858 and accuracy of 0.78\n",
      "Iteration 5017: with minibatch training loss = 0.932 and accuracy of 0.75\n",
      "Iteration 5018: with minibatch training loss = 0.761 and accuracy of 0.8\n",
      "Iteration 5019: with minibatch training loss = 0.957 and accuracy of 0.73\n",
      "Iteration 5020: with minibatch training loss = 1.06 and accuracy of 0.66\n",
      "Iteration 5021: with minibatch training loss = 0.843 and accuracy of 0.77\n",
      "Iteration 5022: with minibatch training loss = 0.979 and accuracy of 0.72\n",
      "Iteration 5023: with minibatch training loss = 1 and accuracy of 0.7\n",
      "Iteration 5024: with minibatch training loss = 0.947 and accuracy of 0.75\n",
      "Iteration 5025: with minibatch training loss = 0.994 and accuracy of 0.72\n",
      "Iteration 5026: with minibatch training loss = 0.896 and accuracy of 0.75\n",
      "Iteration 5027: with minibatch training loss = 0.944 and accuracy of 0.7\n",
      "Iteration 5028: with minibatch training loss = 0.912 and accuracy of 0.72\n",
      "Iteration 5029: with minibatch training loss = 0.673 and accuracy of 0.83\n",
      "Iteration 5030: with minibatch training loss = 0.903 and accuracy of 0.72\n",
      "Iteration 5031: with minibatch training loss = 0.874 and accuracy of 0.73\n",
      "Iteration 5032: with minibatch training loss = 0.831 and accuracy of 0.75\n",
      "Iteration 5033: with minibatch training loss = 0.683 and accuracy of 0.84\n",
      "Iteration 5034: with minibatch training loss = 1.05 and accuracy of 0.72\n",
      "Iteration 5035: with minibatch training loss = 1 and accuracy of 0.73\n",
      "Iteration 5036: with minibatch training loss = 0.984 and accuracy of 0.69\n",
      "Iteration 5037: with minibatch training loss = 0.735 and accuracy of 0.84\n",
      "Iteration 5038: with minibatch training loss = 0.675 and accuracy of 0.81\n",
      "Iteration 5039: with minibatch training loss = 0.672 and accuracy of 0.8\n",
      "Iteration 5040: with minibatch training loss = 0.819 and accuracy of 0.77\n",
      "Iteration 5041: with minibatch training loss = 1.06 and accuracy of 0.73\n",
      "Iteration 5042: with minibatch training loss = 0.607 and accuracy of 0.84\n",
      "Iteration 5043: with minibatch training loss = 0.882 and accuracy of 0.73\n",
      "Iteration 5044: with minibatch training loss = 1.05 and accuracy of 0.72\n",
      "Iteration 5045: with minibatch training loss = 0.869 and accuracy of 0.75\n",
      "Iteration 5046: with minibatch training loss = 0.81 and accuracy of 0.77\n",
      "Iteration 5047: with minibatch training loss = 1.17 and accuracy of 0.69\n",
      "Iteration 5048: with minibatch training loss = 0.922 and accuracy of 0.73\n",
      "Iteration 5049: with minibatch training loss = 0.908 and accuracy of 0.73\n",
      "Iteration 5050: with minibatch training loss = 0.895 and accuracy of 0.75\n",
      "Iteration 5051: with minibatch training loss = 0.748 and accuracy of 0.83\n",
      "Iteration 5052: with minibatch training loss = 0.648 and accuracy of 0.84\n",
      "Iteration 5053: with minibatch training loss = 0.826 and accuracy of 0.77\n",
      "Iteration 5054: with minibatch training loss = 0.858 and accuracy of 0.78\n",
      "Iteration 5055: with minibatch training loss = 1.06 and accuracy of 0.7\n",
      "Iteration 5056: with minibatch training loss = 1.01 and accuracy of 0.7\n",
      "Iteration 5057: with minibatch training loss = 0.866 and accuracy of 0.75\n",
      "Iteration 5058: with minibatch training loss = 1.07 and accuracy of 0.72\n",
      "Iteration 5059: with minibatch training loss = 0.878 and accuracy of 0.78\n",
      "Iteration 5060: with minibatch training loss = 0.677 and accuracy of 0.81\n",
      "Iteration 5061: with minibatch training loss = 0.878 and accuracy of 0.75\n",
      "Iteration 5062: with minibatch training loss = 0.722 and accuracy of 0.81\n",
      "Iteration 5063: with minibatch training loss = 0.671 and accuracy of 0.84\n",
      "Iteration 5064: with minibatch training loss = 0.419 and accuracy of 0.89\n",
      "Iteration 5065: with minibatch training loss = 1.01 and accuracy of 0.7\n",
      "Iteration 5066: with minibatch training loss = 1.12 and accuracy of 0.64\n",
      "Iteration 5067: with minibatch training loss = 0.997 and accuracy of 0.73\n",
      "Iteration 5068: with minibatch training loss = 0.881 and accuracy of 0.75\n",
      "Iteration 5069: with minibatch training loss = 0.907 and accuracy of 0.73\n",
      "Iteration 5070: with minibatch training loss = 0.961 and accuracy of 0.72\n",
      "Iteration 5071: with minibatch training loss = 0.904 and accuracy of 0.75\n",
      "Iteration 5072: with minibatch training loss = 0.751 and accuracy of 0.83\n",
      "Iteration 5073: with minibatch training loss = 0.904 and accuracy of 0.77\n",
      "Iteration 5074: with minibatch training loss = 0.685 and accuracy of 0.81\n",
      "Iteration 5075: with minibatch training loss = 0.801 and accuracy of 0.8\n",
      "Iteration 5076: with minibatch training loss = 0.891 and accuracy of 0.77\n",
      "Iteration 5077: with minibatch training loss = 0.997 and accuracy of 0.72\n",
      "Iteration 5078: with minibatch training loss = 1.03 and accuracy of 0.69\n",
      "Iteration 5079: with minibatch training loss = 0.805 and accuracy of 0.78\n",
      "Iteration 5080: with minibatch training loss = 0.931 and accuracy of 0.73\n",
      "Iteration 5081: with minibatch training loss = 1.14 and accuracy of 0.67\n",
      "Iteration 5082: with minibatch training loss = 0.662 and accuracy of 0.8\n",
      "Iteration 5083: with minibatch training loss = 1.09 and accuracy of 0.67\n",
      "Iteration 5084: with minibatch training loss = 0.797 and accuracy of 0.78\n",
      "Iteration 5085: with minibatch training loss = 0.598 and accuracy of 0.84\n",
      "Iteration 5086: with minibatch training loss = 0.739 and accuracy of 0.81\n",
      "Iteration 5087: with minibatch training loss = 1.01 and accuracy of 0.73\n",
      "Iteration 5088: with minibatch training loss = 0.968 and accuracy of 0.73\n",
      "Iteration 5089: with minibatch training loss = 0.856 and accuracy of 0.73\n",
      "Iteration 5090: with minibatch training loss = 0.615 and accuracy of 0.83\n",
      "Iteration 5091: with minibatch training loss = 1.01 and accuracy of 0.7\n",
      "Iteration 5092: with minibatch training loss = 0.81 and accuracy of 0.75\n",
      "Iteration 5093: with minibatch training loss = 1.12 and accuracy of 0.69\n",
      "Iteration 5094: with minibatch training loss = 0.696 and accuracy of 0.83\n",
      "Iteration 5095: with minibatch training loss = 0.741 and accuracy of 0.81\n",
      "Iteration 5096: with minibatch training loss = 0.655 and accuracy of 0.83\n",
      "Iteration 5097: with minibatch training loss = 0.865 and accuracy of 0.72\n",
      "Iteration 5098: with minibatch training loss = 1.12 and accuracy of 0.67\n",
      "Iteration 5099: with minibatch training loss = 0.882 and accuracy of 0.77\n",
      "Iteration 5100: with minibatch training loss = 0.533 and accuracy of 0.88\n",
      "Iteration 5101: with minibatch training loss = 0.817 and accuracy of 0.77\n",
      "Iteration 5102: with minibatch training loss = 1.1 and accuracy of 0.72\n",
      "Iteration 5103: with minibatch training loss = 0.841 and accuracy of 0.75\n",
      "Iteration 5104: with minibatch training loss = 0.917 and accuracy of 0.73\n",
      "Iteration 5105: with minibatch training loss = 0.51 and accuracy of 0.88\n",
      "Iteration 5106: with minibatch training loss = 1.09 and accuracy of 0.69\n",
      "Iteration 5107: with minibatch training loss = 0.886 and accuracy of 0.75\n",
      "Iteration 5108: with minibatch training loss = 1.25 and accuracy of 0.61\n",
      "Iteration 5109: with minibatch training loss = 0.842 and accuracy of 0.78\n",
      "Iteration 5110: with minibatch training loss = 0.83 and accuracy of 0.81\n",
      "Iteration 5111: with minibatch training loss = 1.03 and accuracy of 0.7\n",
      "Iteration 5112: with minibatch training loss = 0.958 and accuracy of 0.72\n",
      "Iteration 5113: with minibatch training loss = 0.566 and accuracy of 0.84\n",
      "Iteration 5114: with minibatch training loss = 1.22 and accuracy of 0.66\n",
      "Iteration 5115: with minibatch training loss = 0.687 and accuracy of 0.81\n",
      "Iteration 5116: with minibatch training loss = 0.695 and accuracy of 0.78\n",
      "Iteration 5117: with minibatch training loss = 0.86 and accuracy of 0.75\n",
      "Iteration 5118: with minibatch training loss = 1.08 and accuracy of 0.7\n",
      "Iteration 5119: with minibatch training loss = 1.09 and accuracy of 0.72\n",
      "Iteration 5120: with minibatch training loss = 0.717 and accuracy of 0.81\n",
      "Iteration 5121: with minibatch training loss = 1.02 and accuracy of 0.72\n",
      "Iteration 5122: with minibatch training loss = 0.826 and accuracy of 0.8\n",
      "Iteration 5123: with minibatch training loss = 0.943 and accuracy of 0.72\n",
      "Iteration 5124: with minibatch training loss = 1.23 and accuracy of 0.66\n",
      "Iteration 5125: with minibatch training loss = 0.815 and accuracy of 0.77\n",
      "Iteration 5126: with minibatch training loss = 0.844 and accuracy of 0.77\n",
      "Iteration 5127: with minibatch training loss = 1.07 and accuracy of 0.7\n",
      "Iteration 5128: with minibatch training loss = 0.809 and accuracy of 0.8\n",
      "Iteration 5129: with minibatch training loss = 0.585 and accuracy of 0.88\n",
      "Iteration 5130: with minibatch training loss = 0.861 and accuracy of 0.77\n",
      "Iteration 5131: with minibatch training loss = 0.814 and accuracy of 0.77\n",
      "Iteration 5132: with minibatch training loss = 0.892 and accuracy of 0.78\n",
      "Iteration 5133: with minibatch training loss = 0.852 and accuracy of 0.73\n",
      "Iteration 5134: with minibatch training loss = 0.851 and accuracy of 0.78\n",
      "Iteration 5135: with minibatch training loss = 0.864 and accuracy of 0.77\n",
      "Iteration 5136: with minibatch training loss = 1.01 and accuracy of 0.73\n",
      "Iteration 5137: with minibatch training loss = 0.903 and accuracy of 0.77\n",
      "Iteration 5138: with minibatch training loss = 1.02 and accuracy of 0.72\n",
      "Iteration 5139: with minibatch training loss = 0.648 and accuracy of 0.83\n",
      "Iteration 5140: with minibatch training loss = 0.951 and accuracy of 0.72\n",
      "Iteration 5141: with minibatch training loss = 0.728 and accuracy of 0.8\n",
      "Iteration 5142: with minibatch training loss = 0.736 and accuracy of 0.78\n",
      "Iteration 5143: with minibatch training loss = 0.982 and accuracy of 0.72\n",
      "Iteration 5144: with minibatch training loss = 1.2 and accuracy of 0.69\n",
      "Iteration 5145: with minibatch training loss = 1.23 and accuracy of 0.62\n",
      "Iteration 5146: with minibatch training loss = 0.593 and accuracy of 0.86\n",
      "Iteration 5147: with minibatch training loss = 0.974 and accuracy of 0.75\n",
      "Iteration 5148: with minibatch training loss = 0.754 and accuracy of 0.81\n",
      "Iteration 5149: with minibatch training loss = 1.31 and accuracy of 0.67\n",
      "Iteration 5150: with minibatch training loss = 0.893 and accuracy of 0.77\n",
      "Iteration 5151: with minibatch training loss = 0.844 and accuracy of 0.78\n",
      "Iteration 5152: with minibatch training loss = 0.807 and accuracy of 0.8\n",
      "Iteration 5153: with minibatch training loss = 1.06 and accuracy of 0.72\n",
      "Iteration 5154: with minibatch training loss = 0.862 and accuracy of 0.78\n",
      "Iteration 5155: with minibatch training loss = 1.18 and accuracy of 0.69\n",
      "Iteration 5156: with minibatch training loss = 0.896 and accuracy of 0.77\n",
      "Iteration 5157: with minibatch training loss = 0.906 and accuracy of 0.77\n",
      "Iteration 5158: with minibatch training loss = 1.04 and accuracy of 0.7\n",
      "Iteration 5159: with minibatch training loss = 0.911 and accuracy of 0.73\n",
      "Iteration 5160: with minibatch training loss = 1 and accuracy of 0.72\n",
      "Iteration 5161: with minibatch training loss = 0.989 and accuracy of 0.73\n",
      "Iteration 5162: with minibatch training loss = 0.788 and accuracy of 0.77\n",
      "Iteration 5163: with minibatch training loss = 0.79 and accuracy of 0.83\n",
      "Iteration 5164: with minibatch training loss = 0.944 and accuracy of 0.75\n",
      "Iteration 5165: with minibatch training loss = 0.56 and accuracy of 0.88\n",
      "Iteration 5166: with minibatch training loss = 0.73 and accuracy of 0.83\n",
      "Iteration 5167: with minibatch training loss = 1.03 and accuracy of 0.7\n",
      "Iteration 5168: with minibatch training loss = 0.746 and accuracy of 0.8\n",
      "Iteration 5169: with minibatch training loss = 1.15 and accuracy of 0.69\n",
      "Iteration 5170: with minibatch training loss = 0.857 and accuracy of 0.73\n",
      "Iteration 5171: with minibatch training loss = 0.908 and accuracy of 0.75\n",
      "Iteration 5172: with minibatch training loss = 1.25 and accuracy of 0.7\n",
      "Iteration 5173: with minibatch training loss = 0.716 and accuracy of 0.8\n",
      "Iteration 5174: with minibatch training loss = 1.07 and accuracy of 0.69\n",
      "Iteration 5175: with minibatch training loss = 0.948 and accuracy of 0.75\n",
      "Iteration 5176: with minibatch training loss = 0.973 and accuracy of 0.7\n",
      "Iteration 5177: with minibatch training loss = 0.961 and accuracy of 0.73\n",
      "Iteration 5178: with minibatch training loss = 0.96 and accuracy of 0.7\n",
      "Iteration 5179: with minibatch training loss = 0.858 and accuracy of 0.78\n",
      "Iteration 5180: with minibatch training loss = 1.19 and accuracy of 0.66\n",
      "Iteration 5181: with minibatch training loss = 0.9 and accuracy of 0.73\n",
      "Iteration 5182: with minibatch training loss = 0.792 and accuracy of 0.77\n",
      "Iteration 5183: with minibatch training loss = 0.863 and accuracy of 0.77\n",
      "Iteration 5184: with minibatch training loss = 0.906 and accuracy of 0.75\n",
      "Iteration 5185: with minibatch training loss = 0.853 and accuracy of 0.8\n",
      "Iteration 5186: with minibatch training loss = 1.01 and accuracy of 0.7\n",
      "Iteration 5187: with minibatch training loss = 1.2 and accuracy of 0.62\n",
      "Iteration 5188: with minibatch training loss = 0.826 and accuracy of 0.78\n",
      "Iteration 5189: with minibatch training loss = 1.16 and accuracy of 0.7\n",
      "Iteration 5190: with minibatch training loss = 0.76 and accuracy of 0.81\n",
      "Iteration 5191: with minibatch training loss = 1.05 and accuracy of 0.7\n",
      "Iteration 5192: with minibatch training loss = 1.11 and accuracy of 0.7\n",
      "Iteration 5193: with minibatch training loss = 0.934 and accuracy of 0.77\n",
      "Iteration 5194: with minibatch training loss = 0.684 and accuracy of 0.8\n",
      "Iteration 5195: with minibatch training loss = 0.972 and accuracy of 0.75\n",
      "Iteration 5196: with minibatch training loss = 0.559 and accuracy of 0.86\n",
      "Iteration 5197: with minibatch training loss = 0.895 and accuracy of 0.75\n",
      "Iteration 5198: with minibatch training loss = 0.652 and accuracy of 0.83\n",
      "Iteration 5199: with minibatch training loss = 1.19 and accuracy of 0.66\n",
      "Iteration 5200: with minibatch training loss = 0.993 and accuracy of 0.75\n",
      "Iteration 5201: with minibatch training loss = 0.952 and accuracy of 0.72\n",
      "Iteration 5202: with minibatch training loss = 0.953 and accuracy of 0.72\n",
      "Iteration 5203: with minibatch training loss = 0.837 and accuracy of 0.77\n",
      "Iteration 5204: with minibatch training loss = 0.576 and accuracy of 0.88\n",
      "Iteration 5205: with minibatch training loss = 0.721 and accuracy of 0.83\n",
      "Iteration 5206: with minibatch training loss = 0.769 and accuracy of 0.83\n",
      "Iteration 5207: with minibatch training loss = 0.877 and accuracy of 0.75\n",
      "Iteration 5208: with minibatch training loss = 1.1 and accuracy of 0.67\n",
      "Iteration 5209: with minibatch training loss = 1.07 and accuracy of 0.7\n",
      "Iteration 5210: with minibatch training loss = 0.98 and accuracy of 0.77\n",
      "Iteration 5211: with minibatch training loss = 0.552 and accuracy of 0.81\n",
      "Iteration 5212: with minibatch training loss = 0.845 and accuracy of 0.8\n",
      "Iteration 5213: with minibatch training loss = 1.11 and accuracy of 0.72\n",
      "Iteration 5214: with minibatch training loss = 0.895 and accuracy of 0.77\n",
      "Iteration 5215: with minibatch training loss = 0.871 and accuracy of 0.77\n",
      "Iteration 5216: with minibatch training loss = 0.716 and accuracy of 0.83\n",
      "Iteration 5217: with minibatch training loss = 1.29 and accuracy of 0.61\n",
      "Iteration 5218: with minibatch training loss = 0.929 and accuracy of 0.77\n",
      "Iteration 5219: with minibatch training loss = 0.81 and accuracy of 0.8\n",
      "Iteration 5220: with minibatch training loss = 0.727 and accuracy of 0.81\n",
      "Iteration 5221: with minibatch training loss = 0.692 and accuracy of 0.81\n",
      "Iteration 5222: with minibatch training loss = 0.792 and accuracy of 0.78\n",
      "Iteration 5223: with minibatch training loss = 0.809 and accuracy of 0.8\n",
      "Iteration 5224: with minibatch training loss = 0.875 and accuracy of 0.77\n",
      "Iteration 5225: with minibatch training loss = 0.821 and accuracy of 0.77\n",
      "Iteration 5226: with minibatch training loss = 0.801 and accuracy of 0.8\n",
      "Iteration 5227: with minibatch training loss = 0.999 and accuracy of 0.73\n",
      "Iteration 5228: with minibatch training loss = 1.27 and accuracy of 0.66\n",
      "Iteration 5229: with minibatch training loss = 0.997 and accuracy of 0.67\n",
      "Iteration 5230: with minibatch training loss = 0.966 and accuracy of 0.73\n",
      "Iteration 5231: with minibatch training loss = 0.907 and accuracy of 0.75\n",
      "Iteration 5232: with minibatch training loss = 1.01 and accuracy of 0.72\n",
      "Iteration 5233: with minibatch training loss = 1.11 and accuracy of 0.67\n",
      "Iteration 5234: with minibatch training loss = 0.842 and accuracy of 0.73\n",
      "Iteration 5235: with minibatch training loss = 1.1 and accuracy of 0.7\n",
      "Iteration 5236: with minibatch training loss = 0.76 and accuracy of 0.78\n",
      "Iteration 5237: with minibatch training loss = 0.695 and accuracy of 0.78\n",
      "Iteration 5238: with minibatch training loss = 0.516 and accuracy of 0.88\n",
      "Iteration 5239: with minibatch training loss = 1.09 and accuracy of 0.69\n",
      "Iteration 5240: with minibatch training loss = 0.884 and accuracy of 0.77\n",
      "Iteration 5241: with minibatch training loss = 0.902 and accuracy of 0.77\n",
      "Iteration 5242: with minibatch training loss = 1.29 and accuracy of 0.64\n",
      "Iteration 5243: with minibatch training loss = 0.923 and accuracy of 0.75\n",
      "Iteration 5244: with minibatch training loss = 0.86 and accuracy of 0.77\n",
      "Iteration 5245: with minibatch training loss = 0.94 and accuracy of 0.7\n",
      "Iteration 5246: with minibatch training loss = 0.734 and accuracy of 0.81\n",
      "Iteration 5247: with minibatch training loss = 0.807 and accuracy of 0.78\n",
      "Iteration 5248: with minibatch training loss = 0.442 and accuracy of 0.88\n",
      "Iteration 5249: with minibatch training loss = 1.03 and accuracy of 0.73\n",
      "Iteration 5250: with minibatch training loss = 0.549 and accuracy of 0.86\n",
      "Iteration 5251: with minibatch training loss = 1.12 and accuracy of 0.72\n",
      "Iteration 5252: with minibatch training loss = 0.888 and accuracy of 0.77\n",
      "Iteration 5253: with minibatch training loss = 0.889 and accuracy of 0.75\n",
      "Iteration 5254: with minibatch training loss = 0.987 and accuracy of 0.72\n",
      "Iteration 5255: with minibatch training loss = 0.769 and accuracy of 0.8\n",
      "Iteration 5256: with minibatch training loss = 0.923 and accuracy of 0.72\n",
      "Iteration 5257: with minibatch training loss = 1.29 and accuracy of 0.62\n",
      "Iteration 5258: with minibatch training loss = 0.753 and accuracy of 0.81\n",
      "Iteration 5259: with minibatch training loss = 0.828 and accuracy of 0.77\n",
      "Iteration 5260: with minibatch training loss = 0.866 and accuracy of 0.73\n",
      "Iteration 5261: with minibatch training loss = 1.07 and accuracy of 0.72\n",
      "Iteration 5262: with minibatch training loss = 0.939 and accuracy of 0.75\n",
      "Iteration 5263: with minibatch training loss = 0.612 and accuracy of 0.84\n",
      "Iteration 5264: with minibatch training loss = 0.515 and accuracy of 0.88\n",
      "Iteration 5265: with minibatch training loss = 0.93 and accuracy of 0.75\n",
      "Iteration 5266: with minibatch training loss = 0.935 and accuracy of 0.77\n",
      "Iteration 5267: with minibatch training loss = 0.902 and accuracy of 0.75\n",
      "Iteration 5268: with minibatch training loss = 1.03 and accuracy of 0.73\n",
      "Iteration 5269: with minibatch training loss = 0.803 and accuracy of 0.78\n",
      "Iteration 5270: with minibatch training loss = 0.807 and accuracy of 0.8\n",
      "Iteration 5271: with minibatch training loss = 1.23 and accuracy of 0.66\n",
      "Iteration 5272: with minibatch training loss = 0.478 and accuracy of 0.91\n",
      "Iteration 5273: with minibatch training loss = 0.836 and accuracy of 0.77\n",
      "Iteration 5274: with minibatch training loss = 0.962 and accuracy of 0.7\n",
      "Iteration 5275: with minibatch training loss = 0.844 and accuracy of 0.75\n",
      "Iteration 5276: with minibatch training loss = 1.05 and accuracy of 0.72\n",
      "Iteration 5277: with minibatch training loss = 1.07 and accuracy of 0.73\n",
      "Iteration 5278: with minibatch training loss = 0.729 and accuracy of 0.8\n",
      "Iteration 5279: with minibatch training loss = 0.808 and accuracy of 0.78\n",
      "Iteration 5280: with minibatch training loss = 0.545 and accuracy of 0.86\n",
      "Iteration 5281: with minibatch training loss = 1.24 and accuracy of 0.64\n",
      "Iteration 5282: with minibatch training loss = 1.08 and accuracy of 0.69\n",
      "Iteration 5283: with minibatch training loss = 0.559 and accuracy of 0.86\n",
      "Iteration 5284: with minibatch training loss = 0.8 and accuracy of 0.73\n",
      "Iteration 5285: with minibatch training loss = 1.04 and accuracy of 0.67\n",
      "Iteration 5286: with minibatch training loss = 0.583 and accuracy of 0.84\n",
      "Iteration 5287: with minibatch training loss = 0.751 and accuracy of 0.8\n",
      "Iteration 5288: with minibatch training loss = 0.752 and accuracy of 0.8\n",
      "Iteration 5289: with minibatch training loss = 1.09 and accuracy of 0.72\n",
      "Iteration 5290: with minibatch training loss = 1.04 and accuracy of 0.73\n",
      "Iteration 5291: with minibatch training loss = 0.958 and accuracy of 0.72\n",
      "Iteration 5292: with minibatch training loss = 0.784 and accuracy of 0.78\n",
      "Iteration 5293: with minibatch training loss = 1.02 and accuracy of 0.72\n",
      "Iteration 5294: with minibatch training loss = 0.835 and accuracy of 0.77\n",
      "Iteration 5295: with minibatch training loss = 0.966 and accuracy of 0.7\n",
      "Iteration 5296: with minibatch training loss = 0.631 and accuracy of 0.86\n",
      "Iteration 5297: with minibatch training loss = 0.883 and accuracy of 0.77\n",
      "Iteration 5298: with minibatch training loss = 0.84 and accuracy of 0.81\n",
      "Iteration 5299: with minibatch training loss = 0.995 and accuracy of 0.72\n",
      "Iteration 5300: with minibatch training loss = 0.585 and accuracy of 0.83\n",
      "Iteration 5301: with minibatch training loss = 0.777 and accuracy of 0.81\n",
      "Iteration 5302: with minibatch training loss = 0.71 and accuracy of 0.81\n",
      "Iteration 5303: with minibatch training loss = 0.834 and accuracy of 0.78\n",
      "Iteration 5304: with minibatch training loss = 0.771 and accuracy of 0.78\n",
      "Iteration 5305: with minibatch training loss = 0.832 and accuracy of 0.77\n",
      "Iteration 5306: with minibatch training loss = 0.966 and accuracy of 0.72\n",
      "Iteration 5307: with minibatch training loss = 0.84 and accuracy of 0.75\n",
      "Iteration 5308: with minibatch training loss = 0.787 and accuracy of 0.8\n",
      "Iteration 5309: with minibatch training loss = 0.71 and accuracy of 0.81\n",
      "Iteration 5310: with minibatch training loss = 0.863 and accuracy of 0.77\n",
      "Iteration 5311: with minibatch training loss = 0.782 and accuracy of 0.8\n",
      "Iteration 5312: with minibatch training loss = 0.786 and accuracy of 0.77\n",
      "Iteration 5313: with minibatch training loss = 0.919 and accuracy of 0.72\n",
      "Iteration 5314: with minibatch training loss = 0.973 and accuracy of 0.72\n",
      "Iteration 5315: with minibatch training loss = 0.856 and accuracy of 0.77\n",
      "Iteration 5316: with minibatch training loss = 0.918 and accuracy of 0.75\n",
      "Iteration 5317: with minibatch training loss = 1.05 and accuracy of 0.69\n",
      "Iteration 5318: with minibatch training loss = 0.827 and accuracy of 0.78\n",
      "Iteration 5319: with minibatch training loss = 0.678 and accuracy of 0.84\n",
      "Iteration 5320: with minibatch training loss = 0.878 and accuracy of 0.81\n",
      "Iteration 5321: with minibatch training loss = 0.776 and accuracy of 0.81\n",
      "Iteration 5322: with minibatch training loss = 0.702 and accuracy of 0.81\n",
      "Iteration 5323: with minibatch training loss = 0.784 and accuracy of 0.81\n",
      "Iteration 5324: with minibatch training loss = 0.787 and accuracy of 0.8\n",
      "Iteration 5325: with minibatch training loss = 0.995 and accuracy of 0.75\n",
      "Iteration 5326: with minibatch training loss = 1.04 and accuracy of 0.7\n",
      "Iteration 5327: with minibatch training loss = 1.09 and accuracy of 0.69\n",
      "Iteration 5328: with minibatch training loss = 1.05 and accuracy of 0.69\n",
      "Iteration 5329: with minibatch training loss = 0.909 and accuracy of 0.77\n",
      "Iteration 5330: with minibatch training loss = 0.918 and accuracy of 0.75\n",
      "Iteration 5331: with minibatch training loss = 0.827 and accuracy of 0.77\n",
      "Iteration 5332: with minibatch training loss = 0.827 and accuracy of 0.78\n",
      "Iteration 5333: with minibatch training loss = 1.04 and accuracy of 0.69\n",
      "Iteration 5334: with minibatch training loss = 0.713 and accuracy of 0.8\n",
      "Iteration 5335: with minibatch training loss = 0.955 and accuracy of 0.7\n",
      "Iteration 5336: with minibatch training loss = 0.794 and accuracy of 0.8\n",
      "Iteration 5337: with minibatch training loss = 0.659 and accuracy of 0.81\n",
      "Iteration 5338: with minibatch training loss = 0.952 and accuracy of 0.75\n",
      "Iteration 5339: with minibatch training loss = 0.866 and accuracy of 0.77\n",
      "Iteration 5340: with minibatch training loss = 1.14 and accuracy of 0.69\n",
      "Iteration 5341: with minibatch training loss = 0.894 and accuracy of 0.73\n",
      "Iteration 5342: with minibatch training loss = 0.863 and accuracy of 0.8\n",
      "Iteration 5343: with minibatch training loss = 0.744 and accuracy of 0.8\n",
      "Iteration 5344: with minibatch training loss = 0.806 and accuracy of 0.78\n",
      "Iteration 5345: with minibatch training loss = 0.695 and accuracy of 0.83\n",
      "Iteration 5346: with minibatch training loss = 0.83 and accuracy of 0.77\n",
      "Iteration 5347: with minibatch training loss = 0.834 and accuracy of 0.77\n",
      "Iteration 5348: with minibatch training loss = 0.828 and accuracy of 0.78\n",
      "Iteration 5349: with minibatch training loss = 0.925 and accuracy of 0.73\n",
      "Iteration 5350: with minibatch training loss = 0.674 and accuracy of 0.81\n",
      "Iteration 5351: with minibatch training loss = 0.893 and accuracy of 0.75\n",
      "Iteration 5352: with minibatch training loss = 0.978 and accuracy of 0.73\n",
      "Iteration 5353: with minibatch training loss = 0.994 and accuracy of 0.69\n",
      "Iteration 5354: with minibatch training loss = 1.06 and accuracy of 0.69\n",
      "Iteration 5355: with minibatch training loss = 1.02 and accuracy of 0.72\n",
      "Iteration 5356: with minibatch training loss = 0.815 and accuracy of 0.78\n",
      "Iteration 5357: with minibatch training loss = 0.605 and accuracy of 0.86\n",
      "Iteration 5358: with minibatch training loss = 0.961 and accuracy of 0.72\n",
      "Iteration 5359: with minibatch training loss = 0.92 and accuracy of 0.7\n",
      "Iteration 5360: with minibatch training loss = 0.734 and accuracy of 0.8\n",
      "Iteration 5361: with minibatch training loss = 0.835 and accuracy of 0.8\n",
      "Iteration 5362: with minibatch training loss = 0.844 and accuracy of 0.78\n",
      "Iteration 5363: with minibatch training loss = 1.02 and accuracy of 0.73\n",
      "Iteration 5364: with minibatch training loss = 0.948 and accuracy of 0.75\n",
      "Iteration 5365: with minibatch training loss = 1.11 and accuracy of 0.7\n",
      "Iteration 5366: with minibatch training loss = 0.893 and accuracy of 0.77\n",
      "Iteration 5367: with minibatch training loss = 0.592 and accuracy of 0.86\n",
      "Iteration 5368: with minibatch training loss = 0.971 and accuracy of 0.73\n",
      "Iteration 5369: with minibatch training loss = 1.18 and accuracy of 0.64\n",
      "Iteration 5370: with minibatch training loss = 0.928 and accuracy of 0.75\n",
      "Iteration 5371: with minibatch training loss = 0.827 and accuracy of 0.78\n",
      "Iteration 5372: with minibatch training loss = 0.83 and accuracy of 0.73\n",
      "Iteration 5373: with minibatch training loss = 0.934 and accuracy of 0.73\n",
      "Iteration 5374: with minibatch training loss = 1.03 and accuracy of 0.7\n",
      "Iteration 5375: with minibatch training loss = 0.984 and accuracy of 0.72\n",
      "Iteration 5376: with minibatch training loss = 0.816 and accuracy of 0.8\n",
      "Iteration 5377: with minibatch training loss = 0.729 and accuracy of 0.8\n",
      "Iteration 5378: with minibatch training loss = 0.949 and accuracy of 0.75\n",
      "Iteration 5379: with minibatch training loss = 0.823 and accuracy of 0.77\n",
      "Iteration 5380: with minibatch training loss = 0.736 and accuracy of 0.83\n",
      "Iteration 5381: with minibatch training loss = 0.729 and accuracy of 0.78\n",
      "Iteration 5382: with minibatch training loss = 0.857 and accuracy of 0.77\n",
      "Iteration 5383: with minibatch training loss = 0.952 and accuracy of 0.72\n",
      "Iteration 5384: with minibatch training loss = 1.2 and accuracy of 0.64\n",
      "Iteration 5385: with minibatch training loss = 0.756 and accuracy of 0.78\n",
      "Iteration 5386: with minibatch training loss = 0.684 and accuracy of 0.8\n",
      "Iteration 5387: with minibatch training loss = 1.32 and accuracy of 0.64\n",
      "Iteration 5388: with minibatch training loss = 0.885 and accuracy of 0.77\n",
      "Iteration 5389: with minibatch training loss = 1.04 and accuracy of 0.72\n",
      "Iteration 5390: with minibatch training loss = 0.869 and accuracy of 0.78\n",
      "Iteration 5391: with minibatch training loss = 0.525 and accuracy of 0.88\n",
      "Iteration 5392: with minibatch training loss = 1 and accuracy of 0.75\n",
      "Iteration 5393: with minibatch training loss = 1.21 and accuracy of 0.62\n",
      "Iteration 5394: with minibatch training loss = 1.04 and accuracy of 0.73\n",
      "Iteration 5395: with minibatch training loss = 0.976 and accuracy of 0.75\n",
      "Iteration 5396: with minibatch training loss = 0.985 and accuracy of 0.72\n",
      "Iteration 5397: with minibatch training loss = 0.599 and accuracy of 0.86\n",
      "Iteration 5398: with minibatch training loss = 0.833 and accuracy of 0.75\n",
      "Iteration 5399: with minibatch training loss = 0.682 and accuracy of 0.77\n",
      "Iteration 5400: with minibatch training loss = 0.615 and accuracy of 0.84\n",
      "Iteration 5401: with minibatch training loss = 0.898 and accuracy of 0.78\n",
      "Iteration 5402: with minibatch training loss = 0.87 and accuracy of 0.75\n",
      "Iteration 5403: with minibatch training loss = 0.513 and accuracy of 0.88\n",
      "Iteration 5404: with minibatch training loss = 0.913 and accuracy of 0.73\n",
      "Iteration 5405: with minibatch training loss = 0.975 and accuracy of 0.72\n",
      "Iteration 5406: with minibatch training loss = 0.768 and accuracy of 0.77\n",
      "Iteration 5407: with minibatch training loss = 0.963 and accuracy of 0.73\n",
      "Iteration 5408: with minibatch training loss = 0.911 and accuracy of 0.73\n",
      "Iteration 5409: with minibatch training loss = 0.963 and accuracy of 0.75\n",
      "Iteration 5410: with minibatch training loss = 0.845 and accuracy of 0.73\n",
      "Iteration 5411: with minibatch training loss = 0.92 and accuracy of 0.73\n",
      "Iteration 5412: with minibatch training loss = 1.15 and accuracy of 0.67\n",
      "Iteration 5413: with minibatch training loss = 0.947 and accuracy of 0.75\n",
      "Iteration 5414: with minibatch training loss = 1.15 and accuracy of 0.67\n",
      "Iteration 5415: with minibatch training loss = 1.1 and accuracy of 0.7\n",
      "Iteration 5416: with minibatch training loss = 0.941 and accuracy of 0.73\n",
      "Iteration 5417: with minibatch training loss = 1.15 and accuracy of 0.66\n",
      "Iteration 5418: with minibatch training loss = 0.625 and accuracy of 0.81\n",
      "Iteration 5419: with minibatch training loss = 0.904 and accuracy of 0.72\n",
      "Iteration 5420: with minibatch training loss = 1.08 and accuracy of 0.7\n",
      "Iteration 5421: with minibatch training loss = 0.638 and accuracy of 0.84\n",
      "Iteration 5422: with minibatch training loss = 1.06 and accuracy of 0.7\n",
      "Iteration 5423: with minibatch training loss = 1.19 and accuracy of 0.69\n",
      "Iteration 5424: with minibatch training loss = 0.669 and accuracy of 0.78\n",
      "Iteration 5425: with minibatch training loss = 0.676 and accuracy of 0.83\n",
      "Iteration 5426: with minibatch training loss = 1.1 and accuracy of 0.69\n",
      "Iteration 5427: with minibatch training loss = 0.92 and accuracy of 0.77\n",
      "Iteration 5428: with minibatch training loss = 0.757 and accuracy of 0.78\n",
      "Iteration 5429: with minibatch training loss = 0.665 and accuracy of 0.84\n",
      "Iteration 5430: with minibatch training loss = 0.923 and accuracy of 0.75\n",
      "Iteration 5431: with minibatch training loss = 0.89 and accuracy of 0.77\n",
      "Iteration 5432: with minibatch training loss = 0.92 and accuracy of 0.75\n",
      "Iteration 5433: with minibatch training loss = 0.97 and accuracy of 0.73\n",
      "Iteration 5434: with minibatch training loss = 0.739 and accuracy of 0.81\n",
      "Iteration 5435: with minibatch training loss = 0.849 and accuracy of 0.73\n",
      "Iteration 5436: with minibatch training loss = 0.528 and accuracy of 0.88\n",
      "Iteration 5437: with minibatch training loss = 1.11 and accuracy of 0.67\n",
      "Iteration 5438: with minibatch training loss = 0.922 and accuracy of 0.73\n",
      "Iteration 5439: with minibatch training loss = 0.918 and accuracy of 0.75\n",
      "Iteration 5440: with minibatch training loss = 0.702 and accuracy of 0.81\n",
      "Iteration 5441: with minibatch training loss = 0.719 and accuracy of 0.78\n",
      "Iteration 5442: with minibatch training loss = 1.07 and accuracy of 0.72\n",
      "Iteration 5443: with minibatch training loss = 0.713 and accuracy of 0.83\n",
      "Iteration 5444: with minibatch training loss = 0.65 and accuracy of 0.81\n",
      "Iteration 5445: with minibatch training loss = 0.822 and accuracy of 0.77\n",
      "Iteration 5446: with minibatch training loss = 0.88 and accuracy of 0.73\n",
      "Iteration 5447: with minibatch training loss = 0.922 and accuracy of 0.73\n",
      "Iteration 5448: with minibatch training loss = 1.19 and accuracy of 0.67\n",
      "Iteration 5449: with minibatch training loss = 0.888 and accuracy of 0.77\n",
      "Iteration 5450: with minibatch training loss = 0.886 and accuracy of 0.77\n",
      "Iteration 5451: with minibatch training loss = 0.768 and accuracy of 0.81\n",
      "Iteration 5452: with minibatch training loss = 1.12 and accuracy of 0.69\n",
      "Iteration 5453: with minibatch training loss = 0.935 and accuracy of 0.75\n",
      "Iteration 5454: with minibatch training loss = 0.586 and accuracy of 0.86\n",
      "Iteration 5455: with minibatch training loss = 0.911 and accuracy of 0.73\n",
      "Iteration 5456: with minibatch training loss = 1.12 and accuracy of 0.66\n",
      "Iteration 5457: with minibatch training loss = 0.785 and accuracy of 0.8\n",
      "Iteration 5458: with minibatch training loss = 0.796 and accuracy of 0.75\n",
      "Iteration 5459: with minibatch training loss = 0.805 and accuracy of 0.78\n",
      "Iteration 5460: with minibatch training loss = 0.583 and accuracy of 0.86\n",
      "Iteration 5461: with minibatch training loss = 1.17 and accuracy of 0.69\n",
      "Iteration 5462: with minibatch training loss = 0.761 and accuracy of 0.81\n",
      "Iteration 5463: with minibatch training loss = 0.716 and accuracy of 0.81\n",
      "Iteration 5464: with minibatch training loss = 0.992 and accuracy of 0.72\n",
      "Iteration 5465: with minibatch training loss = 1.02 and accuracy of 0.72\n",
      "Iteration 5466: with minibatch training loss = 0.729 and accuracy of 0.8\n",
      "Iteration 5467: with minibatch training loss = 0.591 and accuracy of 0.86\n",
      "Iteration 5468: with minibatch training loss = 0.91 and accuracy of 0.72\n",
      "Iteration 5469: with minibatch training loss = 0.811 and accuracy of 0.77\n",
      "Iteration 5470: with minibatch training loss = 0.851 and accuracy of 0.77\n",
      "Iteration 5471: with minibatch training loss = 0.549 and accuracy of 0.86\n",
      "Iteration 5472: with minibatch training loss = 1.04 and accuracy of 0.67\n",
      "Iteration 5473: with minibatch training loss = 0.676 and accuracy of 0.8\n",
      "Iteration 5474: with minibatch training loss = 1.2 and accuracy of 0.67\n",
      "Iteration 5475: with minibatch training loss = 0.85 and accuracy of 0.78\n",
      "Iteration 5476: with minibatch training loss = 0.843 and accuracy of 0.75\n",
      "Iteration 5477: with minibatch training loss = 0.779 and accuracy of 0.78\n",
      "Iteration 5478: with minibatch training loss = 0.686 and accuracy of 0.81\n",
      "Iteration 5479: with minibatch training loss = 1.09 and accuracy of 0.69\n",
      "Iteration 5480: with minibatch training loss = 1.13 and accuracy of 0.67\n",
      "Iteration 5481: with minibatch training loss = 1.05 and accuracy of 0.78\n",
      "Iteration 5482: with minibatch training loss = 0.758 and accuracy of 0.78\n",
      "Iteration 5483: with minibatch training loss = 0.903 and accuracy of 0.72\n",
      "Iteration 5484: with minibatch training loss = 0.728 and accuracy of 0.8\n",
      "Iteration 5485: with minibatch training loss = 0.592 and accuracy of 0.86\n",
      "Iteration 5486: with minibatch training loss = 0.96 and accuracy of 0.72\n",
      "Iteration 5487: with minibatch training loss = 0.932 and accuracy of 0.75\n",
      "Iteration 5488: with minibatch training loss = 0.653 and accuracy of 0.83\n",
      "Iteration 5489: with minibatch training loss = 1.11 and accuracy of 0.73\n",
      "Iteration 5490: with minibatch training loss = 0.719 and accuracy of 0.8\n",
      "Iteration 5491: with minibatch training loss = 0.953 and accuracy of 0.73\n",
      "Iteration 5492: with minibatch training loss = 0.807 and accuracy of 0.77\n",
      "Iteration 5493: with minibatch training loss = 1.11 and accuracy of 0.67\n",
      "Iteration 5494: with minibatch training loss = 0.933 and accuracy of 0.75\n",
      "Iteration 5495: with minibatch training loss = 0.757 and accuracy of 0.8\n",
      "Iteration 5496: with minibatch training loss = 0.877 and accuracy of 0.73\n",
      "Iteration 5497: with minibatch training loss = 0.942 and accuracy of 0.75\n",
      "Iteration 5498: with minibatch training loss = 0.844 and accuracy of 0.81\n",
      "Iteration 5499: with minibatch training loss = 0.509 and accuracy of 0.89\n",
      "Iteration 5500: with minibatch training loss = 1.03 and accuracy of 0.73\n",
      "Iteration 5501: with minibatch training loss = 0.445 and accuracy of 0.88\n",
      "Iteration 5502: with minibatch training loss = 0.9 and accuracy of 0.77\n",
      "Iteration 5503: with minibatch training loss = 1.03 and accuracy of 0.72\n",
      "Iteration 5504: with minibatch training loss = 0.837 and accuracy of 0.77\n",
      "Iteration 5505: with minibatch training loss = 0.91 and accuracy of 0.72\n",
      "Iteration 5506: with minibatch training loss = 0.927 and accuracy of 0.77\n",
      "Iteration 5507: with minibatch training loss = 0.802 and accuracy of 0.78\n",
      "Iteration 5508: with minibatch training loss = 0.994 and accuracy of 0.7\n",
      "Iteration 5509: with minibatch training loss = 0.855 and accuracy of 0.72\n",
      "Iteration 5510: with minibatch training loss = 0.766 and accuracy of 0.8\n",
      "Iteration 5511: with minibatch training loss = 0.864 and accuracy of 0.77\n",
      "Iteration 5512: with minibatch training loss = 0.503 and accuracy of 0.86\n",
      "Iteration 5513: with minibatch training loss = 0.702 and accuracy of 0.81\n",
      "Iteration 5514: with minibatch training loss = 1.29 and accuracy of 0.64\n",
      "Iteration 5515: with minibatch training loss = 0.816 and accuracy of 0.78\n",
      "Iteration 5516: with minibatch training loss = 0.76 and accuracy of 0.8\n",
      "Iteration 5517: with minibatch training loss = 0.902 and accuracy of 0.73\n",
      "Iteration 5518: with minibatch training loss = 0.913 and accuracy of 0.73\n",
      "Iteration 5519: with minibatch training loss = 0.882 and accuracy of 0.77\n",
      "Iteration 5520: with minibatch training loss = 0.75 and accuracy of 0.8\n",
      "Iteration 5521: with minibatch training loss = 0.734 and accuracy of 0.75\n",
      "Iteration 5522: with minibatch training loss = 0.999 and accuracy of 0.7\n",
      "Iteration 5523: with minibatch training loss = 0.718 and accuracy of 0.83\n",
      "Iteration 5524: with minibatch training loss = 0.919 and accuracy of 0.72\n",
      "Iteration 5525: with minibatch training loss = 0.85 and accuracy of 0.75\n",
      "Iteration 5526: with minibatch training loss = 0.972 and accuracy of 0.7\n",
      "Iteration 5527: with minibatch training loss = 0.921 and accuracy of 0.75\n",
      "Iteration 5528: with minibatch training loss = 0.887 and accuracy of 0.73\n",
      "Iteration 5529: with minibatch training loss = 0.851 and accuracy of 0.75\n",
      "Iteration 5530: with minibatch training loss = 0.98 and accuracy of 0.73\n",
      "Iteration 5531: with minibatch training loss = 1.05 and accuracy of 0.67\n",
      "Iteration 5532: with minibatch training loss = 0.915 and accuracy of 0.77\n",
      "Iteration 5533: with minibatch training loss = 1 and accuracy of 0.72\n",
      "Iteration 5534: with minibatch training loss = 0.892 and accuracy of 0.77\n",
      "Iteration 5535: with minibatch training loss = 0.848 and accuracy of 0.75\n",
      "Iteration 5536: with minibatch training loss = 0.821 and accuracy of 0.8\n",
      "Iteration 5537: with minibatch training loss = 0.79 and accuracy of 0.8\n",
      "Iteration 5538: with minibatch training loss = 0.872 and accuracy of 0.75\n",
      "Iteration 5539: with minibatch training loss = 0.83 and accuracy of 0.77\n",
      "Iteration 5540: with minibatch training loss = 0.848 and accuracy of 0.77\n",
      "Iteration 5541: with minibatch training loss = 0.912 and accuracy of 0.75\n",
      "Iteration 5542: with minibatch training loss = 1.06 and accuracy of 0.7\n",
      "Iteration 5543: with minibatch training loss = 0.758 and accuracy of 0.83\n",
      "Iteration 5544: with minibatch training loss = 0.913 and accuracy of 0.73\n",
      "Iteration 5545: with minibatch training loss = 0.973 and accuracy of 0.73\n",
      "Iteration 5546: with minibatch training loss = 0.984 and accuracy of 0.77\n",
      "Iteration 5547: with minibatch training loss = 0.914 and accuracy of 0.72\n",
      "Iteration 5548: with minibatch training loss = 1.26 and accuracy of 0.66\n",
      "Iteration 5549: with minibatch training loss = 0.815 and accuracy of 0.8\n",
      "Iteration 5550: with minibatch training loss = 1.04 and accuracy of 0.67\n",
      "Validation loss: 0.2672463\n",
      "Model's weights saved at /Users/nhat/Documents/Projects/LetterClassifier/weights/model_se.ckpt\n",
      "Epoch 4, Overall loss = 0.903 and accuracy of 0.753\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13807ffd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5552: with minibatch training loss = 1.13 and accuracy of 0.66\n",
      "Iteration 5553: with minibatch training loss = 0.939 and accuracy of 0.75\n",
      "Iteration 5554: with minibatch training loss = 0.85 and accuracy of 0.77\n",
      "Iteration 5555: with minibatch training loss = 0.905 and accuracy of 0.75\n",
      "Iteration 5556: with minibatch training loss = 0.784 and accuracy of 0.78\n",
      "Iteration 5557: with minibatch training loss = 1 and accuracy of 0.73\n",
      "Iteration 5558: with minibatch training loss = 0.806 and accuracy of 0.8\n",
      "Iteration 5559: with minibatch training loss = 0.794 and accuracy of 0.77\n",
      "Iteration 5560: with minibatch training loss = 0.808 and accuracy of 0.78\n",
      "Iteration 5561: with minibatch training loss = 0.914 and accuracy of 0.72\n",
      "Iteration 5562: with minibatch training loss = 0.714 and accuracy of 0.8\n",
      "Iteration 5563: with minibatch training loss = 1.08 and accuracy of 0.72\n",
      "Iteration 5564: with minibatch training loss = 0.891 and accuracy of 0.75\n",
      "Iteration 5565: with minibatch training loss = 0.99 and accuracy of 0.75\n",
      "Iteration 5566: with minibatch training loss = 0.973 and accuracy of 0.72\n",
      "Iteration 5567: with minibatch training loss = 0.634 and accuracy of 0.84\n",
      "Iteration 5568: with minibatch training loss = 0.858 and accuracy of 0.77\n",
      "Iteration 5569: with minibatch training loss = 0.897 and accuracy of 0.77\n",
      "Iteration 5570: with minibatch training loss = 0.742 and accuracy of 0.8\n",
      "Iteration 5571: with minibatch training loss = 0.8 and accuracy of 0.81\n",
      "Iteration 5572: with minibatch training loss = 0.855 and accuracy of 0.78\n",
      "Iteration 5573: with minibatch training loss = 0.637 and accuracy of 0.83\n",
      "Iteration 5574: with minibatch training loss = 0.67 and accuracy of 0.83\n",
      "Iteration 5575: with minibatch training loss = 0.96 and accuracy of 0.72\n",
      "Iteration 5576: with minibatch training loss = 0.757 and accuracy of 0.78\n",
      "Iteration 5577: with minibatch training loss = 0.923 and accuracy of 0.72\n",
      "Iteration 5578: with minibatch training loss = 0.781 and accuracy of 0.81\n",
      "Iteration 5579: with minibatch training loss = 0.865 and accuracy of 0.77\n",
      "Iteration 5580: with minibatch training loss = 0.759 and accuracy of 0.8\n",
      "Iteration 5581: with minibatch training loss = 0.87 and accuracy of 0.77\n",
      "Iteration 5582: with minibatch training loss = 0.907 and accuracy of 0.77\n",
      "Iteration 5583: with minibatch training loss = 1.06 and accuracy of 0.72\n",
      "Iteration 5584: with minibatch training loss = 0.833 and accuracy of 0.78\n",
      "Iteration 5585: with minibatch training loss = 1.06 and accuracy of 0.67\n",
      "Iteration 5586: with minibatch training loss = 0.948 and accuracy of 0.75\n",
      "Iteration 5587: with minibatch training loss = 0.748 and accuracy of 0.78\n",
      "Iteration 5588: with minibatch training loss = 0.975 and accuracy of 0.72\n",
      "Iteration 5589: with minibatch training loss = 0.945 and accuracy of 0.75\n",
      "Iteration 5590: with minibatch training loss = 1.04 and accuracy of 0.69\n",
      "Iteration 5591: with minibatch training loss = 0.809 and accuracy of 0.75\n",
      "Iteration 5592: with minibatch training loss = 1.18 and accuracy of 0.62\n",
      "Iteration 5593: with minibatch training loss = 0.974 and accuracy of 0.7\n",
      "Iteration 5594: with minibatch training loss = 0.968 and accuracy of 0.72\n",
      "Iteration 5595: with minibatch training loss = 0.861 and accuracy of 0.78\n",
      "Iteration 5596: with minibatch training loss = 0.947 and accuracy of 0.73\n",
      "Iteration 5597: with minibatch training loss = 0.841 and accuracy of 0.75\n",
      "Iteration 5598: with minibatch training loss = 0.514 and accuracy of 0.84\n",
      "Iteration 5599: with minibatch training loss = 0.657 and accuracy of 0.83\n",
      "Iteration 5600: with minibatch training loss = 0.848 and accuracy of 0.78\n",
      "Iteration 5601: with minibatch training loss = 0.633 and accuracy of 0.83\n",
      "Iteration 5602: with minibatch training loss = 0.822 and accuracy of 0.8\n",
      "Iteration 5603: with minibatch training loss = 1.12 and accuracy of 0.67\n",
      "Iteration 5604: with minibatch training loss = 0.871 and accuracy of 0.77\n",
      "Iteration 5605: with minibatch training loss = 1.03 and accuracy of 0.72\n",
      "Iteration 5606: with minibatch training loss = 0.881 and accuracy of 0.75\n",
      "Iteration 5607: with minibatch training loss = 0.849 and accuracy of 0.77\n",
      "Iteration 5608: with minibatch training loss = 0.785 and accuracy of 0.8\n",
      "Iteration 5609: with minibatch training loss = 1.35 and accuracy of 0.64\n",
      "Iteration 5610: with minibatch training loss = 0.672 and accuracy of 0.81\n",
      "Iteration 5611: with minibatch training loss = 0.871 and accuracy of 0.77\n",
      "Iteration 5612: with minibatch training loss = 0.715 and accuracy of 0.83\n",
      "Iteration 5613: with minibatch training loss = 0.885 and accuracy of 0.77\n",
      "Iteration 5614: with minibatch training loss = 0.951 and accuracy of 0.75\n",
      "Iteration 5615: with minibatch training loss = 0.641 and accuracy of 0.8\n",
      "Iteration 5616: with minibatch training loss = 0.88 and accuracy of 0.77\n",
      "Iteration 5617: with minibatch training loss = 0.973 and accuracy of 0.73\n",
      "Iteration 5618: with minibatch training loss = 0.67 and accuracy of 0.84\n",
      "Iteration 5619: with minibatch training loss = 0.988 and accuracy of 0.7\n",
      "Iteration 5620: with minibatch training loss = 1.21 and accuracy of 0.67\n",
      "Iteration 5621: with minibatch training loss = 1.22 and accuracy of 0.64\n",
      "Iteration 5622: with minibatch training loss = 1 and accuracy of 0.75\n",
      "Iteration 5623: with minibatch training loss = 0.774 and accuracy of 0.78\n",
      "Iteration 5624: with minibatch training loss = 0.953 and accuracy of 0.78\n",
      "Iteration 5625: with minibatch training loss = 0.618 and accuracy of 0.84\n",
      "Iteration 5626: with minibatch training loss = 0.552 and accuracy of 0.86\n",
      "Iteration 5627: with minibatch training loss = 0.751 and accuracy of 0.8\n",
      "Iteration 5628: with minibatch training loss = 0.906 and accuracy of 0.77\n",
      "Iteration 5629: with minibatch training loss = 1.07 and accuracy of 0.7\n",
      "Iteration 5630: with minibatch training loss = 0.785 and accuracy of 0.81\n",
      "Iteration 5631: with minibatch training loss = 0.726 and accuracy of 0.81\n",
      "Iteration 5632: with minibatch training loss = 1.19 and accuracy of 0.69\n",
      "Iteration 5633: with minibatch training loss = 1.12 and accuracy of 0.72\n",
      "Iteration 5634: with minibatch training loss = 0.707 and accuracy of 0.8\n",
      "Iteration 5635: with minibatch training loss = 0.859 and accuracy of 0.77\n",
      "Iteration 5636: with minibatch training loss = 0.943 and accuracy of 0.72\n",
      "Iteration 5637: with minibatch training loss = 0.826 and accuracy of 0.77\n",
      "Iteration 5638: with minibatch training loss = 0.684 and accuracy of 0.78\n",
      "Iteration 5639: with minibatch training loss = 1.09 and accuracy of 0.7\n",
      "Iteration 5640: with minibatch training loss = 1.07 and accuracy of 0.69\n",
      "Iteration 5641: with minibatch training loss = 0.877 and accuracy of 0.77\n",
      "Iteration 5642: with minibatch training loss = 0.703 and accuracy of 0.84\n",
      "Iteration 5643: with minibatch training loss = 0.695 and accuracy of 0.81\n",
      "Iteration 5644: with minibatch training loss = 1 and accuracy of 0.7\n",
      "Iteration 5645: with minibatch training loss = 1.03 and accuracy of 0.75\n",
      "Iteration 5646: with minibatch training loss = 0.772 and accuracy of 0.78\n",
      "Iteration 5647: with minibatch training loss = 0.89 and accuracy of 0.72\n",
      "Iteration 5648: with minibatch training loss = 0.81 and accuracy of 0.78\n",
      "Iteration 5649: with minibatch training loss = 0.923 and accuracy of 0.75\n",
      "Iteration 5650: with minibatch training loss = 0.881 and accuracy of 0.78\n",
      "Iteration 5651: with minibatch training loss = 0.871 and accuracy of 0.75\n",
      "Iteration 5652: with minibatch training loss = 0.951 and accuracy of 0.75\n",
      "Iteration 5653: with minibatch training loss = 1.07 and accuracy of 0.72\n",
      "Iteration 5654: with minibatch training loss = 0.83 and accuracy of 0.77\n",
      "Iteration 5655: with minibatch training loss = 0.955 and accuracy of 0.75\n",
      "Iteration 5656: with minibatch training loss = 0.954 and accuracy of 0.7\n",
      "Iteration 5657: with minibatch training loss = 0.656 and accuracy of 0.84\n",
      "Iteration 5658: with minibatch training loss = 1.15 and accuracy of 0.66\n",
      "Iteration 5659: with minibatch training loss = 0.786 and accuracy of 0.75\n",
      "Iteration 5660: with minibatch training loss = 0.995 and accuracy of 0.72\n",
      "Iteration 5661: with minibatch training loss = 0.612 and accuracy of 0.81\n",
      "Iteration 5662: with minibatch training loss = 0.851 and accuracy of 0.75\n",
      "Iteration 5663: with minibatch training loss = 0.895 and accuracy of 0.8\n",
      "Iteration 5664: with minibatch training loss = 0.735 and accuracy of 0.8\n",
      "Iteration 5665: with minibatch training loss = 0.874 and accuracy of 0.75\n",
      "Iteration 5666: with minibatch training loss = 0.749 and accuracy of 0.8\n",
      "Iteration 5667: with minibatch training loss = 1.05 and accuracy of 0.67\n",
      "Iteration 5668: with minibatch training loss = 0.733 and accuracy of 0.81\n",
      "Iteration 5669: with minibatch training loss = 0.915 and accuracy of 0.78\n",
      "Iteration 5670: with minibatch training loss = 0.927 and accuracy of 0.75\n",
      "Iteration 5671: with minibatch training loss = 1.03 and accuracy of 0.72\n",
      "Iteration 5672: with minibatch training loss = 0.657 and accuracy of 0.83\n",
      "Iteration 5673: with minibatch training loss = 0.877 and accuracy of 0.73\n",
      "Iteration 5674: with minibatch training loss = 0.942 and accuracy of 0.73\n",
      "Iteration 5675: with minibatch training loss = 1.14 and accuracy of 0.7\n",
      "Iteration 5676: with minibatch training loss = 0.689 and accuracy of 0.81\n",
      "Iteration 5677: with minibatch training loss = 0.78 and accuracy of 0.77\n",
      "Iteration 5678: with minibatch training loss = 0.744 and accuracy of 0.8\n",
      "Iteration 5679: with minibatch training loss = 0.608 and accuracy of 0.84\n",
      "Iteration 5680: with minibatch training loss = 1.08 and accuracy of 0.69\n",
      "Iteration 5681: with minibatch training loss = 0.796 and accuracy of 0.8\n",
      "Iteration 5682: with minibatch training loss = 1.02 and accuracy of 0.73\n",
      "Iteration 5683: with minibatch training loss = 0.586 and accuracy of 0.84\n",
      "Iteration 5684: with minibatch training loss = 0.793 and accuracy of 0.77\n",
      "Iteration 5685: with minibatch training loss = 1.15 and accuracy of 0.67\n",
      "Iteration 5686: with minibatch training loss = 0.648 and accuracy of 0.83\n",
      "Iteration 5687: with minibatch training loss = 0.659 and accuracy of 0.81\n",
      "Iteration 5688: with minibatch training loss = 0.798 and accuracy of 0.78\n",
      "Iteration 5689: with minibatch training loss = 0.909 and accuracy of 0.77\n",
      "Iteration 5690: with minibatch training loss = 0.858 and accuracy of 0.77\n",
      "Iteration 5691: with minibatch training loss = 1.03 and accuracy of 0.64\n",
      "Iteration 5692: with minibatch training loss = 0.754 and accuracy of 0.78\n",
      "Iteration 5693: with minibatch training loss = 0.768 and accuracy of 0.8\n",
      "Iteration 5694: with minibatch training loss = 1.13 and accuracy of 0.64\n",
      "Iteration 5695: with minibatch training loss = 0.835 and accuracy of 0.8\n",
      "Iteration 5696: with minibatch training loss = 1.12 and accuracy of 0.66\n",
      "Iteration 5697: with minibatch training loss = 0.508 and accuracy of 0.89\n",
      "Iteration 5698: with minibatch training loss = 0.876 and accuracy of 0.77\n",
      "Iteration 5699: with minibatch training loss = 0.705 and accuracy of 0.83\n",
      "Iteration 5700: with minibatch training loss = 0.998 and accuracy of 0.75\n",
      "Iteration 5701: with minibatch training loss = 1.16 and accuracy of 0.64\n",
      "Iteration 5702: with minibatch training loss = 0.902 and accuracy of 0.78\n",
      "Iteration 5703: with minibatch training loss = 1.11 and accuracy of 0.69\n",
      "Iteration 5704: with minibatch training loss = 0.955 and accuracy of 0.75\n",
      "Iteration 5705: with minibatch training loss = 0.776 and accuracy of 0.81\n",
      "Iteration 5706: with minibatch training loss = 0.839 and accuracy of 0.8\n",
      "Iteration 5707: with minibatch training loss = 0.658 and accuracy of 0.8\n",
      "Iteration 5708: with minibatch training loss = 1.01 and accuracy of 0.7\n",
      "Iteration 5709: with minibatch training loss = 0.65 and accuracy of 0.83\n",
      "Iteration 5710: with minibatch training loss = 1.03 and accuracy of 0.77\n",
      "Iteration 5711: with minibatch training loss = 0.649 and accuracy of 0.83\n",
      "Iteration 5712: with minibatch training loss = 1.03 and accuracy of 0.72\n",
      "Iteration 5713: with minibatch training loss = 0.972 and accuracy of 0.72\n",
      "Iteration 5714: with minibatch training loss = 0.877 and accuracy of 0.78\n",
      "Iteration 5715: with minibatch training loss = 1.15 and accuracy of 0.64\n",
      "Iteration 5716: with minibatch training loss = 0.512 and accuracy of 0.86\n",
      "Iteration 5717: with minibatch training loss = 0.885 and accuracy of 0.73\n",
      "Iteration 5718: with minibatch training loss = 1.03 and accuracy of 0.72\n",
      "Iteration 5719: with minibatch training loss = 0.928 and accuracy of 0.75\n",
      "Iteration 5720: with minibatch training loss = 0.945 and accuracy of 0.75\n",
      "Iteration 5721: with minibatch training loss = 0.802 and accuracy of 0.8\n",
      "Iteration 5722: with minibatch training loss = 0.974 and accuracy of 0.75\n",
      "Iteration 5723: with minibatch training loss = 0.679 and accuracy of 0.83\n",
      "Iteration 5724: with minibatch training loss = 0.906 and accuracy of 0.75\n",
      "Iteration 5725: with minibatch training loss = 1.02 and accuracy of 0.73\n",
      "Iteration 5726: with minibatch training loss = 0.801 and accuracy of 0.75\n",
      "Iteration 5727: with minibatch training loss = 0.944 and accuracy of 0.75\n",
      "Iteration 5728: with minibatch training loss = 0.749 and accuracy of 0.75\n",
      "Iteration 5729: with minibatch training loss = 0.8 and accuracy of 0.77\n",
      "Iteration 5730: with minibatch training loss = 1.05 and accuracy of 0.72\n",
      "Iteration 5731: with minibatch training loss = 1.18 and accuracy of 0.67\n",
      "Iteration 5732: with minibatch training loss = 0.884 and accuracy of 0.77\n",
      "Iteration 5733: with minibatch training loss = 1.04 and accuracy of 0.72\n",
      "Iteration 5734: with minibatch training loss = 0.958 and accuracy of 0.75\n",
      "Iteration 5735: with minibatch training loss = 0.928 and accuracy of 0.75\n",
      "Iteration 5736: with minibatch training loss = 0.819 and accuracy of 0.78\n",
      "Iteration 5737: with minibatch training loss = 0.893 and accuracy of 0.75\n",
      "Iteration 5738: with minibatch training loss = 0.741 and accuracy of 0.83\n",
      "Iteration 5739: with minibatch training loss = 0.78 and accuracy of 0.8\n",
      "Iteration 5740: with minibatch training loss = 0.746 and accuracy of 0.8\n",
      "Iteration 5741: with minibatch training loss = 0.762 and accuracy of 0.8\n",
      "Iteration 5742: with minibatch training loss = 0.735 and accuracy of 0.8\n",
      "Iteration 5743: with minibatch training loss = 0.897 and accuracy of 0.75\n",
      "Iteration 5744: with minibatch training loss = 0.835 and accuracy of 0.77\n",
      "Iteration 5745: with minibatch training loss = 0.934 and accuracy of 0.77\n",
      "Iteration 5746: with minibatch training loss = 1.17 and accuracy of 0.67\n",
      "Iteration 5747: with minibatch training loss = 0.917 and accuracy of 0.72\n",
      "Iteration 5748: with minibatch training loss = 0.87 and accuracy of 0.77\n",
      "Iteration 5749: with minibatch training loss = 0.779 and accuracy of 0.77\n",
      "Iteration 5750: with minibatch training loss = 0.815 and accuracy of 0.77\n",
      "Iteration 5751: with minibatch training loss = 1.03 and accuracy of 0.69\n",
      "Iteration 5752: with minibatch training loss = 1.16 and accuracy of 0.69\n",
      "Iteration 5753: with minibatch training loss = 1.06 and accuracy of 0.69\n",
      "Iteration 5754: with minibatch training loss = 1.15 and accuracy of 0.7\n",
      "Iteration 5755: with minibatch training loss = 0.922 and accuracy of 0.77\n",
      "Iteration 5756: with minibatch training loss = 0.959 and accuracy of 0.73\n",
      "Iteration 5757: with minibatch training loss = 0.567 and accuracy of 0.88\n",
      "Iteration 5758: with minibatch training loss = 1.05 and accuracy of 0.7\n",
      "Iteration 5759: with minibatch training loss = 0.884 and accuracy of 0.72\n",
      "Iteration 5760: with minibatch training loss = 1.04 and accuracy of 0.69\n",
      "Iteration 5761: with minibatch training loss = 1.11 and accuracy of 0.7\n",
      "Iteration 5762: with minibatch training loss = 0.852 and accuracy of 0.77\n",
      "Iteration 5763: with minibatch training loss = 0.991 and accuracy of 0.7\n",
      "Iteration 5764: with minibatch training loss = 0.714 and accuracy of 0.8\n",
      "Iteration 5765: with minibatch training loss = 0.781 and accuracy of 0.77\n",
      "Iteration 5766: with minibatch training loss = 0.71 and accuracy of 0.8\n",
      "Iteration 5767: with minibatch training loss = 0.735 and accuracy of 0.83\n",
      "Iteration 5768: with minibatch training loss = 0.832 and accuracy of 0.8\n",
      "Iteration 5769: with minibatch training loss = 0.972 and accuracy of 0.72\n",
      "Iteration 5770: with minibatch training loss = 1.11 and accuracy of 0.7\n",
      "Iteration 5771: with minibatch training loss = 0.712 and accuracy of 0.8\n",
      "Iteration 5772: with minibatch training loss = 0.653 and accuracy of 0.86\n",
      "Iteration 5773: with minibatch training loss = 0.931 and accuracy of 0.75\n",
      "Iteration 5774: with minibatch training loss = 0.901 and accuracy of 0.75\n",
      "Iteration 5775: with minibatch training loss = 0.856 and accuracy of 0.78\n",
      "Iteration 5776: with minibatch training loss = 0.846 and accuracy of 0.78\n",
      "Iteration 5777: with minibatch training loss = 0.922 and accuracy of 0.75\n",
      "Iteration 5778: with minibatch training loss = 0.821 and accuracy of 0.77\n",
      "Iteration 5779: with minibatch training loss = 0.975 and accuracy of 0.73\n",
      "Iteration 5780: with minibatch training loss = 1.1 and accuracy of 0.67\n",
      "Iteration 5781: with minibatch training loss = 0.928 and accuracy of 0.75\n",
      "Iteration 5782: with minibatch training loss = 0.799 and accuracy of 0.8\n",
      "Iteration 5783: with minibatch training loss = 0.847 and accuracy of 0.78\n",
      "Iteration 5784: with minibatch training loss = 0.98 and accuracy of 0.72\n",
      "Iteration 5785: with minibatch training loss = 0.796 and accuracy of 0.78\n",
      "Iteration 5786: with minibatch training loss = 0.947 and accuracy of 0.73\n",
      "Iteration 5787: with minibatch training loss = 0.742 and accuracy of 0.78\n",
      "Iteration 5788: with minibatch training loss = 0.939 and accuracy of 0.73\n",
      "Iteration 5789: with minibatch training loss = 1.02 and accuracy of 0.7\n",
      "Iteration 5790: with minibatch training loss = 0.65 and accuracy of 0.81\n",
      "Iteration 5791: with minibatch training loss = 0.82 and accuracy of 0.77\n",
      "Iteration 5792: with minibatch training loss = 0.907 and accuracy of 0.72\n",
      "Iteration 5793: with minibatch training loss = 0.948 and accuracy of 0.75\n",
      "Iteration 5794: with minibatch training loss = 0.725 and accuracy of 0.78\n",
      "Iteration 5795: with minibatch training loss = 0.927 and accuracy of 0.75\n",
      "Iteration 5796: with minibatch training loss = 0.969 and accuracy of 0.73\n",
      "Iteration 5797: with minibatch training loss = 0.651 and accuracy of 0.81\n",
      "Iteration 5798: with minibatch training loss = 0.72 and accuracy of 0.81\n",
      "Iteration 5799: with minibatch training loss = 0.801 and accuracy of 0.81\n",
      "Iteration 5800: with minibatch training loss = 1.06 and accuracy of 0.69\n",
      "Iteration 5801: with minibatch training loss = 0.969 and accuracy of 0.69\n",
      "Iteration 5802: with minibatch training loss = 0.76 and accuracy of 0.8\n",
      "Iteration 5803: with minibatch training loss = 0.687 and accuracy of 0.81\n",
      "Iteration 5804: with minibatch training loss = 0.556 and accuracy of 0.86\n",
      "Iteration 5805: with minibatch training loss = 0.832 and accuracy of 0.8\n",
      "Iteration 5806: with minibatch training loss = 1.18 and accuracy of 0.66\n",
      "Iteration 5807: with minibatch training loss = 0.785 and accuracy of 0.78\n",
      "Iteration 5808: with minibatch training loss = 0.888 and accuracy of 0.78\n",
      "Iteration 5809: with minibatch training loss = 0.647 and accuracy of 0.84\n",
      "Iteration 5810: with minibatch training loss = 0.865 and accuracy of 0.75\n",
      "Iteration 5811: with minibatch training loss = 0.864 and accuracy of 0.77\n",
      "Iteration 5812: with minibatch training loss = 0.746 and accuracy of 0.8\n",
      "Iteration 5813: with minibatch training loss = 1.23 and accuracy of 0.64\n",
      "Iteration 5814: with minibatch training loss = 0.727 and accuracy of 0.8\n",
      "Iteration 5815: with minibatch training loss = 0.799 and accuracy of 0.78\n",
      "Iteration 5816: with minibatch training loss = 0.938 and accuracy of 0.75\n",
      "Iteration 5817: with minibatch training loss = 0.921 and accuracy of 0.7\n",
      "Iteration 5818: with minibatch training loss = 0.834 and accuracy of 0.77\n",
      "Iteration 5819: with minibatch training loss = 1.24 and accuracy of 0.62\n",
      "Iteration 5820: with minibatch training loss = 0.9 and accuracy of 0.75\n",
      "Iteration 5821: with minibatch training loss = 0.83 and accuracy of 0.83\n",
      "Iteration 5822: with minibatch training loss = 1.03 and accuracy of 0.75\n",
      "Iteration 5823: with minibatch training loss = 0.992 and accuracy of 0.72\n",
      "Iteration 5824: with minibatch training loss = 0.69 and accuracy of 0.83\n",
      "Iteration 5825: with minibatch training loss = 0.567 and accuracy of 0.86\n",
      "Iteration 5826: with minibatch training loss = 0.958 and accuracy of 0.73\n",
      "Iteration 5827: with minibatch training loss = 0.898 and accuracy of 0.75\n",
      "Iteration 5828: with minibatch training loss = 0.598 and accuracy of 0.84\n",
      "Iteration 5829: with minibatch training loss = 1.07 and accuracy of 0.7\n",
      "Iteration 5830: with minibatch training loss = 0.72 and accuracy of 0.83\n",
      "Iteration 5831: with minibatch training loss = 0.718 and accuracy of 0.78\n",
      "Iteration 5832: with minibatch training loss = 1.02 and accuracy of 0.72\n",
      "Iteration 5833: with minibatch training loss = 0.717 and accuracy of 0.81\n",
      "Iteration 5834: with minibatch training loss = 0.816 and accuracy of 0.78\n",
      "Iteration 5835: with minibatch training loss = 1.24 and accuracy of 0.67\n",
      "Iteration 5836: with minibatch training loss = 0.538 and accuracy of 0.86\n",
      "Iteration 5837: with minibatch training loss = 1.02 and accuracy of 0.7\n",
      "Iteration 5838: with minibatch training loss = 1.02 and accuracy of 0.7\n",
      "Iteration 5839: with minibatch training loss = 1.09 and accuracy of 0.7\n",
      "Iteration 5840: with minibatch training loss = 0.829 and accuracy of 0.78\n",
      "Iteration 5841: with minibatch training loss = 0.88 and accuracy of 0.77\n",
      "Iteration 5842: with minibatch training loss = 0.672 and accuracy of 0.81\n",
      "Iteration 5843: with minibatch training loss = 1.08 and accuracy of 0.7\n",
      "Iteration 5844: with minibatch training loss = 0.808 and accuracy of 0.8\n",
      "Iteration 5845: with minibatch training loss = 0.949 and accuracy of 0.78\n",
      "Iteration 5846: with minibatch training loss = 0.713 and accuracy of 0.84\n",
      "Iteration 5847: with minibatch training loss = 0.902 and accuracy of 0.75\n",
      "Iteration 5848: with minibatch training loss = 0.892 and accuracy of 0.81\n",
      "Iteration 5849: with minibatch training loss = 0.862 and accuracy of 0.77\n",
      "Iteration 5850: with minibatch training loss = 0.601 and accuracy of 0.84\n",
      "Iteration 5851: with minibatch training loss = 0.888 and accuracy of 0.78\n",
      "Iteration 5852: with minibatch training loss = 1.05 and accuracy of 0.72\n",
      "Iteration 5853: with minibatch training loss = 1.1 and accuracy of 0.67\n",
      "Iteration 5854: with minibatch training loss = 0.839 and accuracy of 0.73\n",
      "Iteration 5855: with minibatch training loss = 0.835 and accuracy of 0.8\n",
      "Iteration 5856: with minibatch training loss = 0.928 and accuracy of 0.75\n",
      "Iteration 5857: with minibatch training loss = 0.935 and accuracy of 0.72\n",
      "Iteration 5858: with minibatch training loss = 0.717 and accuracy of 0.8\n",
      "Iteration 5859: with minibatch training loss = 1.12 and accuracy of 0.64\n",
      "Iteration 5860: with minibatch training loss = 0.892 and accuracy of 0.73\n",
      "Iteration 5861: with minibatch training loss = 0.669 and accuracy of 0.81\n",
      "Iteration 5862: with minibatch training loss = 1.13 and accuracy of 0.69\n",
      "Iteration 5863: with minibatch training loss = 0.635 and accuracy of 0.81\n",
      "Iteration 5864: with minibatch training loss = 0.798 and accuracy of 0.8\n",
      "Iteration 5865: with minibatch training loss = 0.523 and accuracy of 0.89\n",
      "Iteration 5866: with minibatch training loss = 0.642 and accuracy of 0.83\n",
      "Iteration 5867: with minibatch training loss = 0.875 and accuracy of 0.78\n",
      "Iteration 5868: with minibatch training loss = 0.85 and accuracy of 0.75\n",
      "Iteration 5869: with minibatch training loss = 0.788 and accuracy of 0.8\n",
      "Iteration 5870: with minibatch training loss = 0.704 and accuracy of 0.83\n",
      "Iteration 5871: with minibatch training loss = 1.02 and accuracy of 0.7\n",
      "Iteration 5872: with minibatch training loss = 0.731 and accuracy of 0.8\n",
      "Iteration 5873: with minibatch training loss = 0.674 and accuracy of 0.78\n",
      "Iteration 5874: with minibatch training loss = 0.889 and accuracy of 0.73\n",
      "Iteration 5875: with minibatch training loss = 0.744 and accuracy of 0.83\n",
      "Iteration 5876: with minibatch training loss = 0.935 and accuracy of 0.73\n",
      "Iteration 5877: with minibatch training loss = 1.28 and accuracy of 0.66\n",
      "Iteration 5878: with minibatch training loss = 0.558 and accuracy of 0.86\n",
      "Iteration 5879: with minibatch training loss = 0.745 and accuracy of 0.8\n",
      "Iteration 5880: with minibatch training loss = 0.787 and accuracy of 0.78\n",
      "Iteration 5881: with minibatch training loss = 0.939 and accuracy of 0.7\n",
      "Iteration 5882: with minibatch training loss = 1.21 and accuracy of 0.66\n",
      "Iteration 5883: with minibatch training loss = 0.718 and accuracy of 0.75\n",
      "Iteration 5884: with minibatch training loss = 0.735 and accuracy of 0.8\n",
      "Iteration 5885: with minibatch training loss = 0.895 and accuracy of 0.73\n",
      "Iteration 5886: with minibatch training loss = 0.964 and accuracy of 0.73\n",
      "Iteration 5887: with minibatch training loss = 0.914 and accuracy of 0.77\n",
      "Iteration 5888: with minibatch training loss = 0.828 and accuracy of 0.77\n",
      "Iteration 5889: with minibatch training loss = 0.921 and accuracy of 0.77\n",
      "Iteration 5890: with minibatch training loss = 0.748 and accuracy of 0.78\n",
      "Iteration 5891: with minibatch training loss = 0.737 and accuracy of 0.83\n",
      "Iteration 5892: with minibatch training loss = 1.16 and accuracy of 0.67\n",
      "Iteration 5893: with minibatch training loss = 0.798 and accuracy of 0.77\n",
      "Iteration 5894: with minibatch training loss = 0.832 and accuracy of 0.8\n",
      "Iteration 5895: with minibatch training loss = 0.815 and accuracy of 0.78\n",
      "Iteration 5896: with minibatch training loss = 1.1 and accuracy of 0.66\n",
      "Iteration 5897: with minibatch training loss = 1.01 and accuracy of 0.73\n",
      "Iteration 5898: with minibatch training loss = 0.678 and accuracy of 0.83\n",
      "Iteration 5899: with minibatch training loss = 0.771 and accuracy of 0.78\n",
      "Iteration 5900: with minibatch training loss = 0.945 and accuracy of 0.77\n",
      "Iteration 5901: with minibatch training loss = 0.885 and accuracy of 0.77\n",
      "Iteration 5902: with minibatch training loss = 0.929 and accuracy of 0.7\n",
      "Iteration 5903: with minibatch training loss = 1.14 and accuracy of 0.66\n",
      "Iteration 5904: with minibatch training loss = 0.842 and accuracy of 0.8\n",
      "Iteration 5905: with minibatch training loss = 0.738 and accuracy of 0.81\n",
      "Iteration 5906: with minibatch training loss = 0.891 and accuracy of 0.73\n",
      "Iteration 5907: with minibatch training loss = 0.887 and accuracy of 0.77\n",
      "Iteration 5908: with minibatch training loss = 0.795 and accuracy of 0.78\n",
      "Iteration 5909: with minibatch training loss = 0.908 and accuracy of 0.7\n",
      "Iteration 5910: with minibatch training loss = 0.734 and accuracy of 0.8\n",
      "Iteration 5911: with minibatch training loss = 0.805 and accuracy of 0.81\n",
      "Iteration 5912: with minibatch training loss = 1.04 and accuracy of 0.69\n",
      "Iteration 5913: with minibatch training loss = 0.901 and accuracy of 0.75\n",
      "Iteration 5914: with minibatch training loss = 0.876 and accuracy of 0.75\n",
      "Iteration 5915: with minibatch training loss = 0.664 and accuracy of 0.84\n",
      "Iteration 5916: with minibatch training loss = 1.03 and accuracy of 0.73\n",
      "Iteration 5917: with minibatch training loss = 1.08 and accuracy of 0.69\n",
      "Iteration 5918: with minibatch training loss = 0.885 and accuracy of 0.75\n",
      "Iteration 5919: with minibatch training loss = 0.977 and accuracy of 0.69\n",
      "Iteration 5920: with minibatch training loss = 0.804 and accuracy of 0.77\n",
      "Iteration 5921: with minibatch training loss = 0.979 and accuracy of 0.7\n",
      "Iteration 5922: with minibatch training loss = 0.685 and accuracy of 0.81\n",
      "Iteration 5923: with minibatch training loss = 1.05 and accuracy of 0.67\n",
      "Iteration 5924: with minibatch training loss = 0.434 and accuracy of 0.89\n",
      "Iteration 5925: with minibatch training loss = 0.703 and accuracy of 0.81\n",
      "Iteration 5926: with minibatch training loss = 1.15 and accuracy of 0.64\n",
      "Iteration 5927: with minibatch training loss = 1.02 and accuracy of 0.72\n",
      "Iteration 5928: with minibatch training loss = 0.937 and accuracy of 0.73\n",
      "Iteration 5929: with minibatch training loss = 0.915 and accuracy of 0.77\n",
      "Iteration 5930: with minibatch training loss = 0.683 and accuracy of 0.81\n",
      "Iteration 5931: with minibatch training loss = 0.936 and accuracy of 0.73\n",
      "Iteration 5932: with minibatch training loss = 0.94 and accuracy of 0.73\n",
      "Iteration 5933: with minibatch training loss = 0.434 and accuracy of 0.89\n",
      "Iteration 5934: with minibatch training loss = 0.992 and accuracy of 0.75\n",
      "Iteration 5935: with minibatch training loss = 1.3 and accuracy of 0.61\n",
      "Iteration 5936: with minibatch training loss = 0.874 and accuracy of 0.75\n",
      "Iteration 5937: with minibatch training loss = 0.921 and accuracy of 0.72\n",
      "Iteration 5938: with minibatch training loss = 0.841 and accuracy of 0.77\n",
      "Iteration 5939: with minibatch training loss = 0.921 and accuracy of 0.75\n",
      "Iteration 5940: with minibatch training loss = 0.898 and accuracy of 0.73\n",
      "Iteration 5941: with minibatch training loss = 0.748 and accuracy of 0.81\n",
      "Iteration 5942: with minibatch training loss = 0.953 and accuracy of 0.75\n",
      "Iteration 5943: with minibatch training loss = 0.799 and accuracy of 0.78\n",
      "Iteration 5944: with minibatch training loss = 0.629 and accuracy of 0.83\n",
      "Iteration 5945: with minibatch training loss = 0.868 and accuracy of 0.75\n",
      "Iteration 5946: with minibatch training loss = 0.959 and accuracy of 0.72\n",
      "Iteration 5947: with minibatch training loss = 0.956 and accuracy of 0.73\n",
      "Iteration 5948: with minibatch training loss = 1.03 and accuracy of 0.7\n",
      "Iteration 5949: with minibatch training loss = 0.762 and accuracy of 0.84\n",
      "Iteration 5950: with minibatch training loss = 1.12 and accuracy of 0.7\n",
      "Iteration 5951: with minibatch training loss = 0.831 and accuracy of 0.77\n",
      "Iteration 5952: with minibatch training loss = 1.01 and accuracy of 0.73\n",
      "Iteration 5953: with minibatch training loss = 0.623 and accuracy of 0.84\n",
      "Iteration 5954: with minibatch training loss = 0.718 and accuracy of 0.78\n",
      "Iteration 5955: with minibatch training loss = 0.747 and accuracy of 0.77\n",
      "Iteration 5956: with minibatch training loss = 0.701 and accuracy of 0.81\n",
      "Iteration 5957: with minibatch training loss = 0.992 and accuracy of 0.73\n",
      "Iteration 5958: with minibatch training loss = 0.555 and accuracy of 0.84\n",
      "Iteration 5959: with minibatch training loss = 0.922 and accuracy of 0.75\n",
      "Iteration 5960: with minibatch training loss = 0.634 and accuracy of 0.84\n",
      "Iteration 5961: with minibatch training loss = 0.839 and accuracy of 0.78\n",
      "Iteration 5962: with minibatch training loss = 0.809 and accuracy of 0.78\n",
      "Iteration 5963: with minibatch training loss = 1.06 and accuracy of 0.7\n",
      "Iteration 5964: with minibatch training loss = 0.765 and accuracy of 0.8\n",
      "Iteration 5965: with minibatch training loss = 0.899 and accuracy of 0.77\n",
      "Iteration 5966: with minibatch training loss = 0.881 and accuracy of 0.75\n",
      "Iteration 5967: with minibatch training loss = 0.755 and accuracy of 0.78\n",
      "Iteration 5968: with minibatch training loss = 0.906 and accuracy of 0.75\n",
      "Iteration 5969: with minibatch training loss = 0.751 and accuracy of 0.81\n",
      "Iteration 5970: with minibatch training loss = 0.715 and accuracy of 0.81\n",
      "Iteration 5971: with minibatch training loss = 0.914 and accuracy of 0.73\n",
      "Iteration 5972: with minibatch training loss = 0.82 and accuracy of 0.78\n",
      "Iteration 5973: with minibatch training loss = 0.937 and accuracy of 0.7\n",
      "Iteration 5974: with minibatch training loss = 0.809 and accuracy of 0.8\n",
      "Iteration 5975: with minibatch training loss = 1.1 and accuracy of 0.7\n",
      "Iteration 5976: with minibatch training loss = 0.825 and accuracy of 0.77\n",
      "Iteration 5977: with minibatch training loss = 0.466 and accuracy of 0.88\n",
      "Iteration 5978: with minibatch training loss = 0.918 and accuracy of 0.72\n",
      "Iteration 5979: with minibatch training loss = 0.691 and accuracy of 0.81\n",
      "Iteration 5980: with minibatch training loss = 0.845 and accuracy of 0.77\n",
      "Iteration 5981: with minibatch training loss = 0.543 and accuracy of 0.88\n",
      "Iteration 5982: with minibatch training loss = 0.736 and accuracy of 0.8\n",
      "Iteration 5983: with minibatch training loss = 0.839 and accuracy of 0.77\n",
      "Iteration 5984: with minibatch training loss = 1 and accuracy of 0.72\n",
      "Iteration 5985: with minibatch training loss = 0.95 and accuracy of 0.72\n",
      "Iteration 5986: with minibatch training loss = 0.706 and accuracy of 0.83\n",
      "Iteration 5987: with minibatch training loss = 0.817 and accuracy of 0.78\n",
      "Iteration 5988: with minibatch training loss = 0.967 and accuracy of 0.72\n",
      "Iteration 5989: with minibatch training loss = 0.684 and accuracy of 0.81\n",
      "Iteration 5990: with minibatch training loss = 0.864 and accuracy of 0.7\n",
      "Iteration 5991: with minibatch training loss = 0.816 and accuracy of 0.75\n",
      "Iteration 5992: with minibatch training loss = 0.926 and accuracy of 0.73\n",
      "Iteration 5993: with minibatch training loss = 1.07 and accuracy of 0.7\n",
      "Iteration 5994: with minibatch training loss = 0.798 and accuracy of 0.78\n",
      "Iteration 5995: with minibatch training loss = 1.11 and accuracy of 0.67\n",
      "Iteration 5996: with minibatch training loss = 0.744 and accuracy of 0.83\n",
      "Iteration 5997: with minibatch training loss = 1.13 and accuracy of 0.66\n",
      "Iteration 5998: with minibatch training loss = 0.692 and accuracy of 0.78\n",
      "Iteration 5999: with minibatch training loss = 0.486 and accuracy of 0.86\n",
      "Iteration 6000: with minibatch training loss = 0.762 and accuracy of 0.78\n",
      "Iteration 6001: with minibatch training loss = 0.988 and accuracy of 0.72\n",
      "Iteration 6002: with minibatch training loss = 0.977 and accuracy of 0.72\n",
      "Iteration 6003: with minibatch training loss = 0.914 and accuracy of 0.77\n",
      "Iteration 6004: with minibatch training loss = 0.923 and accuracy of 0.75\n",
      "Iteration 6005: with minibatch training loss = 0.781 and accuracy of 0.78\n",
      "Iteration 6006: with minibatch training loss = 0.823 and accuracy of 0.8\n",
      "Iteration 6007: with minibatch training loss = 1 and accuracy of 0.73\n",
      "Iteration 6008: with minibatch training loss = 0.724 and accuracy of 0.77\n",
      "Iteration 6009: with minibatch training loss = 0.519 and accuracy of 0.86\n",
      "Iteration 6010: with minibatch training loss = 1.02 and accuracy of 0.72\n",
      "Iteration 6011: with minibatch training loss = 0.86 and accuracy of 0.75\n",
      "Iteration 6012: with minibatch training loss = 0.707 and accuracy of 0.8\n",
      "Iteration 6013: with minibatch training loss = 0.89 and accuracy of 0.75\n",
      "Iteration 6014: with minibatch training loss = 0.624 and accuracy of 0.84\n",
      "Iteration 6015: with minibatch training loss = 0.633 and accuracy of 0.83\n",
      "Iteration 6016: with minibatch training loss = 0.823 and accuracy of 0.78\n",
      "Iteration 6017: with minibatch training loss = 0.93 and accuracy of 0.73\n",
      "Iteration 6018: with minibatch training loss = 0.877 and accuracy of 0.75\n",
      "Iteration 6019: with minibatch training loss = 0.778 and accuracy of 0.75\n",
      "Iteration 6020: with minibatch training loss = 0.676 and accuracy of 0.83\n",
      "Iteration 6021: with minibatch training loss = 0.711 and accuracy of 0.81\n",
      "Iteration 6022: with minibatch training loss = 0.599 and accuracy of 0.83\n",
      "Iteration 6023: with minibatch training loss = 1.01 and accuracy of 0.7\n",
      "Iteration 6024: with minibatch training loss = 0.954 and accuracy of 0.72\n",
      "Iteration 6025: with minibatch training loss = 0.975 and accuracy of 0.75\n",
      "Iteration 6026: with minibatch training loss = 0.654 and accuracy of 0.84\n",
      "Iteration 6027: with minibatch training loss = 1.19 and accuracy of 0.64\n",
      "Iteration 6028: with minibatch training loss = 0.703 and accuracy of 0.81\n",
      "Iteration 6029: with minibatch training loss = 0.672 and accuracy of 0.81\n",
      "Iteration 6030: with minibatch training loss = 0.654 and accuracy of 0.81\n",
      "Iteration 6031: with minibatch training loss = 1.11 and accuracy of 0.72\n",
      "Iteration 6032: with minibatch training loss = 0.895 and accuracy of 0.73\n",
      "Iteration 6033: with minibatch training loss = 0.723 and accuracy of 0.8\n",
      "Iteration 6034: with minibatch training loss = 0.936 and accuracy of 0.73\n",
      "Iteration 6035: with minibatch training loss = 0.939 and accuracy of 0.75\n",
      "Iteration 6036: with minibatch training loss = 0.879 and accuracy of 0.8\n",
      "Iteration 6037: with minibatch training loss = 0.766 and accuracy of 0.78\n",
      "Iteration 6038: with minibatch training loss = 0.633 and accuracy of 0.81\n",
      "Iteration 6039: with minibatch training loss = 0.831 and accuracy of 0.77\n",
      "Iteration 6040: with minibatch training loss = 0.621 and accuracy of 0.81\n",
      "Iteration 6041: with minibatch training loss = 0.984 and accuracy of 0.75\n",
      "Iteration 6042: with minibatch training loss = 1.09 and accuracy of 0.7\n",
      "Iteration 6043: with minibatch training loss = 1.08 and accuracy of 0.69\n",
      "Iteration 6044: with minibatch training loss = 0.618 and accuracy of 0.83\n",
      "Iteration 6045: with minibatch training loss = 0.857 and accuracy of 0.77\n",
      "Iteration 6046: with minibatch training loss = 0.879 and accuracy of 0.75\n",
      "Iteration 6047: with minibatch training loss = 0.984 and accuracy of 0.72\n",
      "Iteration 6048: with minibatch training loss = 0.783 and accuracy of 0.8\n",
      "Iteration 6049: with minibatch training loss = 0.691 and accuracy of 0.81\n",
      "Iteration 6050: with minibatch training loss = 0.712 and accuracy of 0.8\n",
      "Iteration 6051: with minibatch training loss = 0.769 and accuracy of 0.81\n",
      "Iteration 6052: with minibatch training loss = 1.03 and accuracy of 0.69\n",
      "Iteration 6053: with minibatch training loss = 1.01 and accuracy of 0.7\n",
      "Iteration 6054: with minibatch training loss = 0.736 and accuracy of 0.8\n",
      "Iteration 6055: with minibatch training loss = 0.777 and accuracy of 0.78\n",
      "Iteration 6056: with minibatch training loss = 1.26 and accuracy of 0.67\n",
      "Iteration 6057: with minibatch training loss = 0.77 and accuracy of 0.75\n",
      "Iteration 6058: with minibatch training loss = 1.07 and accuracy of 0.72\n",
      "Iteration 6059: with minibatch training loss = 0.832 and accuracy of 0.75\n",
      "Iteration 6060: with minibatch training loss = 0.923 and accuracy of 0.73\n",
      "Iteration 6061: with minibatch training loss = 0.947 and accuracy of 0.78\n",
      "Iteration 6062: with minibatch training loss = 0.638 and accuracy of 0.81\n",
      "Iteration 6063: with minibatch training loss = 0.956 and accuracy of 0.73\n",
      "Iteration 6064: with minibatch training loss = 0.693 and accuracy of 0.81\n",
      "Iteration 6065: with minibatch training loss = 0.871 and accuracy of 0.77\n",
      "Iteration 6066: with minibatch training loss = 0.781 and accuracy of 0.75\n",
      "Iteration 6067: with minibatch training loss = 0.604 and accuracy of 0.81\n",
      "Iteration 6068: with minibatch training loss = 0.852 and accuracy of 0.81\n",
      "Iteration 6069: with minibatch training loss = 0.903 and accuracy of 0.77\n",
      "Iteration 6070: with minibatch training loss = 0.733 and accuracy of 0.83\n",
      "Iteration 6071: with minibatch training loss = 0.748 and accuracy of 0.81\n",
      "Iteration 6072: with minibatch training loss = 0.799 and accuracy of 0.75\n",
      "Iteration 6073: with minibatch training loss = 0.959 and accuracy of 0.73\n",
      "Iteration 6074: with minibatch training loss = 0.455 and accuracy of 0.89\n",
      "Iteration 6075: with minibatch training loss = 0.825 and accuracy of 0.77\n",
      "Iteration 6076: with minibatch training loss = 1.03 and accuracy of 0.75\n",
      "Iteration 6077: with minibatch training loss = 0.991 and accuracy of 0.69\n",
      "Iteration 6078: with minibatch training loss = 0.9 and accuracy of 0.72\n",
      "Iteration 6079: with minibatch training loss = 0.904 and accuracy of 0.75\n",
      "Iteration 6080: with minibatch training loss = 0.653 and accuracy of 0.83\n",
      "Iteration 6081: with minibatch training loss = 0.983 and accuracy of 0.73\n",
      "Iteration 6082: with minibatch training loss = 0.593 and accuracy of 0.86\n",
      "Iteration 6083: with minibatch training loss = 0.616 and accuracy of 0.84\n",
      "Iteration 6084: with minibatch training loss = 0.885 and accuracy of 0.73\n",
      "Iteration 6085: with minibatch training loss = 0.769 and accuracy of 0.81\n",
      "Iteration 6086: with minibatch training loss = 0.818 and accuracy of 0.77\n",
      "Iteration 6087: with minibatch training loss = 0.967 and accuracy of 0.72\n",
      "Iteration 6088: with minibatch training loss = 0.687 and accuracy of 0.81\n",
      "Iteration 6089: with minibatch training loss = 0.841 and accuracy of 0.77\n",
      "Iteration 6090: with minibatch training loss = 0.763 and accuracy of 0.8\n",
      "Iteration 6091: with minibatch training loss = 0.756 and accuracy of 0.8\n",
      "Iteration 6092: with minibatch training loss = 0.899 and accuracy of 0.7\n",
      "Iteration 6093: with minibatch training loss = 0.691 and accuracy of 0.81\n",
      "Iteration 6094: with minibatch training loss = 0.905 and accuracy of 0.77\n",
      "Iteration 6095: with minibatch training loss = 0.626 and accuracy of 0.81\n",
      "Iteration 6096: with minibatch training loss = 1.16 and accuracy of 0.62\n",
      "Iteration 6097: with minibatch training loss = 0.928 and accuracy of 0.72\n",
      "Iteration 6098: with minibatch training loss = 0.841 and accuracy of 0.75\n",
      "Iteration 6099: with minibatch training loss = 0.87 and accuracy of 0.78\n",
      "Iteration 6100: with minibatch training loss = 1.13 and accuracy of 0.69\n",
      "Iteration 6101: with minibatch training loss = 0.864 and accuracy of 0.77\n",
      "Iteration 6102: with minibatch training loss = 0.819 and accuracy of 0.77\n",
      "Iteration 6103: with minibatch training loss = 1.14 and accuracy of 0.67\n",
      "Iteration 6104: with minibatch training loss = 0.817 and accuracy of 0.78\n",
      "Iteration 6105: with minibatch training loss = 0.864 and accuracy of 0.77\n",
      "Iteration 6106: with minibatch training loss = 0.758 and accuracy of 0.8\n",
      "Iteration 6107: with minibatch training loss = 0.736 and accuracy of 0.81\n",
      "Iteration 6108: with minibatch training loss = 0.526 and accuracy of 0.86\n",
      "Iteration 6109: with minibatch training loss = 0.768 and accuracy of 0.78\n",
      "Iteration 6110: with minibatch training loss = 0.926 and accuracy of 0.72\n",
      "Iteration 6111: with minibatch training loss = 0.877 and accuracy of 0.73\n",
      "Iteration 6112: with minibatch training loss = 0.708 and accuracy of 0.81\n",
      "Iteration 6113: with minibatch training loss = 0.69 and accuracy of 0.83\n",
      "Iteration 6114: with minibatch training loss = 0.947 and accuracy of 0.73\n",
      "Iteration 6115: with minibatch training loss = 0.904 and accuracy of 0.75\n",
      "Iteration 6116: with minibatch training loss = 1.12 and accuracy of 0.67\n",
      "Iteration 6117: with minibatch training loss = 1.29 and accuracy of 0.64\n",
      "Iteration 6118: with minibatch training loss = 1.19 and accuracy of 0.67\n",
      "Iteration 6119: with minibatch training loss = 0.663 and accuracy of 0.84\n",
      "Iteration 6120: with minibatch training loss = 0.71 and accuracy of 0.8\n",
      "Iteration 6121: with minibatch training loss = 0.916 and accuracy of 0.72\n",
      "Iteration 6122: with minibatch training loss = 0.898 and accuracy of 0.73\n",
      "Iteration 6123: with minibatch training loss = 0.637 and accuracy of 0.81\n",
      "Iteration 6124: with minibatch training loss = 1.06 and accuracy of 0.7\n",
      "Iteration 6125: with minibatch training loss = 1.05 and accuracy of 0.77\n",
      "Iteration 6126: with minibatch training loss = 0.945 and accuracy of 0.73\n",
      "Iteration 6127: with minibatch training loss = 0.869 and accuracy of 0.78\n",
      "Iteration 6128: with minibatch training loss = 0.943 and accuracy of 0.72\n",
      "Iteration 6129: with minibatch training loss = 0.82 and accuracy of 0.8\n",
      "Iteration 6130: with minibatch training loss = 0.826 and accuracy of 0.75\n",
      "Iteration 6131: with minibatch training loss = 0.966 and accuracy of 0.72\n",
      "Iteration 6132: with minibatch training loss = 0.719 and accuracy of 0.78\n",
      "Iteration 6133: with minibatch training loss = 1.09 and accuracy of 0.69\n",
      "Iteration 6134: with minibatch training loss = 0.596 and accuracy of 0.83\n",
      "Iteration 6135: with minibatch training loss = 1 and accuracy of 0.72\n",
      "Iteration 6136: with minibatch training loss = 1.1 and accuracy of 0.69\n",
      "Iteration 6137: with minibatch training loss = 0.912 and accuracy of 0.75\n",
      "Iteration 6138: with minibatch training loss = 0.578 and accuracy of 0.84\n",
      "Iteration 6139: with minibatch training loss = 0.941 and accuracy of 0.73\n",
      "Iteration 6140: with minibatch training loss = 0.754 and accuracy of 0.78\n",
      "Iteration 6141: with minibatch training loss = 1.19 and accuracy of 0.64\n",
      "Iteration 6142: with minibatch training loss = 0.777 and accuracy of 0.77\n",
      "Iteration 6143: with minibatch training loss = 0.666 and accuracy of 0.84\n",
      "Iteration 6144: with minibatch training loss = 0.669 and accuracy of 0.81\n",
      "Iteration 6145: with minibatch training loss = 0.922 and accuracy of 0.75\n",
      "Iteration 6146: with minibatch training loss = 0.767 and accuracy of 0.77\n",
      "Iteration 6147: with minibatch training loss = 1.04 and accuracy of 0.75\n",
      "Iteration 6148: with minibatch training loss = 0.982 and accuracy of 0.77\n",
      "Iteration 6149: with minibatch training loss = 0.625 and accuracy of 0.8\n",
      "Iteration 6150: with minibatch training loss = 1.1 and accuracy of 0.69\n",
      "Iteration 6151: with minibatch training loss = 0.763 and accuracy of 0.8\n",
      "Iteration 6152: with minibatch training loss = 0.94 and accuracy of 0.75\n",
      "Iteration 6153: with minibatch training loss = 0.882 and accuracy of 0.77\n",
      "Iteration 6154: with minibatch training loss = 0.666 and accuracy of 0.84\n",
      "Iteration 6155: with minibatch training loss = 0.703 and accuracy of 0.8\n",
      "Iteration 6156: with minibatch training loss = 0.835 and accuracy of 0.78\n",
      "Iteration 6157: with minibatch training loss = 1.15 and accuracy of 0.69\n",
      "Iteration 6158: with minibatch training loss = 0.953 and accuracy of 0.73\n",
      "Iteration 6159: with minibatch training loss = 0.619 and accuracy of 0.84\n",
      "Iteration 6160: with minibatch training loss = 0.838 and accuracy of 0.75\n",
      "Iteration 6161: with minibatch training loss = 0.727 and accuracy of 0.81\n",
      "Iteration 6162: with minibatch training loss = 0.718 and accuracy of 0.81\n",
      "Iteration 6163: with minibatch training loss = 1 and accuracy of 0.67\n",
      "Iteration 6164: with minibatch training loss = 0.877 and accuracy of 0.77\n",
      "Iteration 6165: with minibatch training loss = 0.97 and accuracy of 0.73\n",
      "Iteration 6166: with minibatch training loss = 1.08 and accuracy of 0.7\n",
      "Iteration 6167: with minibatch training loss = 0.7 and accuracy of 0.81\n",
      "Iteration 6168: with minibatch training loss = 0.776 and accuracy of 0.75\n",
      "Iteration 6169: with minibatch training loss = 0.842 and accuracy of 0.77\n",
      "Iteration 6170: with minibatch training loss = 1.15 and accuracy of 0.66\n",
      "Iteration 6171: with minibatch training loss = 1.07 and accuracy of 0.69\n",
      "Iteration 6172: with minibatch training loss = 1 and accuracy of 0.72\n",
      "Iteration 6173: with minibatch training loss = 0.828 and accuracy of 0.78\n",
      "Iteration 6174: with minibatch training loss = 0.723 and accuracy of 0.81\n",
      "Iteration 6175: with minibatch training loss = 0.849 and accuracy of 0.75\n",
      "Iteration 6176: with minibatch training loss = 0.821 and accuracy of 0.78\n",
      "Iteration 6177: with minibatch training loss = 1.14 and accuracy of 0.67\n",
      "Iteration 6178: with minibatch training loss = 0.857 and accuracy of 0.77\n",
      "Iteration 6179: with minibatch training loss = 1.16 and accuracy of 0.62\n",
      "Iteration 6180: with minibatch training loss = 0.805 and accuracy of 0.77\n",
      "Iteration 6181: with minibatch training loss = 0.802 and accuracy of 0.77\n",
      "Iteration 6182: with minibatch training loss = 0.774 and accuracy of 0.8\n",
      "Iteration 6183: with minibatch training loss = 0.916 and accuracy of 0.75\n",
      "Iteration 6184: with minibatch training loss = 0.944 and accuracy of 0.73\n",
      "Iteration 6185: with minibatch training loss = 0.751 and accuracy of 0.78\n",
      "Iteration 6186: with minibatch training loss = 0.615 and accuracy of 0.86\n",
      "Iteration 6187: with minibatch training loss = 1 and accuracy of 0.75\n",
      "Iteration 6188: with minibatch training loss = 0.86 and accuracy of 0.78\n",
      "Iteration 6189: with minibatch training loss = 0.809 and accuracy of 0.78\n",
      "Iteration 6190: with minibatch training loss = 0.686 and accuracy of 0.8\n",
      "Iteration 6191: with minibatch training loss = 0.719 and accuracy of 0.8\n",
      "Iteration 6192: with minibatch training loss = 0.698 and accuracy of 0.81\n",
      "Iteration 6193: with minibatch training loss = 0.944 and accuracy of 0.72\n",
      "Iteration 6194: with minibatch training loss = 0.949 and accuracy of 0.73\n",
      "Iteration 6195: with minibatch training loss = 0.807 and accuracy of 0.8\n",
      "Iteration 6196: with minibatch training loss = 1.31 and accuracy of 0.62\n",
      "Iteration 6197: with minibatch training loss = 0.773 and accuracy of 0.77\n",
      "Iteration 6198: with minibatch training loss = 0.736 and accuracy of 0.81\n",
      "Iteration 6199: with minibatch training loss = 1.08 and accuracy of 0.69\n",
      "Iteration 6200: with minibatch training loss = 0.473 and accuracy of 0.86\n",
      "Iteration 6201: with minibatch training loss = 1.03 and accuracy of 0.72\n",
      "Iteration 6202: with minibatch training loss = 0.596 and accuracy of 0.84\n",
      "Iteration 6203: with minibatch training loss = 0.872 and accuracy of 0.7\n",
      "Iteration 6204: with minibatch training loss = 0.822 and accuracy of 0.75\n",
      "Iteration 6205: with minibatch training loss = 0.842 and accuracy of 0.73\n",
      "Iteration 6206: with minibatch training loss = 0.961 and accuracy of 0.75\n",
      "Iteration 6207: with minibatch training loss = 0.905 and accuracy of 0.72\n",
      "Iteration 6208: with minibatch training loss = 1.13 and accuracy of 0.72\n",
      "Iteration 6209: with minibatch training loss = 0.953 and accuracy of 0.72\n",
      "Iteration 6210: with minibatch training loss = 1.12 and accuracy of 0.62\n",
      "Iteration 6211: with minibatch training loss = 0.669 and accuracy of 0.81\n",
      "Iteration 6212: with minibatch training loss = 0.798 and accuracy of 0.78\n",
      "Iteration 6213: with minibatch training loss = 0.838 and accuracy of 0.75\n",
      "Iteration 6214: with minibatch training loss = 0.811 and accuracy of 0.81\n",
      "Iteration 6215: with minibatch training loss = 0.797 and accuracy of 0.77\n",
      "Iteration 6216: with minibatch training loss = 0.982 and accuracy of 0.73\n",
      "Iteration 6217: with minibatch training loss = 0.816 and accuracy of 0.78\n",
      "Iteration 6218: with minibatch training loss = 0.515 and accuracy of 0.86\n",
      "Iteration 6219: with minibatch training loss = 0.82 and accuracy of 0.75\n",
      "Iteration 6220: with minibatch training loss = 1.05 and accuracy of 0.7\n",
      "Iteration 6221: with minibatch training loss = 0.912 and accuracy of 0.75\n",
      "Iteration 6222: with minibatch training loss = 0.643 and accuracy of 0.83\n",
      "Iteration 6223: with minibatch training loss = 0.992 and accuracy of 0.72\n",
      "Iteration 6224: with minibatch training loss = 0.719 and accuracy of 0.83\n",
      "Iteration 6225: with minibatch training loss = 0.924 and accuracy of 0.72\n",
      "Iteration 6226: with minibatch training loss = 0.779 and accuracy of 0.77\n",
      "Iteration 6227: with minibatch training loss = 0.776 and accuracy of 0.8\n",
      "Iteration 6228: with minibatch training loss = 0.689 and accuracy of 0.81\n",
      "Iteration 6229: with minibatch training loss = 0.873 and accuracy of 0.77\n",
      "Iteration 6230: with minibatch training loss = 1.06 and accuracy of 0.69\n",
      "Iteration 6231: with minibatch training loss = 1.12 and accuracy of 0.67\n",
      "Iteration 6232: with minibatch training loss = 1.02 and accuracy of 0.72\n",
      "Iteration 6233: with minibatch training loss = 1.03 and accuracy of 0.7\n",
      "Iteration 6234: with minibatch training loss = 0.81 and accuracy of 0.78\n",
      "Iteration 6235: with minibatch training loss = 0.95 and accuracy of 0.77\n",
      "Iteration 6236: with minibatch training loss = 1.06 and accuracy of 0.73\n",
      "Iteration 6237: with minibatch training loss = 0.724 and accuracy of 0.8\n",
      "Iteration 6238: with minibatch training loss = 0.696 and accuracy of 0.78\n",
      "Iteration 6239: with minibatch training loss = 0.904 and accuracy of 0.75\n",
      "Iteration 6240: with minibatch training loss = 1.14 and accuracy of 0.66\n",
      "Iteration 6241: with minibatch training loss = 1.2 and accuracy of 0.69\n",
      "Iteration 6242: with minibatch training loss = 0.869 and accuracy of 0.77\n",
      "Iteration 6243: with minibatch training loss = 1.06 and accuracy of 0.7\n",
      "Iteration 6244: with minibatch training loss = 0.848 and accuracy of 0.77\n",
      "Iteration 6245: with minibatch training loss = 0.722 and accuracy of 0.8\n",
      "Iteration 6246: with minibatch training loss = 0.896 and accuracy of 0.75\n",
      "Iteration 6247: with minibatch training loss = 0.756 and accuracy of 0.83\n",
      "Iteration 6248: with minibatch training loss = 0.834 and accuracy of 0.73\n",
      "Iteration 6249: with minibatch training loss = 0.627 and accuracy of 0.83\n",
      "Iteration 6250: with minibatch training loss = 0.96 and accuracy of 0.75\n",
      "Iteration 6251: with minibatch training loss = 0.868 and accuracy of 0.75\n",
      "Iteration 6252: with minibatch training loss = 0.926 and accuracy of 0.73\n",
      "Iteration 6253: with minibatch training loss = 0.728 and accuracy of 0.78\n",
      "Iteration 6254: with minibatch training loss = 0.77 and accuracy of 0.78\n",
      "Iteration 6255: with minibatch training loss = 1.14 and accuracy of 0.66\n",
      "Iteration 6256: with minibatch training loss = 1.03 and accuracy of 0.69\n",
      "Iteration 6257: with minibatch training loss = 1.1 and accuracy of 0.7\n",
      "Iteration 6258: with minibatch training loss = 1.02 and accuracy of 0.69\n",
      "Iteration 6259: with minibatch training loss = 0.83 and accuracy of 0.72\n",
      "Iteration 6260: with minibatch training loss = 1.12 and accuracy of 0.66\n",
      "Iteration 6261: with minibatch training loss = 1.19 and accuracy of 0.62\n",
      "Iteration 6262: with minibatch training loss = 0.918 and accuracy of 0.73\n",
      "Iteration 6263: with minibatch training loss = 0.914 and accuracy of 0.73\n",
      "Iteration 6264: with minibatch training loss = 0.814 and accuracy of 0.77\n",
      "Iteration 6265: with minibatch training loss = 0.837 and accuracy of 0.75\n",
      "Iteration 6266: with minibatch training loss = 0.951 and accuracy of 0.75\n",
      "Iteration 6267: with minibatch training loss = 0.642 and accuracy of 0.84\n",
      "Iteration 6268: with minibatch training loss = 0.941 and accuracy of 0.73\n",
      "Iteration 6269: with minibatch training loss = 0.856 and accuracy of 0.78\n",
      "Iteration 6270: with minibatch training loss = 0.937 and accuracy of 0.75\n",
      "Iteration 6271: with minibatch training loss = 1.07 and accuracy of 0.72\n",
      "Iteration 6272: with minibatch training loss = 0.831 and accuracy of 0.75\n",
      "Iteration 6273: with minibatch training loss = 0.828 and accuracy of 0.77\n",
      "Iteration 6274: with minibatch training loss = 0.967 and accuracy of 0.73\n",
      "Iteration 6275: with minibatch training loss = 0.577 and accuracy of 0.88\n",
      "Iteration 6276: with minibatch training loss = 1.06 and accuracy of 0.67\n",
      "Iteration 6277: with minibatch training loss = 0.836 and accuracy of 0.73\n",
      "Iteration 6278: with minibatch training loss = 0.672 and accuracy of 0.83\n",
      "Iteration 6279: with minibatch training loss = 0.633 and accuracy of 0.83\n",
      "Iteration 6280: with minibatch training loss = 1.25 and accuracy of 0.62\n",
      "Iteration 6281: with minibatch training loss = 0.882 and accuracy of 0.75\n",
      "Iteration 6282: with minibatch training loss = 0.928 and accuracy of 0.75\n",
      "Iteration 6283: with minibatch training loss = 0.75 and accuracy of 0.77\n",
      "Iteration 6284: with minibatch training loss = 0.694 and accuracy of 0.81\n",
      "Iteration 6285: with minibatch training loss = 0.682 and accuracy of 0.8\n",
      "Iteration 6286: with minibatch training loss = 0.839 and accuracy of 0.75\n",
      "Iteration 6287: with minibatch training loss = 1.06 and accuracy of 0.72\n",
      "Iteration 6288: with minibatch training loss = 0.815 and accuracy of 0.73\n",
      "Iteration 6289: with minibatch training loss = 0.634 and accuracy of 0.84\n",
      "Iteration 6290: with minibatch training loss = 0.912 and accuracy of 0.73\n",
      "Iteration 6291: with minibatch training loss = 0.779 and accuracy of 0.77\n",
      "Iteration 6292: with minibatch training loss = 0.947 and accuracy of 0.73\n",
      "Iteration 6293: with minibatch training loss = 0.757 and accuracy of 0.8\n",
      "Iteration 6294: with minibatch training loss = 0.904 and accuracy of 0.73\n",
      "Iteration 6295: with minibatch training loss = 0.937 and accuracy of 0.75\n",
      "Iteration 6296: with minibatch training loss = 0.672 and accuracy of 0.81\n",
      "Iteration 6297: with minibatch training loss = 1.15 and accuracy of 0.7\n",
      "Iteration 6298: with minibatch training loss = 0.907 and accuracy of 0.73\n",
      "Iteration 6299: with minibatch training loss = 0.822 and accuracy of 0.8\n",
      "Iteration 6300: with minibatch training loss = 0.889 and accuracy of 0.78\n",
      "Iteration 6301: with minibatch training loss = 0.634 and accuracy of 0.81\n",
      "Iteration 6302: with minibatch training loss = 0.995 and accuracy of 0.73\n",
      "Iteration 6303: with minibatch training loss = 0.698 and accuracy of 0.8\n",
      "Iteration 6304: with minibatch training loss = 0.749 and accuracy of 0.8\n",
      "Iteration 6305: with minibatch training loss = 0.712 and accuracy of 0.78\n",
      "Iteration 6306: with minibatch training loss = 0.841 and accuracy of 0.72\n",
      "Iteration 6307: with minibatch training loss = 1.08 and accuracy of 0.69\n",
      "Iteration 6308: with minibatch training loss = 0.982 and accuracy of 0.73\n",
      "Iteration 6309: with minibatch training loss = 1.05 and accuracy of 0.7\n",
      "Iteration 6310: with minibatch training loss = 0.931 and accuracy of 0.75\n",
      "Iteration 6311: with minibatch training loss = 1.18 and accuracy of 0.69\n",
      "Iteration 6312: with minibatch training loss = 1.02 and accuracy of 0.7\n",
      "Iteration 6313: with minibatch training loss = 0.916 and accuracy of 0.7\n",
      "Iteration 6314: with minibatch training loss = 0.951 and accuracy of 0.73\n",
      "Iteration 6315: with minibatch training loss = 0.751 and accuracy of 0.78\n",
      "Iteration 6316: with minibatch training loss = 0.921 and accuracy of 0.73\n",
      "Iteration 6317: with minibatch training loss = 1.06 and accuracy of 0.72\n",
      "Iteration 6318: with minibatch training loss = 1.13 and accuracy of 0.67\n",
      "Iteration 6319: with minibatch training loss = 0.585 and accuracy of 0.86\n",
      "Iteration 6320: with minibatch training loss = 0.825 and accuracy of 0.8\n",
      "Iteration 6321: with minibatch training loss = 0.854 and accuracy of 0.77\n",
      "Iteration 6322: with minibatch training loss = 1.01 and accuracy of 0.72\n",
      "Iteration 6323: with minibatch training loss = 0.74 and accuracy of 0.8\n",
      "Iteration 6324: with minibatch training loss = 0.846 and accuracy of 0.75\n",
      "Iteration 6325: with minibatch training loss = 0.75 and accuracy of 0.77\n",
      "Iteration 6326: with minibatch training loss = 1.16 and accuracy of 0.67\n",
      "Iteration 6327: with minibatch training loss = 0.645 and accuracy of 0.81\n",
      "Iteration 6328: with minibatch training loss = 0.768 and accuracy of 0.77\n",
      "Iteration 6329: with minibatch training loss = 0.856 and accuracy of 0.75\n",
      "Iteration 6330: with minibatch training loss = 1.07 and accuracy of 0.69\n",
      "Iteration 6331: with minibatch training loss = 0.712 and accuracy of 0.78\n",
      "Iteration 6332: with minibatch training loss = 0.954 and accuracy of 0.75\n",
      "Iteration 6333: with minibatch training loss = 0.899 and accuracy of 0.73\n",
      "Iteration 6334: with minibatch training loss = 1.11 and accuracy of 0.66\n",
      "Iteration 6335: with minibatch training loss = 0.744 and accuracy of 0.8\n",
      "Iteration 6336: with minibatch training loss = 0.862 and accuracy of 0.75\n",
      "Iteration 6337: with minibatch training loss = 1.28 and accuracy of 0.66\n",
      "Iteration 6338: with minibatch training loss = 0.633 and accuracy of 0.84\n",
      "Iteration 6339: with minibatch training loss = 0.769 and accuracy of 0.77\n",
      "Iteration 6340: with minibatch training loss = 1.11 and accuracy of 0.7\n",
      "Iteration 6341: with minibatch training loss = 0.582 and accuracy of 0.86\n",
      "Iteration 6342: with minibatch training loss = 0.94 and accuracy of 0.75\n",
      "Iteration 6343: with minibatch training loss = 0.848 and accuracy of 0.75\n",
      "Iteration 6344: with minibatch training loss = 0.725 and accuracy of 0.78\n",
      "Iteration 6345: with minibatch training loss = 0.605 and accuracy of 0.81\n",
      "Iteration 6346: with minibatch training loss = 1.07 and accuracy of 0.67\n",
      "Iteration 6347: with minibatch training loss = 0.826 and accuracy of 0.75\n",
      "Iteration 6348: with minibatch training loss = 1.01 and accuracy of 0.72\n",
      "Iteration 6349: with minibatch training loss = 1.15 and accuracy of 0.72\n",
      "Iteration 6350: with minibatch training loss = 0.777 and accuracy of 0.78\n",
      "Iteration 6351: with minibatch training loss = 0.78 and accuracy of 0.78\n",
      "Iteration 6352: with minibatch training loss = 0.837 and accuracy of 0.73\n",
      "Iteration 6353: with minibatch training loss = 0.732 and accuracy of 0.81\n",
      "Iteration 6354: with minibatch training loss = 0.793 and accuracy of 0.73\n",
      "Iteration 6355: with minibatch training loss = 1.01 and accuracy of 0.72\n",
      "Iteration 6356: with minibatch training loss = 0.663 and accuracy of 0.84\n",
      "Iteration 6357: with minibatch training loss = 1.08 and accuracy of 0.69\n",
      "Iteration 6358: with minibatch training loss = 0.835 and accuracy of 0.78\n",
      "Iteration 6359: with minibatch training loss = 0.881 and accuracy of 0.77\n",
      "Iteration 6360: with minibatch training loss = 0.852 and accuracy of 0.78\n",
      "Iteration 6361: with minibatch training loss = 0.867 and accuracy of 0.75\n",
      "Iteration 6362: with minibatch training loss = 0.925 and accuracy of 0.73\n",
      "Iteration 6363: with minibatch training loss = 0.73 and accuracy of 0.78\n",
      "Iteration 6364: with minibatch training loss = 1.01 and accuracy of 0.67\n",
      "Iteration 6365: with minibatch training loss = 0.741 and accuracy of 0.8\n",
      "Iteration 6366: with minibatch training loss = 0.766 and accuracy of 0.81\n",
      "Iteration 6367: with minibatch training loss = 0.864 and accuracy of 0.75\n",
      "Iteration 6368: with minibatch training loss = 0.932 and accuracy of 0.75\n",
      "Iteration 6369: with minibatch training loss = 0.829 and accuracy of 0.78\n",
      "Iteration 6370: with minibatch training loss = 0.972 and accuracy of 0.72\n",
      "Iteration 6371: with minibatch training loss = 1.07 and accuracy of 0.69\n",
      "Iteration 6372: with minibatch training loss = 0.833 and accuracy of 0.77\n",
      "Iteration 6373: with minibatch training loss = 0.764 and accuracy of 0.8\n",
      "Iteration 6374: with minibatch training loss = 0.855 and accuracy of 0.77\n",
      "Iteration 6375: with minibatch training loss = 0.963 and accuracy of 0.72\n",
      "Iteration 6376: with minibatch training loss = 0.706 and accuracy of 0.78\n",
      "Iteration 6377: with minibatch training loss = 0.559 and accuracy of 0.84\n",
      "Iteration 6378: with minibatch training loss = 0.727 and accuracy of 0.8\n",
      "Iteration 6379: with minibatch training loss = 0.952 and accuracy of 0.73\n",
      "Iteration 6380: with minibatch training loss = 0.752 and accuracy of 0.8\n",
      "Iteration 6381: with minibatch training loss = 0.57 and accuracy of 0.81\n",
      "Iteration 6382: with minibatch training loss = 0.964 and accuracy of 0.75\n",
      "Iteration 6383: with minibatch training loss = 0.76 and accuracy of 0.8\n",
      "Iteration 6384: with minibatch training loss = 0.884 and accuracy of 0.77\n",
      "Iteration 6385: with minibatch training loss = 0.764 and accuracy of 0.83\n",
      "Iteration 6386: with minibatch training loss = 0.947 and accuracy of 0.75\n",
      "Iteration 6387: with minibatch training loss = 0.727 and accuracy of 0.8\n",
      "Iteration 6388: with minibatch training loss = 0.886 and accuracy of 0.75\n",
      "Iteration 6389: with minibatch training loss = 0.842 and accuracy of 0.75\n",
      "Iteration 6390: with minibatch training loss = 1.03 and accuracy of 0.72\n",
      "Iteration 6391: with minibatch training loss = 1.07 and accuracy of 0.66\n",
      "Iteration 6392: with minibatch training loss = 1.08 and accuracy of 0.67\n",
      "Iteration 6393: with minibatch training loss = 0.925 and accuracy of 0.73\n",
      "Iteration 6394: with minibatch training loss = 0.682 and accuracy of 0.81\n",
      "Iteration 6395: with minibatch training loss = 0.733 and accuracy of 0.78\n",
      "Iteration 6396: with minibatch training loss = 0.744 and accuracy of 0.77\n",
      "Iteration 6397: with minibatch training loss = 0.73 and accuracy of 0.78\n",
      "Iteration 6398: with minibatch training loss = 0.73 and accuracy of 0.78\n",
      "Iteration 6399: with minibatch training loss = 0.956 and accuracy of 0.7\n",
      "Iteration 6400: with minibatch training loss = 0.996 and accuracy of 0.73\n",
      "Iteration 6401: with minibatch training loss = 0.694 and accuracy of 0.81\n",
      "Iteration 6402: with minibatch training loss = 0.569 and accuracy of 0.86\n",
      "Iteration 6403: with minibatch training loss = 0.667 and accuracy of 0.84\n",
      "Iteration 6404: with minibatch training loss = 0.709 and accuracy of 0.81\n",
      "Iteration 6405: with minibatch training loss = 0.858 and accuracy of 0.77\n",
      "Iteration 6406: with minibatch training loss = 0.717 and accuracy of 0.81\n",
      "Iteration 6407: with minibatch training loss = 0.738 and accuracy of 0.78\n",
      "Iteration 6408: with minibatch training loss = 0.683 and accuracy of 0.77\n",
      "Iteration 6409: with minibatch training loss = 1.02 and accuracy of 0.69\n",
      "Iteration 6410: with minibatch training loss = 0.783 and accuracy of 0.77\n",
      "Iteration 6411: with minibatch training loss = 1.13 and accuracy of 0.64\n",
      "Iteration 6412: with minibatch training loss = 1.17 and accuracy of 0.7\n",
      "Iteration 6413: with minibatch training loss = 0.923 and accuracy of 0.75\n",
      "Iteration 6414: with minibatch training loss = 0.963 and accuracy of 0.75\n",
      "Iteration 6415: with minibatch training loss = 0.946 and accuracy of 0.72\n",
      "Iteration 6416: with minibatch training loss = 0.525 and accuracy of 0.86\n",
      "Iteration 6417: with minibatch training loss = 0.78 and accuracy of 0.78\n",
      "Iteration 6418: with minibatch training loss = 0.77 and accuracy of 0.77\n",
      "Iteration 6419: with minibatch training loss = 0.928 and accuracy of 0.73\n",
      "Iteration 6420: with minibatch training loss = 0.73 and accuracy of 0.78\n",
      "Iteration 6421: with minibatch training loss = 0.961 and accuracy of 0.7\n",
      "Iteration 6422: with minibatch training loss = 0.88 and accuracy of 0.77\n",
      "Iteration 6423: with minibatch training loss = 1.02 and accuracy of 0.7\n",
      "Iteration 6424: with minibatch training loss = 0.836 and accuracy of 0.73\n",
      "Iteration 6425: with minibatch training loss = 0.998 and accuracy of 0.73\n",
      "Iteration 6426: with minibatch training loss = 0.775 and accuracy of 0.83\n",
      "Iteration 6427: with minibatch training loss = 1.01 and accuracy of 0.73\n",
      "Iteration 6428: with minibatch training loss = 0.753 and accuracy of 0.75\n",
      "Iteration 6429: with minibatch training loss = 0.663 and accuracy of 0.83\n",
      "Iteration 6430: with minibatch training loss = 1.11 and accuracy of 0.7\n",
      "Iteration 6431: with minibatch training loss = 0.805 and accuracy of 0.8\n",
      "Iteration 6432: with minibatch training loss = 0.693 and accuracy of 0.83\n",
      "Iteration 6433: with minibatch training loss = 0.788 and accuracy of 0.8\n",
      "Iteration 6434: with minibatch training loss = 0.858 and accuracy of 0.73\n",
      "Iteration 6435: with minibatch training loss = 0.94 and accuracy of 0.73\n",
      "Iteration 6436: with minibatch training loss = 1.16 and accuracy of 0.7\n",
      "Iteration 6437: with minibatch training loss = 0.893 and accuracy of 0.77\n",
      "Iteration 6438: with minibatch training loss = 0.79 and accuracy of 0.78\n",
      "Iteration 6439: with minibatch training loss = 0.755 and accuracy of 0.8\n",
      "Iteration 6440: with minibatch training loss = 0.6 and accuracy of 0.84\n",
      "Iteration 6441: with minibatch training loss = 0.71 and accuracy of 0.81\n",
      "Iteration 6442: with minibatch training loss = 0.907 and accuracy of 0.77\n",
      "Iteration 6443: with minibatch training loss = 0.952 and accuracy of 0.73\n",
      "Iteration 6444: with minibatch training loss = 0.777 and accuracy of 0.78\n",
      "Iteration 6445: with minibatch training loss = 1.07 and accuracy of 0.69\n",
      "Iteration 6446: with minibatch training loss = 0.905 and accuracy of 0.78\n",
      "Iteration 6447: with minibatch training loss = 1.1 and accuracy of 0.67\n",
      "Iteration 6448: with minibatch training loss = 0.836 and accuracy of 0.77\n",
      "Iteration 6449: with minibatch training loss = 0.831 and accuracy of 0.78\n",
      "Iteration 6450: with minibatch training loss = 1.1 and accuracy of 0.7\n",
      "Iteration 6451: with minibatch training loss = 0.931 and accuracy of 0.72\n",
      "Iteration 6452: with minibatch training loss = 0.563 and accuracy of 0.83\n",
      "Iteration 6453: with minibatch training loss = 1.2 and accuracy of 0.62\n",
      "Iteration 6454: with minibatch training loss = 0.855 and accuracy of 0.75\n",
      "Iteration 6455: with minibatch training loss = 0.73 and accuracy of 0.81\n",
      "Iteration 6456: with minibatch training loss = 0.838 and accuracy of 0.75\n",
      "Iteration 6457: with minibatch training loss = 0.92 and accuracy of 0.75\n",
      "Iteration 6458: with minibatch training loss = 0.967 and accuracy of 0.75\n",
      "Iteration 6459: with minibatch training loss = 0.617 and accuracy of 0.81\n",
      "Iteration 6460: with minibatch training loss = 0.867 and accuracy of 0.75\n",
      "Iteration 6461: with minibatch training loss = 0.56 and accuracy of 0.86\n",
      "Iteration 6462: with minibatch training loss = 1.14 and accuracy of 0.66\n",
      "Iteration 6463: with minibatch training loss = 0.777 and accuracy of 0.77\n",
      "Iteration 6464: with minibatch training loss = 0.711 and accuracy of 0.78\n",
      "Iteration 6465: with minibatch training loss = 0.967 and accuracy of 0.73\n",
      "Iteration 6466: with minibatch training loss = 0.935 and accuracy of 0.73\n",
      "Iteration 6467: with minibatch training loss = 0.98 and accuracy of 0.7\n",
      "Iteration 6468: with minibatch training loss = 0.72 and accuracy of 0.81\n",
      "Iteration 6469: with minibatch training loss = 0.994 and accuracy of 0.69\n",
      "Iteration 6470: with minibatch training loss = 0.758 and accuracy of 0.78\n",
      "Iteration 6471: with minibatch training loss = 1.04 and accuracy of 0.67\n",
      "Iteration 6472: with minibatch training loss = 0.927 and accuracy of 0.72\n",
      "Iteration 6473: with minibatch training loss = 0.71 and accuracy of 0.8\n",
      "Iteration 6474: with minibatch training loss = 0.822 and accuracy of 0.81\n",
      "Iteration 6475: with minibatch training loss = 0.752 and accuracy of 0.8\n",
      "Iteration 6476: with minibatch training loss = 0.912 and accuracy of 0.73\n",
      "Iteration 6477: with minibatch training loss = 0.594 and accuracy of 0.81\n",
      "Iteration 6478: with minibatch training loss = 1.01 and accuracy of 0.7\n",
      "Iteration 6479: with minibatch training loss = 1.02 and accuracy of 0.73\n",
      "Iteration 6480: with minibatch training loss = 1.28 and accuracy of 0.61\n",
      "Iteration 6481: with minibatch training loss = 0.862 and accuracy of 0.75\n",
      "Iteration 6482: with minibatch training loss = 0.914 and accuracy of 0.72\n",
      "Iteration 6483: with minibatch training loss = 0.711 and accuracy of 0.81\n",
      "Iteration 6484: with minibatch training loss = 0.932 and accuracy of 0.75\n",
      "Iteration 6485: with minibatch training loss = 0.467 and accuracy of 0.88\n",
      "Iteration 6486: with minibatch training loss = 0.802 and accuracy of 0.77\n",
      "Iteration 6487: with minibatch training loss = 1.05 and accuracy of 0.72\n",
      "Iteration 6488: with minibatch training loss = 0.833 and accuracy of 0.77\n",
      "Iteration 6489: with minibatch training loss = 0.761 and accuracy of 0.77\n",
      "Iteration 6490: with minibatch training loss = 0.686 and accuracy of 0.84\n",
      "Iteration 6491: with minibatch training loss = 0.71 and accuracy of 0.8\n",
      "Iteration 6492: with minibatch training loss = 0.82 and accuracy of 0.78\n",
      "Iteration 6493: with minibatch training loss = 0.732 and accuracy of 0.8\n",
      "Iteration 6494: with minibatch training loss = 0.671 and accuracy of 0.81\n",
      "Iteration 6495: with minibatch training loss = 0.668 and accuracy of 0.83\n",
      "Iteration 6496: with minibatch training loss = 1.03 and accuracy of 0.7\n",
      "Iteration 6497: with minibatch training loss = 1.29 and accuracy of 0.62\n",
      "Iteration 6498: with minibatch training loss = 0.896 and accuracy of 0.77\n",
      "Iteration 6499: with minibatch training loss = 0.812 and accuracy of 0.77\n",
      "Iteration 6500: with minibatch training loss = 1.07 and accuracy of 0.67\n",
      "Iteration 6501: with minibatch training loss = 0.932 and accuracy of 0.7\n",
      "Iteration 6502: with minibatch training loss = 0.892 and accuracy of 0.73\n",
      "Iteration 6503: with minibatch training loss = 0.663 and accuracy of 0.83\n",
      "Iteration 6504: with minibatch training loss = 0.72 and accuracy of 0.77\n",
      "Iteration 6505: with minibatch training loss = 0.762 and accuracy of 0.8\n",
      "Iteration 6506: with minibatch training loss = 0.978 and accuracy of 0.72\n",
      "Iteration 6507: with minibatch training loss = 0.828 and accuracy of 0.77\n",
      "Iteration 6508: with minibatch training loss = 0.87 and accuracy of 0.75\n",
      "Iteration 6509: with minibatch training loss = 0.817 and accuracy of 0.77\n",
      "Iteration 6510: with minibatch training loss = 0.981 and accuracy of 0.72\n",
      "Iteration 6511: with minibatch training loss = 1.07 and accuracy of 0.73\n",
      "Iteration 6512: with minibatch training loss = 0.959 and accuracy of 0.7\n",
      "Iteration 6513: with minibatch training loss = 1.16 and accuracy of 0.67\n",
      "Iteration 6514: with minibatch training loss = 0.917 and accuracy of 0.73\n",
      "Iteration 6515: with minibatch training loss = 0.834 and accuracy of 0.75\n",
      "Iteration 6516: with minibatch training loss = 0.869 and accuracy of 0.75\n",
      "Iteration 6517: with minibatch training loss = 0.575 and accuracy of 0.88\n",
      "Iteration 6518: with minibatch training loss = 0.498 and accuracy of 0.88\n",
      "Iteration 6519: with minibatch training loss = 0.881 and accuracy of 0.75\n",
      "Iteration 6520: with minibatch training loss = 0.861 and accuracy of 0.75\n",
      "Iteration 6521: with minibatch training loss = 0.958 and accuracy of 0.73\n",
      "Iteration 6522: with minibatch training loss = 0.765 and accuracy of 0.8\n",
      "Iteration 6523: with minibatch training loss = 0.697 and accuracy of 0.83\n",
      "Iteration 6524: with minibatch training loss = 0.984 and accuracy of 0.7\n",
      "Iteration 6525: with minibatch training loss = 0.934 and accuracy of 0.73\n",
      "Iteration 6526: with minibatch training loss = 0.92 and accuracy of 0.77\n",
      "Iteration 6527: with minibatch training loss = 1.01 and accuracy of 0.72\n",
      "Iteration 6528: with minibatch training loss = 0.897 and accuracy of 0.75\n",
      "Iteration 6529: with minibatch training loss = 0.614 and accuracy of 0.84\n",
      "Iteration 6530: with minibatch training loss = 0.988 and accuracy of 0.72\n",
      "Iteration 6531: with minibatch training loss = 0.978 and accuracy of 0.72\n",
      "Iteration 6532: with minibatch training loss = 1.2 and accuracy of 0.69\n",
      "Iteration 6533: with minibatch training loss = 1.25 and accuracy of 0.61\n",
      "Iteration 6534: with minibatch training loss = 0.767 and accuracy of 0.78\n",
      "Iteration 6535: with minibatch training loss = 0.852 and accuracy of 0.78\n",
      "Iteration 6536: with minibatch training loss = 0.755 and accuracy of 0.83\n",
      "Iteration 6537: with minibatch training loss = 1.03 and accuracy of 0.72\n",
      "Iteration 6538: with minibatch training loss = 0.881 and accuracy of 0.78\n",
      "Iteration 6539: with minibatch training loss = 0.864 and accuracy of 0.7\n",
      "Iteration 6540: with minibatch training loss = 0.791 and accuracy of 0.8\n",
      "Iteration 6541: with minibatch training loss = 1.35 and accuracy of 0.59\n",
      "Iteration 6542: with minibatch training loss = 0.971 and accuracy of 0.73\n",
      "Iteration 6543: with minibatch training loss = 0.641 and accuracy of 0.83\n",
      "Iteration 6544: with minibatch training loss = 1.09 and accuracy of 0.69\n",
      "Iteration 6545: with minibatch training loss = 0.841 and accuracy of 0.73\n",
      "Iteration 6546: with minibatch training loss = 0.904 and accuracy of 0.75\n",
      "Iteration 6547: with minibatch training loss = 0.741 and accuracy of 0.8\n",
      "Iteration 6548: with minibatch training loss = 0.848 and accuracy of 0.73\n",
      "Iteration 6549: with minibatch training loss = 0.623 and accuracy of 0.84\n",
      "Iteration 6550: with minibatch training loss = 0.951 and accuracy of 0.72\n",
      "Iteration 6551: with minibatch training loss = 0.724 and accuracy of 0.83\n",
      "Iteration 6552: with minibatch training loss = 1.01 and accuracy of 0.73\n",
      "Iteration 6553: with minibatch training loss = 0.825 and accuracy of 0.77\n",
      "Iteration 6554: with minibatch training loss = 0.893 and accuracy of 0.73\n",
      "Iteration 6555: with minibatch training loss = 0.606 and accuracy of 0.83\n",
      "Iteration 6556: with minibatch training loss = 0.745 and accuracy of 0.8\n",
      "Iteration 6557: with minibatch training loss = 1.04 and accuracy of 0.7\n",
      "Iteration 6558: with minibatch training loss = 0.937 and accuracy of 0.73\n",
      "Iteration 6559: with minibatch training loss = 0.747 and accuracy of 0.77\n",
      "Iteration 6560: with minibatch training loss = 1.01 and accuracy of 0.72\n",
      "Iteration 6561: with minibatch training loss = 0.857 and accuracy of 0.77\n",
      "Iteration 6562: with minibatch training loss = 0.85 and accuracy of 0.77\n",
      "Iteration 6563: with minibatch training loss = 0.583 and accuracy of 0.86\n",
      "Iteration 6564: with minibatch training loss = 0.715 and accuracy of 0.8\n",
      "Iteration 6565: with minibatch training loss = 1.11 and accuracy of 0.67\n",
      "Iteration 6566: with minibatch training loss = 0.871 and accuracy of 0.73\n",
      "Iteration 6567: with minibatch training loss = 0.65 and accuracy of 0.86\n",
      "Iteration 6568: with minibatch training loss = 1.03 and accuracy of 0.7\n",
      "Iteration 6569: with minibatch training loss = 0.758 and accuracy of 0.81\n",
      "Iteration 6570: with minibatch training loss = 0.834 and accuracy of 0.77\n",
      "Iteration 6571: with minibatch training loss = 0.847 and accuracy of 0.77\n",
      "Iteration 6572: with minibatch training loss = 0.86 and accuracy of 0.77\n",
      "Iteration 6573: with minibatch training loss = 0.859 and accuracy of 0.75\n",
      "Iteration 6574: with minibatch training loss = 0.789 and accuracy of 0.8\n",
      "Iteration 6575: with minibatch training loss = 0.883 and accuracy of 0.72\n",
      "Iteration 6576: with minibatch training loss = 0.904 and accuracy of 0.75\n",
      "Iteration 6577: with minibatch training loss = 0.709 and accuracy of 0.84\n",
      "Iteration 6578: with minibatch training loss = 1.05 and accuracy of 0.72\n",
      "Iteration 6579: with minibatch training loss = 0.703 and accuracy of 0.78\n",
      "Iteration 6580: with minibatch training loss = 0.861 and accuracy of 0.77\n",
      "Iteration 6581: with minibatch training loss = 0.883 and accuracy of 0.73\n",
      "Iteration 6582: with minibatch training loss = 0.96 and accuracy of 0.73\n",
      "Iteration 6583: with minibatch training loss = 0.97 and accuracy of 0.75\n",
      "Iteration 6584: with minibatch training loss = 0.873 and accuracy of 0.75\n",
      "Iteration 6585: with minibatch training loss = 0.832 and accuracy of 0.77\n",
      "Iteration 6586: with minibatch training loss = 0.725 and accuracy of 0.78\n",
      "Iteration 6587: with minibatch training loss = 0.935 and accuracy of 0.75\n",
      "Iteration 6588: with minibatch training loss = 1.14 and accuracy of 0.69\n",
      "Iteration 6589: with minibatch training loss = 0.857 and accuracy of 0.72\n",
      "Iteration 6590: with minibatch training loss = 0.84 and accuracy of 0.72\n",
      "Iteration 6591: with minibatch training loss = 0.765 and accuracy of 0.8\n",
      "Iteration 6592: with minibatch training loss = 0.813 and accuracy of 0.77\n",
      "Iteration 6593: with minibatch training loss = 0.923 and accuracy of 0.72\n",
      "Iteration 6594: with minibatch training loss = 0.792 and accuracy of 0.75\n",
      "Iteration 6595: with minibatch training loss = 0.867 and accuracy of 0.77\n",
      "Iteration 6596: with minibatch training loss = 1.02 and accuracy of 0.7\n",
      "Iteration 6597: with minibatch training loss = 0.617 and accuracy of 0.84\n",
      "Iteration 6598: with minibatch training loss = 0.978 and accuracy of 0.73\n",
      "Iteration 6599: with minibatch training loss = 0.555 and accuracy of 0.84\n",
      "Iteration 6600: with minibatch training loss = 0.797 and accuracy of 0.77\n",
      "Iteration 6601: with minibatch training loss = 1.03 and accuracy of 0.77\n",
      "Iteration 6602: with minibatch training loss = 0.948 and accuracy of 0.75\n",
      "Iteration 6603: with minibatch training loss = 0.751 and accuracy of 0.8\n",
      "Iteration 6604: with minibatch training loss = 0.604 and accuracy of 0.81\n",
      "Iteration 6605: with minibatch training loss = 0.95 and accuracy of 0.75\n",
      "Iteration 6606: with minibatch training loss = 0.983 and accuracy of 0.73\n",
      "Iteration 6607: with minibatch training loss = 0.794 and accuracy of 0.78\n",
      "Iteration 6608: with minibatch training loss = 0.829 and accuracy of 0.8\n",
      "Iteration 6609: with minibatch training loss = 0.801 and accuracy of 0.78\n",
      "Iteration 6610: with minibatch training loss = 0.642 and accuracy of 0.83\n",
      "Iteration 6611: with minibatch training loss = 0.716 and accuracy of 0.78\n",
      "Iteration 6612: with minibatch training loss = 0.904 and accuracy of 0.73\n",
      "Iteration 6613: with minibatch training loss = 0.774 and accuracy of 0.78\n",
      "Iteration 6614: with minibatch training loss = 0.881 and accuracy of 0.77\n",
      "Iteration 6615: with minibatch training loss = 0.94 and accuracy of 0.73\n",
      "Iteration 6616: with minibatch training loss = 0.856 and accuracy of 0.77\n",
      "Iteration 6617: with minibatch training loss = 0.982 and accuracy of 0.73\n",
      "Iteration 6618: with minibatch training loss = 0.893 and accuracy of 0.73\n",
      "Iteration 6619: with minibatch training loss = 0.843 and accuracy of 0.75\n",
      "Iteration 6620: with minibatch training loss = 0.955 and accuracy of 0.69\n",
      "Iteration 6621: with minibatch training loss = 0.933 and accuracy of 0.75\n",
      "Iteration 6622: with minibatch training loss = 0.86 and accuracy of 0.72\n",
      "Iteration 6623: with minibatch training loss = 0.763 and accuracy of 0.8\n",
      "Iteration 6624: with minibatch training loss = 0.934 and accuracy of 0.73\n",
      "Iteration 6625: with minibatch training loss = 1.02 and accuracy of 0.7\n",
      "Iteration 6626: with minibatch training loss = 0.885 and accuracy of 0.75\n",
      "Iteration 6627: with minibatch training loss = 0.976 and accuracy of 0.72\n",
      "Iteration 6628: with minibatch training loss = 0.901 and accuracy of 0.75\n",
      "Iteration 6629: with minibatch training loss = 0.621 and accuracy of 0.84\n",
      "Iteration 6630: with minibatch training loss = 0.652 and accuracy of 0.86\n",
      "Iteration 6631: with minibatch training loss = 1.02 and accuracy of 0.69\n",
      "Iteration 6632: with minibatch training loss = 0.742 and accuracy of 0.78\n",
      "Iteration 6633: with minibatch training loss = 0.874 and accuracy of 0.73\n",
      "Iteration 6634: with minibatch training loss = 1.02 and accuracy of 0.72\n",
      "Iteration 6635: with minibatch training loss = 0.797 and accuracy of 0.75\n",
      "Iteration 6636: with minibatch training loss = 0.849 and accuracy of 0.77\n",
      "Iteration 6637: with minibatch training loss = 0.936 and accuracy of 0.72\n",
      "Iteration 6638: with minibatch training loss = 0.777 and accuracy of 0.81\n",
      "Iteration 6639: with minibatch training loss = 0.806 and accuracy of 0.77\n",
      "Iteration 6640: with minibatch training loss = 0.824 and accuracy of 0.75\n",
      "Iteration 6641: with minibatch training loss = 0.726 and accuracy of 0.8\n",
      "Iteration 6642: with minibatch training loss = 0.603 and accuracy of 0.81\n",
      "Iteration 6643: with minibatch training loss = 0.909 and accuracy of 0.75\n",
      "Iteration 6644: with minibatch training loss = 0.874 and accuracy of 0.77\n",
      "Iteration 6645: with minibatch training loss = 1.05 and accuracy of 0.72\n",
      "Iteration 6646: with minibatch training loss = 0.916 and accuracy of 0.75\n",
      "Iteration 6647: with minibatch training loss = 1.19 and accuracy of 0.64\n",
      "Iteration 6648: with minibatch training loss = 0.799 and accuracy of 0.75\n",
      "Iteration 6649: with minibatch training loss = 0.814 and accuracy of 0.73\n",
      "Iteration 6650: with minibatch training loss = 0.65 and accuracy of 0.84\n",
      "Iteration 6651: with minibatch training loss = 0.801 and accuracy of 0.77\n",
      "Iteration 6652: with minibatch training loss = 0.812 and accuracy of 0.77\n",
      "Iteration 6653: with minibatch training loss = 0.881 and accuracy of 0.73\n",
      "Iteration 6654: with minibatch training loss = 1.05 and accuracy of 0.75\n",
      "Iteration 6655: with minibatch training loss = 1.04 and accuracy of 0.7\n",
      "Iteration 6656: with minibatch training loss = 1.18 and accuracy of 0.64\n",
      "Iteration 6657: with minibatch training loss = 0.833 and accuracy of 0.75\n",
      "Iteration 6658: with minibatch training loss = 0.634 and accuracy of 0.83\n",
      "Iteration 6659: with minibatch training loss = 0.843 and accuracy of 0.75\n",
      "Iteration 6660: with minibatch training loss = 0.614 and accuracy of 0.83\n",
      "Iteration 6661: with minibatch training loss = 0.894 and accuracy of 0.75\n",
      "Iteration 6662: with minibatch training loss = 0.762 and accuracy of 0.81\n",
      "Iteration 6663: with minibatch training loss = 0.913 and accuracy of 0.73\n",
      "Iteration 6664: with minibatch training loss = 0.833 and accuracy of 0.81\n",
      "Iteration 6665: with minibatch training loss = 1.01 and accuracy of 0.75\n",
      "Iteration 6666: with minibatch training loss = 0.871 and accuracy of 0.73\n",
      "Iteration 6667: with minibatch training loss = 0.691 and accuracy of 0.8\n",
      "Iteration 6668: with minibatch training loss = 0.716 and accuracy of 0.81\n",
      "Iteration 6669: with minibatch training loss = 1.05 and accuracy of 0.69\n",
      "Iteration 6670: with minibatch training loss = 1.02 and accuracy of 0.7\n",
      "Iteration 6671: with minibatch training loss = 1.09 and accuracy of 0.69\n",
      "Iteration 6672: with minibatch training loss = 0.805 and accuracy of 0.77\n",
      "Iteration 6673: with minibatch training loss = 0.834 and accuracy of 0.78\n",
      "Iteration 6674: with minibatch training loss = 0.732 and accuracy of 0.81\n",
      "Iteration 6675: with minibatch training loss = 1.02 and accuracy of 0.73\n",
      "Iteration 6676: with minibatch training loss = 0.834 and accuracy of 0.75\n",
      "Iteration 6677: with minibatch training loss = 0.882 and accuracy of 0.72\n",
      "Iteration 6678: with minibatch training loss = 0.856 and accuracy of 0.81\n",
      "Iteration 6679: with minibatch training loss = 0.948 and accuracy of 0.73\n",
      "Iteration 6680: with minibatch training loss = 0.705 and accuracy of 0.78\n",
      "Iteration 6681: with minibatch training loss = 0.817 and accuracy of 0.77\n",
      "Iteration 6682: with minibatch training loss = 0.763 and accuracy of 0.8\n",
      "Iteration 6683: with minibatch training loss = 0.947 and accuracy of 0.7\n",
      "Iteration 6684: with minibatch training loss = 0.774 and accuracy of 0.78\n",
      "Iteration 6685: with minibatch training loss = 0.872 and accuracy of 0.78\n",
      "Iteration 6686: with minibatch training loss = 0.594 and accuracy of 0.86\n",
      "Iteration 6687: with minibatch training loss = 0.713 and accuracy of 0.83\n",
      "Iteration 6688: with minibatch training loss = 0.658 and accuracy of 0.78\n",
      "Iteration 6689: with minibatch training loss = 0.806 and accuracy of 0.78\n",
      "Iteration 6690: with minibatch training loss = 1.05 and accuracy of 0.69\n",
      "Iteration 6691: with minibatch training loss = 0.773 and accuracy of 0.8\n",
      "Iteration 6692: with minibatch training loss = 0.772 and accuracy of 0.81\n",
      "Iteration 6693: with minibatch training loss = 0.73 and accuracy of 0.8\n",
      "Iteration 6694: with minibatch training loss = 0.962 and accuracy of 0.67\n",
      "Iteration 6695: with minibatch training loss = 0.812 and accuracy of 0.75\n",
      "Iteration 6696: with minibatch training loss = 0.852 and accuracy of 0.77\n",
      "Iteration 6697: with minibatch training loss = 0.796 and accuracy of 0.78\n",
      "Iteration 6698: with minibatch training loss = 0.459 and accuracy of 0.89\n",
      "Iteration 6699: with minibatch training loss = 0.726 and accuracy of 0.77\n",
      "Iteration 6700: with minibatch training loss = 1.04 and accuracy of 0.69\n",
      "Iteration 6701: with minibatch training loss = 0.891 and accuracy of 0.73\n",
      "Iteration 6702: with minibatch training loss = 0.903 and accuracy of 0.73\n",
      "Iteration 6703: with minibatch training loss = 0.751 and accuracy of 0.78\n",
      "Iteration 6704: with minibatch training loss = 1.02 and accuracy of 0.72\n",
      "Iteration 6705: with minibatch training loss = 1.19 and accuracy of 0.62\n",
      "Iteration 6706: with minibatch training loss = 0.673 and accuracy of 0.84\n",
      "Iteration 6707: with minibatch training loss = 0.743 and accuracy of 0.8\n",
      "Iteration 6708: with minibatch training loss = 0.911 and accuracy of 0.73\n",
      "Iteration 6709: with minibatch training loss = 0.878 and accuracy of 0.75\n",
      "Iteration 6710: with minibatch training loss = 0.717 and accuracy of 0.78\n",
      "Iteration 6711: with minibatch training loss = 1.06 and accuracy of 0.69\n",
      "Iteration 6712: with minibatch training loss = 0.616 and accuracy of 0.84\n",
      "Iteration 6713: with minibatch training loss = 1.16 and accuracy of 0.62\n",
      "Iteration 6714: with minibatch training loss = 0.798 and accuracy of 0.8\n",
      "Iteration 6715: with minibatch training loss = 0.559 and accuracy of 0.84\n",
      "Iteration 6716: with minibatch training loss = 0.908 and accuracy of 0.73\n",
      "Iteration 6717: with minibatch training loss = 0.648 and accuracy of 0.8\n",
      "Iteration 6718: with minibatch training loss = 0.877 and accuracy of 0.75\n",
      "Iteration 6719: with minibatch training loss = 0.813 and accuracy of 0.78\n",
      "Iteration 6720: with minibatch training loss = 0.915 and accuracy of 0.73\n",
      "Iteration 6721: with minibatch training loss = 0.746 and accuracy of 0.78\n",
      "Iteration 6722: with minibatch training loss = 0.888 and accuracy of 0.77\n",
      "Iteration 6723: with minibatch training loss = 0.946 and accuracy of 0.73\n",
      "Iteration 6724: with minibatch training loss = 0.478 and accuracy of 0.88\n",
      "Iteration 6725: with minibatch training loss = 0.585 and accuracy of 0.86\n",
      "Iteration 6726: with minibatch training loss = 0.686 and accuracy of 0.83\n",
      "Iteration 6727: with minibatch training loss = 0.95 and accuracy of 0.73\n",
      "Iteration 6728: with minibatch training loss = 1.28 and accuracy of 0.62\n",
      "Iteration 6729: with minibatch training loss = 0.646 and accuracy of 0.81\n",
      "Iteration 6730: with minibatch training loss = 0.828 and accuracy of 0.78\n",
      "Iteration 6731: with minibatch training loss = 0.682 and accuracy of 0.8\n",
      "Iteration 6732: with minibatch training loss = 0.942 and accuracy of 0.72\n",
      "Iteration 6733: with minibatch training loss = 0.875 and accuracy of 0.77\n",
      "Iteration 6734: with minibatch training loss = 1.02 and accuracy of 0.7\n",
      "Iteration 6735: with minibatch training loss = 1.05 and accuracy of 0.72\n",
      "Iteration 6736: with minibatch training loss = 0.752 and accuracy of 0.78\n",
      "Iteration 6737: with minibatch training loss = 0.762 and accuracy of 0.81\n",
      "Iteration 6738: with minibatch training loss = 0.967 and accuracy of 0.75\n",
      "Iteration 6739: with minibatch training loss = 0.815 and accuracy of 0.8\n",
      "Iteration 6740: with minibatch training loss = 0.834 and accuracy of 0.78\n",
      "Iteration 6741: with minibatch training loss = 0.891 and accuracy of 0.75\n",
      "Iteration 6742: with minibatch training loss = 1.04 and accuracy of 0.75\n",
      "Iteration 6743: with minibatch training loss = 0.723 and accuracy of 0.8\n",
      "Iteration 6744: with minibatch training loss = 0.81 and accuracy of 0.78\n",
      "Iteration 6745: with minibatch training loss = 0.84 and accuracy of 0.75\n",
      "Iteration 6746: with minibatch training loss = 0.903 and accuracy of 0.72\n",
      "Iteration 6747: with minibatch training loss = 0.801 and accuracy of 0.78\n",
      "Iteration 6748: with minibatch training loss = 0.666 and accuracy of 0.83\n",
      "Iteration 6749: with minibatch training loss = 0.632 and accuracy of 0.83\n",
      "Iteration 6750: with minibatch training loss = 0.82 and accuracy of 0.77\n",
      "Iteration 6751: with minibatch training loss = 0.966 and accuracy of 0.73\n",
      "Iteration 6752: with minibatch training loss = 0.973 and accuracy of 0.75\n",
      "Iteration 6753: with minibatch training loss = 0.919 and accuracy of 0.73\n",
      "Iteration 6754: with minibatch training loss = 0.87 and accuracy of 0.8\n",
      "Iteration 6755: with minibatch training loss = 1.03 and accuracy of 0.7\n",
      "Iteration 6756: with minibatch training loss = 0.959 and accuracy of 0.7\n",
      "Iteration 6757: with minibatch training loss = 0.976 and accuracy of 0.7\n",
      "Iteration 6758: with minibatch training loss = 1.42 and accuracy of 0.61\n",
      "Iteration 6759: with minibatch training loss = 0.987 and accuracy of 0.72\n",
      "Iteration 6760: with minibatch training loss = 0.56 and accuracy of 0.84\n",
      "Iteration 6761: with minibatch training loss = 0.743 and accuracy of 0.81\n",
      "Iteration 6762: with minibatch training loss = 0.805 and accuracy of 0.78\n",
      "Iteration 6763: with minibatch training loss = 0.886 and accuracy of 0.73\n",
      "Iteration 6764: with minibatch training loss = 0.748 and accuracy of 0.8\n",
      "Iteration 6765: with minibatch training loss = 0.81 and accuracy of 0.78\n",
      "Iteration 6766: with minibatch training loss = 0.758 and accuracy of 0.77\n",
      "Iteration 6767: with minibatch training loss = 0.701 and accuracy of 0.83\n",
      "Iteration 6768: with minibatch training loss = 0.68 and accuracy of 0.83\n",
      "Iteration 6769: with minibatch training loss = 0.718 and accuracy of 0.83\n",
      "Iteration 6770: with minibatch training loss = 0.745 and accuracy of 0.81\n",
      "Iteration 6771: with minibatch training loss = 0.809 and accuracy of 0.78\n",
      "Iteration 6772: with minibatch training loss = 1.19 and accuracy of 0.67\n",
      "Iteration 6773: with minibatch training loss = 0.92 and accuracy of 0.7\n",
      "Iteration 6774: with minibatch training loss = 0.628 and accuracy of 0.83\n",
      "Iteration 6775: with minibatch training loss = 0.934 and accuracy of 0.77\n",
      "Iteration 6776: with minibatch training loss = 1.1 and accuracy of 0.67\n",
      "Iteration 6777: with minibatch training loss = 0.574 and accuracy of 0.86\n",
      "Iteration 6778: with minibatch training loss = 0.582 and accuracy of 0.86\n",
      "Iteration 6779: with minibatch training loss = 0.435 and accuracy of 0.89\n",
      "Iteration 6780: with minibatch training loss = 0.982 and accuracy of 0.72\n",
      "Iteration 6781: with minibatch training loss = 0.954 and accuracy of 0.75\n",
      "Iteration 6782: with minibatch training loss = 0.893 and accuracy of 0.77\n",
      "Iteration 6783: with minibatch training loss = 0.704 and accuracy of 0.81\n",
      "Iteration 6784: with minibatch training loss = 0.755 and accuracy of 0.81\n",
      "Iteration 6785: with minibatch training loss = 1.03 and accuracy of 0.67\n",
      "Iteration 6786: with minibatch training loss = 0.763 and accuracy of 0.77\n",
      "Iteration 6787: with minibatch training loss = 0.773 and accuracy of 0.75\n",
      "Iteration 6788: with minibatch training loss = 0.839 and accuracy of 0.73\n",
      "Iteration 6789: with minibatch training loss = 1.29 and accuracy of 0.62\n",
      "Iteration 6790: with minibatch training loss = 1.19 and accuracy of 0.66\n",
      "Iteration 6791: with minibatch training loss = 0.615 and accuracy of 0.84\n",
      "Iteration 6792: with minibatch training loss = 0.853 and accuracy of 0.75\n",
      "Iteration 6793: with minibatch training loss = 0.706 and accuracy of 0.78\n",
      "Iteration 6794: with minibatch training loss = 0.817 and accuracy of 0.75\n",
      "Iteration 6795: with minibatch training loss = 0.798 and accuracy of 0.75\n",
      "Iteration 6796: with minibatch training loss = 0.624 and accuracy of 0.83\n",
      "Iteration 6797: with minibatch training loss = 0.821 and accuracy of 0.78\n",
      "Iteration 6798: with minibatch training loss = 0.704 and accuracy of 0.8\n",
      "Iteration 6799: with minibatch training loss = 0.649 and accuracy of 0.8\n",
      "Iteration 6800: with minibatch training loss = 0.916 and accuracy of 0.75\n",
      "Iteration 6801: with minibatch training loss = 1.21 and accuracy of 0.64\n",
      "Iteration 6802: with minibatch training loss = 1.06 and accuracy of 0.67\n",
      "Iteration 6803: with minibatch training loss = 0.918 and accuracy of 0.78\n",
      "Iteration 6804: with minibatch training loss = 0.934 and accuracy of 0.73\n",
      "Iteration 6805: with minibatch training loss = 0.994 and accuracy of 0.7\n",
      "Iteration 6806: with minibatch training loss = 0.786 and accuracy of 0.75\n",
      "Iteration 6807: with minibatch training loss = 0.881 and accuracy of 0.75\n",
      "Iteration 6808: with minibatch training loss = 0.949 and accuracy of 0.73\n",
      "Iteration 6809: with minibatch training loss = 0.749 and accuracy of 0.8\n",
      "Iteration 6810: with minibatch training loss = 0.91 and accuracy of 0.75\n",
      "Iteration 6811: with minibatch training loss = 1.11 and accuracy of 0.67\n",
      "Iteration 6812: with minibatch training loss = 0.879 and accuracy of 0.72\n",
      "Iteration 6813: with minibatch training loss = 0.746 and accuracy of 0.8\n",
      "Iteration 6814: with minibatch training loss = 1.26 and accuracy of 0.67\n",
      "Iteration 6815: with minibatch training loss = 0.948 and accuracy of 0.72\n",
      "Iteration 6816: with minibatch training loss = 0.742 and accuracy of 0.81\n",
      "Iteration 6817: with minibatch training loss = 0.662 and accuracy of 0.81\n",
      "Iteration 6818: with minibatch training loss = 1.06 and accuracy of 0.7\n",
      "Iteration 6819: with minibatch training loss = 0.862 and accuracy of 0.77\n",
      "Iteration 6820: with minibatch training loss = 1.03 and accuracy of 0.67\n",
      "Iteration 6821: with minibatch training loss = 0.967 and accuracy of 0.77\n",
      "Iteration 6822: with minibatch training loss = 1.13 and accuracy of 0.72\n",
      "Iteration 6823: with minibatch training loss = 1.03 and accuracy of 0.72\n",
      "Iteration 6824: with minibatch training loss = 0.986 and accuracy of 0.77\n",
      "Iteration 6825: with minibatch training loss = 0.592 and accuracy of 0.83\n",
      "Iteration 6826: with minibatch training loss = 0.96 and accuracy of 0.75\n",
      "Iteration 6827: with minibatch training loss = 0.843 and accuracy of 0.77\n",
      "Iteration 6828: with minibatch training loss = 0.674 and accuracy of 0.8\n",
      "Iteration 6829: with minibatch training loss = 0.728 and accuracy of 0.78\n",
      "Iteration 6830: with minibatch training loss = 0.576 and accuracy of 0.84\n",
      "Iteration 6831: with minibatch training loss = 0.749 and accuracy of 0.81\n",
      "Iteration 6832: with minibatch training loss = 0.769 and accuracy of 0.78\n",
      "Iteration 6833: with minibatch training loss = 0.81 and accuracy of 0.77\n",
      "Iteration 6834: with minibatch training loss = 0.829 and accuracy of 0.73\n",
      "Iteration 6835: with minibatch training loss = 0.823 and accuracy of 0.73\n",
      "Iteration 6836: with minibatch training loss = 0.932 and accuracy of 0.72\n",
      "Iteration 6837: with minibatch training loss = 0.843 and accuracy of 0.77\n",
      "Iteration 6838: with minibatch training loss = 0.833 and accuracy of 0.77\n",
      "Iteration 6839: with minibatch training loss = 0.693 and accuracy of 0.81\n",
      "Iteration 6840: with minibatch training loss = 0.612 and accuracy of 0.83\n",
      "Iteration 6841: with minibatch training loss = 0.444 and accuracy of 0.89\n",
      "Iteration 6842: with minibatch training loss = 0.457 and accuracy of 0.89\n",
      "Iteration 6843: with minibatch training loss = 0.623 and accuracy of 0.81\n",
      "Iteration 6844: with minibatch training loss = 1.13 and accuracy of 0.67\n",
      "Iteration 6845: with minibatch training loss = 0.982 and accuracy of 0.72\n",
      "Iteration 6846: with minibatch training loss = 0.919 and accuracy of 0.7\n",
      "Iteration 6847: with minibatch training loss = 0.532 and accuracy of 0.81\n",
      "Iteration 6848: with minibatch training loss = 0.979 and accuracy of 0.7\n",
      "Iteration 6849: with minibatch training loss = 0.955 and accuracy of 0.73\n",
      "Iteration 6850: with minibatch training loss = 1.01 and accuracy of 0.72\n",
      "Iteration 6851: with minibatch training loss = 0.741 and accuracy of 0.81\n",
      "Iteration 6852: with minibatch training loss = 0.55 and accuracy of 0.84\n",
      "Iteration 6853: with minibatch training loss = 0.681 and accuracy of 0.81\n",
      "Iteration 6854: with minibatch training loss = 0.909 and accuracy of 0.75\n",
      "Iteration 6855: with minibatch training loss = 0.535 and accuracy of 0.86\n",
      "Iteration 6856: with minibatch training loss = 0.854 and accuracy of 0.75\n",
      "Iteration 6857: with minibatch training loss = 0.755 and accuracy of 0.8\n",
      "Iteration 6858: with minibatch training loss = 0.588 and accuracy of 0.86\n",
      "Iteration 6859: with minibatch training loss = 0.842 and accuracy of 0.77\n",
      "Iteration 6860: with minibatch training loss = 0.883 and accuracy of 0.72\n",
      "Iteration 6861: with minibatch training loss = 0.875 and accuracy of 0.72\n",
      "Iteration 6862: with minibatch training loss = 0.694 and accuracy of 0.81\n",
      "Iteration 6863: with minibatch training loss = 0.746 and accuracy of 0.78\n",
      "Iteration 6864: with minibatch training loss = 0.892 and accuracy of 0.72\n",
      "Iteration 6865: with minibatch training loss = 0.896 and accuracy of 0.73\n",
      "Iteration 6866: with minibatch training loss = 0.843 and accuracy of 0.73\n",
      "Iteration 6867: with minibatch training loss = 0.957 and accuracy of 0.72\n",
      "Iteration 6868: with minibatch training loss = 0.752 and accuracy of 0.77\n",
      "Iteration 6869: with minibatch training loss = 0.857 and accuracy of 0.75\n",
      "Iteration 6870: with minibatch training loss = 0.721 and accuracy of 0.8\n",
      "Iteration 6871: with minibatch training loss = 0.661 and accuracy of 0.8\n",
      "Iteration 6872: with minibatch training loss = 0.818 and accuracy of 0.75\n",
      "Iteration 6873: with minibatch training loss = 0.915 and accuracy of 0.72\n",
      "Iteration 6874: with minibatch training loss = 0.839 and accuracy of 0.8\n",
      "Iteration 6875: with minibatch training loss = 0.615 and accuracy of 0.8\n",
      "Iteration 6876: with minibatch training loss = 1.05 and accuracy of 0.69\n",
      "Iteration 6877: with minibatch training loss = 0.979 and accuracy of 0.69\n",
      "Iteration 6878: with minibatch training loss = 0.685 and accuracy of 0.83\n",
      "Iteration 6879: with minibatch training loss = 1.05 and accuracy of 0.67\n",
      "Iteration 6880: with minibatch training loss = 0.761 and accuracy of 0.77\n",
      "Iteration 6881: with minibatch training loss = 1 and accuracy of 0.7\n",
      "Iteration 6882: with minibatch training loss = 0.947 and accuracy of 0.77\n",
      "Iteration 6883: with minibatch training loss = 0.951 and accuracy of 0.77\n",
      "Iteration 6884: with minibatch training loss = 1.03 and accuracy of 0.72\n",
      "Iteration 6885: with minibatch training loss = 1.03 and accuracy of 0.72\n",
      "Iteration 6886: with minibatch training loss = 1.14 and accuracy of 0.73\n",
      "Iteration 6887: with minibatch training loss = 1.06 and accuracy of 0.7\n",
      "Iteration 6888: with minibatch training loss = 0.77 and accuracy of 0.78\n",
      "Iteration 6889: with minibatch training loss = 0.631 and accuracy of 0.84\n",
      "Iteration 6890: with minibatch training loss = 0.809 and accuracy of 0.75\n",
      "Iteration 6891: with minibatch training loss = 0.468 and accuracy of 0.89\n",
      "Iteration 6892: with minibatch training loss = 0.661 and accuracy of 0.81\n",
      "Iteration 6893: with minibatch training loss = 0.56 and accuracy of 0.84\n",
      "Iteration 6894: with minibatch training loss = 0.878 and accuracy of 0.78\n",
      "Iteration 6895: with minibatch training loss = 0.704 and accuracy of 0.83\n",
      "Iteration 6896: with minibatch training loss = 0.818 and accuracy of 0.78\n",
      "Iteration 6897: with minibatch training loss = 0.96 and accuracy of 0.73\n",
      "Iteration 6898: with minibatch training loss = 0.988 and accuracy of 0.77\n",
      "Iteration 6899: with minibatch training loss = 0.562 and accuracy of 0.84\n",
      "Iteration 6900: with minibatch training loss = 0.754 and accuracy of 0.8\n",
      "Iteration 6901: with minibatch training loss = 1.15 and accuracy of 0.66\n",
      "Iteration 6902: with minibatch training loss = 0.782 and accuracy of 0.77\n",
      "Iteration 6903: with minibatch training loss = 0.737 and accuracy of 0.8\n",
      "Iteration 6904: with minibatch training loss = 0.755 and accuracy of 0.78\n",
      "Iteration 6905: with minibatch training loss = 1 and accuracy of 0.72\n",
      "Iteration 6906: with minibatch training loss = 0.702 and accuracy of 0.8\n",
      "Iteration 6907: with minibatch training loss = 0.959 and accuracy of 0.72\n",
      "Iteration 6908: with minibatch training loss = 0.853 and accuracy of 0.73\n",
      "Iteration 6909: with minibatch training loss = 0.78 and accuracy of 0.77\n",
      "Iteration 6910: with minibatch training loss = 0.95 and accuracy of 0.7\n",
      "Iteration 6911: with minibatch training loss = 0.9 and accuracy of 0.78\n",
      "Iteration 6912: with minibatch training loss = 0.941 and accuracy of 0.69\n",
      "Iteration 6913: with minibatch training loss = 0.807 and accuracy of 0.8\n",
      "Iteration 6914: with minibatch training loss = 0.434 and accuracy of 0.89\n",
      "Iteration 6915: with minibatch training loss = 1.06 and accuracy of 0.69\n",
      "Iteration 6916: with minibatch training loss = 0.916 and accuracy of 0.77\n",
      "Iteration 6917: with minibatch training loss = 0.821 and accuracy of 0.75\n",
      "Iteration 6918: with minibatch training loss = 0.546 and accuracy of 0.86\n",
      "Iteration 6919: with minibatch training loss = 0.592 and accuracy of 0.89\n",
      "Iteration 6920: with minibatch training loss = 0.897 and accuracy of 0.77\n",
      "Iteration 6921: with minibatch training loss = 0.73 and accuracy of 0.81\n",
      "Iteration 6922: with minibatch training loss = 0.913 and accuracy of 0.75\n",
      "Iteration 6923: with minibatch training loss = 1.13 and accuracy of 0.69\n",
      "Iteration 6924: with minibatch training loss = 1.08 and accuracy of 0.69\n",
      "Iteration 6925: with minibatch training loss = 0.787 and accuracy of 0.84\n",
      "Iteration 6926: with minibatch training loss = 0.916 and accuracy of 0.73\n",
      "Iteration 6927: with minibatch training loss = 1.01 and accuracy of 0.7\n",
      "Iteration 6928: with minibatch training loss = 0.683 and accuracy of 0.78\n",
      "Iteration 6929: with minibatch training loss = 1.04 and accuracy of 0.73\n",
      "Iteration 6930: with minibatch training loss = 1.16 and accuracy of 0.62\n",
      "Iteration 6931: with minibatch training loss = 0.596 and accuracy of 0.84\n",
      "Iteration 6932: with minibatch training loss = 0.917 and accuracy of 0.72\n",
      "Iteration 6933: with minibatch training loss = 0.994 and accuracy of 0.69\n",
      "Iteration 6934: with minibatch training loss = 1.03 and accuracy of 0.73\n",
      "Iteration 6935: with minibatch training loss = 0.702 and accuracy of 0.78\n",
      "Iteration 6936: with minibatch training loss = 0.914 and accuracy of 0.72\n",
      "Iteration 6937: with minibatch training loss = 0.658 and accuracy of 0.83\n",
      "Iteration 6938: with minibatch training loss = 0.745 and accuracy of 0.78\n",
      "Validation loss: 0.28986305\n",
      "Epoch 5, Overall loss = 0.86 and accuracy of 0.759\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4xLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvAOZPmwAAIABJREFUeJzsnXd8HMXZx3+Punu33JErxt1GNm7A\n2TQDpoTQSWIngN+EEAghBANJIFRDQggJhIQYCL3XYHDFZ3DHxsa9d7nJXZKtctK8f+zu3d7eltl2\nt6ebrz/6+LbNPjs7O8/MM888Q4wxCAQCgSBzyUq1AAKBQCBILUIRCAQCQYYjFIFAIBBkOEIRCAQC\nQYYjFIFAIBBkOEIRCAQCQYYjFIFAYAARMSLqkWo5BAK/EYpAkBYQ0Q4iOkVE5aq/51ItlwIRTSSi\nWo18IYNzi2Qlk5NkMQUCXURBFKQTlzHGZqdaCBMWMcZGp1oIgcAuokcgSHvk1vgCInqOiI4T0QYi\nOk91vAMRfUZER4hoCxHdqjqWTUT3E9FWIiojouVE1FmV/PlEtJmIjhHR80REPj9LPhH9jYj2yn9/\nI6J8+VhrIvpcluUIEX1DRFnysXuJqER+ho3q5xcIrBCKQFBfOAvAVgCtATwI4CMiaikfewfAHgAd\nAFwN4HEiGisf+w2AGwBcAqApgJ8BOKlKdzyAoQAGALgWwEUmMgwmokNEtImI/uDQ9PMAgOEABgEY\nCGAYgN/Lx+6Wn6MNgEIA9wNgRHQ6gNsBDGWMNZFl3OHg3oIMRSgCQTrxidwaVv5uVR07COBvjLEa\nxti7ADYCuFRu3Y8CcC9jrJIxthLAVAA/ka+7BcDvGWMbmcT3jLHDqnSnMMaOMcZ2AZgLqYLW42sA\n/QC0BfBDSMrlHgfPeBOAhxljBxljpQD+BODH8rEaAO0BnCY/5zdMChZWCyAfQB8iymWM7WCMbXVw\nb0GGIhSBIJ24kjHWXPX3H9WxEhYfQXEnpB5ABwBHGGNlmmMd5d+dIfUkjNiv+n0SQGO9kxhj2xhj\n2xljdYyx1QAehtT7sEsHWT61rB3k338GsAXATCLaRkST5XtvAfBrAA8BOEhE7xBRBwgEnAhFIKgv\ndNTY77sA2Cv/tSSiJppjJfLv3QC6+yAPA+BkPGEvgNNU28pzgDFWxhi7mzHWDcDlAH6jjAUwxt6S\nB6pPk+/9pBvhBZmFUASC+kJbAHcQUS4RXQPgDABfMMZ2A1gI4AkiKiCiAQBuBvCGfN1UAI8QUU+S\nGEBErezenIguJqJC+XdvAH8A8KnFZfmyTMpfFoC3AfyeiNoQUWsAf1RkJaLxRNRDVnjHIZmE6ojo\ndCIaKw8qVwI4BaDO7jMIMhfhPipIJ/5HRLWq7VmMsR/Iv5cA6AngEIADAK5W2fpvAPAvSC3rowAe\nVLmh/hWSfX0mpIHmDQCUNO1wHoD/ElFj+f5vAHjc4ppyzfYFAB6FNGi9St73vrwPkJ7vOUiDxUcB\n/JMxNldWblMgKb8aSIpvkoNnEGQoJBamEaQ7RDQRwC3Ch18gcIYwDQkEAkGGIxSBQCAQZDjCNCQQ\nCAQZjugRCAQCQYaTFl5DrVu3ZkVFRY6uraioQKNGjbwVyEeEvP4i5PUXIa9/OJF1+fLlhxhjbSxP\nZIwF/u/MM89kTpk7d67ja1OBkNdfhLz+IuT1DyeyAljGOOpYYRoSCASCDEcoAoFAIMhwhCIQCASC\nDEcoAoFAIMhwhCIQCASCDEcoAoFAIMhwhCIQCASCDMdXRUBEdxLRGiJaS0S/lve1JKJZ8oLgs4io\nhZ8yCAQCgRHrD9dia6k2Gnjm4ZsiIKJ+AG6FtPj2QADjiagHgMkA5jDGegKYI28LBAJB0nny20qc\n9/S8VIuRcvzsEZwBYAlj7CRjLAJgHoCrAFwB4FX5nFcBXOmjDAKBQCCwwE9FsAbA2UTUiogaArgE\n0kLhhYyxffI5+wEU+iiDQCAQCCzwNQw1Ed0M4DYAFQDWAqgCMJEx1lx1zlHGWMI4ARFNgrzcXmFh\n4ZnvvPOOIxnKy8vRuHFjR9emAiGvvwh5/SXd5J04vQIA8N9xwQ885yRvx4wZs5wxVmx5Ik9AIi/+\nIK3fehuAjQDay/vaA9hoda0IOhdchLz+IuT1l9Pu/Zyddu/nqRaDi7QNOkdEbeX/u0AaH3gLwGcA\nJsinTADwqZ8yCAQCgcAcv9cj+JCIWgGoAfBLxtgxIpoC4D3ZbLQTwLU+yyAQCAQCE3xVBIyxs3X2\nHQZwnp/3FQgEAgE/YmaxQCAQZDhCEQgEAkGGIxSBQCAQZDhCEQgEAkGGIxSBQCBwzO4jJ/Hl6n3W\nJwoCjVAEKmrrGJ6fuwUVVZFUiyIQpAXj/zEfv3jzu1SLIXCJUAQqPl+1F3+esRF/nrEx1aIIBGnB\n8VM1qRZB4AFCEaiorKkFAJysFj0CgUCQOQhFIBAIBBmOUAQCgUCQ4QhFIBAIBBmOUAQCgUCQ4QhF\noMLHNXoEAoEgsAhFIBCoWL7zCIomT8OWg+WpFkUgSBpCEQgEKj5buRcAMH9zaYolSS+Y6E6nNUIR\nCLg5VV2Lw+VVqRZDEECEHkhvhCIQcHPl8wtw5qOzUy2GIIDUNz3AGMML4a3Yc/RkqkVJCkIR6ECg\nVIsQSDYeKEu1CKYcP1mDvcdOpVqMegFjDB+v2IPqSB33+fWJvccr8eT0DfjZf79NtShJQSiCDOVw\neRUe/XwdIrV8H3o6EPrLXIyc8lWqxXDF0YrqVIsAAJi57gDuevd7PDN7U6pFcU15VQTLdx6xdY2i\n2MorMyPcjFAEGcpD/1uHqfO3Y/b6g6kWxTOOnkzvAGjvL9uNwY/Mwtq9x1Ny/wVbDmH3EckUogST\nKy3jGxMKcn/gtje/ww9fWIQTlfzlI4skq0BdkB/MQ4QiyFCqI1KAvfrWpXdLKnNj/pZDAIDNB1Lj\nunrT1CU4+6m50obNjAhyMVq95xgAoIbTzAWoFUGAH8xDhCLIcEgMhwhM4C0eLNB9AgmyUdiz5FNF\nj0BQr8mQhk5aId6JPzjJ1ixZE2RKj9lXRUBEdxHRWiJaQ0RvE1EBEXUloiVEtIWI3iWiPD9lEOgT\nK97B7BJsLS2Prg+RaaRjLy0d6ks72aqcK0xDLiGijgDuAFDMGOsHIBvA9QCeBPAMY6wHgKMAbvZL\nBrtkxisPPpU1tTjv6Xm44+0VqRYlY7Ey9dTVMYQ3poejgZu6XGsaevh/69DzgS/cCRRA/DYN5QBo\nQEQ5ABoC2AdgLIAP5OOvArjSZxkEaUa17NK6aOvhFEvijsqaWtTYcM9Np4bIywu2Y+IrMR/7IDec\nFfOOnZ6W8jjaHsHLC7ajpjbAD+uQHL8SZoyVENFfAOwCcArATADLARxjjCnOuXsAdNS7nogmAZgE\nAIWFhQiHw47kKC8v5752427JvWzf/n0Ih+35HXuFHXmNqGMMjAHZWcYl/9ChSgDA2rVrUHBog630\n1fJ5Ia+WkzXShxapjThK2+waK3lLSiR3yc1btiBcs9P2vdVMnF6BDo0Jj49uyHX+wQPSO1m/bj2a\nHdvMJa8fhMNhbNwjfQv79+9HOHw04ZwFa+PdSr/++mvk51BK5LUiEpGqmwULFqBRrv43oZX5eBWL\nXqv3PKl4Rj/z1jdFQEQtAFwBoCuAYwDeBzCO93rG2IsAXgSA4uJiFgqFHMkRDofBe+2+pbuAtavR\nvl17hEIDHN3PLXbkVYjU1uH/Xl+OX53XE4M6N8cv3/oO01btw44plxpe88bOZcDBA+jXrx9Cfdvx\n3Wj6NACIk8+JvFacqKwB5sxETnaOvbR15NNiJe9Xx9cAu3aiZ48eCI3qyn9vA3n2ljPuZ/hw3wpg\n/16c0ecMhAZ15JLXU1T5d/Db3cCaVWjXrh1CoYEJp35x6Htg957o9tnnnI2GeTnJlZeT7PAMIBLB\n6FGj0axhbvxBgzJTWlYFzJ2NrKzs+GMcZcwv/MxbP01D5wPYzhgrZYzVAPgIwCgAzWVTEQB0AlDi\nowz1noqqCF6avx1zNhzEb95dCQCYtmofx5Vyd9lH2QTpi9UYQURjPA+yaSiKLdOQ9EDCfdQ9uwAM\nJ6KGJDnwngdgHYC5AK6Wz5kA4FMfZXBEOnlt3PfRajzxpT3TjsAaOz7nXhFEV0WjbIho7OTBk1yF\nq8HiQD+ZZ/imCBhjSyANCn8HYLV8rxcB3AvgN0S0BUArAC/5JUMmsPNIZkRHzCRSoYTsoh0ED6IS\nc4X8OPXtsYzwbYwAABhjDwJ4ULN7G4Bhft7XCyK1daiurUPDPF+zyFPslFmlgKdDpWOXujoWnRAk\ncIZVBVgfPWfUGHkN1VfEzGID/u/15ejzxxmpFsN3glhduv320vXjDaLURiHZI3WaHkEyhHGIG9kk\nDzxmyw04HRGKwIA5G9JjsoxTlI/jlteW4cCJypTK4jW1aaoI0omEMYJ0yHIbMirPU8eA5+duQc8H\nvrQVvTTdEIpAgNcW7Ui1CHG4tVbVedB4s7J5H6moxhuL3c0zSGcSWsgBVgRuxy/eWya5yR4pD8Za\nEX4gFIGg3q3IlgzT0B1vr8DvP1mDzT6s2ub32/h81V4s2+FuwqQ2j9Mh+qgdGdXnKg2T4D+hc9Jn\nJDQJpEX31iPqnZeHimSYhg6WVSbtXl5z+1tSDCezCYdWpNNjOxFV/XwU3ZdGD20T0SOwoK6O4e9z\nNuNIQJYQNMNpQU2V41BtHdMdn3A9WKyaBfT1plIUTZ7m+furle+R7WXmpVE9oxXVjzqyto7h/WW7\no3nNw8b9ZZj6zTbdY05lVDzr0uj12EYoAgsWbzuMv87ahPs/Wp1qUTwlCIX6mVmbcNbjc7D/uLeD\n1eqK48WvpUrB6+UflVsobqp1dQxFk6cZVkJ2SAePXm2jw4/y9NaSnbjng1V4fdEOLNl2mKuhM/4f\n3+DRaevjZeMU7u73vscTX0rXqi+J9Qj40klHhCKwQOn6l1V56zFQURXB7HUHXKVx30er8P3uY65l\nSVW9E94keWYlrIvr8oNLxveq7RFUycsg/nnGRq7rF249hOMBWmPZbcXuh9nksNyLez68Fde9uBhf\nrtlveY3Z/AYrCT/8bg/+PU9S5EzPNhSI5pM/ZLwimPrNNnxrMnCmRPC00z3l4XcfrsItry3D1lLn\n69O+vXS3hxLVH7yok6wm2kUVgVI+WPy2GRVVEdz4nyW45bVvLc+1y/GTNThVbX9BH6M8M8qGBNOQ\n7Ttao8ikNBT2HHU2i97tQLboEWQAj05bj2v+tcjwuNLi81oR7DhUAQA4VV2LIxXVKJo8De99m7yK\nPUiFWvuhpoMHilIeFNNQbS2/IlACtm3YF+9x5MVzD3x4JsY9+7Xt6+z3AGzfwjVOvducyBrXIRBj\nBIKcbH8UgZqdhyWl8ObSXb7dwxQirN5zHCerI9bnBoiKKn15k6FIlMpcMSHY6RFoW9kz1+7H9DU8\nEWP52HnYfss5wTSkycIl2w7jy9XGMiZDMbgdO3HsTOHutmmBUAQ6qAtclssewaYDZSjTmZGY6ha5\n+vbllRFc9tz8wCwNyZM3C7ccQt8HZ2D+5kOu0mSM4Ykv19seTFb86JV0I/IEqxwbMY4UkSa9vhw/\nf+M7W/f3GqMsV76F615cjF+8+Z3h+UHrxW0rLcd2udftlWTqMlTfXEmFIrBAawO2y4XPfI2fykv6\nHamoNl2Q/fvdxzBnl/MBRHtB52JnV0UkmVYaDDyfrI4kNQwFz3Mslcd1lm7XWc7SRkacqqnFv+dt\nw9UvGJsH9dA2DJQeQhZHs9XILz0aCNDjNujsdQfw+BfrTc+xXby1F/hQL7pJcuzT8zDmL2HT9Mwq\n83jTkHJ9bKef6xScqq7F2U99ldSlWoUisCDWI4jfP3Ptfpz5yKyEiv39Zbtx9lNfxe1btlNa6m/I\nI7Nw7b+lCkddjtS/X18XvPkKV7+wCGc9PifVYsRht7JUPua75p7Er+Sez5qS4yirdGYOq9MqAnmM\nwKhHUB2pw4SXl2Lt3uPR952sNuUtry2LutEaYTQb27An5VYoBziOlOtSWKWsed0jqI7U6bpObzxQ\nht1HTmHKl+bK20uEIrBAKXu1mgA2j05bj8MV1dGW8tbScqwpOY57PliF3UdOAdAvLKv2xJsg9Mr2\nUYeTn5y2I9Uy7D5yMqFnsG7fCYcpO4PnI1NkPlldmzBWoHe1kuTRKob/fb8XADD+H/Pxk5eWOpIx\nNkagbEvlwyj89dq9xzFvUynu/2i1KqCZ8XP+a95WPPTZWkeypQKrN8YYw2ff73UVxdNtPymhE2Mi\ntG6ICbUicCkLANz74SoMf2JOQmNSKRfJDBEvFIEFSmvAaozgvKfnYfw/5ke3mbyAvBFmld13uxIX\nC+fBi8J59lNzceXzCzxIyV+UT2Tq/O3o+6D9cOFK/m90GCtIMRUqFcbRk5LyNhosjvuomSKDftpE\nwJQvN+C/C3fYkknbS7GivCoSNQvadR/VYqW7v1i9H3e8vQIvhLfyC6hJ1Ot60bY1THWF0w7Boq2H\nsXS7ZNZU5hFVGyzyk8wlNeq1Ipi+Zh8W7uXv+usNeCn7jBSB8SAk37iCnomDx87sJXpdXy+pqa3T\nraSMzDtuxdB7Dm2Was85ZTJ2o0edqkfwwfI9+KE8xmDlNcQQK1NeZ3eNzbCr/R6cgSueWyDLYk+a\nxCEC8+uPVEhzAdyMNTn9KpwMZOu6j8b1CJy9vRv+szhqHjZC+VREj8Aj3v12N2btcDd7U3n52krd\n6h3d//Fq9HzgS0f39Ov9/+rtFegvt571BsP8oucDX+KBTxJDdBh9TDwKya7MvGObvMpQbecPb4yt\nXWEUe0i9V7lHdaTOcIDeCdUR+2aXDfvL4mRSsKroEuZ+cOabnfemTdJtxZgos8lgsfq+esd1Lr3k\n2W/w4fI9zoSTKS2rwtaD0iRT0SPwCCKC29D0ir3Oboz7dzgnhxElFjQ/egQ1tXX43/d7UWbge+83\nyZwFzdNa4xmH+PU7KzBqylem5zDG4t6XZY+AxVcyajOc2x6ZdrEYOxhfadBrS8FoseOxYpcD3npj\nBBNeXorfaxo36/adwN3vf29fQBWjpnyFyXJcM9Ej8Ag72RiprUO5jgeJ0k3TLs3nFrOBJz8UgZlt\n1sndVuw6iqLJ03DolPdL+PFU5HY/kgTTEMc1n6zci5Jjp3SPqRWJuu43enfq3VbrJTg3gTjHrReM\n5WCxkzQ1F7n+KmwIoc4PPffRJduP4I3F3kwAVT+nerwgmT2Cer0eARFxt1zu+WAVPl5RkrBf+Wj9\nWrJUr97wowAkBHZzydvyLOi1h2pxtacpu0fvndvxGNEyb1Mpzu3VBgDw3rLdWLztcJxpSF35W+kn\nBnMnAgD4giO4mm7aLipz2wOnCfnJl4KrORIOG0huzYCKzH7OHdAjmWOF9VwR8BdwPSUAWBcWp2VD\naV3ofRg3Tl2CwV2aozpSh74dmuKpqwfypWkiTGKLWN3isR9LhfeD/ug7a5tpQk/M5RiB19/r3A0H\no4rgdx+sirs/Y+BqqqoH5I16PDvkUCOKe6td9FK1Mm1Fr7WpKLWHkxJiwuX1dkSMGyOIuir7Y1Y1\nKsvJVAS+mYaI6HQiWqn6O0FEvyailkQ0i4g2y/+38EuGLHLf5TXqxnvl02z0rlfsOoa1e09E10t1\ni/Y2ybLxPjtns+U5N05dErcdFc1lJpdV1mDZTv3Ist6FRGB8s4njRov1z1EGbr1Ea9oyDH3t0n1U\nzcGySvzwhYVxvVBHgd80QjkfIzBwSjApA3qXvL5opzMBHJJM50HfFAFjbCNjbBBjbBCAMwGcBPAx\ngMkA5jDGegKYI2/7AoEcfu6xN2DXN5uXZPYya+sYqjkGEhVfeDe8vXQXZq7lM21Y9irMejgm1yof\n/s/fWI7KGsmm9+WafSiaPE11DpeIJveI/W/HlKcdLPYSnme6cepi/WttenCZNbBeX7QTy3cexVtL\nEm3obio3t6E3nJoHlbs68criwUiOetEj0HAegK2MsZ0ArgDwqrz/VQBX+nXTrCwPfNI9kcQYnlf9\nx0/X4KJn7IcWVvjR1CVRm74ZXvQS7vtoNSa9vtxRmm8s3omjFdWe9VZWq2Zxu/FaMvsgtWMEPPjV\nG7Pq5ew5ehJr9+rPEte2d9y45/pVfSUrwsQjn6+Lv4qUMYLkDhIks0dgOUZARHcCeAVAGYCpAAYD\nmMwYm2njPtcDeFv+XcgYU+LZ7gdQaHDfSQAmAUBhYSHC4bCN20mUllaitrbO9rX79sZstCtWrAQA\nVFdXx6Vz6pTU5V6yZAl2NrLWp+prw+EwKiqkUMFLv/0Wp2rMC9hrcpfU6jlOnTqVcB8AWLStIkGW\no0djJoOSkkTzk9m95s6di337pd5DVVWV7rnKPiWf9NIsK4sde/vzr/D7+afw1jfrcGv/fABApDZi\nKMf2bfG9F/V5ixYtRpuGWThhEkdo3teJijUcDqNkr2TO2Lw5ZtIq2bMb4fDBhPMB4Nul32Lfvpi5\npaysPEHmcDiMnSekCWvl5eVYuGihoVxayssT0zPiWGWsxap3zZvT9e8bDodxoprFbW+Ugx/u27sX\n4fDhuGOSXPGhrm98IYxHRzdEeXk5duyTyuuOHdsRDktjb5t2SumVlJQgHOaLGLtzZ/w73rRxI8In\nt2HtoVp0bExoXsD33Sn196JFC9FCdY26l6zOr5fmb0c3xHq1ZSck5Xn4cKKZUblO3UPieV/hcBiR\nWql8zp8/H41yE2v9o0eOxKVlpyzYhWew+GeMsWeJ6CIALQD8GMDrALgUARHlAbgcwH3aY4wxRkS6\ntSBj7EUALwJAcXExC4VCPLeL4/2932F32X6YXjt9WsKu9h06AHukFvSAgQOBb5fgRDXD3gbdcONZ\nXQAADZeFgZMVGDZsGLq1aaybjppQKBQ9JxQKoeHyMFBRgaFDh0qBz5ZYVw4Jz6G5Z4MGDeLusx6d\n8YtQ94TzQqEQ/r1pMXBE+sA7deoE7NxhfC+d66cfXg3s2Y38/Hzdc5V9BUu/AmRloJW/yer5wAmp\n1T565HBg/lwcqs7F8BEjgPBXyMnOMXx3G2krsGlDvLzyvYcPHy7Fb5lu3Is6++yzgVnxoSlCoRDm\nHFsD7NqJXr16AuulWD+dO3dGKNRHNy+Khw7F2siOaHlp3LgxQqGzE/JiTclxYOF8NGrcGMOHFwNh\nvkHcA5EGGDZkONo2LbA+90QlEJ4TvadW3jP69AG+Tww1HgqFcKi8CvhqdnR7z+KdwLo16NCxA0Kh\n/gnvtdGKr4Hy2JjGnnKGUCiEcDiMoqIOwNbNOK2oCKFQLwDAjgXbgfXr0KljR4RC/biefWnlBmBb\nzO25d+/TERraBRMnT0PH5g2wYHIo8SKdshotFyNGoH2zBtFjp6prgVnTE84DgGFDhwLzpfLTrFlT\n4PgxNG/ZEjhUmpg+ZBPyjC/i9umiysecuTOASASjR41Gs4a5CfK3ad0KodDQ6HY4HDZP2wU8piFF\nVV0C4HXG2FrY6/1dDOA7xpiyQO8BImoPAPL/+k0tD8iy4zZkgLo7eP/H3i1gH3U/9LG3+eT0DYZu\nozyDpdsPVegumGJHZu6ZuvJ5x07WeJInh8rNxzuUeC9W8gBWHkqMa4wg5mXEbJkYHllciQv/ZqzQ\nvHILtpvnZuVHyQ6vyzaBoi1vo/kdXhHnNaTs8/6B5HsZOKQEbIxgORHNhKQIZhBRE8DWhN0bEDML\nAcBnACbIvycA+NRGWrYgSC903/FT+NHUJTh+yn64Ca3tNOLxhAKn3it6hVIvraGPzXaUPgCM+UtY\nd8EUW254nCcrE/Z488PtNzLxFfP1gh9URf40+yClwWIOryFV2+kfc7ZwSBjjmOzpU14ViYtUOX3N\nPgx9bDaWbDsclcUM85AKTLMt4VVMfE+qUHK3UqDdMBrR23KMEbh5PuPBYheJ2oRHEdwMybNnKGPs\nJIBcAD/lSZyIGgG4AMBHqt1TAFxARJsBnC9v+0KW3CF47qstmL/lED5bqT9XwIwvVsW3iI+fqsHa\nvcejqx/xYujCxpxVakaFx4my8xPelpvygRPFPL2chsNwmqdGmCXFWOLx2jqW4G2mlufdZc4Grvs9\nOAOX/v2b6PaH30nl+aDcK7Cq2GauPWB80OBSo3LutHFsp5WrvQUhFv7bC8yjA8ffF7AfZsYtQZtQ\nNgLASsZYBRH9CMAQAM/yJM4YqwDQSrPvMCQvIt9RZhbHpojHOFVdi91Hrdd21X60p2pqcenfVeGm\nOWVJKHTMYD9vejr7CISBf+Ibw3fTy1UrNa8+S+UD5y36Zq6Ekbo6b80SFqYhbeXW/f4v0LtdE/3z\nXcq1tTRWMSu9gyYFOVxpTzNbc9jkus06obp5HsNrqycRqRoM9q+31ZONm3Ap/W/aI/BhVnegvIYA\nvABgIBENBHA3JM+h1wCc66dgXqAMEehVGr9+dwVmmLWQDHD6vg1XgOIIOaCloioiu7glpuUE+/Zh\n7wtprWqpRzsL0+gx9ul5OP8MXWc0R5gpncqa2jhZFMm1k8P8+KiVIHNOZoZrMctyu73M6LOqGwwe\naQWlwWAU5dUOvCL5HabdqLwHbYwgwiRJrwDwHGPseQD6zZ2AIQ0uxbbVv50oAW0adtD2aN0MFr88\nfzt3dFMjvCzT/5q3FUWTp5mux2yFsnKVV3bR2eudvV89zL7HiS9/a/kOiyZPi65a5yWK8lQaGe5a\npdoCmtgijj9sNlgcPyHTK3MlIbZ6n9FKcF4R93jyrczWF3GlhCHNxtYGhgzahLIyIroPktvoNCLK\ngjROEHiiPQKVx4ZbHLe6Pax6fXQ04ru/RoCp30jr4Tpd/xfQ9ggcJ+MbP399ORZsSfR/L6uKcHkB\nKdd6WQ6UhWg8Kdec9vLoPp40AfwzvAUD/zQzOo5hB72IuXe+K8/rUc3yve7fi/Abeb8ZV/1zAV6e\nvz0mH3egPHvn6/H83C1YYbLy4F3vrsST0zfE7QvaYPF1AKogzSfYD6ATgD/7KpVHKBkZfZEepKlt\n2T8/d0tc6AIjjKI1OilbRgUkWRWol5WZQsSm7TeZ3eaaSB2mr92Pn/5X39OIRxHoxbR3i2IaUoe7\ncIr60gEPzUB5Vax3pzs+a3Afgh41AAAgAElEQVSvLUdrUVYZ6wFMl8ONuFmZTIEIKDma2LNasv0I\nPjIIGqnmwIkqPPz5umjP1Sy79PLSbJzaKu//PGMjfvBP/blCjAEVVYm96UD1COTK/00AzYhoPIBK\nxthrvkvmAQRCHXMfu16NtlXw0Xd8nkhGlcWG/SdgV0V50i22uOUtr5q7V3pNtEeQzGYQJ9HFZgzy\njMeRJWpn9kgmIGZOc+tI8+63u7B8Z6y1eqIyIpdLCd6WcKS2Do8uqcRUudWtd5mrWEMEZJvUWH/6\n31rjgyp6/2E69h83V0z6i9f709KSHA4S9wcq6BwRXQtgKYBrAFwLYAkRBS0EvS5Zmqfz4j3yxOzR\nQ+v/rGzd88Eq23LxLH7iltnrjef5ee2eCcR6BFlEhhFD1aRCXRgHZkuNLSuSMEbgLJ17P1yNO95O\nnHGswKto/I7Xn0VkOkj8yoId3GntOXrS0/UIXPWSmf437TbInh14TEMPQJpDMIEx9hMAwwD8wV+x\nvILiXp4X5fQ/32y3PkmH73fHAqCpW1uAfbm8MA35Yd6xi7rs19bFBovvetd6ub9ktpYUjPKXx7/c\nD3mVxkXUzOjlOFTcd5OYLt8YgdprKP73Xoczg7OzvclIUmabGnCkojr+XFi5j7qTR++b/vC7PThR\nmZx5QTyKIIsxpm4eHua8LuVImct879rx8KOXYjH3Z6w5EFdweENdV9ZI9lejHsEeHfupHpsOlOHb\nHcYDVzws2BKbcepFtmpdIbWs2HUU01ST+5KqB8wtQ/YWZPewDEbX05aTPFzhPoy4HrqDxRwz29/U\nC0MNwkvzt2PklK+wUeNi+92uo1L8HxM5vHAbBaRytnCrcfC7H7+0VHWu9L9fPR4G43L/L5MlZr2E\np0KfTkQziGgiEU0EMA3AF/6K5Q1EyszPYNmdmfwvts3HeU/PQ/+HZroeKL3QRUhrQMrTXUdORn8r\nWIlVFanFLa8uw5aD5QnHYl5D+tf+4J8L8cu3EsNdJIXoYKz+m+rfqZl1Ej62QRSFcN2/F0X3VdbU\nxoXhtotaXL2WsO74sWbnMZ1FcA6cqMSj09YDiK3IBgD7j1fiqn8uxMXPfo2iydMwb1NpwrUMLDZe\nA2DVnmPmD2ECAfjFm3zlSWl46So/jzy2jMp9dpLGzHgGi++BFAV0gPz3ImPsXr8F8wLnC9P4y1cb\nDsb5lfOWJSVcQ6rHU52aIJbtOIrZ6w/gD5+skdJRJRObWcz3cMn0GlLwoiz5UR6VfKxRhVW++/3v\ncdlz8w2usEZd+Wtbwicqa7hNO9qyrZ7drK5Ey6skpbHjsNTAeHrmRt20Lh3QIbrtJgikk/Kj9526\niX0UTRfGq9zlmo2OewjXmsWMsQ8BfOizLJ6jxBryw3XPDas0LTW7FWsy3cqsWFlaCyCb69yYG2/i\n81r1CBLSSmYWWJQfW+XKYRnUC/OgoNdiX7HTnelPjbbVO/7v8+OUjsL6ffqL3gD6j21Wh+q93orq\nWryyIDZG5+Z7tlN8FKWhN6EsUseQkx0vy7RV+3DpgPa25DH6pvNykqMIDO9CRGXyOsPavzIiMn7j\nASIaa0jeDsIAqS62vYb8EYMXdaH/vlTfpruttBwHNb7j0VAITFo7d3VJTCHG5hFw9gjsCOwzdgZO\nnZbAC3TMeTqRHGzJxItW0ShmQS16fvJmjTDT+Rc65eCZWZviJi0mq2EXDTqnc0O9HoFdE6aZF15e\nqnsEjLG0CCNhBhFwMgJTv+YgYLd3mQqziBojcdX5O/bpeQnHn/hyffS858PxoZijXkMW5Z6xxCBv\nXlE0eRqKT2uhc1NfbucZfi+hGOFY79oKL3oEJ6vjZ667kcpOETJTZkoDxk0j8/efrDE2DaW6R1Af\nCNogsRHpbBoC+D9IxSSmZxPlDSZWpYQW8CkPlumYVCwrWo6KWDnFbvhyMxSFuHbvCfx11ibP0gUQ\n91K9sIProfaW49FlXuo7O3WDcqbeu3OaN4yxaLpfbTho2MvPT5Ii4BojSFe0mbu1NNFbJQjYn1Dm\njxy8aG3Gis81r0JjDMjRPEQtp2moKlKHgly+MQmvsPrWU91heEkVP0fBy16CmzUAzMQwk1E30F1C\n2s7lstcjMD45uqCSTVEYi38ewzGCJJmG6nePQJO37y2TFmlP5XwCPexK43Vj2G5+GJ7NmQwDkKOZ\nGKSYH6xazIfLpeBlydSFHnQIko7bRrxaqfvVI6gxWe1P9/162SPwqABFJ/XZvO5AWWXceIeRsklW\nyJV63SMwylyrDzfZDW67FXGqxwiM4H2KOsYS3OJ4K5uxT8/DjimXJtVr6OUFiS3uoOO2saO+XB1e\nfNoq48Vt9DCbzHnvh6vRvU1jvLJwR0Lh0SvjXjp7ODEN6eFUSa4pife3Marvk9X7t1QERHQVgCcB\ntIWUJwSAMcaa+iyba4wqC6tus9+Db1rs3i3VYwRu3Sj1TEN28zxI4z+p6mGa5YCXjfjJH8X89Tea\nuLE6Ycn2I7aVixc4GSzWIxb51V6Gawe+jb5pr2ZSW8HTI3gKwGWMsfV+C+M1RpWF1Ufy9lJ3i77Y\nxW4h2n/c20VObNcZRoqAd4wAiTMmzRb9CDpBlNwvc06uwybq5waVvZENXO8uXhaR5bbmWXhfGWsX\ncTLylktW759njOBAOioBwCQ4W8A+XbsF/C8zvfUQ0cZ8cQr3c+iYhox7GYkHiiZPw0Zt4L4UKpK4\nAG0GcngpX1lljWmcHMB9r9bo8hybg5dWYuTnGigCrsFiW6LEMX+zef5ZyaLFTBS9d39SE1Mp1T1c\nwx6BbBICgGVE9C6ATyAtUAMAYIx95LNsrtF7gSt2HeUOzpYsUt0YXrLdOuyzGsNwzNzXJw4WGwXe\nk/Im8diHmnUgUpmHczcah+xW8FK8/g/NBAA0a2C8UKCb1eIA43dcFXG+HKkexj0CnTECD19yeRV/\n/phV0QfLKtG5ZUPb99cqAuObJKdgm5mGLlP9PgngQtU2AxB4RaBndzNaJSiVBKt/Yo16RrAa3g+1\njiXOIzCyZLyycAea5CcWU+2rTfa4jppvVK3LZIrhdaXMw99mb7Z1vlVr2k4IhYQegYsvx443jtkz\n/PCFRdgx5VLD9372U1/huuLOCfvVS20CxoPwySpPZjOLf5ocEfwjOMOJ5gTNndUKdYheNXYGi7UT\nZYzGCB75fB2uObNTwn6tIglKDno9mJoqklUkDRWBnmlII1O5i16PukPaxaJF78Zss/vIKV1TLu83\nn6xyzbNC2atE1Fy13YKIXuZJnIiaE9EHRLSBiNYT0QgiaklEs4hos/y/zpx+jwiom6WWoFRiyYIx\noJGmlW/2Yegd0b5ar8Y5/OK1RTs9T7OyhmNFHIckq0xqvccUeL7cvccrLdcL/2ZzYjhrIN5Zwapn\nwVWN2MwwXueIZClknn7ZAMZYNPA3Y+wogMGc6T8LYDpjrDeAgQDWA5gMYA5jrCeAOfK2L6R6Bi4v\n6dYjMIK7R6Czzyxypd74gfbVvu5DRZvJeFUkrdIxmrXsVRvOqPeq7lFazivyoR7hdepKlmML1wpl\n6lY7EbUE3/yDZgDOAfASADDGqmWFcgWAV+XTXgVwpV2heUm1vz0v9UQPJCzBaQRjLOGZzdZI1rP/\na9+tV0sYChSSUyinGiz9unibPQcGu9hZ8IXHNGS3wuZdlTDlYwQqngawiIjel7evAfA4x3VdAZQC\neIWIBgJYDuBOAIWMMWVkZD+AQr2LiWgSgEkAUFhYiHA4zHHLeHbs8GfpPq9Zs3ZtqkXwhJdmreQ6\nb1tpGd7/2npdYoX9Bw4k7KupiV/96sC+vdzpCawpPXTY+iQOysvN43vtLnW+ipobDh+KmYxOnao0\nrV8Olpp7hd37yiyUVcfX2Fb11c5dict46rFu3To0OSqNMZSXlzuqB3mwVASMsdeIaBmAsfKuqxhj\n6zjTHgLgV4yxJUT0LDRmIMYYIyJdnccYexHSymgoLi5moVCI45bxrMdWYPMG29clm95n9AFWrki1\nGK5p07YtsN96lmh1LTBnF/9AX+s2bYF98ekW5OfhRHXUm9lWegJrWrZsCZTq29ft0KRJY+CEcU+x\noKAAqEy+O3dbVVktKChAKBQCpuuPN7S1KNfvbkxscEbrK4M0O3bqDOywDl1yRp8+CA2UVmULh8Nw\nUg/ywDNY/DpjbB1j7Dn5bx0Rvc6R9h4AexhjyqrtH0BSDAeIqL2cdnsA1k7YDklS4D7XpNL10Uv8\negy9dJO1lmumkqwSmarxMTu3dVrSzJ6Nd+Z3svKHp6rsq94gomwAZ1pdxBjbD2A3EZ0u7zoPwDoA\nnwGYIO+bAOBTbmltki5jBH6FA0g2h8qrrE9ygJ6iFIrAX5I1WJwq1GXKqrJ1+ghmnzVv4+9oRXLM\n22ZLVd5HRGUABqiWqCyD1ILnrbx/BeBNIloFYBCksYUpAC4gos0Azpe3fSFdKgs38d6DhN0Zyrzo\nfTRpouPTlmT1UlNV8tWNL0sZHAhZXhUxzUPe/H3ofzxWePcYKgLG2BPycpV/Zow1ZYw1kf9aMcbu\n40mcMbaSMVbMGBvAGLuSMXaUMXaYMXYeY6wnY+x8xphv7gFpowg8WAqwPqMXtj5oYULsMqyoZapF\nSAqlZea9xH3HK02P+4WdtpcTF84hj8wyrexNlmJICTyDxffJ7qM9ARSo9ieuph0wTpyqsT4pAJgt\n0CEAZq9P9BoKqsmBl6D3aLzK34MWiiBVqMuUHwsPVUfqUGfyWQdt7hDPfIBbILl9dgKwEsBwAIsQ\n8yIKLDVp0tL+97ytqRZBkGSCPn4VtAi9fmL1rE7r7D1HTxoeC5qDCM9g8Z0AhgLYyRgbA2lW8THz\nSwR22Jui7rEgdQTdbBmweiqlOFWKN01dYngsaEYAHkVQyRirBAAiymeMbQBwusU1AoHAhGStReuU\n+uLJxsOBE1WGMYkA50rRzCwWtB4Bz8ziPXLQuU8AzCKiowBEYBeBwAVBj4gRsHrKd4xiEgH+eDal\nnSJgjP1A/vkQEc0F0AzAdF+lEgjqOUEfIwhaRZVK/MiKT1cGKyQKT48ARDQEwGhIynEBYyw9gvgI\nBAElWWvROiWd15D2nvqfFzwhJv4IKUpoKwCtIQWR+73fgnlB0Fy0BAKFoIc/yaAhAksyoRrh6RHc\nBGCgasB4CiQ30kf9FMwLRGEWBJWgm4YypRE1pnMO5u42D1iYCTnB0y7ZC9VEMgD5AEoMzg0Uws4p\nCCpB9xoS306MTFCKhj0CIvoHJGV4HMBaIpolb18AwHiIPUCIHoEgqARbDcB0Vmx9guc9HE+TCAVu\nMDMNLZP/Xw7gY9X+sG/SeMz4Ae3xLzFrVyCwjegRxEj3uFY8GCoCxtirRsfShX4dm6VaBIFAl6B7\nDWWMHuB4DZlgWTAzDb3HGLuWiFZDZ7yEMTbAV8kEgnpMsNVA5riP8ryHjB4jgBRjCADGJ0MQgSCT\nCPhYMffi6plAJpjJzExD++T/RTgJgcBjgu4+Wl8WS/KCTMgJngllVxHRZiI6rlqpzHg1akFKaJzP\nNUlc4CNXDe7IfW7Qxwh2HTEOoVyf4HkLx05mtteQwlMALmOMrfdbGIFzAl6vZASVkVruc4NuGhJk\nFjwTyg4IJZBabjqri+U5QTc1+MUvx3RPtQhRurVuzH1uur0unjKYjqTbe/ALHkWwjIjeJaIbZDPR\nVUR0le+SZQj5OdavgGewKugLnfjFiG6tUy1ClDvP78l9brop7tvG9Ei1CL4ghkIkeExDTQGcBHCh\nah8D8JEvEqUBDfOycbKa3wxgBk+FwLNISIbqAWSlIHjbqB6tsGDL4YT9uTYiyaWZHgi8u6tTMsAh\niAue9Qh+mgxBUoWTSr1JQY6HisD6nKLWjTjSqa+fqjnZKXjuPA9ChwZ9sFhLfS1fokcgYTah7HeM\nsadUMYfiYIzdYZU4Ee0AUAagFkCEMVZMRC0BvAugCMAOANcyxo46kj5FePlR8KQ1vFsr9GzbGJsP\nlhuek5OhXYJUmMS8uGe6va56qgeQISGVLDFr2igDxMsgxRvS/vEyhjE2iDFWLG9PBjCHMdYTwBx5\nO2U4Kd9efhM8H1gWEf5yzUDTc3KCHuDeJ1IRxdOL1ny6tbDTS1p+graIfKowm1D2P/l/r2MOXQEg\nJP9+FVIQu3s9vgc3TnqGXnXriYATleax0AG+jzBjewQpqFC9uGe6KYL6qgnqMmK6mDWWYwREVAzg\nAQCnqc/njDXEAMwkIgbg34yxFwEUKrOWAewHUGhw30kAJgFAYWEhwuEwx+0SeWIYw31LjUtxba19\nW39VVaUjWbRkQbKZWfHdd8sti2vlqcyYAKRl5Qo7nVNvOHSoVHe/nTJaUrLHI2mSw+KFi1Itgi9U\n10QQdC2nlKvy8nLH9aAVPF5DbwK4B8Bq2DepjWaMlRBRWwCziGiD+iBjjMlKIgFZabwIAMXFxSwU\nCtm8tYSUcRWGx7OzswGbyqBBgwLglPvQtLk5WaitkbL02uJOuH5YF1z1z4UJ5xWfWSy5kC5aYJhW\n0yaNsbeizLVM6cawoUOBhd8k9Z6tWrcGDhxI2B8KhYDp07jS6NypM7Bzu8eSWXPTWV3w5pJdtq8b\nNWokMHe2DxKllqzsHPA1x1KHUveFw2E4rQet4DEslzLGPmOMbWeM7VT+eBJnjJXI/x+EtKbBMAAH\niKg9AMj/H3Qoe8p47oYhnqSTo/J9bJSfgyFdWuiex2NFyNR5BF4898DOzW2d74WnSape10V92zm6\nzk8vp5tHd03Y17pxvm/3UyO8hiR4FMGDRDTV7oQyImpERE2U35DmIawB8BmACfJpEwB86lB2T3Di\nR2y34jBCXYmVllW5SitjB4s9qKBSMb6SqiECp/nlp7jd2jRCswa5cfuS9UqEIpDgqT1+CmAQgHEA\nLpP/eEJTFwKYT0TfQ1rachpjbDqAKQAuIKLNAM6XtzMSdQX0+ap9hufxfLuZOliciuf2Ij69kwr5\njPZNPbivs+tysr3J5wcuOSNhH4Hw+a9G49IB7aP7kjWY7oUiaJiXzX3ulYM6uL+hD/CMEQxljJ1u\nN2HG2DYACT6PjLHDAM6zm57XvDNpOK5/cXFKZeA1axAIVv5NqagQz+7ZGt9sPpT0+6ppYOMj9Aov\nKg8nppYmBe4jzDo18diZNW2XLAI6t2yIKwZ2wDS5QZROPYKmBbncE0wv6tsOn6zc6/6mHsPzdhcS\nUR/fJUkiedlZGCSbd9o3L/DlHrkcLagBnfhMTFw9Ao9abHaojqTeCduLMQK7KXjRIzira0vb1zTI\nda/0nGaXniK4bGAHTLtjtEuJYuVbraT8GpPo2yG+V+XFojN2FLSTeS/HkxAGm0cRDAewkog2EtEq\nIlpNRKv8FsxvCnKz8Y8bBuPtW4f7kv4gi3GE5g1z8ez1g7jS4jMNJWeMQK3glmw/Ev3NEzzPj1Ze\nKgxitR60Ils2ysM9F9nraHuhCJxWsH46IygyUdw+f+5194W94rbt9AiW3n8eWjTMTdifx1H2FZzM\nQSktdzd+yAPPE4wD0BPSYK8yPnCZn0Ili8sGdkBhU396BGRRRZ3ZpQUa5edYKgwlrd7tm5iekyzT\nUEGOfmXEowjSLb6OEV4s4+jEBv5/53bz4L6uk/AcRSR1lvhVVOo0nVg7b7Jt0wJ8dntiD8iOknSi\nUJPhEWj59apdRu26jwYVloTZhFYFWTn+8W0jAQA3DDOO904E5Odko3/HZobnuC0sQ4tirqtOBrQK\nOFqrvvQIUqBcvDAnOCmDgw3ci+3gZX55tah7tEcgiza4S3PfBotrNTLbfQQ9046db8/JYyVj9nxG\n+hwmI/Qsb0EmImx4ZBweu7Kf8Tny/2aVh9sxgvd/PjL6+2/XD8YbN59l6/r8XI4egQ1Dzke3jbQ+\nySPsfmdelJ8ajiA3PL1FM7rpRK0NcqdMXT6sxLSzLKgat8pLr+d9yIbpxlGPIAnjfxmpCLzmgj6J\nUTKsTfaxl1uQm206iKR8vGZl+L6LE93y3DCgs3HvQw+eVoudSiiXc8zDi0/E7uQlvR7BBz8fYSuN\nmlpmWSndFpJWXxvbuy02P3ZxwvGJI4tMr799bOJiMoGOcaQSzUrOJ6+Oj3Bzw7DOXLewY8/XQ0+u\n3Uf4oww4ad2LHkEaY6f1y5MaYKwI7h3XG51bNnScut7C93YrDJ7z7aTptofTlWMNBwB48of9MeWH\nPGGzYujJVlxkzwuopraOu2eRRfpeOw9d3tf0Or3Wp9fmOd5yPq5vO6x7+CKLtIw2LM6FZDrlIdSr\nbdzypnb7B27H4px4DSXDDyQjFMHoHvHLGXptGdL7oL1U4kpaRrZpnnv1MZmM9MkvRyWmySWZvQvs\nfAO8H5zRs/+ac9nI64Z2SZjVaoWyMM3wbvZdQBUitdajBG7t+XrXe90j4B3ryM/NQsO8HNPzFXkZ\ns5bTad5kZRF+fq7zda7dhj13crXoEXjEf35SjKYeTMYxJrFwe/nBRccIDL4hnpblJf31Y8wUNs1H\nj7aJi67bFZ8AbH/iEtPZr3byhGfw2Qw/TSCKeaHKxTyK0T0T11rW9mLc2rN5K5CfjerqKH2vGlRK\n3Uo6+4zQHi7jCOeuoHa1tpvFbnsEThRYMlzDM0IRNMjLRqcWzk0nI7u3sn2NZUG2OL7+4XGqc+WW\nkg/eTkZde8Muv9FuIunP/Gbc6Jmr9JPUT9RPf4A82QxRWeNcEeRmZyVUQn+/frAbsRLQK4Nac9EZ\n7ZviD+Odjy9ZmYYeukyai9q2ifE4jKK01d+EdY+AU0C9+6lqPbvlxK13nhO5hWnIJ+y2tH5zQS/r\nkzS47dY3yMuO9mKsegRuMBLTSY/ACjut9EacisDpUIybD1oxDVXVeBu+WPvBuy1DemYM7TsgD+5j\nxoSRRfjLNQNx94XGk+cUOeO8hjhMQ1t0BtB54OkpqccR4q71wQfayjssEPMI6gtu6lB1oSzQcZPU\nq6C9eHdKRFEvxgiMMLrUbpp6rTo3abr17rBS9h/9YqRhRWLlRdS+mTQJ8dZznE3wUsYWtD08bx0M\nvPfkcgIR4eozO5ma+ih6rvQ/A59+dxpxV12xGhWT09vpmzjt2uv1ZiJr0RujU5MMT6+MUQRuUL+H\nFX+4MOG4XlnyopWVrWkpeaXMuPbbrJR4XFx5C3S3NnweP+r72iWLyLAiadnI/OPt06Epdky51HQS\noBmDOksTw3h7eE57gjwmBSf5F10/26MearQRod5nIHvTghzdGdZ2zKbqMm/3EexOKPvPT4rjtp0U\n12REDchIRWD75asKDm+0Sy9eXba2lZ3E2Ol+lD3eNL+6O+Qo/d+Nsx0k1zaf3T4Kl/Rvb30iB9rX\n6XXDTym35/Rq42m6XheNaLlQJWzUEHnjlrN058x4EfZDjZ1nzDPpmdj1SNNDmIY8xI0HRhYB0+4Y\njfBvQzauMX95Ra2sB6+1BcCL0AZajFpedns02jABAPDUDwdoolN6U6B/PPy0aAA2dYperGq1+qEL\nTXtDvBFjvcTsVfz2wl64bGAsJMhUVQtUKYPqsq9Ny4k5yuuYQHpjBHbrPi8CATpFz1ysoO5BXNin\nkCu/PvxF/OTEZIRRyRhF4AYCoW+HZigymKSkp2TMuuWvTByKey7qbXlfZeJSrdza8WM1JfXH99at\nZ+HVnw2T99tNJ5E+HZqib4fYDGWvGjZE+hVQrY0MMh4kJ09b5uPMlobUlBuj+5rp/9vH9sSPh58G\nQGqZnqWa26AoAnW++F+l2CdhfIkx2xqGp5F069ldE/Ypl73/8xEY0S3mHWhrFrxJj0A9ptCiYR5X\nel4sQGQXoQg06E28sioUdscIxvRuyzUYqvQIIvKH7CxQmX4L9pnrJDuvWsyR3VvjXNmMYNtrSOd8\n7T4/Br3U+RwJ4LqDejLxvkfe3FIUbHYWxeWH0hgxU5CO3Bk9dmfWsQzZbjQojbH8nCz89kJ9Lz+z\nbzIvO8u0ZW+GmekmscxzDOCnQF1npCIws+l9cefZCfucVGBejhEoH7Jdy9BzNw7G8G76cyAUE4ex\n15B0hDdUg5JFZr7gnvUIoC+3On2nVjRtutPuGO1qHQB1cLl7x8X3Aq1EVDxtrOzMyrvKonj5lZaq\nWbgOoyNf3nk2XvnpUIP7mYpjiq6HXZaSbixho1sYvVclvHSDvGzcPlZ/VrlemkpybkxmZvWD+hB3\nAyAF3baMUQTqAmTXNdHqxegd9mKAJ9YjkEq50UfAeyfF7THuWpOHm3XXOXh5on5lkChDYjoJPus+\nlHC1We7ygfrhs9VrynZuYv7utSL27dDM1RKRZqu4ad+ndntUj1a4qXce/nQFX0whbf6e2aUFfn5u\ndzx9jckCSAbv5Iz2TTHm9LZ4/Af98dYt9iLR2sXJhDItjXnekU6SUUUA4u7fXDW4o7nJT0WrxjFz\nkAOLV9LIHEWges08i6io0SuU6jgzj1/VHzeeFe9KqLYHjjndmddGU01L0MmAt1VYCrNy2bOwCRrx\nekkpPQKTAT+vPgJlFrOW3OwsXKGzloLe+zOf7+Dd12oWblrbQtS+KyLCBUW5aFJg3iNQh2iIq0yz\nCJMv7o12qgaA3Ue78awuGNkjMRyG0/T0zte6jzIH6VpFYlXfh0cus/v/9bpBuFIVBttsPLBhXg6e\nUgU15Hks9b15VzF0S8YoAjVmpiE99ArFO5NiI/vtmzXA4z/oH3e8Y/MG0d8/4Sikejx3w2D89sJe\n0XGLq4Z0snV9/ExN7VEOTaBz3NCUpLcvwTSUeNa/fjQEM359joUQxqjrTifdcO21Xttnq1WKwMoN\n2KnNPSsmfFLsyzFvJHMTnJ7/u1mAxtj/+oreDMX8ZXaVrmnIqQnRQTY7udUVg5ytu2CXzFQEtnsE\n9u/xk5GnRX+POb0t/nnTENtptG1agNvH9ox+FL+5oBfWPzwOO6Zcil6FUqC4Nk3ycf1Q64lNRmYI\nz6oNnS+Dp0cwpnfb6JWN+qcAABw1SURBVLPYup3OvizSrwbt9KS87rqrTUNaPWBlGuLF1SC8g5vy\n3m75Hy7gOk+RvzoiyVKQkxUdH3tl4lCsUKVjJC2PAry4X+L8D+P04vnq7nPjr7OTbRS7hkfB1cvB\nYiLKJqIVRPS5vN2ViJYQ0RYiepeI+HyqXGJ3jEC92IwTU4E2ProXk5Cysig6oS1L9aE0M5nGHtdK\nVuWB8tMyuBdnoTSqmM22lfTdmGLUz0QG++PPZ9H7Wsmm5a7zrWNOTRxZFGc/ruZYiexXY3tg4eSx\nnswgTrYNumOLBobHeCZTXdyvHYrlZVKrIlLspvzc7Ohz5GZnoUUj6yrC6LmVOD43ndUF/TsZL7Zk\nlW/d2thvrETTtnt+PR0svhPAetX2kwCeYYz1AHAUwM1JkCEubMF9l5hHW9wx5dK4qeEBHd8BwOHa\nqqpcxvVrl7Dfq0KnFxYi0RPDHq/Jcxp0MbA133fJGbhsYAfd1p9b2S7nWMv5ocv74l8/PjO6rTZD\ntm0qTXhr1zR+0L4gNxsdmjdwvBiPXogGY+LPcqJ71Eq0aUEudky51EEqEi/86Mxoo0kJ652fk2Xb\nRdXq2Y0UvbphENeo8OGjZ2B8YwTe39oSP4P0g4g6AbgUwGMAfkNS028sgBvlU14F8BCAF/yUAwCe\nvnYQrt9xBGNOb2t4zv9uH627P4jL+6kX8eA7X1qs5b8Ld0jXQb9l7JTHruyfsC/Ra0hfLiO4QiNo\nPt7CpgX4xw3x4Zy5uuMaO7XuOdbSJPCfnxTj+KkaNG+Yi47NG6BRXg7OP0PqbWpfXe92TRzcQTVY\nbOM53aCk4fVE90o5mmt+Tlb0d52qwWJ2P6PnUiaaac2UNwzrgl6FjfHS3PXR63kCxDlB/V46mfSg\n9M5PFn73CP4G4HcAlP5xKwDHGGPKKhJ7ACRlNKRxfo6pEgCgu0ALEFBFwHOO5iT1dHfeHoHe8cX3\nnYcv7oifb6GYrNTn80woc5qzyipT6thPZh/QQBOzgCSH8bVGvuaA5NVhthpa55YN0a9jM3Rq0RBE\nhAv7tou+B+07ICJdF18r1Plqt6g6qcz9+hpiPYJsVUNHEnBAR741tLVlQJmDo93/xFX98dNRXePe\n7SNX9lOnxC03dx4yoBVHGJR61SMgovEADjLGlhNRyMH1kwBMAoDCwkKEw2FHcpSXl3Nf+803XyNP\np3u+ZMlibGtorDP10lfv0x7nkcfqnIoKacHsZcuW4dBmfRfPtWvXYtdx6ePatm0bFkR2R499++23\ncjoVpvc6URVfyiORCDasWAwA+OOIAjy8qDJO3rITsYW8lyxejC0NYvl26mQFAGBE+2ws2ie1+ObN\nm4fsLMI1vXLx/qaauLSMKCnZg9AZ+fjvuEZYOP/r6H6j6yKRCG7vU4NI74Z4+fsK7CknrF25DAc2\nxmSrY3WyPF+jvLwyLs3q6mrpeZYswXZNOWgGYFAOEA7v1b232bPs2iWlu23bNoSxBwBQVVUVd61Z\n+VX2H6iok5+zBt98bZ4fS5csjdsuKyuzLL9a1qxdCwAoPVQaPe+W/nmYurraMo2t243PqSmXnuM0\nKsWmI1Jb8ftVq4B9OThRJpWr5cuX49jWxPK+dKlUnmuqq+PSLCsrBwDsLdmDcLg04braujoAhGXf\nLsP+Jllomkc4Uc2wdu2a6Dm/OTM/4VnWHoitiFZZWQkjwuEwNpZI5Xrfgf1ceT1vXlj3mJ26zC5+\nmoZGAbiciC4BUACgKYBnATQnohy5V9AJQInexYyxFwG8CADFxcUsFAo5EiIcDsPy2unTAADnnntO\n/CCvvH/EiOH6K5zJx6Ppy9vRfQbHTeXhOQdA09XzgRPHMeTMM2OB0FT3B4C+ffsisucYsH0bunXr\nhrNHFAGzZwAAhpxZDCycj8aNGyMUSpxNrXC4vAqYOzu6nZOTE5UtBODhRfHyPrNGkgsARo0cKfmw\ny3I1btwYKC/DHeOHYtF/Fkevy84ihELABWv3Y/+JSoRGFCXkh5rOnTojFOqTcE5Cnsn7s7OzccHY\nMdJvmot7uw9MWGz+sgMr8fGKEowJnYtn1iwAyk5E08xbMBuoqsKI4cPRuSXnSncc73HRqfXA9m3o\n3q07QiGph5O3cA4gVyyhUCix/GrLGIBdh08C38xFXm4uzjnnXGDml4n3lq8bdtYwYP686O7GTeT3\nr8lnXbnlc0YWD8LzK5egf/fOCIWkyW4hAJ89NhsHy6pM0xgyvAbvPTTT8Jybxkv/r3hlKXCoFP37\n90eodyE+2b8C21fuxTkjhqFnocqEJst0lvxcuXl5cd9ew0aNgLIydOmsKTMyWfO/AMAwbNhQ9Cps\ngrz5s4HqKvTr1w95q1egOlKHO645P+G6yjX7gRXLAQAFBQXAqVMJ5yjP12L3Mfxn9QJcOuwMhIaf\npp/Xqn1jxowBZiSWH666zCG+KQLG2H0A7gMAuUfwW8bYTUT0PoCrAbwDYAKAT/2SwS5GJiAjk8Od\n5/VEvsP4JG5RROINr0MGxg8/u6Fau6zeAKD6lAs5Z2s2ynce8iEvmxKUAAA8dfUA3HdJb9MAYp7j\nkY3dzByXcK4H9xvZvRWevX4QLtK8L57HaWoxOU5B+eaU0BGPX9Uflw/qEK8E1Odrtn8wuCM+XlES\nNdsYzvQ3caNecO9YHD9VYymrlWloYOfmmH/vmLi5RUEjFbXYvZAGjrdAGjN4KQUy6GKkCIzK0F0X\n9MJtoR7R7WTNAgRUH4pDH3n+MQLtgC9/VZJ4LfelpvCuCWGH3OwstG1ibp/3a6jIbbpuwpk4GiMg\nwhWDOiasOnbTWc4W6tGjuex6miu7ejfMy8HY3oWG5xuVS+X7MDpuHGtImqNjNG5oF2WMKKgkRREw\nxsKMsfHy722MsWGMsR6MsWsYY1VW1ycLo9fEO1icrFmAgGpyksWHfE5PyfNmWNeWcX0CpVXu50C4\n2YQyxUPG6vbz7x0TjZSq0CjPV2c3Q7z+kGNxbhKZeRf/bOtYrJ5UTEWKced5PfHyRZymMwBv3zrc\n8NhDV/TF/Zf0xjk9jcNbqDF67loDryGF4MWrTQ2p+aICitF3HkQ9HpM1sSj369gUa0okG/eoHq2x\n+bGLkZsdc8kDbPQIXMhoOKGMAW/dOhwb9p+wrFw7tWiIiGbVkca8C9u75JL+8aYPr8tB1Idd3VOT\n36edQHfxsYb0pfzuDxcgUluHiupa3eNeQES2GhYjuutHxgUkE9Kkc/QXkDeVQfO/Us4N5YoWLec9\nX78YKk+0SwZCEaiwu65vKjHrEXRq3hBrSk5Ez9Gze5u1Rr3CKPooA9CyUR5Gdudr7Sk0yM3Gj4Z3\n4ZrYBQBr/nQR+j04w9Y91PxyTI+4ba+LgalfvJ03w3FqS3l27vZDFbrH590Two7DJzHh5aW6x4PK\nrLvOQXlVxPY8AgUz12AzzunVGkWtGmLH4ZP2LuRk06MXJ2WJSoWMjDXEi/Ie7LyPRnnZ+OUY+y0Z\nu6grVf5rYr8ZZ5fAduUXN3IZf0jxzLVrl1aSbN4wFw9c2ie5A7pqOTxWmwPk8Ae927lbkUqRi8i+\nYlfexWmtGkUXJUonehY2weAuLQzfjdUYgYJy9FB5Vdy2EQ3zcvD6zebhubUL19shLycrqYpA9AhM\nyM4i1NUyWz2CtQ+P81GiGGY9AqNp+fFjBPHp+EHiGIG7u9nWSfL/VmGcudPzOLMuH9gBgzs3j3NJ\nff7GIXh+7ha0acK//nKcF1YAOq9/uWag5HacRAx7BLLXkWGIiej18ceVNUCc3FNBHa8s6AhFYIJU\neJhnK2t5SWyav1OvoUT7tBeokzNaocx2iGiH6qpRfg4euqwPxvQ2n1HOLwc/w4pacpmwtPMSiota\n4pWfmsRYMsVaQu0ZfgyWXn2mvXDpfmJpGjJwH+X5rJrLa478cEhH/P2rLQ4lDAZCEZgQC+YVPE0Q\ncx+1cY3qN28YajfPnjBG4DIfnfQoJo7q6uqeTnnv5yOsT/IIJWRzy0a5gRzPSgYJjy1vRxWBRWtO\nub53uybYsL+M67tqnJ+DDY+MQ35OllAE9ZlstTtGkrju9Dw0bmPthpql0yOYOLII2VmEPUelAawE\n32jVDqMusZcYfZyu00kyXgdX84IFk8dGf7dqnI9Hr+yHsRw9H63d2cmqd0FGW1aUKODW3nHSCd3b\nNMaG/WXcvVbtXIp0RQwWmxAtPEn8Vi7umos/jE+cCq9FKbhq0R66vK/ptaTz23JAykUlbGgaSte6\nJwCNbcV1VjtL9UfDT0MHjpmrnVo0wG8v7IV3J0k+/P05g7kFHW2DRgne17SBlF9GYwSD2mbHnRfr\nSfggpA73X9I7OTeyQPQITFAqydoA1lwTRhZh0bbD6KUz5d5wkXvVtzC4SwtMHFmEW8/p5kqOlgWE\nI5X6NzQOMZFuBEfiL+88Gxv2lzm+nohw+1gpWuqnvxyF0x2Gvg4aWueJO8/rhdPbNcWq3cewrXS7\n4RjB9afn4bGbzo3a+2PLcPr/zrc/cUlgTHlCEZjw5A8H4MnpG9DUxuSeZDGuXzvbC4KoC112FuGh\ny/tyXGO+/dCIBujSJz7+v4LRhLJ0NUcEYayoc8uG/IHvLBgou68mm6/vGeN4ER4jtOUyLycLlw/s\ngJW7jgEw7hFkZxHaqhYKMvPG85qgKAFAKAJTLurbLiGwliCepvkUXQ4QMA+A5rbcB+i7EbigSytv\nFJkat/MIFJx6tqU7YoygHhJr/bivOZ2mcGGfQsOAdXY/sWR3INK1x5LJGEaQsHAfTUxH9saznkbg\nC2v+dFFK7it6BPWQh6/oi1aN83DeGd74z9tB+d7+79zEsQe3akn0CARWaMuIMujLO0s3Bf4hcSQr\njpYWoQjqIW2bFuCxHySuIeyEINgxM62bLrCPUSmttWkaUhJy0yv85ndjHF+bKoRpSJA03LrjGtmB\nR5pEscxUzmjvLn5R2uGRacitZ9usu87xdEA/WYgeQYZx/yW9bUf99Ip2sndGQ5sLy1g1zl6/+azo\nIuUCYNodo/WXVq3HGDUSLMNQJ6SjXOesPAWhB+0EoQgyDLsx3r0s1g9e1hfDurbEsK6JS0VyyWIg\nTHYW+Rqp8czTWmDG2gMpW5bULn071I9JYl6gNBBs9wic9lrTUw8IRQBIq0HtPuJPXHE1I7q1wo7D\n+vHg6xt6H1KDvGxcNSQ4Acl4+dt1g7H9UAX3eruC5GM0jqR0FHlb6nbXAtfi54p/fiIUAYBehU10\nZ+i65bZQ97h0355kvDRfUNFby9X8fO8/hFQbfRrkZaNPhwyzuact+rGUuE1D0TECZ6UuiJGKeRCK\nwEd+Ny4YcURSgR+Vd5p+Y4IUokwo413LyG2PIAizz50gFIHAFLsF24/PQEzwCgZv3HwWlu88mmox\nbBE1DXGWzOhZjgeLHV2WcoQiEKQN6eqRUV8Y3bM1RvdMjceZU2IhJvjOd+s+arXuQVBJDzcIQcpw\nWveqG1Rnu6w8RH9AYIlBIWF2ZxYrpiGHtqE01QP+9QiIqADA1wDy5ft8wBh7kIi6AngHQCsAywH8\nmDFW7ZccgtQzdUIxKqpqXaeTpt+YIIkkhpiwN1jstkeQrmMEfvYIqgCMZYwNBDAIwDgiGg7gSQDP\nMMZ6ADgK4GYfZRCkCPV3l5+TjZaN8hynlaohgnT9qAUxfj++D8YPaM+1epsa5+6jzq5LNb4pAiZR\nLm/myn8MwFgAH8j7XwVwpV8yCFKHL5V3kj6yv147CBf1LUTPwsbJuaHANzo2b4DnbhzCvaSk0oAR\nM4s9hIiyIZl/egB4HsBWAMcYYxH5lD0AdBfoJaJJACYBQGFhIcLhsCMZysvLHV+bCoImb3Vt/AdR\nU1MTJ59W3j6NarAMQMnGlQjv9Kadsbdcigl88uRJ13nDm783dAYWfPO1q3t5QbLLw8A22WjbkNLu\neztWKZWR6upqW/fXyluypwoAsGXLVoRrd9mWY9HChWiab64MbuidhwMn63TlNJPdz7z1VREwxmoB\nDCKi5gA+BsDtWM8YexHAiwBQXFzMQqGQIxnC4TCcXpsKgiZvVaQWmDU9up2bmxsnn1becxnD72sZ\n8nK862xuOVgGzP8aDRs2dJ03QctfK5Itr9tbpSp/D5VXAeHZaNe8MUKhc7iv08o7v3wdsHM7unfv\njpCdZVynTwMAjB49ytIMGtLbKV9vlnd+5m1S3EcZY8eIaC6AEQCaE1GO3CvoBKAkGTIIkgMRIS/H\n2+5xIzlG+xntxOxegT6tG+fj8R/0x5jebVylE5tQ5sw05GUDKJn46TXUBkCNrAQaALgA0kDxXABX\nQ/IcmgDgU79kELhH+z2kwgbavlkDvDNpOAZ0EsHUBMbceFYX12m49RpqZDOyblDws0fQHsCr8jhB\nFoD3GGOfE9E6AO8Q0aMAVgB4yUcZBPWE4d3EmgOCJOCyRyAGizUwxlYBGKyzfxuAYX7dVyAQCJzi\nNAz1zLvOkcYp0hQRYkJgitLAaVqQgxOVETTgdMMTCNKRVvJAb9MG9kKO+xXBOFkIRSAwJT8nG89e\nPwjDurbER9+V4NL+7VMtkkDgGxNHFqFRfg6uLe6c1Pt+fNvIlK5VIhSBwJIrBklTPX45pkeKJREI\n/CUnOws3DHM/6GyXwV1aYHCXFkm/r0J6+joJBAKBwDOEIhAIBIIMRygCgUAgyHCEIhAIBIIMRygC\ngUAgyHCEIhAIBIIMRygCgUAgyHCEIhAIBIIMh5yuxJNMiKgUwE6Hl7cGcMhDcfxGyOsvQl5/EfL6\nhxNZT2OMWcbmTgtF4AYiWsYYK061HLwIef1FyOsvQl7/8FNWYRoSCASCDEcoAoFAIMhwMkERvJhq\nAWwi5PUXIa+/CHn9wzdZ6/0YgUAgEAjMyYQegUAgEAhMEIpAIBAIMpx6rQiIaBwRbSSiLUQ0OQDy\ndCaiuUS0jojWEtGd8v6WRDSLiDbL/7eQ9xMR/V2WfxURDUmR3NlEtIKIPpe3uxLRElmud4koT96f\nL29vkY8XpUDW5kT0ARFtIKL1RDQiyPlLRHfJZWENEb1NRAVByl8iepmIDhLRGtU+2/lJRBPk8zcT\n0YQky/tnuTysIqKPiai56th9srwbiegi1f6k1B168qqO3U1EjIhay9v+5S9jrF7+AcgGsBVANwB5\nAL4H0CfFMrUHMET+3QTAJgB9ADwFYLK8fzKAJ+XflwD4EgABGA5gSYrk/g2AtwB8Lm+/B+B6+fe/\nAPxC/n0bgH/Jv68H8G4KZH0VwC3y7zwAzYOavwA6AtgOoIEqXycGKX8BnANgCIA1qn228hNASwDb\n5P9byL9bJFHeCwHkyL+fVMnbR64X8gF0leuL7GTWHXryyvs7A5gBaSJta7/zN2mFPtl/AEYAmKHa\nvg/AfamWSyPjpwAuALARQHt5X3sAG+Xf/wZwg+r86HlJlLETgDkAxgL4XC6Eh1QfVjSf5YI7Qv6d\nI59HSZS1mVyxkmZ/IPMXkiLYLX/AOXL+XhS0/AVQpKlYbeUngBsA/Fu1P+48v+XVHPsBgDfl33F1\ngpK/ya479OQF8AGAgQB2IKYIfMvf+mwaUj4yhT3yvkAgd+sHA1gCoJAxtk8+tB9Aofw7CM/wNwC/\nA1Anb7cCcIwxFtGRKSqvfPy4fH6y6AqgFMArsilrKhE1QkDzlzFWAuAvAHYB2Acpv5YjuPmrYDc/\ng1COFX4GqVUNBFReIroCQAlj7HvNId/krc+KILAQUWMAHwL4NWPshPoYk1R6IHx6iWg8gIOMseWp\nloWTHEjd7BcYY4MBVEAyXUQJWP62AHAFJAXWAUAjAONSKpRNgpSfVhDRAwAiAN5MtSxGEFFDAPcD\n+GMy71ufFUEJJDubQid5X0oholxISuBNxthH8u4DRNRePt4ewEF5f6qfYRSAy4loB4B3IJmHngXQ\nnIhydGSKyisfbwbgcBLl3QNgD2Nsibz9ASTFENT8PR/AdsZYKWOsBsBHkPI8qPmrYDc/U53PIKKJ\nAMYDuElWXjCRK5XydofUMPhe/u46AfiOiNqZyOVa3vqsCL4F0FP2wMiDNLj2WSoFIiIC8BKA9Yyx\nv6oOfQZAGemfAGnsQNn/E9lbYDiA46ouue8wxu5jjHVijBVByr+vGGM3AZgL4GoDeZXnuFo+P2mt\nRcbYfgC7ieh0edd5ANYhoPkLySQ0nIgaymVDkTeQ+avCbn7OAHAhEbWQe0EXyvuSAhGNg2TevJwx\ndlJ16DMA18veWF0B9ASwFCmsOxhjqxljbRljRfJ3tweSg8l++Jm/fg2ABOEP0ij7JkgeAA8EQJ7R\nkLrRqwCslP8ugWTnnQNgM4DZAFrK5xOA52X5VwP/397dhVhVhWEc/z9Q1BR0kd4UGCEJglET2kgg\nZVJBEBVpBEXSBwRBGUQXkkFEBEIfF1EgBSHI4IUFg91Ekk2BNYjSaE5mDTlX3fRNoIU1bxfvmpnF\nccZzZvw6035+cGCfvdfea+3FzH73Wefsd7HiPLZ9NVO/GlpM/sOMAjuAi8r6i8v70bJ98XloZy+w\nr/TxAPkriq7tX+Al4FvgELCN/AVL1/QvsJ38/uIEeVF6fC79SY7Nj5bXo+e4vaPkGPrE/9yWqvym\n0t4jwJ3V+nNy7ZiuvS3bx5j6svis9a9TTJiZNdz/eWjIzMw64EBgZtZwDgRmZg3nQGBm1nAOBGZm\nDedAYPOKpLvbZYOUdKWk98vyI5LemmUdz3dQZqukde3KnS2SBiXNi0nXrfs5ENi8EhE7I2JzmzI/\nRsTpXKTbBoL5rHpq2QxwILAuIenqkjN+q6TvJPVLuk3SnpJjva+Um7zDL2XflPSFpB8m7tDLser8\n7ovKHfT3kl6s6hyQtF85H8ATZd1moEfSsKT+sm59yf9+QNK26rg3t9Y9zTkdlvRuqeNjST1l2+Qd\nvaSFJZ3AxPkNKPP8j0l6StKzJYnekKTLqyoeLu08VPXPpcoc93vLPvdUx90paTf5MJjZJAcC6ybX\nAK8DS8vrQfJp7OeY+S79ilLmLmCmTwp9wFrgOuD+akjlsYhYDqwANkhaEBEbgeMR0RsRD0laBrwA\nrImI64FnZln3EuDtiFgG/F7a0c61wH3AjcArwLHIJHpfAuurcpdERC85T8F7Zd0mMvVEH3Ar8Koy\nAytk3qV1EXFLB22wBnEgsG5yNDLXyjgwAnwS+ej712TO9ukMRMR4RHzDVDrkVrsi4peIOE4mdltV\n1m+QdAAYIpN2LZlm3zXAjoj4GSAifp1l3UcjYrgs7z/FedQ+jYg/I+InMtX0h2V9az9sL236HLhM\nOfPWHcBGScPAIJmW4qpSfldL+82ATNtr1i3+rpbHq/fjzPy3Wu+jGcq05lEJSavJ7J83RcQxSYPk\nRXM2Oqm7LvMv0FOW/2HqRqy13k774aTzKu1YGxFH6g2SVpJpuc1O4k8E1gS3K+fZ7QHuBfaQKZx/\nK0FgKTn134QTynThALvJ4aQFkPP1nqE2jQHLy/Jcv9h+AEDSKjIT5R9k1smnSzZTJN1wmu20BnAg\nsCbYS84BcRD4ICL2AR8BF0g6TI7vD1Xl3wEOSuqPiBFynP6zMoz0BmfGa8CTkr4CFs7xGH+V/beQ\nWTYBXgYuJNs/Ut6bnZKzj5qZNZw/EZiZNZwDgZlZwzkQmJk1nAOBmVnDORCYmTWcA4GZWcM5EJiZ\nNdx/Q40q42Dhmx8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13aa8c160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6940: with minibatch training loss = 0.769 and accuracy of 0.75\n",
      "Iteration 6941: with minibatch training loss = 0.929 and accuracy of 0.75\n",
      "Iteration 6942: with minibatch training loss = 0.687 and accuracy of 0.83\n",
      "Iteration 6943: with minibatch training loss = 0.675 and accuracy of 0.83\n",
      "Iteration 6944: with minibatch training loss = 0.978 and accuracy of 0.72\n",
      "Iteration 6945: with minibatch training loss = 0.796 and accuracy of 0.78\n",
      "Iteration 6946: with minibatch training loss = 0.703 and accuracy of 0.81\n",
      "Iteration 6947: with minibatch training loss = 0.806 and accuracy of 0.78\n",
      "Iteration 6948: with minibatch training loss = 0.78 and accuracy of 0.78\n",
      "Iteration 6949: with minibatch training loss = 0.815 and accuracy of 0.77\n",
      "Iteration 6950: with minibatch training loss = 0.867 and accuracy of 0.77\n",
      "Iteration 6951: with minibatch training loss = 0.618 and accuracy of 0.83\n",
      "Iteration 6952: with minibatch training loss = 0.662 and accuracy of 0.83\n",
      "Iteration 6953: with minibatch training loss = 0.843 and accuracy of 0.77\n",
      "Iteration 6954: with minibatch training loss = 1.15 and accuracy of 0.66\n",
      "Iteration 6955: with minibatch training loss = 0.975 and accuracy of 0.72\n",
      "Iteration 6956: with minibatch training loss = 1.14 and accuracy of 0.66\n",
      "Iteration 6957: with minibatch training loss = 0.601 and accuracy of 0.84\n",
      "Iteration 6958: with minibatch training loss = 0.767 and accuracy of 0.78\n",
      "Iteration 6959: with minibatch training loss = 0.881 and accuracy of 0.75\n",
      "Iteration 6960: with minibatch training loss = 0.481 and accuracy of 0.88\n",
      "Iteration 6961: with minibatch training loss = 1.08 and accuracy of 0.67\n",
      "Iteration 6962: with minibatch training loss = 0.957 and accuracy of 0.72\n",
      "Iteration 6963: with minibatch training loss = 0.804 and accuracy of 0.78\n",
      "Iteration 6964: with minibatch training loss = 0.942 and accuracy of 0.73\n",
      "Iteration 6965: with minibatch training loss = 0.583 and accuracy of 0.84\n",
      "Iteration 6966: with minibatch training loss = 1.1 and accuracy of 0.67\n",
      "Iteration 6967: with minibatch training loss = 0.995 and accuracy of 0.7\n",
      "Iteration 6968: with minibatch training loss = 0.784 and accuracy of 0.77\n",
      "Iteration 6969: with minibatch training loss = 0.75 and accuracy of 0.81\n",
      "Iteration 6970: with minibatch training loss = 1.17 and accuracy of 0.72\n",
      "Iteration 6971: with minibatch training loss = 0.908 and accuracy of 0.77\n",
      "Iteration 6972: with minibatch training loss = 0.717 and accuracy of 0.8\n",
      "Iteration 6973: with minibatch training loss = 0.861 and accuracy of 0.77\n",
      "Iteration 6974: with minibatch training loss = 0.934 and accuracy of 0.75\n",
      "Iteration 6975: with minibatch training loss = 0.66 and accuracy of 0.83\n",
      "Iteration 6976: with minibatch training loss = 0.926 and accuracy of 0.73\n",
      "Iteration 6977: with minibatch training loss = 0.906 and accuracy of 0.75\n",
      "Iteration 6978: with minibatch training loss = 0.733 and accuracy of 0.78\n",
      "Iteration 6979: with minibatch training loss = 0.994 and accuracy of 0.75\n",
      "Iteration 6980: with minibatch training loss = 1.12 and accuracy of 0.72\n",
      "Iteration 6981: with minibatch training loss = 0.778 and accuracy of 0.8\n",
      "Iteration 6982: with minibatch training loss = 0.692 and accuracy of 0.8\n",
      "Iteration 6983: with minibatch training loss = 0.714 and accuracy of 0.86\n",
      "Iteration 6984: with minibatch training loss = 0.81 and accuracy of 0.77\n",
      "Iteration 6985: with minibatch training loss = 0.707 and accuracy of 0.81\n",
      "Iteration 6986: with minibatch training loss = 0.783 and accuracy of 0.75\n",
      "Iteration 6987: with minibatch training loss = 0.828 and accuracy of 0.72\n",
      "Iteration 6988: with minibatch training loss = 0.726 and accuracy of 0.78\n",
      "Iteration 6989: with minibatch training loss = 0.72 and accuracy of 0.78\n",
      "Iteration 6990: with minibatch training loss = 0.998 and accuracy of 0.72\n",
      "Iteration 6991: with minibatch training loss = 1.14 and accuracy of 0.64\n",
      "Iteration 6992: with minibatch training loss = 0.684 and accuracy of 0.83\n",
      "Iteration 6993: with minibatch training loss = 0.881 and accuracy of 0.73\n",
      "Iteration 6994: with minibatch training loss = 0.818 and accuracy of 0.77\n",
      "Iteration 6995: with minibatch training loss = 0.752 and accuracy of 0.8\n",
      "Iteration 6996: with minibatch training loss = 0.884 and accuracy of 0.75\n",
      "Iteration 6997: with minibatch training loss = 0.811 and accuracy of 0.77\n",
      "Iteration 6998: with minibatch training loss = 0.951 and accuracy of 0.72\n",
      "Iteration 6999: with minibatch training loss = 0.966 and accuracy of 0.72\n",
      "Iteration 7000: with minibatch training loss = 1.12 and accuracy of 0.67\n",
      "Iteration 7001: with minibatch training loss = 0.918 and accuracy of 0.73\n",
      "Iteration 7002: with minibatch training loss = 0.9 and accuracy of 0.77\n",
      "Iteration 7003: with minibatch training loss = 0.807 and accuracy of 0.8\n",
      "Iteration 7004: with minibatch training loss = 0.976 and accuracy of 0.7\n",
      "Iteration 7005: with minibatch training loss = 0.785 and accuracy of 0.78\n",
      "Iteration 7006: with minibatch training loss = 0.839 and accuracy of 0.78\n",
      "Iteration 7007: with minibatch training loss = 1 and accuracy of 0.69\n",
      "Iteration 7008: with minibatch training loss = 0.711 and accuracy of 0.83\n",
      "Iteration 7009: with minibatch training loss = 0.844 and accuracy of 0.75\n",
      "Iteration 7010: with minibatch training loss = 0.74 and accuracy of 0.81\n",
      "Iteration 7011: with minibatch training loss = 0.678 and accuracy of 0.81\n",
      "Iteration 7012: with minibatch training loss = 0.895 and accuracy of 0.78\n",
      "Iteration 7013: with minibatch training loss = 1.13 and accuracy of 0.67\n",
      "Iteration 7014: with minibatch training loss = 0.666 and accuracy of 0.81\n",
      "Iteration 7015: with minibatch training loss = 1.02 and accuracy of 0.72\n",
      "Iteration 7016: with minibatch training loss = 0.866 and accuracy of 0.78\n",
      "Iteration 7017: with minibatch training loss = 0.964 and accuracy of 0.72\n",
      "Iteration 7018: with minibatch training loss = 0.653 and accuracy of 0.84\n",
      "Iteration 7019: with minibatch training loss = 0.824 and accuracy of 0.77\n",
      "Iteration 7020: with minibatch training loss = 0.803 and accuracy of 0.78\n",
      "Iteration 7021: with minibatch training loss = 0.805 and accuracy of 0.78\n",
      "Iteration 7022: with minibatch training loss = 0.811 and accuracy of 0.73\n",
      "Iteration 7023: with minibatch training loss = 0.655 and accuracy of 0.81\n",
      "Iteration 7024: with minibatch training loss = 0.671 and accuracy of 0.8\n",
      "Iteration 7025: with minibatch training loss = 0.849 and accuracy of 0.75\n",
      "Iteration 7026: with minibatch training loss = 0.849 and accuracy of 0.73\n",
      "Iteration 7027: with minibatch training loss = 1.12 and accuracy of 0.67\n",
      "Iteration 7028: with minibatch training loss = 0.915 and accuracy of 0.73\n",
      "Iteration 7029: with minibatch training loss = 0.885 and accuracy of 0.77\n",
      "Iteration 7030: with minibatch training loss = 1.08 and accuracy of 0.69\n",
      "Iteration 7031: with minibatch training loss = 1.03 and accuracy of 0.69\n",
      "Iteration 7032: with minibatch training loss = 0.67 and accuracy of 0.81\n",
      "Iteration 7033: with minibatch training loss = 0.667 and accuracy of 0.83\n",
      "Iteration 7034: with minibatch training loss = 0.537 and accuracy of 0.81\n",
      "Iteration 7035: with minibatch training loss = 0.667 and accuracy of 0.83\n",
      "Iteration 7036: with minibatch training loss = 0.591 and accuracy of 0.84\n",
      "Iteration 7037: with minibatch training loss = 0.999 and accuracy of 0.67\n",
      "Iteration 7038: with minibatch training loss = 1.33 and accuracy of 0.59\n",
      "Iteration 7039: with minibatch training loss = 1.14 and accuracy of 0.67\n",
      "Iteration 7040: with minibatch training loss = 0.787 and accuracy of 0.77\n",
      "Iteration 7041: with minibatch training loss = 1.01 and accuracy of 0.7\n",
      "Iteration 7042: with minibatch training loss = 0.728 and accuracy of 0.8\n",
      "Iteration 7043: with minibatch training loss = 1 and accuracy of 0.72\n",
      "Iteration 7044: with minibatch training loss = 0.851 and accuracy of 0.78\n",
      "Iteration 7045: with minibatch training loss = 0.949 and accuracy of 0.72\n",
      "Iteration 7046: with minibatch training loss = 0.682 and accuracy of 0.81\n",
      "Iteration 7047: with minibatch training loss = 0.748 and accuracy of 0.8\n",
      "Iteration 7048: with minibatch training loss = 0.859 and accuracy of 0.72\n",
      "Iteration 7049: with minibatch training loss = 0.86 and accuracy of 0.75\n",
      "Iteration 7050: with minibatch training loss = 0.675 and accuracy of 0.8\n",
      "Iteration 7051: with minibatch training loss = 0.889 and accuracy of 0.77\n",
      "Iteration 7052: with minibatch training loss = 0.833 and accuracy of 0.77\n",
      "Iteration 7053: with minibatch training loss = 0.878 and accuracy of 0.75\n",
      "Iteration 7054: with minibatch training loss = 0.777 and accuracy of 0.81\n",
      "Iteration 7055: with minibatch training loss = 0.802 and accuracy of 0.75\n",
      "Iteration 7056: with minibatch training loss = 1.03 and accuracy of 0.7\n",
      "Iteration 7057: with minibatch training loss = 0.685 and accuracy of 0.81\n",
      "Iteration 7058: with minibatch training loss = 0.852 and accuracy of 0.75\n",
      "Iteration 7059: with minibatch training loss = 0.737 and accuracy of 0.8\n",
      "Iteration 7060: with minibatch training loss = 1.05 and accuracy of 0.7\n",
      "Iteration 7061: with minibatch training loss = 0.918 and accuracy of 0.73\n",
      "Iteration 7062: with minibatch training loss = 0.883 and accuracy of 0.73\n",
      "Iteration 7063: with minibatch training loss = 0.895 and accuracy of 0.73\n",
      "Iteration 7064: with minibatch training loss = 0.716 and accuracy of 0.81\n",
      "Iteration 7065: with minibatch training loss = 0.865 and accuracy of 0.77\n",
      "Iteration 7066: with minibatch training loss = 0.904 and accuracy of 0.75\n",
      "Iteration 7067: with minibatch training loss = 0.815 and accuracy of 0.75\n",
      "Iteration 7068: with minibatch training loss = 0.753 and accuracy of 0.77\n",
      "Iteration 7069: with minibatch training loss = 0.792 and accuracy of 0.8\n",
      "Iteration 7070: with minibatch training loss = 1.25 and accuracy of 0.66\n",
      "Iteration 7071: with minibatch training loss = 0.994 and accuracy of 0.7\n",
      "Iteration 7072: with minibatch training loss = 0.795 and accuracy of 0.75\n",
      "Iteration 7073: with minibatch training loss = 1.01 and accuracy of 0.72\n",
      "Iteration 7074: with minibatch training loss = 0.862 and accuracy of 0.73\n",
      "Iteration 7075: with minibatch training loss = 0.73 and accuracy of 0.78\n",
      "Iteration 7076: with minibatch training loss = 0.846 and accuracy of 0.75\n",
      "Iteration 7077: with minibatch training loss = 1.18 and accuracy of 0.64\n",
      "Iteration 7078: with minibatch training loss = 0.991 and accuracy of 0.72\n",
      "Iteration 7079: with minibatch training loss = 0.58 and accuracy of 0.83\n",
      "Iteration 7080: with minibatch training loss = 0.727 and accuracy of 0.78\n",
      "Iteration 7081: with minibatch training loss = 0.575 and accuracy of 0.86\n",
      "Iteration 7082: with minibatch training loss = 0.868 and accuracy of 0.75\n",
      "Iteration 7083: with minibatch training loss = 0.894 and accuracy of 0.8\n",
      "Iteration 7084: with minibatch training loss = 1 and accuracy of 0.7\n",
      "Iteration 7085: with minibatch training loss = 0.58 and accuracy of 0.83\n",
      "Iteration 7086: with minibatch training loss = 0.595 and accuracy of 0.86\n",
      "Iteration 7087: with minibatch training loss = 0.726 and accuracy of 0.83\n",
      "Iteration 7088: with minibatch training loss = 1.13 and accuracy of 0.66\n",
      "Iteration 7089: with minibatch training loss = 0.674 and accuracy of 0.81\n",
      "Iteration 7090: with minibatch training loss = 0.972 and accuracy of 0.7\n",
      "Iteration 7091: with minibatch training loss = 0.837 and accuracy of 0.77\n",
      "Iteration 7092: with minibatch training loss = 0.82 and accuracy of 0.77\n",
      "Iteration 7093: with minibatch training loss = 0.622 and accuracy of 0.81\n",
      "Iteration 7094: with minibatch training loss = 0.798 and accuracy of 0.77\n",
      "Iteration 7095: with minibatch training loss = 1.04 and accuracy of 0.69\n",
      "Iteration 7096: with minibatch training loss = 1.06 and accuracy of 0.7\n",
      "Iteration 7097: with minibatch training loss = 0.661 and accuracy of 0.81\n",
      "Iteration 7098: with minibatch training loss = 0.725 and accuracy of 0.78\n",
      "Iteration 7099: with minibatch training loss = 0.982 and accuracy of 0.7\n",
      "Iteration 7100: with minibatch training loss = 0.798 and accuracy of 0.78\n",
      "Iteration 7101: with minibatch training loss = 0.832 and accuracy of 0.77\n",
      "Iteration 7102: with minibatch training loss = 1.02 and accuracy of 0.72\n",
      "Iteration 7103: with minibatch training loss = 0.893 and accuracy of 0.73\n",
      "Iteration 7104: with minibatch training loss = 0.826 and accuracy of 0.77\n",
      "Iteration 7105: with minibatch training loss = 0.871 and accuracy of 0.75\n",
      "Iteration 7106: with minibatch training loss = 0.782 and accuracy of 0.78\n",
      "Iteration 7107: with minibatch training loss = 0.804 and accuracy of 0.77\n",
      "Iteration 7108: with minibatch training loss = 1.15 and accuracy of 0.69\n",
      "Iteration 7109: with minibatch training loss = 0.905 and accuracy of 0.72\n",
      "Iteration 7110: with minibatch training loss = 0.524 and accuracy of 0.88\n",
      "Iteration 7111: with minibatch training loss = 0.821 and accuracy of 0.77\n",
      "Iteration 7112: with minibatch training loss = 1.14 and accuracy of 0.69\n",
      "Iteration 7113: with minibatch training loss = 0.8 and accuracy of 0.77\n",
      "Iteration 7114: with minibatch training loss = 1.04 and accuracy of 0.72\n",
      "Iteration 7115: with minibatch training loss = 0.817 and accuracy of 0.78\n",
      "Iteration 7116: with minibatch training loss = 0.903 and accuracy of 0.72\n",
      "Iteration 7117: with minibatch training loss = 0.607 and accuracy of 0.83\n",
      "Iteration 7118: with minibatch training loss = 1.04 and accuracy of 0.7\n",
      "Iteration 7119: with minibatch training loss = 1.1 and accuracy of 0.69\n",
      "Iteration 7120: with minibatch training loss = 1.07 and accuracy of 0.72\n",
      "Iteration 7121: with minibatch training loss = 0.955 and accuracy of 0.72\n",
      "Iteration 7122: with minibatch training loss = 0.579 and accuracy of 0.83\n",
      "Iteration 7123: with minibatch training loss = 0.753 and accuracy of 0.78\n",
      "Iteration 7124: with minibatch training loss = 0.696 and accuracy of 0.8\n",
      "Iteration 7125: with minibatch training loss = 0.951 and accuracy of 0.73\n",
      "Iteration 7126: with minibatch training loss = 1.01 and accuracy of 0.69\n",
      "Iteration 7127: with minibatch training loss = 0.608 and accuracy of 0.81\n",
      "Iteration 7128: with minibatch training loss = 1.04 and accuracy of 0.72\n",
      "Iteration 7129: with minibatch training loss = 0.825 and accuracy of 0.78\n",
      "Iteration 7130: with minibatch training loss = 0.75 and accuracy of 0.78\n",
      "Iteration 7131: with minibatch training loss = 0.745 and accuracy of 0.8\n",
      "Iteration 7132: with minibatch training loss = 0.803 and accuracy of 0.77\n",
      "Iteration 7133: with minibatch training loss = 0.698 and accuracy of 0.83\n",
      "Iteration 7134: with minibatch training loss = 0.6 and accuracy of 0.84\n",
      "Iteration 7135: with minibatch training loss = 0.824 and accuracy of 0.77\n",
      "Iteration 7136: with minibatch training loss = 1.1 and accuracy of 0.69\n",
      "Iteration 7137: with minibatch training loss = 0.694 and accuracy of 0.8\n",
      "Iteration 7138: with minibatch training loss = 0.933 and accuracy of 0.75\n",
      "Iteration 7139: with minibatch training loss = 0.722 and accuracy of 0.81\n",
      "Iteration 7140: with minibatch training loss = 0.985 and accuracy of 0.73\n",
      "Iteration 7141: with minibatch training loss = 0.645 and accuracy of 0.81\n",
      "Iteration 7142: with minibatch training loss = 1.02 and accuracy of 0.75\n",
      "Iteration 7143: with minibatch training loss = 1.12 and accuracy of 0.66\n",
      "Iteration 7144: with minibatch training loss = 0.5 and accuracy of 0.88\n",
      "Iteration 7145: with minibatch training loss = 0.659 and accuracy of 0.81\n",
      "Iteration 7146: with minibatch training loss = 1.01 and accuracy of 0.73\n",
      "Iteration 7147: with minibatch training loss = 0.729 and accuracy of 0.8\n",
      "Iteration 7148: with minibatch training loss = 1.02 and accuracy of 0.72\n",
      "Iteration 7149: with minibatch training loss = 0.568 and accuracy of 0.84\n",
      "Iteration 7150: with minibatch training loss = 0.711 and accuracy of 0.81\n",
      "Iteration 7151: with minibatch training loss = 0.852 and accuracy of 0.73\n",
      "Iteration 7152: with minibatch training loss = 1.13 and accuracy of 0.66\n",
      "Iteration 7153: with minibatch training loss = 0.826 and accuracy of 0.73\n",
      "Iteration 7154: with minibatch training loss = 0.594 and accuracy of 0.83\n",
      "Iteration 7155: with minibatch training loss = 0.437 and accuracy of 0.91\n",
      "Iteration 7156: with minibatch training loss = 1 and accuracy of 0.72\n",
      "Iteration 7157: with minibatch training loss = 0.841 and accuracy of 0.75\n",
      "Iteration 7158: with minibatch training loss = 0.899 and accuracy of 0.75\n",
      "Iteration 7159: with minibatch training loss = 0.786 and accuracy of 0.8\n",
      "Iteration 7160: with minibatch training loss = 1 and accuracy of 0.75\n",
      "Iteration 7161: with minibatch training loss = 0.826 and accuracy of 0.77\n",
      "Iteration 7162: with minibatch training loss = 0.92 and accuracy of 0.73\n",
      "Iteration 7163: with minibatch training loss = 1.04 and accuracy of 0.72\n",
      "Iteration 7164: with minibatch training loss = 0.758 and accuracy of 0.8\n",
      "Iteration 7165: with minibatch training loss = 0.693 and accuracy of 0.78\n",
      "Iteration 7166: with minibatch training loss = 0.644 and accuracy of 0.83\n",
      "Iteration 7167: with minibatch training loss = 1.29 and accuracy of 0.62\n",
      "Iteration 7168: with minibatch training loss = 0.613 and accuracy of 0.83\n",
      "Iteration 7169: with minibatch training loss = 0.965 and accuracy of 0.72\n",
      "Iteration 7170: with minibatch training loss = 0.744 and accuracy of 0.81\n",
      "Iteration 7171: with minibatch training loss = 0.832 and accuracy of 0.77\n",
      "Iteration 7172: with minibatch training loss = 0.951 and accuracy of 0.7\n",
      "Iteration 7173: with minibatch training loss = 0.535 and accuracy of 0.86\n",
      "Iteration 7174: with minibatch training loss = 0.971 and accuracy of 0.72\n",
      "Iteration 7175: with minibatch training loss = 0.955 and accuracy of 0.73\n",
      "Iteration 7176: with minibatch training loss = 0.675 and accuracy of 0.81\n",
      "Iteration 7177: with minibatch training loss = 0.788 and accuracy of 0.78\n",
      "Iteration 7178: with minibatch training loss = 0.767 and accuracy of 0.75\n",
      "Iteration 7179: with minibatch training loss = 0.739 and accuracy of 0.8\n",
      "Iteration 7180: with minibatch training loss = 0.621 and accuracy of 0.81\n",
      "Iteration 7181: with minibatch training loss = 0.895 and accuracy of 0.75\n",
      "Iteration 7182: with minibatch training loss = 1.18 and accuracy of 0.64\n",
      "Iteration 7183: with minibatch training loss = 1 and accuracy of 0.72\n",
      "Iteration 7184: with minibatch training loss = 0.923 and accuracy of 0.72\n",
      "Iteration 7185: with minibatch training loss = 0.623 and accuracy of 0.81\n",
      "Iteration 7186: with minibatch training loss = 0.815 and accuracy of 0.8\n",
      "Iteration 7187: with minibatch training loss = 0.478 and accuracy of 0.89\n",
      "Iteration 7188: with minibatch training loss = 0.956 and accuracy of 0.72\n",
      "Iteration 7189: with minibatch training loss = 0.836 and accuracy of 0.78\n",
      "Iteration 7190: with minibatch training loss = 0.69 and accuracy of 0.81\n",
      "Iteration 7191: with minibatch training loss = 0.816 and accuracy of 0.8\n",
      "Iteration 7192: with minibatch training loss = 0.659 and accuracy of 0.81\n",
      "Iteration 7193: with minibatch training loss = 0.634 and accuracy of 0.83\n",
      "Iteration 7194: with minibatch training loss = 0.873 and accuracy of 0.77\n",
      "Iteration 7195: with minibatch training loss = 1.07 and accuracy of 0.69\n",
      "Iteration 7196: with minibatch training loss = 0.888 and accuracy of 0.75\n",
      "Iteration 7197: with minibatch training loss = 0.976 and accuracy of 0.69\n",
      "Iteration 7198: with minibatch training loss = 0.73 and accuracy of 0.8\n",
      "Iteration 7199: with minibatch training loss = 0.805 and accuracy of 0.77\n",
      "Iteration 7200: with minibatch training loss = 0.805 and accuracy of 0.78\n",
      "Iteration 7201: with minibatch training loss = 0.91 and accuracy of 0.73\n",
      "Iteration 7202: with minibatch training loss = 0.893 and accuracy of 0.7\n",
      "Iteration 7203: with minibatch training loss = 0.552 and accuracy of 0.88\n",
      "Iteration 7204: with minibatch training loss = 0.898 and accuracy of 0.77\n",
      "Iteration 7205: with minibatch training loss = 0.866 and accuracy of 0.77\n",
      "Iteration 7206: with minibatch training loss = 1.01 and accuracy of 0.7\n",
      "Iteration 7207: with minibatch training loss = 0.984 and accuracy of 0.72\n",
      "Iteration 7208: with minibatch training loss = 0.835 and accuracy of 0.75\n",
      "Iteration 7209: with minibatch training loss = 0.822 and accuracy of 0.78\n",
      "Iteration 7210: with minibatch training loss = 0.763 and accuracy of 0.81\n",
      "Iteration 7211: with minibatch training loss = 0.755 and accuracy of 0.78\n",
      "Iteration 7212: with minibatch training loss = 0.997 and accuracy of 0.69\n",
      "Iteration 7213: with minibatch training loss = 0.788 and accuracy of 0.78\n",
      "Iteration 7214: with minibatch training loss = 0.654 and accuracy of 0.81\n",
      "Iteration 7215: with minibatch training loss = 0.786 and accuracy of 0.78\n",
      "Iteration 7216: with minibatch training loss = 0.778 and accuracy of 0.78\n",
      "Iteration 7217: with minibatch training loss = 0.597 and accuracy of 0.84\n",
      "Iteration 7218: with minibatch training loss = 0.817 and accuracy of 0.77\n",
      "Iteration 7219: with minibatch training loss = 0.667 and accuracy of 0.8\n",
      "Iteration 7220: with minibatch training loss = 1.11 and accuracy of 0.69\n",
      "Iteration 7221: with minibatch training loss = 0.807 and accuracy of 0.78\n",
      "Iteration 7222: with minibatch training loss = 0.769 and accuracy of 0.77\n",
      "Iteration 7223: with minibatch training loss = 0.991 and accuracy of 0.72\n",
      "Iteration 7224: with minibatch training loss = 0.533 and accuracy of 0.84\n",
      "Iteration 7225: with minibatch training loss = 0.92 and accuracy of 0.75\n",
      "Iteration 7226: with minibatch training loss = 1.03 and accuracy of 0.7\n",
      "Iteration 7227: with minibatch training loss = 0.56 and accuracy of 0.84\n",
      "Iteration 7228: with minibatch training loss = 0.778 and accuracy of 0.78\n",
      "Iteration 7229: with minibatch training loss = 0.919 and accuracy of 0.73\n",
      "Iteration 7230: with minibatch training loss = 0.696 and accuracy of 0.83\n",
      "Iteration 7231: with minibatch training loss = 0.747 and accuracy of 0.77\n",
      "Iteration 7232: with minibatch training loss = 0.603 and accuracy of 0.83\n",
      "Iteration 7233: with minibatch training loss = 0.963 and accuracy of 0.72\n",
      "Iteration 7234: with minibatch training loss = 0.665 and accuracy of 0.86\n",
      "Iteration 7235: with minibatch training loss = 1.06 and accuracy of 0.72\n",
      "Iteration 7236: with minibatch training loss = 0.833 and accuracy of 0.78\n",
      "Iteration 7237: with minibatch training loss = 0.705 and accuracy of 0.8\n",
      "Iteration 7238: with minibatch training loss = 0.838 and accuracy of 0.75\n",
      "Iteration 7239: with minibatch training loss = 0.845 and accuracy of 0.78\n",
      "Iteration 7240: with minibatch training loss = 0.87 and accuracy of 0.75\n",
      "Iteration 7241: with minibatch training loss = 0.709 and accuracy of 0.8\n",
      "Iteration 7242: with minibatch training loss = 1.07 and accuracy of 0.67\n",
      "Iteration 7243: with minibatch training loss = 0.891 and accuracy of 0.75\n",
      "Iteration 7244: with minibatch training loss = 0.82 and accuracy of 0.81\n",
      "Iteration 7245: with minibatch training loss = 0.69 and accuracy of 0.78\n",
      "Iteration 7246: with minibatch training loss = 0.832 and accuracy of 0.77\n",
      "Iteration 7247: with minibatch training loss = 1.11 and accuracy of 0.66\n",
      "Iteration 7248: with minibatch training loss = 1.01 and accuracy of 0.72\n",
      "Iteration 7249: with minibatch training loss = 1.08 and accuracy of 0.66\n",
      "Iteration 7250: with minibatch training loss = 0.742 and accuracy of 0.78\n",
      "Iteration 7251: with minibatch training loss = 1.03 and accuracy of 0.69\n",
      "Iteration 7252: with minibatch training loss = 0.786 and accuracy of 0.78\n",
      "Iteration 7253: with minibatch training loss = 0.879 and accuracy of 0.73\n",
      "Iteration 7254: with minibatch training loss = 1.23 and accuracy of 0.64\n",
      "Iteration 7255: with minibatch training loss = 1.09 and accuracy of 0.7\n",
      "Iteration 7256: with minibatch training loss = 0.916 and accuracy of 0.7\n",
      "Iteration 7257: with minibatch training loss = 0.853 and accuracy of 0.78\n",
      "Iteration 7258: with minibatch training loss = 0.746 and accuracy of 0.78\n",
      "Iteration 7259: with minibatch training loss = 0.679 and accuracy of 0.8\n",
      "Iteration 7260: with minibatch training loss = 1.06 and accuracy of 0.7\n",
      "Iteration 7261: with minibatch training loss = 0.83 and accuracy of 0.75\n",
      "Iteration 7262: with minibatch training loss = 0.716 and accuracy of 0.78\n",
      "Iteration 7263: with minibatch training loss = 0.988 and accuracy of 0.75\n",
      "Iteration 7264: with minibatch training loss = 0.785 and accuracy of 0.73\n",
      "Iteration 7265: with minibatch training loss = 0.495 and accuracy of 0.88\n",
      "Iteration 7266: with minibatch training loss = 0.573 and accuracy of 0.86\n",
      "Iteration 7267: with minibatch training loss = 0.734 and accuracy of 0.78\n",
      "Iteration 7268: with minibatch training loss = 1.04 and accuracy of 0.69\n",
      "Iteration 7269: with minibatch training loss = 0.905 and accuracy of 0.73\n",
      "Iteration 7270: with minibatch training loss = 0.775 and accuracy of 0.78\n",
      "Iteration 7271: with minibatch training loss = 0.968 and accuracy of 0.73\n",
      "Iteration 7272: with minibatch training loss = 0.635 and accuracy of 0.8\n",
      "Iteration 7273: with minibatch training loss = 0.901 and accuracy of 0.73\n",
      "Iteration 7274: with minibatch training loss = 1.12 and accuracy of 0.69\n",
      "Iteration 7275: with minibatch training loss = 1.14 and accuracy of 0.69\n",
      "Iteration 7276: with minibatch training loss = 0.775 and accuracy of 0.8\n",
      "Iteration 7277: with minibatch training loss = 0.941 and accuracy of 0.77\n",
      "Iteration 7278: with minibatch training loss = 0.87 and accuracy of 0.77\n",
      "Iteration 7279: with minibatch training loss = 0.606 and accuracy of 0.84\n",
      "Iteration 7280: with minibatch training loss = 0.662 and accuracy of 0.84\n",
      "Iteration 7281: with minibatch training loss = 0.47 and accuracy of 0.88\n",
      "Iteration 7282: with minibatch training loss = 0.787 and accuracy of 0.78\n",
      "Iteration 7283: with minibatch training loss = 0.715 and accuracy of 0.81\n",
      "Iteration 7284: with minibatch training loss = 1.14 and accuracy of 0.66\n",
      "Iteration 7285: with minibatch training loss = 0.703 and accuracy of 0.81\n",
      "Iteration 7286: with minibatch training loss = 0.791 and accuracy of 0.78\n",
      "Iteration 7287: with minibatch training loss = 0.858 and accuracy of 0.75\n",
      "Iteration 7288: with minibatch training loss = 0.949 and accuracy of 0.72\n",
      "Iteration 7289: with minibatch training loss = 0.71 and accuracy of 0.81\n",
      "Iteration 7290: with minibatch training loss = 0.548 and accuracy of 0.84\n",
      "Iteration 7291: with minibatch training loss = 0.818 and accuracy of 0.78\n",
      "Iteration 7292: with minibatch training loss = 0.779 and accuracy of 0.81\n",
      "Iteration 7293: with minibatch training loss = 0.81 and accuracy of 0.78\n",
      "Iteration 7294: with minibatch training loss = 0.847 and accuracy of 0.77\n",
      "Iteration 7295: with minibatch training loss = 0.737 and accuracy of 0.78\n",
      "Iteration 7296: with minibatch training loss = 0.967 and accuracy of 0.7\n",
      "Iteration 7297: with minibatch training loss = 0.929 and accuracy of 0.73\n",
      "Iteration 7298: with minibatch training loss = 0.566 and accuracy of 0.86\n",
      "Iteration 7299: with minibatch training loss = 0.925 and accuracy of 0.73\n",
      "Iteration 7300: with minibatch training loss = 0.924 and accuracy of 0.73\n",
      "Iteration 7301: with minibatch training loss = 0.71 and accuracy of 0.77\n",
      "Iteration 7302: with minibatch training loss = 0.696 and accuracy of 0.81\n",
      "Iteration 7303: with minibatch training loss = 0.644 and accuracy of 0.83\n",
      "Iteration 7304: with minibatch training loss = 0.788 and accuracy of 0.8\n",
      "Iteration 7305: with minibatch training loss = 0.819 and accuracy of 0.75\n",
      "Iteration 7306: with minibatch training loss = 0.73 and accuracy of 0.8\n",
      "Iteration 7307: with minibatch training loss = 0.833 and accuracy of 0.75\n",
      "Iteration 7308: with minibatch training loss = 0.987 and accuracy of 0.72\n",
      "Iteration 7309: with minibatch training loss = 0.967 and accuracy of 0.7\n",
      "Iteration 7310: with minibatch training loss = 0.639 and accuracy of 0.83\n",
      "Iteration 7311: with minibatch training loss = 0.84 and accuracy of 0.75\n",
      "Iteration 7312: with minibatch training loss = 0.989 and accuracy of 0.75\n",
      "Iteration 7313: with minibatch training loss = 0.928 and accuracy of 0.73\n",
      "Iteration 7314: with minibatch training loss = 0.806 and accuracy of 0.78\n",
      "Iteration 7315: with minibatch training loss = 0.836 and accuracy of 0.77\n",
      "Iteration 7316: with minibatch training loss = 0.905 and accuracy of 0.73\n",
      "Iteration 7317: with minibatch training loss = 0.828 and accuracy of 0.77\n",
      "Iteration 7318: with minibatch training loss = 0.728 and accuracy of 0.8\n",
      "Iteration 7319: with minibatch training loss = 0.578 and accuracy of 0.81\n",
      "Iteration 7320: with minibatch training loss = 0.716 and accuracy of 0.8\n",
      "Iteration 7321: with minibatch training loss = 0.614 and accuracy of 0.83\n",
      "Iteration 7322: with minibatch training loss = 1.05 and accuracy of 0.73\n",
      "Iteration 7323: with minibatch training loss = 1.02 and accuracy of 0.72\n",
      "Iteration 7324: with minibatch training loss = 0.802 and accuracy of 0.77\n",
      "Iteration 7325: with minibatch training loss = 1.14 and accuracy of 0.72\n",
      "Iteration 7326: with minibatch training loss = 1.14 and accuracy of 0.66\n",
      "Iteration 7327: with minibatch training loss = 0.929 and accuracy of 0.75\n",
      "Iteration 7328: with minibatch training loss = 0.965 and accuracy of 0.7\n",
      "Iteration 7329: with minibatch training loss = 0.733 and accuracy of 0.81\n",
      "Iteration 7330: with minibatch training loss = 0.957 and accuracy of 0.77\n",
      "Iteration 7331: with minibatch training loss = 0.617 and accuracy of 0.81\n",
      "Iteration 7332: with minibatch training loss = 0.49 and accuracy of 0.88\n",
      "Iteration 7333: with minibatch training loss = 0.863 and accuracy of 0.77\n",
      "Iteration 7334: with minibatch training loss = 0.921 and accuracy of 0.72\n",
      "Iteration 7335: with minibatch training loss = 0.841 and accuracy of 0.8\n",
      "Iteration 7336: with minibatch training loss = 0.749 and accuracy of 0.75\n",
      "Iteration 7337: with minibatch training loss = 0.94 and accuracy of 0.78\n",
      "Iteration 7338: with minibatch training loss = 0.782 and accuracy of 0.8\n",
      "Iteration 7339: with minibatch training loss = 1.01 and accuracy of 0.7\n",
      "Iteration 7340: with minibatch training loss = 0.773 and accuracy of 0.8\n",
      "Iteration 7341: with minibatch training loss = 0.899 and accuracy of 0.78\n",
      "Iteration 7342: with minibatch training loss = 0.796 and accuracy of 0.77\n",
      "Iteration 7343: with minibatch training loss = 0.703 and accuracy of 0.8\n",
      "Iteration 7344: with minibatch training loss = 0.891 and accuracy of 0.75\n",
      "Iteration 7345: with minibatch training loss = 0.627 and accuracy of 0.86\n",
      "Iteration 7346: with minibatch training loss = 0.671 and accuracy of 0.81\n",
      "Iteration 7347: with minibatch training loss = 1.08 and accuracy of 0.7\n",
      "Iteration 7348: with minibatch training loss = 0.839 and accuracy of 0.77\n",
      "Iteration 7349: with minibatch training loss = 0.937 and accuracy of 0.77\n",
      "Iteration 7350: with minibatch training loss = 0.73 and accuracy of 0.8\n",
      "Iteration 7351: with minibatch training loss = 0.878 and accuracy of 0.72\n",
      "Iteration 7352: with minibatch training loss = 1.02 and accuracy of 0.69\n",
      "Iteration 7353: with minibatch training loss = 0.866 and accuracy of 0.73\n",
      "Iteration 7354: with minibatch training loss = 0.833 and accuracy of 0.78\n",
      "Iteration 7355: with minibatch training loss = 0.629 and accuracy of 0.83\n",
      "Iteration 7356: with minibatch training loss = 1.01 and accuracy of 0.72\n",
      "Iteration 7357: with minibatch training loss = 0.752 and accuracy of 0.8\n",
      "Iteration 7358: with minibatch training loss = 0.643 and accuracy of 0.84\n",
      "Iteration 7359: with minibatch training loss = 0.797 and accuracy of 0.78\n",
      "Iteration 7360: with minibatch training loss = 0.72 and accuracy of 0.81\n",
      "Iteration 7361: with minibatch training loss = 0.824 and accuracy of 0.73\n",
      "Iteration 7362: with minibatch training loss = 1.02 and accuracy of 0.72\n",
      "Iteration 7363: with minibatch training loss = 0.671 and accuracy of 0.8\n",
      "Iteration 7364: with minibatch training loss = 0.851 and accuracy of 0.75\n",
      "Iteration 7365: with minibatch training loss = 0.841 and accuracy of 0.75\n",
      "Iteration 7366: with minibatch training loss = 0.719 and accuracy of 0.78\n",
      "Iteration 7367: with minibatch training loss = 1.04 and accuracy of 0.69\n",
      "Iteration 7368: with minibatch training loss = 0.719 and accuracy of 0.81\n",
      "Iteration 7369: with minibatch training loss = 0.791 and accuracy of 0.78\n",
      "Iteration 7370: with minibatch training loss = 0.94 and accuracy of 0.7\n",
      "Iteration 7371: with minibatch training loss = 0.809 and accuracy of 0.77\n",
      "Iteration 7372: with minibatch training loss = 0.57 and accuracy of 0.86\n",
      "Iteration 7373: with minibatch training loss = 0.639 and accuracy of 0.83\n",
      "Iteration 7374: with minibatch training loss = 0.913 and accuracy of 0.73\n",
      "Iteration 7375: with minibatch training loss = 0.89 and accuracy of 0.77\n",
      "Iteration 7376: with minibatch training loss = 1.24 and accuracy of 0.66\n",
      "Iteration 7377: with minibatch training loss = 0.712 and accuracy of 0.81\n",
      "Iteration 7378: with minibatch training loss = 0.945 and accuracy of 0.73\n",
      "Iteration 7379: with minibatch training loss = 0.686 and accuracy of 0.8\n",
      "Iteration 7380: with minibatch training loss = 0.701 and accuracy of 0.81\n",
      "Iteration 7381: with minibatch training loss = 0.956 and accuracy of 0.72\n",
      "Iteration 7382: with minibatch training loss = 0.677 and accuracy of 0.81\n",
      "Iteration 7383: with minibatch training loss = 1.08 and accuracy of 0.67\n",
      "Iteration 7384: with minibatch training loss = 1.04 and accuracy of 0.7\n",
      "Iteration 7385: with minibatch training loss = 1.08 and accuracy of 0.67\n",
      "Iteration 7386: with minibatch training loss = 0.813 and accuracy of 0.77\n",
      "Iteration 7387: with minibatch training loss = 0.715 and accuracy of 0.81\n",
      "Iteration 7388: with minibatch training loss = 0.85 and accuracy of 0.77\n",
      "Iteration 7389: with minibatch training loss = 0.631 and accuracy of 0.81\n",
      "Iteration 7390: with minibatch training loss = 1.11 and accuracy of 0.67\n",
      "Iteration 7391: with minibatch training loss = 0.753 and accuracy of 0.8\n",
      "Iteration 7392: with minibatch training loss = 0.826 and accuracy of 0.77\n",
      "Iteration 7393: with minibatch training loss = 0.669 and accuracy of 0.8\n",
      "Iteration 7394: with minibatch training loss = 0.901 and accuracy of 0.78\n",
      "Iteration 7395: with minibatch training loss = 0.922 and accuracy of 0.7\n",
      "Iteration 7396: with minibatch training loss = 1.02 and accuracy of 0.7\n",
      "Iteration 7397: with minibatch training loss = 0.872 and accuracy of 0.75\n",
      "Iteration 7398: with minibatch training loss = 1.01 and accuracy of 0.72\n",
      "Iteration 7399: with minibatch training loss = 0.731 and accuracy of 0.78\n",
      "Iteration 7400: with minibatch training loss = 0.912 and accuracy of 0.75\n",
      "Iteration 7401: with minibatch training loss = 1.16 and accuracy of 0.7\n",
      "Iteration 7402: with minibatch training loss = 0.55 and accuracy of 0.89\n",
      "Iteration 7403: with minibatch training loss = 0.883 and accuracy of 0.73\n",
      "Iteration 7404: with minibatch training loss = 0.82 and accuracy of 0.77\n",
      "Iteration 7405: with minibatch training loss = 1.23 and accuracy of 0.64\n",
      "Iteration 7406: with minibatch training loss = 0.913 and accuracy of 0.72\n",
      "Iteration 7407: with minibatch training loss = 0.814 and accuracy of 0.78\n",
      "Iteration 7408: with minibatch training loss = 0.796 and accuracy of 0.8\n",
      "Iteration 7409: with minibatch training loss = 0.899 and accuracy of 0.73\n",
      "Iteration 7410: with minibatch training loss = 0.883 and accuracy of 0.75\n",
      "Iteration 7411: with minibatch training loss = 0.817 and accuracy of 0.77\n",
      "Iteration 7412: with minibatch training loss = 0.701 and accuracy of 0.81\n",
      "Iteration 7413: with minibatch training loss = 0.664 and accuracy of 0.83\n",
      "Iteration 7414: with minibatch training loss = 0.884 and accuracy of 0.73\n",
      "Iteration 7415: with minibatch training loss = 0.701 and accuracy of 0.78\n",
      "Iteration 7416: with minibatch training loss = 0.783 and accuracy of 0.77\n",
      "Iteration 7417: with minibatch training loss = 0.735 and accuracy of 0.8\n",
      "Iteration 7418: with minibatch training loss = 0.525 and accuracy of 0.84\n",
      "Iteration 7419: with minibatch training loss = 0.948 and accuracy of 0.73\n",
      "Iteration 7420: with minibatch training loss = 0.797 and accuracy of 0.78\n",
      "Iteration 7421: with minibatch training loss = 0.727 and accuracy of 0.8\n",
      "Iteration 7422: with minibatch training loss = 0.719 and accuracy of 0.83\n",
      "Iteration 7423: with minibatch training loss = 0.812 and accuracy of 0.77\n",
      "Iteration 7424: with minibatch training loss = 0.878 and accuracy of 0.78\n",
      "Iteration 7425: with minibatch training loss = 1.18 and accuracy of 0.67\n",
      "Iteration 7426: with minibatch training loss = 0.673 and accuracy of 0.78\n",
      "Iteration 7427: with minibatch training loss = 1.02 and accuracy of 0.7\n",
      "Iteration 7428: with minibatch training loss = 0.977 and accuracy of 0.72\n",
      "Iteration 7429: with minibatch training loss = 0.99 and accuracy of 0.78\n",
      "Iteration 7430: with minibatch training loss = 0.742 and accuracy of 0.8\n",
      "Iteration 7431: with minibatch training loss = 0.929 and accuracy of 0.72\n",
      "Iteration 7432: with minibatch training loss = 0.803 and accuracy of 0.77\n",
      "Iteration 7433: with minibatch training loss = 0.735 and accuracy of 0.8\n",
      "Iteration 7434: with minibatch training loss = 1.02 and accuracy of 0.69\n",
      "Iteration 7435: with minibatch training loss = 0.675 and accuracy of 0.83\n",
      "Iteration 7436: with minibatch training loss = 0.905 and accuracy of 0.72\n",
      "Iteration 7437: with minibatch training loss = 0.798 and accuracy of 0.78\n",
      "Iteration 7438: with minibatch training loss = 0.576 and accuracy of 0.84\n",
      "Iteration 7439: with minibatch training loss = 0.858 and accuracy of 0.73\n",
      "Iteration 7440: with minibatch training loss = 0.796 and accuracy of 0.77\n",
      "Iteration 7441: with minibatch training loss = 1.01 and accuracy of 0.7\n",
      "Iteration 7442: with minibatch training loss = 0.51 and accuracy of 0.86\n",
      "Iteration 7443: with minibatch training loss = 0.933 and accuracy of 0.72\n",
      "Iteration 7444: with minibatch training loss = 0.768 and accuracy of 0.83\n",
      "Iteration 7445: with minibatch training loss = 0.803 and accuracy of 0.73\n",
      "Iteration 7446: with minibatch training loss = 0.853 and accuracy of 0.7\n",
      "Iteration 7447: with minibatch training loss = 1.05 and accuracy of 0.66\n",
      "Iteration 7448: with minibatch training loss = 0.971 and accuracy of 0.72\n",
      "Iteration 7449: with minibatch training loss = 1.14 and accuracy of 0.67\n",
      "Iteration 7450: with minibatch training loss = 0.852 and accuracy of 0.72\n",
      "Iteration 7451: with minibatch training loss = 0.755 and accuracy of 0.78\n",
      "Iteration 7452: with minibatch training loss = 0.614 and accuracy of 0.83\n",
      "Iteration 7453: with minibatch training loss = 0.814 and accuracy of 0.75\n",
      "Iteration 7454: with minibatch training loss = 0.745 and accuracy of 0.78\n",
      "Iteration 7455: with minibatch training loss = 0.821 and accuracy of 0.7\n",
      "Iteration 7456: with minibatch training loss = 1.05 and accuracy of 0.7\n",
      "Iteration 7457: with minibatch training loss = 0.917 and accuracy of 0.73\n",
      "Iteration 7458: with minibatch training loss = 1.15 and accuracy of 0.67\n",
      "Iteration 7459: with minibatch training loss = 1.1 and accuracy of 0.69\n",
      "Iteration 7460: with minibatch training loss = 0.756 and accuracy of 0.73\n",
      "Iteration 7461: with minibatch training loss = 0.746 and accuracy of 0.8\n",
      "Iteration 7462: with minibatch training loss = 0.68 and accuracy of 0.83\n",
      "Iteration 7463: with minibatch training loss = 0.973 and accuracy of 0.7\n",
      "Iteration 7464: with minibatch training loss = 0.506 and accuracy of 0.86\n",
      "Iteration 7465: with minibatch training loss = 0.52 and accuracy of 0.88\n",
      "Iteration 7466: with minibatch training loss = 0.922 and accuracy of 0.73\n",
      "Iteration 7467: with minibatch training loss = 0.692 and accuracy of 0.81\n",
      "Iteration 7468: with minibatch training loss = 0.663 and accuracy of 0.83\n",
      "Iteration 7469: with minibatch training loss = 0.895 and accuracy of 0.73\n",
      "Iteration 7470: with minibatch training loss = 1.05 and accuracy of 0.67\n",
      "Iteration 7471: with minibatch training loss = 0.779 and accuracy of 0.77\n",
      "Iteration 7472: with minibatch training loss = 0.904 and accuracy of 0.73\n",
      "Iteration 7473: with minibatch training loss = 0.762 and accuracy of 0.78\n",
      "Iteration 7474: with minibatch training loss = 0.953 and accuracy of 0.73\n",
      "Iteration 7475: with minibatch training loss = 0.661 and accuracy of 0.84\n",
      "Iteration 7476: with minibatch training loss = 0.815 and accuracy of 0.78\n",
      "Iteration 7477: with minibatch training loss = 0.809 and accuracy of 0.78\n",
      "Iteration 7478: with minibatch training loss = 0.562 and accuracy of 0.84\n",
      "Iteration 7479: with minibatch training loss = 1.04 and accuracy of 0.66\n",
      "Iteration 7480: with minibatch training loss = 0.871 and accuracy of 0.72\n",
      "Iteration 7481: with minibatch training loss = 0.66 and accuracy of 0.84\n",
      "Iteration 7482: with minibatch training loss = 0.583 and accuracy of 0.86\n",
      "Iteration 7483: with minibatch training loss = 0.454 and accuracy of 0.88\n",
      "Iteration 7484: with minibatch training loss = 0.688 and accuracy of 0.8\n",
      "Iteration 7485: with minibatch training loss = 0.916 and accuracy of 0.75\n",
      "Iteration 7486: with minibatch training loss = 1.01 and accuracy of 0.69\n",
      "Iteration 7487: with minibatch training loss = 1.16 and accuracy of 0.66\n",
      "Iteration 7488: with minibatch training loss = 0.991 and accuracy of 0.7\n",
      "Iteration 7489: with minibatch training loss = 0.857 and accuracy of 0.73\n",
      "Iteration 7490: with minibatch training loss = 0.703 and accuracy of 0.81\n",
      "Iteration 7491: with minibatch training loss = 0.906 and accuracy of 0.77\n",
      "Iteration 7492: with minibatch training loss = 0.992 and accuracy of 0.7\n",
      "Iteration 7493: with minibatch training loss = 0.852 and accuracy of 0.77\n",
      "Iteration 7494: with minibatch training loss = 0.796 and accuracy of 0.8\n",
      "Iteration 7495: with minibatch training loss = 1.07 and accuracy of 0.69\n",
      "Iteration 7496: with minibatch training loss = 0.66 and accuracy of 0.8\n",
      "Iteration 7497: with minibatch training loss = 0.868 and accuracy of 0.75\n",
      "Iteration 7498: with minibatch training loss = 0.67 and accuracy of 0.81\n",
      "Iteration 7499: with minibatch training loss = 0.618 and accuracy of 0.84\n",
      "Iteration 7500: with minibatch training loss = 0.832 and accuracy of 0.75\n",
      "Iteration 7501: with minibatch training loss = 0.697 and accuracy of 0.81\n",
      "Iteration 7502: with minibatch training loss = 0.693 and accuracy of 0.8\n",
      "Iteration 7503: with minibatch training loss = 1.17 and accuracy of 0.67\n",
      "Iteration 7504: with minibatch training loss = 0.955 and accuracy of 0.7\n",
      "Iteration 7505: with minibatch training loss = 1.17 and accuracy of 0.64\n",
      "Iteration 7506: with minibatch training loss = 0.844 and accuracy of 0.73\n",
      "Iteration 7507: with minibatch training loss = 0.589 and accuracy of 0.84\n",
      "Iteration 7508: with minibatch training loss = 0.73 and accuracy of 0.77\n",
      "Iteration 7509: with minibatch training loss = 0.793 and accuracy of 0.78\n",
      "Iteration 7510: with minibatch training loss = 1.08 and accuracy of 0.66\n",
      "Iteration 7511: with minibatch training loss = 0.729 and accuracy of 0.8\n",
      "Iteration 7512: with minibatch training loss = 0.871 and accuracy of 0.75\n",
      "Iteration 7513: with minibatch training loss = 0.794 and accuracy of 0.78\n",
      "Iteration 7514: with minibatch training loss = 0.602 and accuracy of 0.84\n",
      "Iteration 7515: with minibatch training loss = 0.904 and accuracy of 0.78\n",
      "Iteration 7516: with minibatch training loss = 0.912 and accuracy of 0.77\n",
      "Iteration 7517: with minibatch training loss = 1.12 and accuracy of 0.7\n",
      "Iteration 7518: with minibatch training loss = 0.792 and accuracy of 0.77\n",
      "Iteration 7519: with minibatch training loss = 0.822 and accuracy of 0.75\n",
      "Iteration 7520: with minibatch training loss = 0.976 and accuracy of 0.72\n",
      "Iteration 7521: with minibatch training loss = 0.778 and accuracy of 0.81\n",
      "Iteration 7522: with minibatch training loss = 0.781 and accuracy of 0.78\n",
      "Iteration 7523: with minibatch training loss = 0.664 and accuracy of 0.81\n",
      "Iteration 7524: with minibatch training loss = 0.662 and accuracy of 0.81\n",
      "Iteration 7525: with minibatch training loss = 1.03 and accuracy of 0.69\n",
      "Iteration 7526: with minibatch training loss = 0.751 and accuracy of 0.8\n",
      "Iteration 7527: with minibatch training loss = 0.863 and accuracy of 0.8\n",
      "Iteration 7528: with minibatch training loss = 0.838 and accuracy of 0.77\n",
      "Iteration 7529: with minibatch training loss = 0.743 and accuracy of 0.81\n",
      "Iteration 7530: with minibatch training loss = 0.562 and accuracy of 0.86\n",
      "Iteration 7531: with minibatch training loss = 0.733 and accuracy of 0.81\n",
      "Iteration 7532: with minibatch training loss = 0.771 and accuracy of 0.8\n",
      "Iteration 7533: with minibatch training loss = 0.981 and accuracy of 0.67\n",
      "Iteration 7534: with minibatch training loss = 0.815 and accuracy of 0.77\n",
      "Iteration 7535: with minibatch training loss = 1.16 and accuracy of 0.67\n",
      "Iteration 7536: with minibatch training loss = 0.762 and accuracy of 0.8\n",
      "Iteration 7537: with minibatch training loss = 0.637 and accuracy of 0.84\n",
      "Iteration 7538: with minibatch training loss = 0.683 and accuracy of 0.83\n",
      "Iteration 7539: with minibatch training loss = 1.04 and accuracy of 0.69\n",
      "Iteration 7540: with minibatch training loss = 0.807 and accuracy of 0.78\n",
      "Iteration 7541: with minibatch training loss = 0.647 and accuracy of 0.81\n",
      "Iteration 7542: with minibatch training loss = 0.685 and accuracy of 0.8\n",
      "Iteration 7543: with minibatch training loss = 0.835 and accuracy of 0.73\n",
      "Iteration 7544: with minibatch training loss = 0.554 and accuracy of 0.86\n",
      "Iteration 7545: with minibatch training loss = 0.84 and accuracy of 0.77\n",
      "Iteration 7546: with minibatch training loss = 0.652 and accuracy of 0.8\n",
      "Iteration 7547: with minibatch training loss = 0.676 and accuracy of 0.83\n",
      "Iteration 7548: with minibatch training loss = 0.657 and accuracy of 0.81\n",
      "Iteration 7549: with minibatch training loss = 0.775 and accuracy of 0.78\n",
      "Iteration 7550: with minibatch training loss = 1.24 and accuracy of 0.66\n",
      "Iteration 7551: with minibatch training loss = 0.862 and accuracy of 0.75\n",
      "Iteration 7552: with minibatch training loss = 1.16 and accuracy of 0.66\n",
      "Iteration 7553: with minibatch training loss = 0.872 and accuracy of 0.75\n",
      "Iteration 7554: with minibatch training loss = 1.01 and accuracy of 0.73\n",
      "Iteration 7555: with minibatch training loss = 0.823 and accuracy of 0.77\n",
      "Iteration 7556: with minibatch training loss = 0.803 and accuracy of 0.8\n",
      "Iteration 7557: with minibatch training loss = 0.871 and accuracy of 0.8\n",
      "Iteration 7558: with minibatch training loss = 0.745 and accuracy of 0.77\n",
      "Iteration 7559: with minibatch training loss = 1.18 and accuracy of 0.64\n",
      "Iteration 7560: with minibatch training loss = 0.883 and accuracy of 0.77\n",
      "Iteration 7561: with minibatch training loss = 0.725 and accuracy of 0.81\n",
      "Iteration 7562: with minibatch training loss = 1.15 and accuracy of 0.66\n",
      "Iteration 7563: with minibatch training loss = 0.727 and accuracy of 0.8\n",
      "Iteration 7564: with minibatch training loss = 0.782 and accuracy of 0.78\n",
      "Iteration 7565: with minibatch training loss = 1.02 and accuracy of 0.7\n",
      "Iteration 7566: with minibatch training loss = 0.837 and accuracy of 0.78\n",
      "Iteration 7567: with minibatch training loss = 0.879 and accuracy of 0.75\n",
      "Iteration 7568: with minibatch training loss = 0.944 and accuracy of 0.75\n",
      "Iteration 7569: with minibatch training loss = 0.752 and accuracy of 0.77\n",
      "Iteration 7570: with minibatch training loss = 1.09 and accuracy of 0.67\n",
      "Iteration 7571: with minibatch training loss = 0.8 and accuracy of 0.78\n",
      "Iteration 7572: with minibatch training loss = 0.449 and accuracy of 0.88\n",
      "Iteration 7573: with minibatch training loss = 0.838 and accuracy of 0.75\n",
      "Iteration 7574: with minibatch training loss = 1.26 and accuracy of 0.64\n",
      "Iteration 7575: with minibatch training loss = 0.791 and accuracy of 0.8\n",
      "Iteration 7576: with minibatch training loss = 0.588 and accuracy of 0.84\n",
      "Iteration 7577: with minibatch training loss = 0.912 and accuracy of 0.73\n",
      "Iteration 7578: with minibatch training loss = 0.525 and accuracy of 0.86\n",
      "Iteration 7579: with minibatch training loss = 0.724 and accuracy of 0.8\n",
      "Iteration 7580: with minibatch training loss = 0.486 and accuracy of 0.88\n",
      "Iteration 7581: with minibatch training loss = 0.965 and accuracy of 0.7\n",
      "Iteration 7582: with minibatch training loss = 1.01 and accuracy of 0.72\n",
      "Iteration 7583: with minibatch training loss = 0.903 and accuracy of 0.77\n",
      "Iteration 7584: with minibatch training loss = 0.868 and accuracy of 0.73\n",
      "Iteration 7585: with minibatch training loss = 0.777 and accuracy of 0.78\n",
      "Iteration 7586: with minibatch training loss = 0.795 and accuracy of 0.77\n",
      "Iteration 7587: with minibatch training loss = 0.543 and accuracy of 0.88\n",
      "Iteration 7588: with minibatch training loss = 0.49 and accuracy of 0.88\n",
      "Iteration 7589: with minibatch training loss = 1.07 and accuracy of 0.7\n",
      "Iteration 7590: with minibatch training loss = 0.899 and accuracy of 0.75\n",
      "Iteration 7591: with minibatch training loss = 0.851 and accuracy of 0.77\n",
      "Iteration 7592: with minibatch training loss = 0.863 and accuracy of 0.77\n",
      "Iteration 7593: with minibatch training loss = 0.845 and accuracy of 0.78\n",
      "Iteration 7594: with minibatch training loss = 0.901 and accuracy of 0.75\n",
      "Iteration 7595: with minibatch training loss = 0.707 and accuracy of 0.8\n",
      "Iteration 7596: with minibatch training loss = 0.913 and accuracy of 0.75\n",
      "Iteration 7597: with minibatch training loss = 1.01 and accuracy of 0.69\n",
      "Iteration 7598: with minibatch training loss = 0.951 and accuracy of 0.72\n",
      "Iteration 7599: with minibatch training loss = 0.968 and accuracy of 0.72\n",
      "Iteration 7600: with minibatch training loss = 0.899 and accuracy of 0.73\n",
      "Iteration 7601: with minibatch training loss = 0.932 and accuracy of 0.72\n",
      "Iteration 7602: with minibatch training loss = 0.864 and accuracy of 0.77\n",
      "Iteration 7603: with minibatch training loss = 0.872 and accuracy of 0.77\n",
      "Iteration 7604: with minibatch training loss = 0.573 and accuracy of 0.81\n",
      "Iteration 7605: with minibatch training loss = 0.961 and accuracy of 0.75\n",
      "Iteration 7606: with minibatch training loss = 1.07 and accuracy of 0.69\n",
      "Iteration 7607: with minibatch training loss = 0.859 and accuracy of 0.77\n",
      "Iteration 7608: with minibatch training loss = 1.16 and accuracy of 0.67\n",
      "Iteration 7609: with minibatch training loss = 1.24 and accuracy of 0.62\n",
      "Iteration 7610: with minibatch training loss = 0.895 and accuracy of 0.75\n",
      "Iteration 7611: with minibatch training loss = 0.993 and accuracy of 0.72\n",
      "Iteration 7612: with minibatch training loss = 0.617 and accuracy of 0.84\n",
      "Iteration 7613: with minibatch training loss = 0.624 and accuracy of 0.81\n",
      "Iteration 7614: with minibatch training loss = 0.661 and accuracy of 0.8\n",
      "Iteration 7615: with minibatch training loss = 0.688 and accuracy of 0.83\n",
      "Iteration 7616: with minibatch training loss = 0.855 and accuracy of 0.77\n",
      "Iteration 7617: with minibatch training loss = 0.708 and accuracy of 0.81\n",
      "Iteration 7618: with minibatch training loss = 0.933 and accuracy of 0.7\n",
      "Iteration 7619: with minibatch training loss = 0.884 and accuracy of 0.75\n",
      "Iteration 7620: with minibatch training loss = 0.554 and accuracy of 0.86\n",
      "Iteration 7621: with minibatch training loss = 0.81 and accuracy of 0.73\n",
      "Iteration 7622: with minibatch training loss = 0.867 and accuracy of 0.77\n",
      "Iteration 7623: with minibatch training loss = 0.732 and accuracy of 0.81\n",
      "Iteration 7624: with minibatch training loss = 0.794 and accuracy of 0.78\n",
      "Iteration 7625: with minibatch training loss = 0.63 and accuracy of 0.81\n",
      "Iteration 7626: with minibatch training loss = 0.619 and accuracy of 0.84\n",
      "Iteration 7627: with minibatch training loss = 0.684 and accuracy of 0.81\n",
      "Iteration 7628: with minibatch training loss = 0.716 and accuracy of 0.81\n",
      "Iteration 7629: with minibatch training loss = 0.649 and accuracy of 0.83\n",
      "Iteration 7630: with minibatch training loss = 0.821 and accuracy of 0.78\n",
      "Iteration 7631: with minibatch training loss = 1.1 and accuracy of 0.7\n",
      "Iteration 7632: with minibatch training loss = 0.498 and accuracy of 0.84\n",
      "Iteration 7633: with minibatch training loss = 0.732 and accuracy of 0.8\n",
      "Iteration 7634: with minibatch training loss = 0.533 and accuracy of 0.86\n",
      "Iteration 7635: with minibatch training loss = 0.802 and accuracy of 0.81\n",
      "Iteration 7636: with minibatch training loss = 0.954 and accuracy of 0.75\n",
      "Iteration 7637: with minibatch training loss = 0.802 and accuracy of 0.77\n",
      "Iteration 7638: with minibatch training loss = 0.975 and accuracy of 0.73\n",
      "Iteration 7639: with minibatch training loss = 0.903 and accuracy of 0.75\n",
      "Iteration 7640: with minibatch training loss = 0.994 and accuracy of 0.7\n",
      "Iteration 7641: with minibatch training loss = 1.15 and accuracy of 0.67\n",
      "Iteration 7642: with minibatch training loss = 0.71 and accuracy of 0.78\n",
      "Iteration 7643: with minibatch training loss = 0.648 and accuracy of 0.84\n",
      "Iteration 7644: with minibatch training loss = 0.922 and accuracy of 0.72\n",
      "Iteration 7645: with minibatch training loss = 0.979 and accuracy of 0.7\n",
      "Iteration 7646: with minibatch training loss = 0.657 and accuracy of 0.83\n",
      "Iteration 7647: with minibatch training loss = 0.978 and accuracy of 0.72\n",
      "Iteration 7648: with minibatch training loss = 1.17 and accuracy of 0.67\n",
      "Iteration 7649: with minibatch training loss = 0.821 and accuracy of 0.77\n",
      "Iteration 7650: with minibatch training loss = 1.33 and accuracy of 0.64\n",
      "Iteration 7651: with minibatch training loss = 0.895 and accuracy of 0.72\n",
      "Iteration 7652: with minibatch training loss = 0.844 and accuracy of 0.75\n",
      "Iteration 7653: with minibatch training loss = 0.912 and accuracy of 0.73\n",
      "Iteration 7654: with minibatch training loss = 0.911 and accuracy of 0.73\n",
      "Iteration 7655: with minibatch training loss = 0.76 and accuracy of 0.78\n",
      "Iteration 7656: with minibatch training loss = 0.865 and accuracy of 0.78\n",
      "Iteration 7657: with minibatch training loss = 0.831 and accuracy of 0.8\n",
      "Iteration 7658: with minibatch training loss = 0.754 and accuracy of 0.78\n",
      "Iteration 7659: with minibatch training loss = 0.961 and accuracy of 0.77\n",
      "Iteration 7660: with minibatch training loss = 0.974 and accuracy of 0.72\n",
      "Iteration 7661: with minibatch training loss = 0.703 and accuracy of 0.8\n",
      "Iteration 7662: with minibatch training loss = 0.613 and accuracy of 0.81\n",
      "Iteration 7663: with minibatch training loss = 0.799 and accuracy of 0.8\n",
      "Iteration 7664: with minibatch training loss = 0.756 and accuracy of 0.78\n",
      "Iteration 7665: with minibatch training loss = 1 and accuracy of 0.73\n",
      "Iteration 7666: with minibatch training loss = 0.703 and accuracy of 0.8\n",
      "Iteration 7667: with minibatch training loss = 0.421 and accuracy of 0.89\n",
      "Iteration 7668: with minibatch training loss = 0.682 and accuracy of 0.83\n",
      "Iteration 7669: with minibatch training loss = 0.86 and accuracy of 0.73\n",
      "Iteration 7670: with minibatch training loss = 0.625 and accuracy of 0.84\n",
      "Iteration 7671: with minibatch training loss = 0.974 and accuracy of 0.7\n",
      "Iteration 7672: with minibatch training loss = 0.767 and accuracy of 0.77\n",
      "Iteration 7673: with minibatch training loss = 0.608 and accuracy of 0.84\n",
      "Iteration 7674: with minibatch training loss = 0.851 and accuracy of 0.72\n",
      "Iteration 7675: with minibatch training loss = 0.577 and accuracy of 0.86\n",
      "Iteration 7676: with minibatch training loss = 0.637 and accuracy of 0.83\n",
      "Iteration 7677: with minibatch training loss = 1.01 and accuracy of 0.7\n",
      "Iteration 7678: with minibatch training loss = 0.927 and accuracy of 0.7\n",
      "Iteration 7679: with minibatch training loss = 0.834 and accuracy of 0.77\n",
      "Iteration 7680: with minibatch training loss = 0.877 and accuracy of 0.75\n",
      "Iteration 7681: with minibatch training loss = 0.547 and accuracy of 0.83\n",
      "Iteration 7682: with minibatch training loss = 0.709 and accuracy of 0.8\n",
      "Iteration 7683: with minibatch training loss = 0.411 and accuracy of 0.92\n",
      "Iteration 7684: with minibatch training loss = 1.01 and accuracy of 0.7\n",
      "Iteration 7685: with minibatch training loss = 0.8 and accuracy of 0.78\n",
      "Iteration 7686: with minibatch training loss = 0.901 and accuracy of 0.73\n",
      "Iteration 7687: with minibatch training loss = 0.744 and accuracy of 0.83\n",
      "Iteration 7688: with minibatch training loss = 0.857 and accuracy of 0.78\n",
      "Iteration 7689: with minibatch training loss = 0.809 and accuracy of 0.77\n",
      "Iteration 7690: with minibatch training loss = 0.838 and accuracy of 0.75\n",
      "Iteration 7691: with minibatch training loss = 0.78 and accuracy of 0.75\n",
      "Iteration 7692: with minibatch training loss = 0.737 and accuracy of 0.8\n",
      "Iteration 7693: with minibatch training loss = 0.814 and accuracy of 0.77\n",
      "Iteration 7694: with minibatch training loss = 0.909 and accuracy of 0.72\n",
      "Iteration 7695: with minibatch training loss = 0.889 and accuracy of 0.77\n",
      "Iteration 7696: with minibatch training loss = 0.874 and accuracy of 0.72\n",
      "Iteration 7697: with minibatch training loss = 0.86 and accuracy of 0.73\n",
      "Iteration 7698: with minibatch training loss = 0.69 and accuracy of 0.81\n",
      "Iteration 7699: with minibatch training loss = 0.935 and accuracy of 0.77\n",
      "Iteration 7700: with minibatch training loss = 0.743 and accuracy of 0.8\n",
      "Iteration 7701: with minibatch training loss = 0.985 and accuracy of 0.69\n",
      "Iteration 7702: with minibatch training loss = 0.771 and accuracy of 0.8\n",
      "Iteration 7703: with minibatch training loss = 0.972 and accuracy of 0.7\n",
      "Iteration 7704: with minibatch training loss = 1.06 and accuracy of 0.72\n",
      "Iteration 7705: with minibatch training loss = 0.713 and accuracy of 0.78\n",
      "Iteration 7706: with minibatch training loss = 1.16 and accuracy of 0.67\n",
      "Iteration 7707: with minibatch training loss = 0.61 and accuracy of 0.84\n",
      "Iteration 7708: with minibatch training loss = 1.17 and accuracy of 0.67\n",
      "Iteration 7709: with minibatch training loss = 0.604 and accuracy of 0.83\n",
      "Iteration 7710: with minibatch training loss = 0.852 and accuracy of 0.77\n",
      "Iteration 7711: with minibatch training loss = 0.652 and accuracy of 0.81\n",
      "Iteration 7712: with minibatch training loss = 0.659 and accuracy of 0.83\n",
      "Iteration 7713: with minibatch training loss = 0.933 and accuracy of 0.73\n",
      "Iteration 7714: with minibatch training loss = 1.3 and accuracy of 0.61\n",
      "Iteration 7715: with minibatch training loss = 0.713 and accuracy of 0.8\n",
      "Iteration 7716: with minibatch training loss = 0.793 and accuracy of 0.77\n",
      "Iteration 7717: with minibatch training loss = 0.833 and accuracy of 0.77\n",
      "Iteration 7718: with minibatch training loss = 0.97 and accuracy of 0.75\n",
      "Iteration 7719: with minibatch training loss = 0.721 and accuracy of 0.78\n",
      "Iteration 7720: with minibatch training loss = 0.426 and accuracy of 0.89\n",
      "Iteration 7721: with minibatch training loss = 0.935 and accuracy of 0.73\n",
      "Iteration 7722: with minibatch training loss = 0.986 and accuracy of 0.7\n",
      "Iteration 7723: with minibatch training loss = 0.8 and accuracy of 0.77\n",
      "Iteration 7724: with minibatch training loss = 0.875 and accuracy of 0.78\n",
      "Iteration 7725: with minibatch training loss = 1.03 and accuracy of 0.72\n",
      "Iteration 7726: with minibatch training loss = 0.778 and accuracy of 0.78\n",
      "Iteration 7727: with minibatch training loss = 0.742 and accuracy of 0.8\n",
      "Iteration 7728: with minibatch training loss = 0.834 and accuracy of 0.77\n",
      "Iteration 7729: with minibatch training loss = 0.895 and accuracy of 0.75\n",
      "Iteration 7730: with minibatch training loss = 0.999 and accuracy of 0.69\n",
      "Iteration 7731: with minibatch training loss = 0.771 and accuracy of 0.77\n",
      "Iteration 7732: with minibatch training loss = 0.857 and accuracy of 0.75\n",
      "Iteration 7733: with minibatch training loss = 0.822 and accuracy of 0.77\n",
      "Iteration 7734: with minibatch training loss = 0.793 and accuracy of 0.81\n",
      "Iteration 7735: with minibatch training loss = 0.81 and accuracy of 0.78\n",
      "Iteration 7736: with minibatch training loss = 0.942 and accuracy of 0.73\n",
      "Iteration 7737: with minibatch training loss = 1.02 and accuracy of 0.69\n",
      "Iteration 7738: with minibatch training loss = 0.861 and accuracy of 0.78\n",
      "Iteration 7739: with minibatch training loss = 0.973 and accuracy of 0.72\n",
      "Iteration 7740: with minibatch training loss = 0.579 and accuracy of 0.83\n",
      "Iteration 7741: with minibatch training loss = 0.668 and accuracy of 0.83\n",
      "Iteration 7742: with minibatch training loss = 0.387 and accuracy of 0.89\n",
      "Iteration 7743: with minibatch training loss = 0.723 and accuracy of 0.8\n",
      "Iteration 7744: with minibatch training loss = 0.642 and accuracy of 0.83\n",
      "Iteration 7745: with minibatch training loss = 1.06 and accuracy of 0.7\n",
      "Iteration 7746: with minibatch training loss = 1.11 and accuracy of 0.66\n",
      "Iteration 7747: with minibatch training loss = 0.713 and accuracy of 0.81\n",
      "Iteration 7748: with minibatch training loss = 0.595 and accuracy of 0.86\n",
      "Iteration 7749: with minibatch training loss = 0.926 and accuracy of 0.75\n",
      "Iteration 7750: with minibatch training loss = 0.583 and accuracy of 0.84\n",
      "Iteration 7751: with minibatch training loss = 0.562 and accuracy of 0.84\n",
      "Iteration 7752: with minibatch training loss = 0.766 and accuracy of 0.78\n",
      "Iteration 7753: with minibatch training loss = 0.652 and accuracy of 0.81\n",
      "Iteration 7754: with minibatch training loss = 0.861 and accuracy of 0.73\n",
      "Iteration 7755: with minibatch training loss = 0.849 and accuracy of 0.75\n",
      "Iteration 7756: with minibatch training loss = 0.968 and accuracy of 0.75\n",
      "Iteration 7757: with minibatch training loss = 0.722 and accuracy of 0.8\n",
      "Iteration 7758: with minibatch training loss = 0.878 and accuracy of 0.75\n",
      "Iteration 7759: with minibatch training loss = 0.619 and accuracy of 0.84\n",
      "Iteration 7760: with minibatch training loss = 0.842 and accuracy of 0.77\n",
      "Iteration 7761: with minibatch training loss = 0.619 and accuracy of 0.81\n",
      "Iteration 7762: with minibatch training loss = 0.675 and accuracy of 0.78\n",
      "Iteration 7763: with minibatch training loss = 0.767 and accuracy of 0.78\n",
      "Iteration 7764: with minibatch training loss = 0.644 and accuracy of 0.86\n",
      "Iteration 7765: with minibatch training loss = 0.526 and accuracy of 0.91\n",
      "Iteration 7766: with minibatch training loss = 1.19 and accuracy of 0.64\n",
      "Iteration 7767: with minibatch training loss = 1.12 and accuracy of 0.67\n",
      "Iteration 7768: with minibatch training loss = 0.737 and accuracy of 0.78\n",
      "Iteration 7769: with minibatch training loss = 0.665 and accuracy of 0.78\n",
      "Iteration 7770: with minibatch training loss = 0.594 and accuracy of 0.84\n",
      "Iteration 7771: with minibatch training loss = 0.767 and accuracy of 0.83\n",
      "Iteration 7772: with minibatch training loss = 0.766 and accuracy of 0.78\n",
      "Iteration 7773: with minibatch training loss = 0.801 and accuracy of 0.83\n",
      "Iteration 7774: with minibatch training loss = 0.958 and accuracy of 0.75\n",
      "Iteration 7775: with minibatch training loss = 0.734 and accuracy of 0.8\n",
      "Iteration 7776: with minibatch training loss = 1.03 and accuracy of 0.67\n",
      "Iteration 7777: with minibatch training loss = 0.912 and accuracy of 0.73\n",
      "Iteration 7778: with minibatch training loss = 1.11 and accuracy of 0.67\n",
      "Iteration 7779: with minibatch training loss = 0.534 and accuracy of 0.83\n",
      "Iteration 7780: with minibatch training loss = 0.612 and accuracy of 0.8\n",
      "Iteration 7781: with minibatch training loss = 0.635 and accuracy of 0.83\n",
      "Iteration 7782: with minibatch training loss = 0.493 and accuracy of 0.89\n",
      "Iteration 7783: with minibatch training loss = 0.659 and accuracy of 0.8\n",
      "Iteration 7784: with minibatch training loss = 0.625 and accuracy of 0.83\n",
      "Iteration 7785: with minibatch training loss = 1.02 and accuracy of 0.72\n",
      "Iteration 7786: with minibatch training loss = 0.772 and accuracy of 0.75\n",
      "Iteration 7787: with minibatch training loss = 0.961 and accuracy of 0.73\n",
      "Iteration 7788: with minibatch training loss = 0.886 and accuracy of 0.77\n",
      "Iteration 7789: with minibatch training loss = 0.813 and accuracy of 0.8\n",
      "Iteration 7790: with minibatch training loss = 0.642 and accuracy of 0.81\n",
      "Iteration 7791: with minibatch training loss = 0.849 and accuracy of 0.75\n",
      "Iteration 7792: with minibatch training loss = 0.585 and accuracy of 0.86\n",
      "Iteration 7793: with minibatch training loss = 0.631 and accuracy of 0.84\n",
      "Iteration 7794: with minibatch training loss = 1.01 and accuracy of 0.7\n",
      "Iteration 7795: with minibatch training loss = 0.637 and accuracy of 0.83\n",
      "Iteration 7796: with minibatch training loss = 1.07 and accuracy of 0.66\n",
      "Iteration 7797: with minibatch training loss = 0.694 and accuracy of 0.8\n",
      "Iteration 7798: with minibatch training loss = 0.837 and accuracy of 0.77\n",
      "Iteration 7799: with minibatch training loss = 0.942 and accuracy of 0.72\n",
      "Iteration 7800: with minibatch training loss = 0.636 and accuracy of 0.84\n",
      "Iteration 7801: with minibatch training loss = 0.522 and accuracy of 0.86\n",
      "Iteration 7802: with minibatch training loss = 0.727 and accuracy of 0.8\n",
      "Iteration 7803: with minibatch training loss = 0.673 and accuracy of 0.81\n",
      "Iteration 7804: with minibatch training loss = 0.96 and accuracy of 0.7\n",
      "Iteration 7805: with minibatch training loss = 0.764 and accuracy of 0.8\n",
      "Iteration 7806: with minibatch training loss = 0.694 and accuracy of 0.81\n",
      "Iteration 7807: with minibatch training loss = 1.37 and accuracy of 0.62\n",
      "Iteration 7808: with minibatch training loss = 1.03 and accuracy of 0.66\n",
      "Iteration 7809: with minibatch training loss = 0.594 and accuracy of 0.84\n",
      "Iteration 7810: with minibatch training loss = 0.68 and accuracy of 0.83\n",
      "Iteration 7811: with minibatch training loss = 0.825 and accuracy of 0.8\n",
      "Iteration 7812: with minibatch training loss = 0.798 and accuracy of 0.78\n",
      "Iteration 7813: with minibatch training loss = 0.905 and accuracy of 0.77\n",
      "Iteration 7814: with minibatch training loss = 1 and accuracy of 0.69\n",
      "Iteration 7815: with minibatch training loss = 0.746 and accuracy of 0.81\n",
      "Iteration 7816: with minibatch training loss = 0.723 and accuracy of 0.8\n",
      "Iteration 7817: with minibatch training loss = 0.903 and accuracy of 0.73\n",
      "Iteration 7818: with minibatch training loss = 0.624 and accuracy of 0.83\n",
      "Iteration 7819: with minibatch training loss = 0.766 and accuracy of 0.78\n",
      "Iteration 7820: with minibatch training loss = 1.07 and accuracy of 0.67\n",
      "Iteration 7821: with minibatch training loss = 0.719 and accuracy of 0.8\n",
      "Iteration 7822: with minibatch training loss = 1.01 and accuracy of 0.7\n",
      "Iteration 7823: with minibatch training loss = 0.696 and accuracy of 0.8\n",
      "Iteration 7824: with minibatch training loss = 1.32 and accuracy of 0.64\n",
      "Iteration 7825: with minibatch training loss = 0.67 and accuracy of 0.81\n",
      "Iteration 7826: with minibatch training loss = 0.711 and accuracy of 0.8\n",
      "Iteration 7827: with minibatch training loss = 0.82 and accuracy of 0.78\n",
      "Iteration 7828: with minibatch training loss = 0.568 and accuracy of 0.84\n",
      "Iteration 7829: with minibatch training loss = 0.678 and accuracy of 0.83\n",
      "Iteration 7830: with minibatch training loss = 0.727 and accuracy of 0.81\n",
      "Iteration 7831: with minibatch training loss = 0.593 and accuracy of 0.84\n",
      "Iteration 7832: with minibatch training loss = 0.729 and accuracy of 0.81\n",
      "Iteration 7833: with minibatch training loss = 0.562 and accuracy of 0.81\n",
      "Iteration 7834: with minibatch training loss = 0.918 and accuracy of 0.77\n",
      "Iteration 7835: with minibatch training loss = 0.71 and accuracy of 0.83\n",
      "Iteration 7836: with minibatch training loss = 0.89 and accuracy of 0.69\n",
      "Iteration 7837: with minibatch training loss = 0.924 and accuracy of 0.75\n",
      "Iteration 7838: with minibatch training loss = 1.11 and accuracy of 0.7\n",
      "Iteration 7839: with minibatch training loss = 0.853 and accuracy of 0.75\n",
      "Iteration 7840: with minibatch training loss = 0.922 and accuracy of 0.73\n",
      "Iteration 7841: with minibatch training loss = 1.11 and accuracy of 0.66\n",
      "Iteration 7842: with minibatch training loss = 0.505 and accuracy of 0.84\n",
      "Iteration 7843: with minibatch training loss = 1.02 and accuracy of 0.69\n",
      "Iteration 7844: with minibatch training loss = 0.751 and accuracy of 0.78\n",
      "Iteration 7845: with minibatch training loss = 0.667 and accuracy of 0.83\n",
      "Iteration 7846: with minibatch training loss = 0.909 and accuracy of 0.73\n",
      "Iteration 7847: with minibatch training loss = 0.785 and accuracy of 0.77\n",
      "Iteration 7848: with minibatch training loss = 0.887 and accuracy of 0.73\n",
      "Iteration 7849: with minibatch training loss = 0.893 and accuracy of 0.77\n",
      "Iteration 7850: with minibatch training loss = 0.539 and accuracy of 0.88\n",
      "Iteration 7851: with minibatch training loss = 0.901 and accuracy of 0.73\n",
      "Iteration 7852: with minibatch training loss = 0.615 and accuracy of 0.83\n",
      "Iteration 7853: with minibatch training loss = 0.548 and accuracy of 0.84\n",
      "Iteration 7854: with minibatch training loss = 1.04 and accuracy of 0.7\n",
      "Iteration 7855: with minibatch training loss = 0.819 and accuracy of 0.75\n",
      "Iteration 7856: with minibatch training loss = 0.747 and accuracy of 0.81\n",
      "Iteration 7857: with minibatch training loss = 0.831 and accuracy of 0.75\n",
      "Iteration 7858: with minibatch training loss = 0.717 and accuracy of 0.81\n",
      "Iteration 7859: with minibatch training loss = 0.552 and accuracy of 0.83\n",
      "Iteration 7860: with minibatch training loss = 0.806 and accuracy of 0.78\n",
      "Iteration 7861: with minibatch training loss = 0.847 and accuracy of 0.75\n",
      "Iteration 7862: with minibatch training loss = 0.932 and accuracy of 0.75\n",
      "Iteration 7863: with minibatch training loss = 1.17 and accuracy of 0.67\n",
      "Iteration 7864: with minibatch training loss = 0.878 and accuracy of 0.75\n",
      "Iteration 7865: with minibatch training loss = 0.941 and accuracy of 0.75\n",
      "Iteration 7866: with minibatch training loss = 1.03 and accuracy of 0.69\n",
      "Iteration 7867: with minibatch training loss = 0.81 and accuracy of 0.8\n",
      "Iteration 7868: with minibatch training loss = 0.747 and accuracy of 0.83\n",
      "Iteration 7869: with minibatch training loss = 0.485 and accuracy of 0.88\n",
      "Iteration 7870: with minibatch training loss = 0.586 and accuracy of 0.81\n",
      "Iteration 7871: with minibatch training loss = 0.622 and accuracy of 0.83\n",
      "Iteration 7872: with minibatch training loss = 0.958 and accuracy of 0.72\n",
      "Iteration 7873: with minibatch training loss = 0.746 and accuracy of 0.75\n",
      "Iteration 7874: with minibatch training loss = 0.857 and accuracy of 0.73\n",
      "Iteration 7875: with minibatch training loss = 0.732 and accuracy of 0.78\n",
      "Iteration 7876: with minibatch training loss = 0.811 and accuracy of 0.78\n",
      "Iteration 7877: with minibatch training loss = 0.921 and accuracy of 0.73\n",
      "Iteration 7878: with minibatch training loss = 0.736 and accuracy of 0.86\n",
      "Iteration 7879: with minibatch training loss = 0.54 and accuracy of 0.88\n",
      "Iteration 7880: with minibatch training loss = 0.988 and accuracy of 0.72\n",
      "Iteration 7881: with minibatch training loss = 0.682 and accuracy of 0.81\n",
      "Iteration 7882: with minibatch training loss = 0.889 and accuracy of 0.75\n",
      "Iteration 7883: with minibatch training loss = 0.975 and accuracy of 0.73\n",
      "Iteration 7884: with minibatch training loss = 1.09 and accuracy of 0.72\n",
      "Iteration 7885: with minibatch training loss = 0.837 and accuracy of 0.78\n",
      "Iteration 7886: with minibatch training loss = 0.986 and accuracy of 0.73\n",
      "Iteration 7887: with minibatch training loss = 0.692 and accuracy of 0.8\n",
      "Iteration 7888: with minibatch training loss = 0.677 and accuracy of 0.8\n",
      "Iteration 7889: with minibatch training loss = 0.837 and accuracy of 0.75\n",
      "Iteration 7890: with minibatch training loss = 0.863 and accuracy of 0.77\n",
      "Iteration 7891: with minibatch training loss = 1.03 and accuracy of 0.69\n",
      "Iteration 7892: with minibatch training loss = 0.866 and accuracy of 0.73\n",
      "Iteration 7893: with minibatch training loss = 0.852 and accuracy of 0.77\n",
      "Iteration 7894: with minibatch training loss = 0.86 and accuracy of 0.77\n",
      "Iteration 7895: with minibatch training loss = 0.824 and accuracy of 0.77\n",
      "Iteration 7896: with minibatch training loss = 1.67 and accuracy of 0.52\n",
      "Iteration 7897: with minibatch training loss = 1.11 and accuracy of 0.66\n",
      "Iteration 7898: with minibatch training loss = 0.652 and accuracy of 0.81\n",
      "Iteration 7899: with minibatch training loss = 1.15 and accuracy of 0.67\n",
      "Iteration 7900: with minibatch training loss = 0.907 and accuracy of 0.78\n",
      "Iteration 7901: with minibatch training loss = 0.616 and accuracy of 0.84\n",
      "Iteration 7902: with minibatch training loss = 0.679 and accuracy of 0.81\n",
      "Iteration 7903: with minibatch training loss = 0.883 and accuracy of 0.72\n",
      "Iteration 7904: with minibatch training loss = 1.1 and accuracy of 0.66\n",
      "Iteration 7905: with minibatch training loss = 0.778 and accuracy of 0.77\n",
      "Iteration 7906: with minibatch training loss = 0.528 and accuracy of 0.86\n",
      "Iteration 7907: with minibatch training loss = 0.683 and accuracy of 0.81\n",
      "Iteration 7908: with minibatch training loss = 0.825 and accuracy of 0.78\n",
      "Iteration 7909: with minibatch training loss = 0.911 and accuracy of 0.77\n",
      "Iteration 7910: with minibatch training loss = 0.883 and accuracy of 0.77\n",
      "Iteration 7911: with minibatch training loss = 0.749 and accuracy of 0.78\n",
      "Iteration 7912: with minibatch training loss = 0.756 and accuracy of 0.77\n",
      "Iteration 7913: with minibatch training loss = 0.832 and accuracy of 0.75\n",
      "Iteration 7914: with minibatch training loss = 1.02 and accuracy of 0.73\n",
      "Iteration 7915: with minibatch training loss = 0.738 and accuracy of 0.8\n",
      "Iteration 7916: with minibatch training loss = 1.06 and accuracy of 0.7\n",
      "Iteration 7917: with minibatch training loss = 0.747 and accuracy of 0.78\n",
      "Iteration 7918: with minibatch training loss = 0.553 and accuracy of 0.84\n",
      "Iteration 7919: with minibatch training loss = 0.863 and accuracy of 0.73\n",
      "Iteration 7920: with minibatch training loss = 0.917 and accuracy of 0.73\n",
      "Iteration 7921: with minibatch training loss = 1.12 and accuracy of 0.69\n",
      "Iteration 7922: with minibatch training loss = 0.845 and accuracy of 0.77\n",
      "Iteration 7923: with minibatch training loss = 1.08 and accuracy of 0.7\n",
      "Iteration 7924: with minibatch training loss = 1.03 and accuracy of 0.75\n",
      "Iteration 7925: with minibatch training loss = 0.907 and accuracy of 0.77\n",
      "Iteration 7926: with minibatch training loss = 0.804 and accuracy of 0.75\n",
      "Iteration 7927: with minibatch training loss = 0.882 and accuracy of 0.75\n",
      "Iteration 7928: with minibatch training loss = 0.94 and accuracy of 0.72\n",
      "Iteration 7929: with minibatch training loss = 0.715 and accuracy of 0.83\n",
      "Iteration 7930: with minibatch training loss = 0.683 and accuracy of 0.81\n",
      "Iteration 7931: with minibatch training loss = 0.867 and accuracy of 0.77\n",
      "Iteration 7932: with minibatch training loss = 0.786 and accuracy of 0.77\n",
      "Iteration 7933: with minibatch training loss = 1 and accuracy of 0.7\n",
      "Iteration 7934: with minibatch training loss = 0.982 and accuracy of 0.75\n",
      "Iteration 7935: with minibatch training loss = 0.963 and accuracy of 0.72\n",
      "Iteration 7936: with minibatch training loss = 0.812 and accuracy of 0.75\n",
      "Iteration 7937: with minibatch training loss = 0.619 and accuracy of 0.84\n",
      "Iteration 7938: with minibatch training loss = 0.906 and accuracy of 0.75\n",
      "Iteration 7939: with minibatch training loss = 0.577 and accuracy of 0.84\n",
      "Iteration 7940: with minibatch training loss = 0.696 and accuracy of 0.8\n",
      "Iteration 7941: with minibatch training loss = 0.6 and accuracy of 0.83\n",
      "Iteration 7942: with minibatch training loss = 0.81 and accuracy of 0.8\n",
      "Iteration 7943: with minibatch training loss = 0.92 and accuracy of 0.72\n",
      "Iteration 7944: with minibatch training loss = 0.768 and accuracy of 0.77\n",
      "Iteration 7945: with minibatch training loss = 0.978 and accuracy of 0.72\n",
      "Iteration 7946: with minibatch training loss = 0.644 and accuracy of 0.83\n",
      "Iteration 7947: with minibatch training loss = 0.804 and accuracy of 0.73\n",
      "Iteration 7948: with minibatch training loss = 0.738 and accuracy of 0.8\n",
      "Iteration 7949: with minibatch training loss = 0.808 and accuracy of 0.77\n",
      "Iteration 7950: with minibatch training loss = 0.842 and accuracy of 0.73\n",
      "Iteration 7951: with minibatch training loss = 0.588 and accuracy of 0.84\n",
      "Iteration 7952: with minibatch training loss = 0.649 and accuracy of 0.81\n",
      "Iteration 7953: with minibatch training loss = 0.821 and accuracy of 0.77\n",
      "Iteration 7954: with minibatch training loss = 0.939 and accuracy of 0.75\n",
      "Iteration 7955: with minibatch training loss = 1.07 and accuracy of 0.69\n",
      "Iteration 7956: with minibatch training loss = 0.745 and accuracy of 0.77\n",
      "Iteration 7957: with minibatch training loss = 0.69 and accuracy of 0.78\n",
      "Iteration 7958: with minibatch training loss = 0.746 and accuracy of 0.78\n",
      "Iteration 7959: with minibatch training loss = 0.854 and accuracy of 0.75\n",
      "Iteration 7960: with minibatch training loss = 0.909 and accuracy of 0.72\n",
      "Iteration 7961: with minibatch training loss = 0.725 and accuracy of 0.81\n",
      "Iteration 7962: with minibatch training loss = 1.03 and accuracy of 0.69\n",
      "Iteration 7963: with minibatch training loss = 0.648 and accuracy of 0.83\n",
      "Iteration 7964: with minibatch training loss = 0.992 and accuracy of 0.73\n",
      "Iteration 7965: with minibatch training loss = 0.586 and accuracy of 0.86\n",
      "Iteration 7966: with minibatch training loss = 0.666 and accuracy of 0.83\n",
      "Iteration 7967: with minibatch training loss = 0.816 and accuracy of 0.73\n",
      "Iteration 7968: with minibatch training loss = 0.776 and accuracy of 0.8\n",
      "Iteration 7969: with minibatch training loss = 0.69 and accuracy of 0.8\n",
      "Iteration 7970: with minibatch training loss = 0.701 and accuracy of 0.83\n",
      "Iteration 7971: with minibatch training loss = 0.722 and accuracy of 0.8\n",
      "Iteration 7972: with minibatch training loss = 0.816 and accuracy of 0.77\n",
      "Iteration 7973: with minibatch training loss = 0.717 and accuracy of 0.8\n",
      "Iteration 7974: with minibatch training loss = 0.963 and accuracy of 0.72\n",
      "Iteration 7975: with minibatch training loss = 1 and accuracy of 0.72\n",
      "Iteration 7976: with minibatch training loss = 0.99 and accuracy of 0.73\n",
      "Iteration 7977: with minibatch training loss = 0.6 and accuracy of 0.81\n",
      "Iteration 7978: with minibatch training loss = 0.837 and accuracy of 0.73\n",
      "Iteration 7979: with minibatch training loss = 0.607 and accuracy of 0.84\n",
      "Iteration 7980: with minibatch training loss = 0.741 and accuracy of 0.78\n",
      "Iteration 7981: with minibatch training loss = 0.791 and accuracy of 0.77\n",
      "Iteration 7982: with minibatch training loss = 0.842 and accuracy of 0.77\n",
      "Iteration 7983: with minibatch training loss = 1.27 and accuracy of 0.64\n",
      "Iteration 7984: with minibatch training loss = 0.856 and accuracy of 0.73\n",
      "Iteration 7985: with minibatch training loss = 0.846 and accuracy of 0.77\n",
      "Iteration 7986: with minibatch training loss = 0.941 and accuracy of 0.73\n",
      "Iteration 7987: with minibatch training loss = 0.644 and accuracy of 0.8\n",
      "Iteration 7988: with minibatch training loss = 0.839 and accuracy of 0.78\n",
      "Iteration 7989: with minibatch training loss = 1.22 and accuracy of 0.66\n",
      "Iteration 7990: with minibatch training loss = 0.911 and accuracy of 0.75\n",
      "Iteration 7991: with minibatch training loss = 0.786 and accuracy of 0.8\n",
      "Iteration 7992: with minibatch training loss = 0.963 and accuracy of 0.73\n",
      "Iteration 7993: with minibatch training loss = 0.679 and accuracy of 0.8\n",
      "Iteration 7994: with minibatch training loss = 0.824 and accuracy of 0.78\n",
      "Iteration 7995: with minibatch training loss = 0.59 and accuracy of 0.84\n",
      "Iteration 7996: with minibatch training loss = 0.889 and accuracy of 0.73\n",
      "Iteration 7997: with minibatch training loss = 0.878 and accuracy of 0.77\n",
      "Iteration 7998: with minibatch training loss = 0.74 and accuracy of 0.8\n",
      "Iteration 7999: with minibatch training loss = 0.791 and accuracy of 0.78\n",
      "Iteration 8000: with minibatch training loss = 0.643 and accuracy of 0.8\n",
      "Iteration 8001: with minibatch training loss = 0.579 and accuracy of 0.83\n",
      "Iteration 8002: with minibatch training loss = 0.827 and accuracy of 0.77\n",
      "Iteration 8003: with minibatch training loss = 1.01 and accuracy of 0.73\n",
      "Iteration 8004: with minibatch training loss = 0.657 and accuracy of 0.83\n",
      "Iteration 8005: with minibatch training loss = 1.08 and accuracy of 0.66\n",
      "Iteration 8006: with minibatch training loss = 1.12 and accuracy of 0.67\n",
      "Iteration 8007: with minibatch training loss = 0.766 and accuracy of 0.77\n",
      "Iteration 8008: with minibatch training loss = 0.85 and accuracy of 0.75\n",
      "Iteration 8009: with minibatch training loss = 1.02 and accuracy of 0.72\n",
      "Iteration 8010: with minibatch training loss = 0.495 and accuracy of 0.84\n",
      "Iteration 8011: with minibatch training loss = 0.608 and accuracy of 0.84\n",
      "Iteration 8012: with minibatch training loss = 0.815 and accuracy of 0.77\n",
      "Iteration 8013: with minibatch training loss = 0.863 and accuracy of 0.77\n",
      "Iteration 8014: with minibatch training loss = 0.809 and accuracy of 0.75\n",
      "Iteration 8015: with minibatch training loss = 0.7 and accuracy of 0.8\n",
      "Iteration 8016: with minibatch training loss = 0.712 and accuracy of 0.81\n",
      "Iteration 8017: with minibatch training loss = 0.788 and accuracy of 0.75\n",
      "Iteration 8018: with minibatch training loss = 0.846 and accuracy of 0.78\n",
      "Iteration 8019: with minibatch training loss = 0.844 and accuracy of 0.75\n",
      "Iteration 8020: with minibatch training loss = 0.889 and accuracy of 0.73\n",
      "Iteration 8021: with minibatch training loss = 0.935 and accuracy of 0.72\n",
      "Iteration 8022: with minibatch training loss = 0.906 and accuracy of 0.75\n",
      "Iteration 8023: with minibatch training loss = 0.43 and accuracy of 0.88\n",
      "Iteration 8024: with minibatch training loss = 0.952 and accuracy of 0.72\n",
      "Iteration 8025: with minibatch training loss = 0.536 and accuracy of 0.83\n",
      "Iteration 8026: with minibatch training loss = 0.862 and accuracy of 0.73\n",
      "Iteration 8027: with minibatch training loss = 0.884 and accuracy of 0.7\n",
      "Iteration 8028: with minibatch training loss = 0.77 and accuracy of 0.78\n",
      "Iteration 8029: with minibatch training loss = 0.965 and accuracy of 0.73\n",
      "Iteration 8030: with minibatch training loss = 0.888 and accuracy of 0.77\n",
      "Iteration 8031: with minibatch training loss = 0.791 and accuracy of 0.78\n",
      "Iteration 8032: with minibatch training loss = 0.807 and accuracy of 0.8\n",
      "Iteration 8033: with minibatch training loss = 1.16 and accuracy of 0.66\n",
      "Iteration 8034: with minibatch training loss = 0.724 and accuracy of 0.81\n",
      "Iteration 8035: with minibatch training loss = 0.928 and accuracy of 0.73\n",
      "Iteration 8036: with minibatch training loss = 0.874 and accuracy of 0.72\n",
      "Iteration 8037: with minibatch training loss = 0.701 and accuracy of 0.78\n",
      "Iteration 8038: with minibatch training loss = 0.746 and accuracy of 0.78\n",
      "Iteration 8039: with minibatch training loss = 0.808 and accuracy of 0.78\n",
      "Iteration 8040: with minibatch training loss = 0.824 and accuracy of 0.73\n",
      "Iteration 8041: with minibatch training loss = 0.623 and accuracy of 0.81\n",
      "Iteration 8042: with minibatch training loss = 0.95 and accuracy of 0.77\n",
      "Iteration 8043: with minibatch training loss = 0.942 and accuracy of 0.73\n",
      "Iteration 8044: with minibatch training loss = 0.907 and accuracy of 0.77\n",
      "Iteration 8045: with minibatch training loss = 1.14 and accuracy of 0.66\n",
      "Iteration 8046: with minibatch training loss = 0.765 and accuracy of 0.77\n",
      "Iteration 8047: with minibatch training loss = 0.722 and accuracy of 0.8\n",
      "Iteration 8048: with minibatch training loss = 1.24 and accuracy of 0.64\n",
      "Iteration 8049: with minibatch training loss = 0.622 and accuracy of 0.83\n",
      "Iteration 8050: with minibatch training loss = 0.565 and accuracy of 0.86\n",
      "Iteration 8051: with minibatch training loss = 0.914 and accuracy of 0.72\n",
      "Iteration 8052: with minibatch training loss = 0.903 and accuracy of 0.75\n",
      "Iteration 8053: with minibatch training loss = 1.19 and accuracy of 0.66\n",
      "Iteration 8054: with minibatch training loss = 0.74 and accuracy of 0.78\n",
      "Iteration 8055: with minibatch training loss = 0.718 and accuracy of 0.83\n",
      "Iteration 8056: with minibatch training loss = 0.91 and accuracy of 0.75\n",
      "Iteration 8057: with minibatch training loss = 0.807 and accuracy of 0.77\n",
      "Iteration 8058: with minibatch training loss = 0.96 and accuracy of 0.72\n",
      "Iteration 8059: with minibatch training loss = 0.956 and accuracy of 0.7\n",
      "Iteration 8060: with minibatch training loss = 0.809 and accuracy of 0.75\n",
      "Iteration 8061: with minibatch training loss = 0.885 and accuracy of 0.75\n",
      "Iteration 8062: with minibatch training loss = 0.92 and accuracy of 0.73\n",
      "Iteration 8063: with minibatch training loss = 0.79 and accuracy of 0.77\n",
      "Iteration 8064: with minibatch training loss = 0.966 and accuracy of 0.72\n",
      "Iteration 8065: with minibatch training loss = 1.05 and accuracy of 0.7\n",
      "Iteration 8066: with minibatch training loss = 0.733 and accuracy of 0.81\n",
      "Iteration 8067: with minibatch training loss = 0.535 and accuracy of 0.83\n",
      "Iteration 8068: with minibatch training loss = 0.801 and accuracy of 0.73\n",
      "Iteration 8069: with minibatch training loss = 0.963 and accuracy of 0.72\n",
      "Iteration 8070: with minibatch training loss = 0.74 and accuracy of 0.77\n",
      "Iteration 8071: with minibatch training loss = 0.738 and accuracy of 0.75\n",
      "Iteration 8072: with minibatch training loss = 0.802 and accuracy of 0.78\n",
      "Iteration 8073: with minibatch training loss = 0.966 and accuracy of 0.73\n",
      "Iteration 8074: with minibatch training loss = 0.9 and accuracy of 0.73\n",
      "Iteration 8075: with minibatch training loss = 1.09 and accuracy of 0.69\n",
      "Iteration 8076: with minibatch training loss = 0.561 and accuracy of 0.84\n",
      "Iteration 8077: with minibatch training loss = 0.548 and accuracy of 0.84\n",
      "Iteration 8078: with minibatch training loss = 1.13 and accuracy of 0.64\n",
      "Iteration 8079: with minibatch training loss = 0.797 and accuracy of 0.73\n",
      "Iteration 8080: with minibatch training loss = 0.919 and accuracy of 0.72\n",
      "Iteration 8081: with minibatch training loss = 0.886 and accuracy of 0.73\n",
      "Iteration 8082: with minibatch training loss = 1.03 and accuracy of 0.69\n",
      "Iteration 8083: with minibatch training loss = 0.772 and accuracy of 0.75\n",
      "Iteration 8084: with minibatch training loss = 0.686 and accuracy of 0.8\n",
      "Iteration 8085: with minibatch training loss = 0.756 and accuracy of 0.77\n",
      "Iteration 8086: with minibatch training loss = 0.71 and accuracy of 0.8\n",
      "Iteration 8087: with minibatch training loss = 0.848 and accuracy of 0.73\n",
      "Iteration 8088: with minibatch training loss = 0.889 and accuracy of 0.75\n",
      "Iteration 8089: with minibatch training loss = 0.695 and accuracy of 0.8\n",
      "Iteration 8090: with minibatch training loss = 0.681 and accuracy of 0.8\n",
      "Iteration 8091: with minibatch training loss = 0.641 and accuracy of 0.83\n",
      "Iteration 8092: with minibatch training loss = 0.825 and accuracy of 0.8\n",
      "Iteration 8093: with minibatch training loss = 0.907 and accuracy of 0.73\n",
      "Iteration 8094: with minibatch training loss = 0.751 and accuracy of 0.83\n",
      "Iteration 8095: with minibatch training loss = 0.857 and accuracy of 0.75\n",
      "Iteration 8096: with minibatch training loss = 0.654 and accuracy of 0.83\n",
      "Iteration 8097: with minibatch training loss = 0.781 and accuracy of 0.78\n",
      "Iteration 8098: with minibatch training loss = 0.802 and accuracy of 0.78\n",
      "Iteration 8099: with minibatch training loss = 0.731 and accuracy of 0.77\n",
      "Iteration 8100: with minibatch training loss = 0.701 and accuracy of 0.8\n",
      "Iteration 8101: with minibatch training loss = 1.01 and accuracy of 0.7\n",
      "Iteration 8102: with minibatch training loss = 0.725 and accuracy of 0.83\n",
      "Iteration 8103: with minibatch training loss = 0.974 and accuracy of 0.7\n",
      "Iteration 8104: with minibatch training loss = 0.669 and accuracy of 0.8\n",
      "Iteration 8105: with minibatch training loss = 0.837 and accuracy of 0.77\n",
      "Iteration 8106: with minibatch training loss = 0.683 and accuracy of 0.83\n",
      "Iteration 8107: with minibatch training loss = 0.831 and accuracy of 0.77\n",
      "Iteration 8108: with minibatch training loss = 0.857 and accuracy of 0.73\n",
      "Iteration 8109: with minibatch training loss = 0.629 and accuracy of 0.83\n",
      "Iteration 8110: with minibatch training loss = 0.667 and accuracy of 0.83\n",
      "Iteration 8111: with minibatch training loss = 0.834 and accuracy of 0.73\n",
      "Iteration 8112: with minibatch training loss = 0.575 and accuracy of 0.83\n",
      "Iteration 8113: with minibatch training loss = 0.543 and accuracy of 0.84\n",
      "Iteration 8114: with minibatch training loss = 0.831 and accuracy of 0.77\n",
      "Iteration 8115: with minibatch training loss = 0.644 and accuracy of 0.83\n",
      "Iteration 8116: with minibatch training loss = 0.781 and accuracy of 0.78\n",
      "Iteration 8117: with minibatch training loss = 0.72 and accuracy of 0.8\n",
      "Iteration 8118: with minibatch training loss = 0.837 and accuracy of 0.77\n",
      "Iteration 8119: with minibatch training loss = 0.797 and accuracy of 0.77\n",
      "Iteration 8120: with minibatch training loss = 0.701 and accuracy of 0.8\n",
      "Iteration 8121: with minibatch training loss = 0.871 and accuracy of 0.73\n",
      "Iteration 8122: with minibatch training loss = 1.01 and accuracy of 0.7\n",
      "Iteration 8123: with minibatch training loss = 0.75 and accuracy of 0.77\n",
      "Iteration 8124: with minibatch training loss = 0.693 and accuracy of 0.78\n",
      "Iteration 8125: with minibatch training loss = 0.822 and accuracy of 0.77\n",
      "Iteration 8126: with minibatch training loss = 0.797 and accuracy of 0.77\n",
      "Iteration 8127: with minibatch training loss = 0.845 and accuracy of 0.8\n",
      "Iteration 8128: with minibatch training loss = 0.936 and accuracy of 0.73\n",
      "Iteration 8129: with minibatch training loss = 0.876 and accuracy of 0.72\n",
      "Iteration 8130: with minibatch training loss = 0.989 and accuracy of 0.72\n",
      "Iteration 8131: with minibatch training loss = 0.729 and accuracy of 0.78\n",
      "Iteration 8132: with minibatch training loss = 0.952 and accuracy of 0.7\n",
      "Iteration 8133: with minibatch training loss = 0.539 and accuracy of 0.86\n",
      "Iteration 8134: with minibatch training loss = 0.806 and accuracy of 0.78\n",
      "Iteration 8135: with minibatch training loss = 0.61 and accuracy of 0.78\n",
      "Iteration 8136: with minibatch training loss = 0.745 and accuracy of 0.78\n",
      "Iteration 8137: with minibatch training loss = 0.714 and accuracy of 0.8\n",
      "Iteration 8138: with minibatch training loss = 0.779 and accuracy of 0.75\n",
      "Iteration 8139: with minibatch training loss = 0.904 and accuracy of 0.72\n",
      "Iteration 8140: with minibatch training loss = 0.833 and accuracy of 0.77\n",
      "Iteration 8141: with minibatch training loss = 0.752 and accuracy of 0.77\n",
      "Iteration 8142: with minibatch training loss = 0.513 and accuracy of 0.86\n",
      "Iteration 8143: with minibatch training loss = 0.73 and accuracy of 0.81\n",
      "Iteration 8144: with minibatch training loss = 0.642 and accuracy of 0.81\n",
      "Iteration 8145: with minibatch training loss = 0.764 and accuracy of 0.8\n",
      "Iteration 8146: with minibatch training loss = 0.888 and accuracy of 0.8\n",
      "Iteration 8147: with minibatch training loss = 0.777 and accuracy of 0.78\n",
      "Iteration 8148: with minibatch training loss = 1.05 and accuracy of 0.69\n",
      "Iteration 8149: with minibatch training loss = 0.839 and accuracy of 0.77\n",
      "Iteration 8150: with minibatch training loss = 0.696 and accuracy of 0.8\n",
      "Iteration 8151: with minibatch training loss = 0.99 and accuracy of 0.72\n",
      "Iteration 8152: with minibatch training loss = 1.02 and accuracy of 0.73\n",
      "Iteration 8153: with minibatch training loss = 0.905 and accuracy of 0.77\n",
      "Iteration 8154: with minibatch training loss = 0.961 and accuracy of 0.7\n",
      "Iteration 8155: with minibatch training loss = 0.535 and accuracy of 0.84\n",
      "Iteration 8156: with minibatch training loss = 0.879 and accuracy of 0.77\n",
      "Iteration 8157: with minibatch training loss = 0.711 and accuracy of 0.8\n",
      "Iteration 8158: with minibatch training loss = 0.631 and accuracy of 0.81\n",
      "Iteration 8159: with minibatch training loss = 1.02 and accuracy of 0.72\n",
      "Iteration 8160: with minibatch training loss = 1 and accuracy of 0.69\n",
      "Iteration 8161: with minibatch training loss = 0.855 and accuracy of 0.73\n",
      "Iteration 8162: with minibatch training loss = 0.876 and accuracy of 0.77\n",
      "Iteration 8163: with minibatch training loss = 0.825 and accuracy of 0.77\n",
      "Iteration 8164: with minibatch training loss = 0.959 and accuracy of 0.77\n",
      "Iteration 8165: with minibatch training loss = 0.753 and accuracy of 0.77\n",
      "Iteration 8166: with minibatch training loss = 0.925 and accuracy of 0.77\n",
      "Iteration 8167: with minibatch training loss = 0.512 and accuracy of 0.88\n",
      "Iteration 8168: with minibatch training loss = 0.806 and accuracy of 0.77\n",
      "Iteration 8169: with minibatch training loss = 0.996 and accuracy of 0.73\n",
      "Iteration 8170: with minibatch training loss = 0.914 and accuracy of 0.77\n",
      "Iteration 8171: with minibatch training loss = 0.635 and accuracy of 0.83\n",
      "Iteration 8172: with minibatch training loss = 1.1 and accuracy of 0.66\n",
      "Iteration 8173: with minibatch training loss = 0.857 and accuracy of 0.73\n",
      "Iteration 8174: with minibatch training loss = 0.717 and accuracy of 0.78\n",
      "Iteration 8175: with minibatch training loss = 0.379 and accuracy of 0.89\n",
      "Iteration 8176: with minibatch training loss = 0.864 and accuracy of 0.77\n",
      "Iteration 8177: with minibatch training loss = 0.779 and accuracy of 0.78\n",
      "Iteration 8178: with minibatch training loss = 0.798 and accuracy of 0.77\n",
      "Iteration 8179: with minibatch training loss = 0.778 and accuracy of 0.78\n",
      "Iteration 8180: with minibatch training loss = 0.905 and accuracy of 0.77\n",
      "Iteration 8181: with minibatch training loss = 0.999 and accuracy of 0.69\n",
      "Iteration 8182: with minibatch training loss = 0.899 and accuracy of 0.73\n",
      "Iteration 8183: with minibatch training loss = 0.759 and accuracy of 0.77\n",
      "Iteration 8184: with minibatch training loss = 0.676 and accuracy of 0.81\n",
      "Iteration 8185: with minibatch training loss = 1.02 and accuracy of 0.69\n",
      "Iteration 8186: with minibatch training loss = 0.914 and accuracy of 0.73\n",
      "Iteration 8187: with minibatch training loss = 0.687 and accuracy of 0.78\n",
      "Iteration 8188: with minibatch training loss = 1.02 and accuracy of 0.73\n",
      "Iteration 8189: with minibatch training loss = 0.731 and accuracy of 0.77\n",
      "Iteration 8190: with minibatch training loss = 0.833 and accuracy of 0.8\n",
      "Iteration 8191: with minibatch training loss = 1.14 and accuracy of 0.67\n",
      "Iteration 8192: with minibatch training loss = 0.623 and accuracy of 0.81\n",
      "Iteration 8193: with minibatch training loss = 0.817 and accuracy of 0.77\n",
      "Iteration 8194: with minibatch training loss = 0.623 and accuracy of 0.81\n",
      "Iteration 8195: with minibatch training loss = 0.541 and accuracy of 0.86\n",
      "Iteration 8196: with minibatch training loss = 0.618 and accuracy of 0.88\n",
      "Iteration 8197: with minibatch training loss = 0.646 and accuracy of 0.83\n",
      "Iteration 8198: with minibatch training loss = 0.77 and accuracy of 0.78\n",
      "Iteration 8199: with minibatch training loss = 1.21 and accuracy of 0.64\n",
      "Iteration 8200: with minibatch training loss = 0.957 and accuracy of 0.69\n",
      "Iteration 8201: with minibatch training loss = 0.604 and accuracy of 0.86\n",
      "Iteration 8202: with minibatch training loss = 0.776 and accuracy of 0.78\n",
      "Iteration 8203: with minibatch training loss = 0.708 and accuracy of 0.8\n",
      "Iteration 8204: with minibatch training loss = 1.11 and accuracy of 0.66\n",
      "Iteration 8205: with minibatch training loss = 0.997 and accuracy of 0.7\n",
      "Iteration 8206: with minibatch training loss = 1.04 and accuracy of 0.7\n",
      "Iteration 8207: with minibatch training loss = 0.747 and accuracy of 0.8\n",
      "Iteration 8208: with minibatch training loss = 0.859 and accuracy of 0.75\n",
      "Iteration 8209: with minibatch training loss = 0.84 and accuracy of 0.81\n",
      "Iteration 8210: with minibatch training loss = 0.587 and accuracy of 0.86\n",
      "Iteration 8211: with minibatch training loss = 0.847 and accuracy of 0.72\n",
      "Iteration 8212: with minibatch training loss = 0.688 and accuracy of 0.8\n",
      "Iteration 8213: with minibatch training loss = 0.971 and accuracy of 0.73\n",
      "Iteration 8214: with minibatch training loss = 0.676 and accuracy of 0.8\n",
      "Iteration 8215: with minibatch training loss = 0.91 and accuracy of 0.73\n",
      "Iteration 8216: with minibatch training loss = 0.786 and accuracy of 0.77\n",
      "Iteration 8217: with minibatch training loss = 0.573 and accuracy of 0.83\n",
      "Iteration 8218: with minibatch training loss = 0.874 and accuracy of 0.73\n",
      "Iteration 8219: with minibatch training loss = 1.09 and accuracy of 0.7\n",
      "Iteration 8220: with minibatch training loss = 0.737 and accuracy of 0.78\n",
      "Iteration 8221: with minibatch training loss = 0.729 and accuracy of 0.81\n",
      "Iteration 8222: with minibatch training loss = 0.479 and accuracy of 0.86\n",
      "Iteration 8223: with minibatch training loss = 0.994 and accuracy of 0.7\n",
      "Iteration 8224: with minibatch training loss = 0.701 and accuracy of 0.83\n",
      "Iteration 8225: with minibatch training loss = 0.761 and accuracy of 0.78\n",
      "Iteration 8226: with minibatch training loss = 1.19 and accuracy of 0.66\n",
      "Iteration 8227: with minibatch training loss = 0.858 and accuracy of 0.78\n",
      "Iteration 8228: with minibatch training loss = 0.629 and accuracy of 0.86\n",
      "Iteration 8229: with minibatch training loss = 0.979 and accuracy of 0.75\n",
      "Iteration 8230: with minibatch training loss = 0.565 and accuracy of 0.88\n",
      "Iteration 8231: with minibatch training loss = 0.655 and accuracy of 0.81\n",
      "Iteration 8232: with minibatch training loss = 0.699 and accuracy of 0.8\n",
      "Iteration 8233: with minibatch training loss = 0.669 and accuracy of 0.84\n",
      "Iteration 8234: with minibatch training loss = 0.438 and accuracy of 0.89\n",
      "Iteration 8235: with minibatch training loss = 0.706 and accuracy of 0.81\n",
      "Iteration 8236: with minibatch training loss = 0.783 and accuracy of 0.78\n",
      "Iteration 8237: with minibatch training loss = 0.914 and accuracy of 0.72\n",
      "Iteration 8238: with minibatch training loss = 0.733 and accuracy of 0.8\n",
      "Iteration 8239: with minibatch training loss = 0.691 and accuracy of 0.83\n",
      "Iteration 8240: with minibatch training loss = 0.917 and accuracy of 0.75\n",
      "Iteration 8241: with minibatch training loss = 0.941 and accuracy of 0.72\n",
      "Iteration 8242: with minibatch training loss = 0.772 and accuracy of 0.78\n",
      "Iteration 8243: with minibatch training loss = 0.574 and accuracy of 0.84\n",
      "Iteration 8244: with minibatch training loss = 0.726 and accuracy of 0.77\n",
      "Iteration 8245: with minibatch training loss = 0.662 and accuracy of 0.83\n",
      "Iteration 8246: with minibatch training loss = 0.723 and accuracy of 0.8\n",
      "Iteration 8247: with minibatch training loss = 0.573 and accuracy of 0.84\n",
      "Iteration 8248: with minibatch training loss = 0.878 and accuracy of 0.77\n",
      "Iteration 8249: with minibatch training loss = 0.975 and accuracy of 0.7\n",
      "Iteration 8250: with minibatch training loss = 1.01 and accuracy of 0.72\n",
      "Iteration 8251: with minibatch training loss = 0.984 and accuracy of 0.72\n",
      "Iteration 8252: with minibatch training loss = 0.973 and accuracy of 0.7\n",
      "Iteration 8253: with minibatch training loss = 1.05 and accuracy of 0.7\n",
      "Iteration 8254: with minibatch training loss = 0.781 and accuracy of 0.77\n",
      "Iteration 8255: with minibatch training loss = 0.858 and accuracy of 0.73\n",
      "Iteration 8256: with minibatch training loss = 0.746 and accuracy of 0.77\n",
      "Iteration 8257: with minibatch training loss = 0.905 and accuracy of 0.75\n",
      "Iteration 8258: with minibatch training loss = 0.557 and accuracy of 0.84\n",
      "Iteration 8259: with minibatch training loss = 0.875 and accuracy of 0.75\n",
      "Iteration 8260: with minibatch training loss = 0.669 and accuracy of 0.8\n",
      "Iteration 8261: with minibatch training loss = 0.535 and accuracy of 0.88\n",
      "Iteration 8262: with minibatch training loss = 0.894 and accuracy of 0.75\n",
      "Iteration 8263: with minibatch training loss = 0.812 and accuracy of 0.78\n",
      "Iteration 8264: with minibatch training loss = 1.03 and accuracy of 0.7\n",
      "Iteration 8265: with minibatch training loss = 0.948 and accuracy of 0.77\n",
      "Iteration 8266: with minibatch training loss = 0.713 and accuracy of 0.77\n",
      "Iteration 8267: with minibatch training loss = 1.1 and accuracy of 0.69\n",
      "Iteration 8268: with minibatch training loss = 0.695 and accuracy of 0.78\n",
      "Iteration 8269: with minibatch training loss = 1.01 and accuracy of 0.69\n",
      "Iteration 8270: with minibatch training loss = 0.837 and accuracy of 0.8\n",
      "Iteration 8271: with minibatch training loss = 0.93 and accuracy of 0.73\n",
      "Iteration 8272: with minibatch training loss = 0.587 and accuracy of 0.78\n",
      "Iteration 8273: with minibatch training loss = 0.698 and accuracy of 0.83\n",
      "Iteration 8274: with minibatch training loss = 0.848 and accuracy of 0.8\n",
      "Iteration 8275: with minibatch training loss = 0.634 and accuracy of 0.83\n",
      "Iteration 8276: with minibatch training loss = 1 and accuracy of 0.7\n",
      "Iteration 8277: with minibatch training loss = 0.562 and accuracy of 0.84\n",
      "Iteration 8278: with minibatch training loss = 0.973 and accuracy of 0.7\n",
      "Iteration 8279: with minibatch training loss = 0.599 and accuracy of 0.84\n",
      "Iteration 8280: with minibatch training loss = 0.732 and accuracy of 0.84\n",
      "Iteration 8281: with minibatch training loss = 0.785 and accuracy of 0.77\n",
      "Iteration 8282: with minibatch training loss = 1.23 and accuracy of 0.64\n",
      "Iteration 8283: with minibatch training loss = 0.854 and accuracy of 0.77\n",
      "Iteration 8284: with minibatch training loss = 0.912 and accuracy of 0.72\n",
      "Iteration 8285: with minibatch training loss = 0.96 and accuracy of 0.72\n",
      "Iteration 8286: with minibatch training loss = 0.682 and accuracy of 0.83\n",
      "Iteration 8287: with minibatch training loss = 0.687 and accuracy of 0.8\n",
      "Iteration 8288: with minibatch training loss = 0.971 and accuracy of 0.69\n",
      "Iteration 8289: with minibatch training loss = 0.712 and accuracy of 0.83\n",
      "Iteration 8290: with minibatch training loss = 0.856 and accuracy of 0.8\n",
      "Iteration 8291: with minibatch training loss = 0.601 and accuracy of 0.83\n",
      "Iteration 8292: with minibatch training loss = 0.758 and accuracy of 0.77\n",
      "Iteration 8293: with minibatch training loss = 0.861 and accuracy of 0.78\n",
      "Iteration 8294: with minibatch training loss = 0.569 and accuracy of 0.8\n",
      "Iteration 8295: with minibatch training loss = 0.693 and accuracy of 0.83\n",
      "Iteration 8296: with minibatch training loss = 0.869 and accuracy of 0.75\n",
      "Iteration 8297: with minibatch training loss = 0.79 and accuracy of 0.73\n",
      "Iteration 8298: with minibatch training loss = 0.954 and accuracy of 0.69\n",
      "Iteration 8299: with minibatch training loss = 0.892 and accuracy of 0.78\n",
      "Iteration 8300: with minibatch training loss = 0.837 and accuracy of 0.75\n",
      "Iteration 8301: with minibatch training loss = 0.947 and accuracy of 0.72\n",
      "Iteration 8302: with minibatch training loss = 0.839 and accuracy of 0.77\n",
      "Iteration 8303: with minibatch training loss = 0.891 and accuracy of 0.75\n",
      "Iteration 8304: with minibatch training loss = 0.862 and accuracy of 0.78\n",
      "Iteration 8305: with minibatch training loss = 0.934 and accuracy of 0.7\n",
      "Iteration 8306: with minibatch training loss = 0.821 and accuracy of 0.78\n",
      "Iteration 8307: with minibatch training loss = 0.768 and accuracy of 0.8\n",
      "Iteration 8308: with minibatch training loss = 0.958 and accuracy of 0.72\n",
      "Iteration 8309: with minibatch training loss = 1.17 and accuracy of 0.66\n",
      "Iteration 8310: with minibatch training loss = 0.622 and accuracy of 0.84\n",
      "Iteration 8311: with minibatch training loss = 0.734 and accuracy of 0.77\n",
      "Iteration 8312: with minibatch training loss = 1.22 and accuracy of 0.59\n",
      "Iteration 8313: with minibatch training loss = 0.708 and accuracy of 0.8\n",
      "Iteration 8314: with minibatch training loss = 0.729 and accuracy of 0.81\n",
      "Iteration 8315: with minibatch training loss = 0.835 and accuracy of 0.77\n",
      "Iteration 8316: with minibatch training loss = 1 and accuracy of 0.7\n",
      "Iteration 8317: with minibatch training loss = 0.704 and accuracy of 0.81\n",
      "Iteration 8318: with minibatch training loss = 0.585 and accuracy of 0.84\n",
      "Iteration 8319: with minibatch training loss = 0.861 and accuracy of 0.8\n",
      "Iteration 8320: with minibatch training loss = 1.17 and accuracy of 0.66\n",
      "Iteration 8321: with minibatch training loss = 0.802 and accuracy of 0.75\n",
      "Iteration 8322: with minibatch training loss = 0.736 and accuracy of 0.78\n",
      "Iteration 8323: with minibatch training loss = 0.858 and accuracy of 0.73\n",
      "Iteration 8324: with minibatch training loss = 0.716 and accuracy of 0.8\n",
      "Iteration 8325: with minibatch training loss = 0.671 and accuracy of 0.83\n",
      "Iteration 8326: with minibatch training loss = 0.546 and accuracy of 0.84\n",
      "Validation loss: 0.28716648\n",
      "Epoch 6, Overall loss = 0.826 and accuracy of 0.766\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCoJhGIaRwgqCYRiGkfL/Ab4wQqfXOpLPAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x121599278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8328: with minibatch training loss = 1.01 and accuracy of 0.69\n",
      "Iteration 8329: with minibatch training loss = 0.779 and accuracy of 0.77\n",
      "Iteration 8330: with minibatch training loss = 0.828 and accuracy of 0.77\n",
      "Iteration 8331: with minibatch training loss = 0.982 and accuracy of 0.72\n",
      "Iteration 8332: with minibatch training loss = 0.703 and accuracy of 0.81\n",
      "Iteration 8333: with minibatch training loss = 0.764 and accuracy of 0.8\n",
      "Iteration 8334: with minibatch training loss = 0.968 and accuracy of 0.73\n",
      "Iteration 8335: with minibatch training loss = 0.657 and accuracy of 0.83\n",
      "Iteration 8336: with minibatch training loss = 0.697 and accuracy of 0.78\n",
      "Iteration 8337: with minibatch training loss = 0.731 and accuracy of 0.77\n",
      "Iteration 8338: with minibatch training loss = 0.815 and accuracy of 0.78\n",
      "Iteration 8339: with minibatch training loss = 0.569 and accuracy of 0.86\n",
      "Iteration 8340: with minibatch training loss = 0.833 and accuracy of 0.77\n",
      "Iteration 8341: with minibatch training loss = 0.94 and accuracy of 0.73\n",
      "Iteration 8342: with minibatch training loss = 0.831 and accuracy of 0.77\n",
      "Iteration 8343: with minibatch training loss = 0.624 and accuracy of 0.81\n",
      "Iteration 8344: with minibatch training loss = 0.85 and accuracy of 0.78\n",
      "Iteration 8345: with minibatch training loss = 0.583 and accuracy of 0.86\n",
      "Iteration 8346: with minibatch training loss = 0.731 and accuracy of 0.78\n",
      "Iteration 8347: with minibatch training loss = 0.977 and accuracy of 0.73\n",
      "Iteration 8348: with minibatch training loss = 0.614 and accuracy of 0.83\n",
      "Iteration 8349: with minibatch training loss = 0.758 and accuracy of 0.78\n",
      "Iteration 8350: with minibatch training loss = 0.758 and accuracy of 0.78\n",
      "Iteration 8351: with minibatch training loss = 0.755 and accuracy of 0.8\n",
      "Iteration 8352: with minibatch training loss = 0.486 and accuracy of 0.89\n",
      "Iteration 8353: with minibatch training loss = 0.784 and accuracy of 0.78\n",
      "Iteration 8354: with minibatch training loss = 0.999 and accuracy of 0.73\n",
      "Iteration 8355: with minibatch training loss = 0.888 and accuracy of 0.77\n",
      "Iteration 8356: with minibatch training loss = 0.739 and accuracy of 0.8\n",
      "Iteration 8357: with minibatch training loss = 0.873 and accuracy of 0.75\n",
      "Iteration 8358: with minibatch training loss = 0.769 and accuracy of 0.86\n",
      "Iteration 8359: with minibatch training loss = 0.953 and accuracy of 0.72\n",
      "Iteration 8360: with minibatch training loss = 0.806 and accuracy of 0.75\n",
      "Iteration 8361: with minibatch training loss = 0.754 and accuracy of 0.81\n",
      "Iteration 8362: with minibatch training loss = 1.01 and accuracy of 0.69\n",
      "Iteration 8363: with minibatch training loss = 1.08 and accuracy of 0.7\n",
      "Iteration 8364: with minibatch training loss = 0.812 and accuracy of 0.75\n",
      "Iteration 8365: with minibatch training loss = 0.833 and accuracy of 0.8\n",
      "Iteration 8366: with minibatch training loss = 0.713 and accuracy of 0.83\n",
      "Iteration 8367: with minibatch training loss = 0.823 and accuracy of 0.78\n",
      "Iteration 8368: with minibatch training loss = 1.1 and accuracy of 0.7\n",
      "Iteration 8369: with minibatch training loss = 0.973 and accuracy of 0.69\n",
      "Iteration 8370: with minibatch training loss = 0.669 and accuracy of 0.83\n",
      "Iteration 8371: with minibatch training loss = 0.654 and accuracy of 0.8\n",
      "Iteration 8372: with minibatch training loss = 0.812 and accuracy of 0.73\n",
      "Iteration 8373: with minibatch training loss = 0.91 and accuracy of 0.75\n",
      "Iteration 8374: with minibatch training loss = 0.776 and accuracy of 0.77\n",
      "Iteration 8375: with minibatch training loss = 0.8 and accuracy of 0.75\n",
      "Iteration 8376: with minibatch training loss = 0.944 and accuracy of 0.73\n",
      "Iteration 8377: with minibatch training loss = 0.517 and accuracy of 0.84\n",
      "Iteration 8378: with minibatch training loss = 0.866 and accuracy of 0.73\n",
      "Iteration 8379: with minibatch training loss = 0.771 and accuracy of 0.75\n",
      "Iteration 8380: with minibatch training loss = 0.732 and accuracy of 0.78\n",
      "Iteration 8381: with minibatch training loss = 1.12 and accuracy of 0.67\n",
      "Iteration 8382: with minibatch training loss = 0.605 and accuracy of 0.83\n",
      "Iteration 8383: with minibatch training loss = 0.777 and accuracy of 0.78\n",
      "Iteration 8384: with minibatch training loss = 0.817 and accuracy of 0.78\n",
      "Iteration 8385: with minibatch training loss = 0.997 and accuracy of 0.7\n",
      "Iteration 8386: with minibatch training loss = 0.745 and accuracy of 0.77\n",
      "Iteration 8387: with minibatch training loss = 0.949 and accuracy of 0.72\n",
      "Iteration 8388: with minibatch training loss = 0.764 and accuracy of 0.78\n",
      "Iteration 8389: with minibatch training loss = 1.07 and accuracy of 0.67\n",
      "Iteration 8390: with minibatch training loss = 1.01 and accuracy of 0.72\n",
      "Iteration 8391: with minibatch training loss = 1 and accuracy of 0.67\n",
      "Iteration 8392: with minibatch training loss = 0.765 and accuracy of 0.78\n",
      "Iteration 8393: with minibatch training loss = 0.905 and accuracy of 0.73\n",
      "Iteration 8394: with minibatch training loss = 0.727 and accuracy of 0.77\n",
      "Iteration 8395: with minibatch training loss = 0.734 and accuracy of 0.81\n",
      "Iteration 8396: with minibatch training loss = 0.768 and accuracy of 0.77\n",
      "Iteration 8397: with minibatch training loss = 1.07 and accuracy of 0.66\n",
      "Iteration 8398: with minibatch training loss = 0.483 and accuracy of 0.88\n",
      "Iteration 8399: with minibatch training loss = 0.971 and accuracy of 0.73\n",
      "Iteration 8400: with minibatch training loss = 0.693 and accuracy of 0.84\n",
      "Iteration 8401: with minibatch training loss = 0.645 and accuracy of 0.81\n",
      "Iteration 8402: with minibatch training loss = 0.839 and accuracy of 0.72\n",
      "Iteration 8403: with minibatch training loss = 0.317 and accuracy of 0.94\n",
      "Iteration 8404: with minibatch training loss = 1.02 and accuracy of 0.75\n",
      "Iteration 8405: with minibatch training loss = 0.846 and accuracy of 0.73\n",
      "Iteration 8406: with minibatch training loss = 0.923 and accuracy of 0.73\n",
      "Iteration 8407: with minibatch training loss = 0.654 and accuracy of 0.78\n",
      "Iteration 8408: with minibatch training loss = 0.777 and accuracy of 0.8\n",
      "Iteration 8409: with minibatch training loss = 1.17 and accuracy of 0.67\n",
      "Iteration 8410: with minibatch training loss = 0.829 and accuracy of 0.77\n",
      "Iteration 8411: with minibatch training loss = 0.655 and accuracy of 0.8\n",
      "Iteration 8412: with minibatch training loss = 0.83 and accuracy of 0.75\n",
      "Iteration 8413: with minibatch training loss = 0.725 and accuracy of 0.8\n",
      "Iteration 8414: with minibatch training loss = 0.729 and accuracy of 0.77\n",
      "Iteration 8415: with minibatch training loss = 0.985 and accuracy of 0.73\n",
      "Iteration 8416: with minibatch training loss = 0.593 and accuracy of 0.83\n",
      "Iteration 8417: with minibatch training loss = 0.699 and accuracy of 0.8\n",
      "Iteration 8418: with minibatch training loss = 0.945 and accuracy of 0.77\n",
      "Iteration 8419: with minibatch training loss = 0.813 and accuracy of 0.75\n",
      "Iteration 8420: with minibatch training loss = 0.565 and accuracy of 0.86\n",
      "Iteration 8421: with minibatch training loss = 0.891 and accuracy of 0.77\n",
      "Iteration 8422: with minibatch training loss = 0.802 and accuracy of 0.73\n",
      "Iteration 8423: with minibatch training loss = 0.696 and accuracy of 0.78\n",
      "Iteration 8424: with minibatch training loss = 0.803 and accuracy of 0.77\n",
      "Iteration 8425: with minibatch training loss = 0.75 and accuracy of 0.78\n",
      "Iteration 8426: with minibatch training loss = 1.07 and accuracy of 0.67\n",
      "Iteration 8427: with minibatch training loss = 0.723 and accuracy of 0.77\n",
      "Iteration 8428: with minibatch training loss = 0.764 and accuracy of 0.78\n",
      "Iteration 8429: with minibatch training loss = 0.857 and accuracy of 0.73\n",
      "Iteration 8430: with minibatch training loss = 0.554 and accuracy of 0.83\n",
      "Iteration 8431: with minibatch training loss = 0.809 and accuracy of 0.8\n",
      "Iteration 8432: with minibatch training loss = 0.499 and accuracy of 0.86\n",
      "Iteration 8433: with minibatch training loss = 0.616 and accuracy of 0.84\n",
      "Iteration 8434: with minibatch training loss = 0.752 and accuracy of 0.75\n",
      "Iteration 8435: with minibatch training loss = 0.818 and accuracy of 0.73\n",
      "Iteration 8436: with minibatch training loss = 0.871 and accuracy of 0.75\n",
      "Iteration 8437: with minibatch training loss = 0.677 and accuracy of 0.8\n",
      "Iteration 8438: with minibatch training loss = 0.778 and accuracy of 0.77\n",
      "Iteration 8439: with minibatch training loss = 0.866 and accuracy of 0.77\n",
      "Iteration 8440: with minibatch training loss = 1 and accuracy of 0.72\n",
      "Iteration 8441: with minibatch training loss = 0.497 and accuracy of 0.86\n",
      "Iteration 8442: with minibatch training loss = 0.779 and accuracy of 0.78\n",
      "Iteration 8443: with minibatch training loss = 0.879 and accuracy of 0.75\n",
      "Iteration 8444: with minibatch training loss = 0.639 and accuracy of 0.83\n",
      "Iteration 8445: with minibatch training loss = 0.942 and accuracy of 0.77\n",
      "Iteration 8446: with minibatch training loss = 0.994 and accuracy of 0.69\n",
      "Iteration 8447: with minibatch training loss = 0.7 and accuracy of 0.8\n",
      "Iteration 8448: with minibatch training loss = 1.09 and accuracy of 0.72\n",
      "Iteration 8449: with minibatch training loss = 0.556 and accuracy of 0.84\n",
      "Iteration 8450: with minibatch training loss = 0.664 and accuracy of 0.8\n",
      "Iteration 8451: with minibatch training loss = 0.606 and accuracy of 0.81\n",
      "Iteration 8452: with minibatch training loss = 0.828 and accuracy of 0.77\n",
      "Iteration 8453: with minibatch training loss = 0.729 and accuracy of 0.81\n",
      "Iteration 8454: with minibatch training loss = 1.13 and accuracy of 0.7\n",
      "Iteration 8455: with minibatch training loss = 0.778 and accuracy of 0.78\n",
      "Iteration 8456: with minibatch training loss = 0.54 and accuracy of 0.86\n",
      "Iteration 8457: with minibatch training loss = 0.959 and accuracy of 0.72\n",
      "Iteration 8458: with minibatch training loss = 0.766 and accuracy of 0.78\n",
      "Iteration 8459: with minibatch training loss = 0.572 and accuracy of 0.84\n",
      "Iteration 8460: with minibatch training loss = 0.969 and accuracy of 0.7\n",
      "Iteration 8461: with minibatch training loss = 1.09 and accuracy of 0.69\n",
      "Iteration 8462: with minibatch training loss = 0.788 and accuracy of 0.75\n",
      "Iteration 8463: with minibatch training loss = 0.763 and accuracy of 0.77\n",
      "Iteration 8464: with minibatch training loss = 0.83 and accuracy of 0.78\n",
      "Iteration 8465: with minibatch training loss = 0.978 and accuracy of 0.72\n",
      "Iteration 8466: with minibatch training loss = 0.824 and accuracy of 0.77\n",
      "Iteration 8467: with minibatch training loss = 0.874 and accuracy of 0.77\n",
      "Iteration 8468: with minibatch training loss = 0.819 and accuracy of 0.75\n",
      "Iteration 8469: with minibatch training loss = 0.891 and accuracy of 0.75\n",
      "Iteration 8470: with minibatch training loss = 1.04 and accuracy of 0.72\n",
      "Iteration 8471: with minibatch training loss = 0.664 and accuracy of 0.81\n",
      "Iteration 8472: with minibatch training loss = 0.897 and accuracy of 0.73\n",
      "Iteration 8473: with minibatch training loss = 0.716 and accuracy of 0.78\n",
      "Iteration 8474: with minibatch training loss = 0.746 and accuracy of 0.8\n",
      "Iteration 8475: with minibatch training loss = 0.38 and accuracy of 0.91\n",
      "Iteration 8476: with minibatch training loss = 0.663 and accuracy of 0.83\n",
      "Iteration 8477: with minibatch training loss = 0.642 and accuracy of 0.81\n",
      "Iteration 8478: with minibatch training loss = 0.955 and accuracy of 0.73\n",
      "Iteration 8479: with minibatch training loss = 0.717 and accuracy of 0.81\n",
      "Iteration 8480: with minibatch training loss = 0.761 and accuracy of 0.77\n",
      "Iteration 8481: with minibatch training loss = 0.919 and accuracy of 0.73\n",
      "Iteration 8482: with minibatch training loss = 0.61 and accuracy of 0.86\n",
      "Iteration 8483: with minibatch training loss = 0.848 and accuracy of 0.77\n",
      "Iteration 8484: with minibatch training loss = 0.691 and accuracy of 0.78\n",
      "Iteration 8485: with minibatch training loss = 0.381 and accuracy of 0.91\n",
      "Iteration 8486: with minibatch training loss = 0.896 and accuracy of 0.77\n",
      "Iteration 8487: with minibatch training loss = 0.985 and accuracy of 0.69\n",
      "Iteration 8488: with minibatch training loss = 0.664 and accuracy of 0.81\n",
      "Iteration 8489: with minibatch training loss = 0.963 and accuracy of 0.7\n",
      "Iteration 8490: with minibatch training loss = 0.654 and accuracy of 0.84\n",
      "Iteration 8491: with minibatch training loss = 0.749 and accuracy of 0.77\n",
      "Iteration 8492: with minibatch training loss = 0.54 and accuracy of 0.84\n",
      "Iteration 8493: with minibatch training loss = 0.666 and accuracy of 0.83\n",
      "Iteration 8494: with minibatch training loss = 0.813 and accuracy of 0.78\n",
      "Iteration 8495: with minibatch training loss = 0.78 and accuracy of 0.77\n",
      "Iteration 8496: with minibatch training loss = 0.731 and accuracy of 0.78\n",
      "Iteration 8497: with minibatch training loss = 0.788 and accuracy of 0.8\n",
      "Iteration 8498: with minibatch training loss = 0.79 and accuracy of 0.78\n",
      "Iteration 8499: with minibatch training loss = 0.772 and accuracy of 0.77\n",
      "Iteration 8500: with minibatch training loss = 0.728 and accuracy of 0.81\n",
      "Iteration 8501: with minibatch training loss = 0.643 and accuracy of 0.81\n",
      "Iteration 8502: with minibatch training loss = 0.845 and accuracy of 0.73\n",
      "Iteration 8503: with minibatch training loss = 0.677 and accuracy of 0.83\n",
      "Iteration 8504: with minibatch training loss = 0.829 and accuracy of 0.72\n",
      "Iteration 8505: with minibatch training loss = 0.602 and accuracy of 0.84\n",
      "Iteration 8506: with minibatch training loss = 0.741 and accuracy of 0.83\n",
      "Iteration 8507: with minibatch training loss = 0.924 and accuracy of 0.75\n",
      "Iteration 8508: with minibatch training loss = 0.806 and accuracy of 0.78\n",
      "Iteration 8509: with minibatch training loss = 0.771 and accuracy of 0.78\n",
      "Iteration 8510: with minibatch training loss = 0.759 and accuracy of 0.8\n",
      "Iteration 8511: with minibatch training loss = 1.09 and accuracy of 0.7\n",
      "Iteration 8512: with minibatch training loss = 0.897 and accuracy of 0.75\n",
      "Iteration 8513: with minibatch training loss = 0.817 and accuracy of 0.77\n",
      "Iteration 8514: with minibatch training loss = 0.916 and accuracy of 0.73\n",
      "Iteration 8515: with minibatch training loss = 0.757 and accuracy of 0.77\n",
      "Iteration 8516: with minibatch training loss = 1.08 and accuracy of 0.7\n",
      "Iteration 8517: with minibatch training loss = 0.786 and accuracy of 0.78\n",
      "Iteration 8518: with minibatch training loss = 0.546 and accuracy of 0.86\n",
      "Iteration 8519: with minibatch training loss = 0.687 and accuracy of 0.78\n",
      "Iteration 8520: with minibatch training loss = 0.749 and accuracy of 0.8\n",
      "Iteration 8521: with minibatch training loss = 0.917 and accuracy of 0.75\n",
      "Iteration 8522: with minibatch training loss = 0.746 and accuracy of 0.78\n",
      "Iteration 8523: with minibatch training loss = 0.66 and accuracy of 0.83\n",
      "Iteration 8524: with minibatch training loss = 0.888 and accuracy of 0.73\n",
      "Iteration 8525: with minibatch training loss = 0.689 and accuracy of 0.8\n",
      "Iteration 8526: with minibatch training loss = 0.881 and accuracy of 0.77\n",
      "Iteration 8527: with minibatch training loss = 0.907 and accuracy of 0.72\n",
      "Iteration 8528: with minibatch training loss = 0.965 and accuracy of 0.75\n",
      "Iteration 8529: with minibatch training loss = 0.762 and accuracy of 0.8\n",
      "Iteration 8530: with minibatch training loss = 0.836 and accuracy of 0.77\n",
      "Iteration 8531: with minibatch training loss = 0.754 and accuracy of 0.78\n",
      "Iteration 8532: with minibatch training loss = 0.808 and accuracy of 0.75\n",
      "Iteration 8533: with minibatch training loss = 0.829 and accuracy of 0.77\n",
      "Iteration 8534: with minibatch training loss = 0.707 and accuracy of 0.83\n",
      "Iteration 8535: with minibatch training loss = 0.678 and accuracy of 0.8\n",
      "Iteration 8536: with minibatch training loss = 0.582 and accuracy of 0.86\n",
      "Iteration 8537: with minibatch training loss = 0.823 and accuracy of 0.75\n",
      "Iteration 8538: with minibatch training loss = 0.748 and accuracy of 0.78\n",
      "Iteration 8539: with minibatch training loss = 0.892 and accuracy of 0.77\n",
      "Iteration 8540: with minibatch training loss = 0.671 and accuracy of 0.81\n",
      "Iteration 8541: with minibatch training loss = 0.858 and accuracy of 0.72\n",
      "Iteration 8542: with minibatch training loss = 0.674 and accuracy of 0.81\n",
      "Iteration 8543: with minibatch training loss = 0.671 and accuracy of 0.81\n",
      "Iteration 8544: with minibatch training loss = 0.572 and accuracy of 0.84\n",
      "Iteration 8545: with minibatch training loss = 1 and accuracy of 0.72\n",
      "Iteration 8546: with minibatch training loss = 0.948 and accuracy of 0.73\n",
      "Iteration 8547: with minibatch training loss = 0.745 and accuracy of 0.78\n",
      "Iteration 8548: with minibatch training loss = 0.678 and accuracy of 0.81\n",
      "Iteration 8549: with minibatch training loss = 0.933 and accuracy of 0.73\n",
      "Iteration 8550: with minibatch training loss = 0.996 and accuracy of 0.67\n",
      "Iteration 8551: with minibatch training loss = 0.694 and accuracy of 0.8\n",
      "Iteration 8552: with minibatch training loss = 0.711 and accuracy of 0.8\n",
      "Iteration 8553: with minibatch training loss = 0.686 and accuracy of 0.8\n",
      "Iteration 8554: with minibatch training loss = 0.546 and accuracy of 0.86\n",
      "Iteration 8555: with minibatch training loss = 1.18 and accuracy of 0.62\n",
      "Iteration 8556: with minibatch training loss = 0.857 and accuracy of 0.72\n",
      "Iteration 8557: with minibatch training loss = 0.814 and accuracy of 0.78\n",
      "Iteration 8558: with minibatch training loss = 0.686 and accuracy of 0.81\n",
      "Iteration 8559: with minibatch training loss = 0.938 and accuracy of 0.72\n",
      "Iteration 8560: with minibatch training loss = 0.975 and accuracy of 0.69\n",
      "Iteration 8561: with minibatch training loss = 0.883 and accuracy of 0.73\n",
      "Iteration 8562: with minibatch training loss = 0.825 and accuracy of 0.77\n",
      "Iteration 8563: with minibatch training loss = 0.8 and accuracy of 0.73\n",
      "Iteration 8564: with minibatch training loss = 0.814 and accuracy of 0.77\n",
      "Iteration 8565: with minibatch training loss = 0.935 and accuracy of 0.73\n",
      "Iteration 8566: with minibatch training loss = 0.47 and accuracy of 0.88\n",
      "Iteration 8567: with minibatch training loss = 1.16 and accuracy of 0.66\n",
      "Iteration 8568: with minibatch training loss = 0.622 and accuracy of 0.8\n",
      "Iteration 8569: with minibatch training loss = 0.772 and accuracy of 0.75\n",
      "Iteration 8570: with minibatch training loss = 0.685 and accuracy of 0.8\n",
      "Iteration 8571: with minibatch training loss = 0.537 and accuracy of 0.88\n",
      "Iteration 8572: with minibatch training loss = 0.467 and accuracy of 0.86\n",
      "Iteration 8573: with minibatch training loss = 0.842 and accuracy of 0.75\n",
      "Iteration 8574: with minibatch training loss = 0.765 and accuracy of 0.77\n",
      "Iteration 8575: with minibatch training loss = 0.678 and accuracy of 0.8\n",
      "Iteration 8576: with minibatch training loss = 0.665 and accuracy of 0.84\n",
      "Iteration 8577: with minibatch training loss = 1.01 and accuracy of 0.69\n",
      "Iteration 8578: with minibatch training loss = 0.707 and accuracy of 0.78\n",
      "Iteration 8579: with minibatch training loss = 0.695 and accuracy of 0.8\n",
      "Iteration 8580: with minibatch training loss = 0.728 and accuracy of 0.75\n",
      "Iteration 8581: with minibatch training loss = 0.649 and accuracy of 0.83\n",
      "Iteration 8582: with minibatch training loss = 0.661 and accuracy of 0.8\n",
      "Iteration 8583: with minibatch training loss = 0.912 and accuracy of 0.72\n",
      "Iteration 8584: with minibatch training loss = 0.812 and accuracy of 0.77\n",
      "Iteration 8585: with minibatch training loss = 0.952 and accuracy of 0.73\n",
      "Iteration 8586: with minibatch training loss = 0.778 and accuracy of 0.8\n",
      "Iteration 8587: with minibatch training loss = 0.969 and accuracy of 0.72\n",
      "Iteration 8588: with minibatch training loss = 0.7 and accuracy of 0.81\n",
      "Iteration 8589: with minibatch training loss = 0.864 and accuracy of 0.75\n",
      "Iteration 8590: with minibatch training loss = 0.92 and accuracy of 0.73\n",
      "Iteration 8591: with minibatch training loss = 0.763 and accuracy of 0.78\n",
      "Iteration 8592: with minibatch training loss = 0.647 and accuracy of 0.84\n",
      "Iteration 8593: with minibatch training loss = 0.772 and accuracy of 0.77\n",
      "Iteration 8594: with minibatch training loss = 0.701 and accuracy of 0.8\n",
      "Iteration 8595: with minibatch training loss = 1.11 and accuracy of 0.7\n",
      "Iteration 8596: with minibatch training loss = 0.733 and accuracy of 0.8\n",
      "Iteration 8597: with minibatch training loss = 0.825 and accuracy of 0.78\n",
      "Iteration 8598: with minibatch training loss = 1.12 and accuracy of 0.66\n",
      "Iteration 8599: with minibatch training loss = 0.988 and accuracy of 0.77\n",
      "Iteration 8600: with minibatch training loss = 0.798 and accuracy of 0.75\n",
      "Iteration 8601: with minibatch training loss = 1.07 and accuracy of 0.69\n",
      "Iteration 8602: with minibatch training loss = 0.884 and accuracy of 0.73\n",
      "Iteration 8603: with minibatch training loss = 0.767 and accuracy of 0.78\n",
      "Iteration 8604: with minibatch training loss = 0.585 and accuracy of 0.84\n",
      "Iteration 8605: with minibatch training loss = 0.717 and accuracy of 0.81\n",
      "Iteration 8606: with minibatch training loss = 1.09 and accuracy of 0.67\n",
      "Iteration 8607: with minibatch training loss = 0.636 and accuracy of 0.8\n",
      "Iteration 8608: with minibatch training loss = 0.953 and accuracy of 0.75\n",
      "Iteration 8609: with minibatch training loss = 1.08 and accuracy of 0.67\n",
      "Iteration 8610: with minibatch training loss = 1.04 and accuracy of 0.7\n",
      "Iteration 8611: with minibatch training loss = 1.06 and accuracy of 0.69\n",
      "Iteration 8612: with minibatch training loss = 0.541 and accuracy of 0.84\n",
      "Iteration 8613: with minibatch training loss = 0.59 and accuracy of 0.83\n",
      "Iteration 8614: with minibatch training loss = 0.92 and accuracy of 0.72\n",
      "Iteration 8615: with minibatch training loss = 0.458 and accuracy of 0.88\n",
      "Iteration 8616: with minibatch training loss = 0.757 and accuracy of 0.8\n",
      "Iteration 8617: with minibatch training loss = 0.737 and accuracy of 0.81\n",
      "Iteration 8618: with minibatch training loss = 0.646 and accuracy of 0.8\n",
      "Iteration 8619: with minibatch training loss = 1.02 and accuracy of 0.73\n",
      "Iteration 8620: with minibatch training loss = 0.859 and accuracy of 0.75\n",
      "Iteration 8621: with minibatch training loss = 0.898 and accuracy of 0.73\n",
      "Iteration 8622: with minibatch training loss = 0.979 and accuracy of 0.69\n",
      "Iteration 8623: with minibatch training loss = 1.04 and accuracy of 0.67\n",
      "Iteration 8624: with minibatch training loss = 0.985 and accuracy of 0.7\n",
      "Iteration 8625: with minibatch training loss = 0.564 and accuracy of 0.84\n",
      "Iteration 8626: with minibatch training loss = 0.522 and accuracy of 0.84\n",
      "Iteration 8627: with minibatch training loss = 0.681 and accuracy of 0.84\n",
      "Iteration 8628: with minibatch training loss = 1.09 and accuracy of 0.64\n",
      "Iteration 8629: with minibatch training loss = 0.893 and accuracy of 0.75\n",
      "Iteration 8630: with minibatch training loss = 0.579 and accuracy of 0.84\n",
      "Iteration 8631: with minibatch training loss = 0.753 and accuracy of 0.77\n",
      "Iteration 8632: with minibatch training loss = 0.931 and accuracy of 0.78\n",
      "Iteration 8633: with minibatch training loss = 0.826 and accuracy of 0.75\n",
      "Iteration 8634: with minibatch training loss = 0.763 and accuracy of 0.8\n",
      "Iteration 8635: with minibatch training loss = 0.606 and accuracy of 0.84\n",
      "Iteration 8636: with minibatch training loss = 0.917 and accuracy of 0.7\n",
      "Iteration 8637: with minibatch training loss = 0.728 and accuracy of 0.77\n",
      "Iteration 8638: with minibatch training loss = 0.854 and accuracy of 0.77\n",
      "Iteration 8639: with minibatch training loss = 0.806 and accuracy of 0.73\n",
      "Iteration 8640: with minibatch training loss = 0.88 and accuracy of 0.75\n",
      "Iteration 8641: with minibatch training loss = 0.75 and accuracy of 0.78\n",
      "Iteration 8642: with minibatch training loss = 0.851 and accuracy of 0.75\n",
      "Iteration 8643: with minibatch training loss = 0.624 and accuracy of 0.83\n",
      "Iteration 8644: with minibatch training loss = 0.744 and accuracy of 0.75\n",
      "Iteration 8645: with minibatch training loss = 0.833 and accuracy of 0.78\n",
      "Iteration 8646: with minibatch training loss = 0.986 and accuracy of 0.72\n",
      "Iteration 8647: with minibatch training loss = 0.793 and accuracy of 0.77\n",
      "Iteration 8648: with minibatch training loss = 1.11 and accuracy of 0.69\n",
      "Iteration 8649: with minibatch training loss = 0.82 and accuracy of 0.78\n",
      "Iteration 8650: with minibatch training loss = 0.649 and accuracy of 0.83\n",
      "Iteration 8651: with minibatch training loss = 0.774 and accuracy of 0.77\n",
      "Iteration 8652: with minibatch training loss = 0.849 and accuracy of 0.77\n",
      "Iteration 8653: with minibatch training loss = 0.669 and accuracy of 0.81\n",
      "Iteration 8654: with minibatch training loss = 0.824 and accuracy of 0.73\n",
      "Iteration 8655: with minibatch training loss = 0.878 and accuracy of 0.73\n",
      "Iteration 8656: with minibatch training loss = 0.857 and accuracy of 0.72\n",
      "Iteration 8657: with minibatch training loss = 0.807 and accuracy of 0.77\n",
      "Iteration 8658: with minibatch training loss = 0.815 and accuracy of 0.8\n",
      "Iteration 8659: with minibatch training loss = 0.791 and accuracy of 0.77\n",
      "Iteration 8660: with minibatch training loss = 0.605 and accuracy of 0.84\n",
      "Iteration 8661: with minibatch training loss = 0.73 and accuracy of 0.8\n",
      "Iteration 8662: with minibatch training loss = 0.656 and accuracy of 0.83\n",
      "Iteration 8663: with minibatch training loss = 0.997 and accuracy of 0.7\n",
      "Iteration 8664: with minibatch training loss = 1.02 and accuracy of 0.72\n",
      "Iteration 8665: with minibatch training loss = 0.821 and accuracy of 0.77\n",
      "Iteration 8666: with minibatch training loss = 0.835 and accuracy of 0.77\n",
      "Iteration 8667: with minibatch training loss = 0.936 and accuracy of 0.73\n",
      "Iteration 8668: with minibatch training loss = 0.973 and accuracy of 0.75\n",
      "Iteration 8669: with minibatch training loss = 0.502 and accuracy of 0.91\n",
      "Iteration 8670: with minibatch training loss = 0.889 and accuracy of 0.73\n",
      "Iteration 8671: with minibatch training loss = 0.703 and accuracy of 0.81\n",
      "Iteration 8672: with minibatch training loss = 0.941 and accuracy of 0.75\n",
      "Iteration 8673: with minibatch training loss = 0.605 and accuracy of 0.86\n",
      "Iteration 8674: with minibatch training loss = 0.801 and accuracy of 0.75\n",
      "Iteration 8675: with minibatch training loss = 0.929 and accuracy of 0.72\n",
      "Iteration 8676: with minibatch training loss = 0.802 and accuracy of 0.75\n",
      "Iteration 8677: with minibatch training loss = 0.805 and accuracy of 0.77\n",
      "Iteration 8678: with minibatch training loss = 0.479 and accuracy of 0.84\n",
      "Iteration 8679: with minibatch training loss = 0.805 and accuracy of 0.75\n",
      "Iteration 8680: with minibatch training loss = 0.693 and accuracy of 0.83\n",
      "Iteration 8681: with minibatch training loss = 0.88 and accuracy of 0.75\n",
      "Iteration 8682: with minibatch training loss = 0.531 and accuracy of 0.86\n",
      "Iteration 8683: with minibatch training loss = 0.751 and accuracy of 0.77\n",
      "Iteration 8684: with minibatch training loss = 0.942 and accuracy of 0.7\n",
      "Iteration 8685: with minibatch training loss = 1.05 and accuracy of 0.69\n",
      "Iteration 8686: with minibatch training loss = 0.953 and accuracy of 0.7\n",
      "Iteration 8687: with minibatch training loss = 0.636 and accuracy of 0.83\n",
      "Iteration 8688: with minibatch training loss = 0.932 and accuracy of 0.75\n",
      "Iteration 8689: with minibatch training loss = 0.843 and accuracy of 0.75\n",
      "Iteration 8690: with minibatch training loss = 0.963 and accuracy of 0.73\n",
      "Iteration 8691: with minibatch training loss = 0.636 and accuracy of 0.81\n",
      "Iteration 8692: with minibatch training loss = 0.874 and accuracy of 0.73\n",
      "Iteration 8693: with minibatch training loss = 0.973 and accuracy of 0.73\n",
      "Iteration 8694: with minibatch training loss = 0.749 and accuracy of 0.8\n",
      "Iteration 8695: with minibatch training loss = 0.702 and accuracy of 0.81\n",
      "Iteration 8696: with minibatch training loss = 0.74 and accuracy of 0.78\n",
      "Iteration 8697: with minibatch training loss = 0.701 and accuracy of 0.81\n",
      "Iteration 8698: with minibatch training loss = 0.602 and accuracy of 0.83\n",
      "Iteration 8699: with minibatch training loss = 0.701 and accuracy of 0.77\n",
      "Iteration 8700: with minibatch training loss = 0.701 and accuracy of 0.83\n",
      "Iteration 8701: with minibatch training loss = 0.695 and accuracy of 0.78\n",
      "Iteration 8702: with minibatch training loss = 0.685 and accuracy of 0.8\n",
      "Iteration 8703: with minibatch training loss = 0.682 and accuracy of 0.81\n",
      "Iteration 8704: with minibatch training loss = 0.911 and accuracy of 0.72\n",
      "Iteration 8705: with minibatch training loss = 1.07 and accuracy of 0.7\n",
      "Iteration 8706: with minibatch training loss = 1.24 and accuracy of 0.62\n",
      "Iteration 8707: with minibatch training loss = 0.723 and accuracy of 0.78\n",
      "Iteration 8708: with minibatch training loss = 0.73 and accuracy of 0.78\n",
      "Iteration 8709: with minibatch training loss = 0.819 and accuracy of 0.78\n",
      "Iteration 8710: with minibatch training loss = 0.962 and accuracy of 0.69\n",
      "Iteration 8711: with minibatch training loss = 0.833 and accuracy of 0.75\n",
      "Iteration 8712: with minibatch training loss = 0.612 and accuracy of 0.81\n",
      "Iteration 8713: with minibatch training loss = 0.97 and accuracy of 0.7\n",
      "Iteration 8714: with minibatch training loss = 0.636 and accuracy of 0.84\n",
      "Iteration 8715: with minibatch training loss = 0.831 and accuracy of 0.78\n",
      "Iteration 8716: with minibatch training loss = 0.889 and accuracy of 0.7\n",
      "Iteration 8717: with minibatch training loss = 0.837 and accuracy of 0.75\n",
      "Iteration 8718: with minibatch training loss = 0.848 and accuracy of 0.77\n",
      "Iteration 8719: with minibatch training loss = 0.802 and accuracy of 0.77\n",
      "Iteration 8720: with minibatch training loss = 0.793 and accuracy of 0.75\n",
      "Iteration 8721: with minibatch training loss = 0.747 and accuracy of 0.8\n",
      "Iteration 8722: with minibatch training loss = 0.687 and accuracy of 0.8\n",
      "Iteration 8723: with minibatch training loss = 0.929 and accuracy of 0.77\n",
      "Iteration 8724: with minibatch training loss = 0.749 and accuracy of 0.8\n",
      "Iteration 8725: with minibatch training loss = 0.981 and accuracy of 0.72\n",
      "Iteration 8726: with minibatch training loss = 0.849 and accuracy of 0.77\n",
      "Iteration 8727: with minibatch training loss = 0.968 and accuracy of 0.73\n",
      "Iteration 8728: with minibatch training loss = 0.933 and accuracy of 0.75\n",
      "Iteration 8729: with minibatch training loss = 0.531 and accuracy of 0.84\n",
      "Iteration 8730: with minibatch training loss = 0.597 and accuracy of 0.81\n",
      "Iteration 8731: with minibatch training loss = 0.691 and accuracy of 0.81\n",
      "Iteration 8732: with minibatch training loss = 0.875 and accuracy of 0.7\n",
      "Iteration 8733: with minibatch training loss = 0.582 and accuracy of 0.83\n",
      "Iteration 8734: with minibatch training loss = 0.358 and accuracy of 0.89\n",
      "Iteration 8735: with minibatch training loss = 1.04 and accuracy of 0.7\n",
      "Iteration 8736: with minibatch training loss = 0.637 and accuracy of 0.84\n",
      "Iteration 8737: with minibatch training loss = 0.918 and accuracy of 0.72\n",
      "Iteration 8738: with minibatch training loss = 0.983 and accuracy of 0.69\n",
      "Iteration 8739: with minibatch training loss = 0.744 and accuracy of 0.78\n",
      "Iteration 8740: with minibatch training loss = 0.685 and accuracy of 0.8\n",
      "Iteration 8741: with minibatch training loss = 0.934 and accuracy of 0.72\n",
      "Iteration 8742: with minibatch training loss = 0.849 and accuracy of 0.75\n",
      "Iteration 8743: with minibatch training loss = 0.937 and accuracy of 0.72\n",
      "Iteration 8744: with minibatch training loss = 0.855 and accuracy of 0.75\n",
      "Iteration 8745: with minibatch training loss = 0.642 and accuracy of 0.81\n",
      "Iteration 8746: with minibatch training loss = 0.546 and accuracy of 0.88\n",
      "Iteration 8747: with minibatch training loss = 0.79 and accuracy of 0.78\n",
      "Iteration 8748: with minibatch training loss = 0.816 and accuracy of 0.75\n",
      "Iteration 8749: with minibatch training loss = 0.872 and accuracy of 0.75\n",
      "Iteration 8750: with minibatch training loss = 0.927 and accuracy of 0.7\n",
      "Iteration 8751: with minibatch training loss = 0.757 and accuracy of 0.77\n",
      "Iteration 8752: with minibatch training loss = 0.727 and accuracy of 0.78\n",
      "Iteration 8753: with minibatch training loss = 0.961 and accuracy of 0.7\n",
      "Iteration 8754: with minibatch training loss = 0.745 and accuracy of 0.77\n",
      "Iteration 8755: with minibatch training loss = 0.964 and accuracy of 0.73\n",
      "Iteration 8756: with minibatch training loss = 0.767 and accuracy of 0.78\n",
      "Iteration 8757: with minibatch training loss = 0.688 and accuracy of 0.8\n",
      "Iteration 8758: with minibatch training loss = 0.495 and accuracy of 0.86\n",
      "Iteration 8759: with minibatch training loss = 0.793 and accuracy of 0.77\n",
      "Iteration 8760: with minibatch training loss = 0.99 and accuracy of 0.73\n",
      "Iteration 8761: with minibatch training loss = 0.747 and accuracy of 0.78\n",
      "Iteration 8762: with minibatch training loss = 0.809 and accuracy of 0.77\n",
      "Iteration 8763: with minibatch training loss = 0.64 and accuracy of 0.83\n",
      "Iteration 8764: with minibatch training loss = 0.919 and accuracy of 0.72\n",
      "Iteration 8765: with minibatch training loss = 1 and accuracy of 0.72\n",
      "Iteration 8766: with minibatch training loss = 0.771 and accuracy of 0.77\n",
      "Iteration 8767: with minibatch training loss = 1.02 and accuracy of 0.67\n",
      "Iteration 8768: with minibatch training loss = 0.517 and accuracy of 0.84\n",
      "Iteration 8769: with minibatch training loss = 0.829 and accuracy of 0.77\n",
      "Iteration 8770: with minibatch training loss = 0.746 and accuracy of 0.77\n",
      "Iteration 8771: with minibatch training loss = 0.821 and accuracy of 0.77\n",
      "Iteration 8772: with minibatch training loss = 0.844 and accuracy of 0.73\n",
      "Iteration 8773: with minibatch training loss = 0.85 and accuracy of 0.72\n",
      "Iteration 8774: with minibatch training loss = 0.532 and accuracy of 0.86\n",
      "Iteration 8775: with minibatch training loss = 0.873 and accuracy of 0.73\n",
      "Iteration 8776: with minibatch training loss = 0.945 and accuracy of 0.73\n",
      "Iteration 8777: with minibatch training loss = 0.58 and accuracy of 0.84\n",
      "Iteration 8778: with minibatch training loss = 0.79 and accuracy of 0.78\n",
      "Iteration 8779: with minibatch training loss = 0.679 and accuracy of 0.81\n",
      "Iteration 8780: with minibatch training loss = 0.835 and accuracy of 0.77\n",
      "Iteration 8781: with minibatch training loss = 1.02 and accuracy of 0.67\n",
      "Iteration 8782: with minibatch training loss = 0.685 and accuracy of 0.8\n",
      "Iteration 8783: with minibatch training loss = 0.834 and accuracy of 0.73\n",
      "Iteration 8784: with minibatch training loss = 0.782 and accuracy of 0.78\n",
      "Iteration 8785: with minibatch training loss = 0.851 and accuracy of 0.73\n",
      "Iteration 8786: with minibatch training loss = 0.876 and accuracy of 0.75\n",
      "Iteration 8787: with minibatch training loss = 0.981 and accuracy of 0.7\n",
      "Iteration 8788: with minibatch training loss = 0.867 and accuracy of 0.75\n",
      "Iteration 8789: with minibatch training loss = 0.922 and accuracy of 0.73\n",
      "Iteration 8790: with minibatch training loss = 0.982 and accuracy of 0.69\n",
      "Iteration 8791: with minibatch training loss = 0.727 and accuracy of 0.8\n",
      "Iteration 8792: with minibatch training loss = 0.517 and accuracy of 0.86\n",
      "Iteration 8793: with minibatch training loss = 0.869 and accuracy of 0.75\n",
      "Iteration 8794: with minibatch training loss = 0.606 and accuracy of 0.8\n",
      "Iteration 8795: with minibatch training loss = 0.62 and accuracy of 0.83\n",
      "Iteration 8796: with minibatch training loss = 0.941 and accuracy of 0.72\n",
      "Iteration 8797: with minibatch training loss = 0.849 and accuracy of 0.77\n",
      "Iteration 8798: with minibatch training loss = 0.749 and accuracy of 0.78\n",
      "Iteration 8799: with minibatch training loss = 0.59 and accuracy of 0.84\n",
      "Iteration 8800: with minibatch training loss = 0.716 and accuracy of 0.78\n",
      "Iteration 8801: with minibatch training loss = 0.584 and accuracy of 0.88\n",
      "Iteration 8802: with minibatch training loss = 0.958 and accuracy of 0.72\n",
      "Iteration 8803: with minibatch training loss = 0.805 and accuracy of 0.73\n",
      "Iteration 8804: with minibatch training loss = 0.892 and accuracy of 0.77\n",
      "Iteration 8805: with minibatch training loss = 0.701 and accuracy of 0.78\n",
      "Iteration 8806: with minibatch training loss = 0.776 and accuracy of 0.75\n",
      "Iteration 8807: with minibatch training loss = 0.831 and accuracy of 0.77\n",
      "Iteration 8808: with minibatch training loss = 0.769 and accuracy of 0.81\n",
      "Iteration 8809: with minibatch training loss = 1.08 and accuracy of 0.7\n",
      "Iteration 8810: with minibatch training loss = 0.682 and accuracy of 0.83\n",
      "Iteration 8811: with minibatch training loss = 0.942 and accuracy of 0.75\n",
      "Iteration 8812: with minibatch training loss = 0.994 and accuracy of 0.75\n",
      "Iteration 8813: with minibatch training loss = 0.849 and accuracy of 0.77\n",
      "Iteration 8814: with minibatch training loss = 0.919 and accuracy of 0.7\n",
      "Iteration 8815: with minibatch training loss = 0.894 and accuracy of 0.77\n",
      "Iteration 8816: with minibatch training loss = 0.704 and accuracy of 0.81\n",
      "Iteration 8817: with minibatch training loss = 0.801 and accuracy of 0.77\n",
      "Iteration 8818: with minibatch training loss = 0.768 and accuracy of 0.77\n",
      "Iteration 8819: with minibatch training loss = 0.99 and accuracy of 0.7\n",
      "Iteration 8820: with minibatch training loss = 0.642 and accuracy of 0.81\n",
      "Iteration 8821: with minibatch training loss = 0.81 and accuracy of 0.77\n",
      "Iteration 8822: with minibatch training loss = 0.505 and accuracy of 0.88\n",
      "Iteration 8823: with minibatch training loss = 0.713 and accuracy of 0.78\n",
      "Iteration 8824: with minibatch training loss = 0.752 and accuracy of 0.77\n",
      "Iteration 8825: with minibatch training loss = 0.857 and accuracy of 0.73\n",
      "Iteration 8826: with minibatch training loss = 0.795 and accuracy of 0.78\n",
      "Iteration 8827: with minibatch training loss = 0.786 and accuracy of 0.81\n",
      "Iteration 8828: with minibatch training loss = 0.713 and accuracy of 0.81\n",
      "Iteration 8829: with minibatch training loss = 1.07 and accuracy of 0.67\n",
      "Iteration 8830: with minibatch training loss = 0.732 and accuracy of 0.8\n",
      "Iteration 8831: with minibatch training loss = 0.854 and accuracy of 0.77\n",
      "Iteration 8832: with minibatch training loss = 0.819 and accuracy of 0.84\n",
      "Iteration 8833: with minibatch training loss = 0.626 and accuracy of 0.81\n",
      "Iteration 8834: with minibatch training loss = 0.825 and accuracy of 0.75\n",
      "Iteration 8835: with minibatch training loss = 0.968 and accuracy of 0.7\n",
      "Iteration 8836: with minibatch training loss = 0.646 and accuracy of 0.81\n",
      "Iteration 8837: with minibatch training loss = 0.868 and accuracy of 0.75\n",
      "Iteration 8838: with minibatch training loss = 0.655 and accuracy of 0.78\n",
      "Iteration 8839: with minibatch training loss = 0.911 and accuracy of 0.72\n",
      "Iteration 8840: with minibatch training loss = 0.814 and accuracy of 0.77\n",
      "Iteration 8841: with minibatch training loss = 0.912 and accuracy of 0.72\n",
      "Iteration 8842: with minibatch training loss = 0.586 and accuracy of 0.84\n",
      "Iteration 8843: with minibatch training loss = 0.927 and accuracy of 0.7\n",
      "Iteration 8844: with minibatch training loss = 0.989 and accuracy of 0.69\n",
      "Iteration 8845: with minibatch training loss = 0.874 and accuracy of 0.8\n",
      "Iteration 8846: with minibatch training loss = 0.72 and accuracy of 0.78\n",
      "Iteration 8847: with minibatch training loss = 0.815 and accuracy of 0.8\n",
      "Iteration 8848: with minibatch training loss = 0.883 and accuracy of 0.77\n",
      "Iteration 8849: with minibatch training loss = 0.7 and accuracy of 0.78\n",
      "Iteration 8850: with minibatch training loss = 0.735 and accuracy of 0.8\n",
      "Iteration 8851: with minibatch training loss = 0.771 and accuracy of 0.78\n",
      "Iteration 8852: with minibatch training loss = 0.696 and accuracy of 0.83\n",
      "Iteration 8853: with minibatch training loss = 0.793 and accuracy of 0.75\n",
      "Iteration 8854: with minibatch training loss = 0.62 and accuracy of 0.84\n",
      "Iteration 8855: with minibatch training loss = 0.804 and accuracy of 0.8\n",
      "Iteration 8856: with minibatch training loss = 0.636 and accuracy of 0.83\n",
      "Iteration 8857: with minibatch training loss = 1.19 and accuracy of 0.64\n",
      "Iteration 8858: with minibatch training loss = 0.904 and accuracy of 0.73\n",
      "Iteration 8859: with minibatch training loss = 0.637 and accuracy of 0.81\n",
      "Iteration 8860: with minibatch training loss = 0.709 and accuracy of 0.78\n",
      "Iteration 8861: with minibatch training loss = 0.685 and accuracy of 0.78\n",
      "Iteration 8862: with minibatch training loss = 0.776 and accuracy of 0.81\n",
      "Iteration 8863: with minibatch training loss = 0.634 and accuracy of 0.81\n",
      "Iteration 8864: with minibatch training loss = 0.522 and accuracy of 0.86\n",
      "Iteration 8865: with minibatch training loss = 0.704 and accuracy of 0.78\n",
      "Iteration 8866: with minibatch training loss = 0.794 and accuracy of 0.77\n",
      "Iteration 8867: with minibatch training loss = 0.892 and accuracy of 0.73\n",
      "Iteration 8868: with minibatch training loss = 0.688 and accuracy of 0.78\n",
      "Iteration 8869: with minibatch training loss = 0.799 and accuracy of 0.75\n",
      "Iteration 8870: with minibatch training loss = 0.765 and accuracy of 0.8\n",
      "Iteration 8871: with minibatch training loss = 0.906 and accuracy of 0.72\n",
      "Iteration 8872: with minibatch training loss = 0.899 and accuracy of 0.73\n",
      "Iteration 8873: with minibatch training loss = 0.748 and accuracy of 0.78\n",
      "Iteration 8874: with minibatch training loss = 0.679 and accuracy of 0.83\n",
      "Iteration 8875: with minibatch training loss = 0.763 and accuracy of 0.78\n",
      "Iteration 8876: with minibatch training loss = 0.817 and accuracy of 0.77\n",
      "Iteration 8877: with minibatch training loss = 0.888 and accuracy of 0.73\n",
      "Iteration 8878: with minibatch training loss = 0.472 and accuracy of 0.88\n",
      "Iteration 8879: with minibatch training loss = 1.02 and accuracy of 0.7\n",
      "Iteration 8880: with minibatch training loss = 0.948 and accuracy of 0.7\n",
      "Iteration 8881: with minibatch training loss = 0.823 and accuracy of 0.75\n",
      "Iteration 8882: with minibatch training loss = 0.921 and accuracy of 0.75\n",
      "Iteration 8883: with minibatch training loss = 0.848 and accuracy of 0.75\n",
      "Iteration 8884: with minibatch training loss = 0.769 and accuracy of 0.78\n",
      "Iteration 8885: with minibatch training loss = 0.749 and accuracy of 0.78\n",
      "Iteration 8886: with minibatch training loss = 0.673 and accuracy of 0.83\n",
      "Iteration 8887: with minibatch training loss = 1.04 and accuracy of 0.72\n",
      "Iteration 8888: with minibatch training loss = 0.703 and accuracy of 0.83\n",
      "Iteration 8889: with minibatch training loss = 0.708 and accuracy of 0.83\n",
      "Iteration 8890: with minibatch training loss = 0.646 and accuracy of 0.81\n",
      "Iteration 8891: with minibatch training loss = 0.84 and accuracy of 0.75\n",
      "Iteration 8892: with minibatch training loss = 0.782 and accuracy of 0.8\n",
      "Iteration 8893: with minibatch training loss = 0.865 and accuracy of 0.78\n",
      "Iteration 8894: with minibatch training loss = 0.899 and accuracy of 0.77\n",
      "Iteration 8895: with minibatch training loss = 0.689 and accuracy of 0.8\n",
      "Iteration 8896: with minibatch training loss = 0.629 and accuracy of 0.8\n",
      "Iteration 8897: with minibatch training loss = 1.03 and accuracy of 0.72\n",
      "Iteration 8898: with minibatch training loss = 0.957 and accuracy of 0.73\n",
      "Iteration 8899: with minibatch training loss = 0.72 and accuracy of 0.8\n",
      "Iteration 8900: with minibatch training loss = 0.591 and accuracy of 0.83\n",
      "Iteration 8901: with minibatch training loss = 0.847 and accuracy of 0.78\n",
      "Iteration 8902: with minibatch training loss = 1.07 and accuracy of 0.69\n",
      "Iteration 8903: with minibatch training loss = 0.999 and accuracy of 0.73\n",
      "Iteration 8904: with minibatch training loss = 0.767 and accuracy of 0.73\n",
      "Iteration 8905: with minibatch training loss = 0.464 and accuracy of 0.91\n",
      "Iteration 8906: with minibatch training loss = 0.535 and accuracy of 0.86\n",
      "Iteration 8907: with minibatch training loss = 0.831 and accuracy of 0.73\n",
      "Iteration 8908: with minibatch training loss = 0.656 and accuracy of 0.8\n",
      "Iteration 8909: with minibatch training loss = 1.13 and accuracy of 0.72\n",
      "Iteration 8910: with minibatch training loss = 0.764 and accuracy of 0.78\n",
      "Iteration 8911: with minibatch training loss = 0.619 and accuracy of 0.81\n",
      "Iteration 8912: with minibatch training loss = 0.841 and accuracy of 0.75\n",
      "Iteration 8913: with minibatch training loss = 0.619 and accuracy of 0.86\n",
      "Iteration 8914: with minibatch training loss = 0.613 and accuracy of 0.83\n",
      "Iteration 8915: with minibatch training loss = 1 and accuracy of 0.72\n",
      "Iteration 8916: with minibatch training loss = 0.856 and accuracy of 0.77\n",
      "Iteration 8917: with minibatch training loss = 0.806 and accuracy of 0.78\n",
      "Iteration 8918: with minibatch training loss = 0.83 and accuracy of 0.75\n",
      "Iteration 8919: with minibatch training loss = 0.571 and accuracy of 0.86\n",
      "Iteration 8920: with minibatch training loss = 1.01 and accuracy of 0.72\n",
      "Iteration 8921: with minibatch training loss = 0.654 and accuracy of 0.83\n",
      "Iteration 8922: with minibatch training loss = 0.957 and accuracy of 0.7\n",
      "Iteration 8923: with minibatch training loss = 0.93 and accuracy of 0.78\n",
      "Iteration 8924: with minibatch training loss = 0.695 and accuracy of 0.83\n",
      "Iteration 8925: with minibatch training loss = 1.1 and accuracy of 0.67\n",
      "Iteration 8926: with minibatch training loss = 1.11 and accuracy of 0.67\n",
      "Iteration 8927: with minibatch training loss = 0.843 and accuracy of 0.77\n",
      "Iteration 8928: with minibatch training loss = 0.709 and accuracy of 0.8\n",
      "Iteration 8929: with minibatch training loss = 0.699 and accuracy of 0.81\n",
      "Iteration 8930: with minibatch training loss = 1.05 and accuracy of 0.69\n",
      "Iteration 8931: with minibatch training loss = 0.42 and accuracy of 0.91\n",
      "Iteration 8932: with minibatch training loss = 0.838 and accuracy of 0.77\n",
      "Iteration 8933: with minibatch training loss = 1.05 and accuracy of 0.7\n",
      "Iteration 8934: with minibatch training loss = 0.849 and accuracy of 0.78\n",
      "Iteration 8935: with minibatch training loss = 0.943 and accuracy of 0.73\n",
      "Iteration 8936: with minibatch training loss = 0.603 and accuracy of 0.81\n",
      "Iteration 8937: with minibatch training loss = 0.613 and accuracy of 0.84\n",
      "Iteration 8938: with minibatch training loss = 0.92 and accuracy of 0.7\n",
      "Iteration 8939: with minibatch training loss = 1.13 and accuracy of 0.66\n",
      "Iteration 8940: with minibatch training loss = 0.781 and accuracy of 0.78\n",
      "Iteration 8941: with minibatch training loss = 0.695 and accuracy of 0.81\n",
      "Iteration 8942: with minibatch training loss = 0.796 and accuracy of 0.8\n",
      "Iteration 8943: with minibatch training loss = 0.678 and accuracy of 0.81\n",
      "Iteration 8944: with minibatch training loss = 0.907 and accuracy of 0.69\n",
      "Iteration 8945: with minibatch training loss = 1.12 and accuracy of 0.64\n",
      "Iteration 8946: with minibatch training loss = 0.778 and accuracy of 0.78\n",
      "Iteration 8947: with minibatch training loss = 1.18 and accuracy of 0.67\n",
      "Iteration 8948: with minibatch training loss = 0.926 and accuracy of 0.7\n",
      "Iteration 8949: with minibatch training loss = 0.796 and accuracy of 0.78\n",
      "Iteration 8950: with minibatch training loss = 0.979 and accuracy of 0.72\n",
      "Iteration 8951: with minibatch training loss = 0.924 and accuracy of 0.75\n",
      "Iteration 8952: with minibatch training loss = 0.667 and accuracy of 0.8\n",
      "Iteration 8953: with minibatch training loss = 1.15 and accuracy of 0.67\n",
      "Iteration 8954: with minibatch training loss = 0.724 and accuracy of 0.78\n",
      "Iteration 8955: with minibatch training loss = 0.578 and accuracy of 0.86\n",
      "Iteration 8956: with minibatch training loss = 0.575 and accuracy of 0.86\n",
      "Iteration 8957: with minibatch training loss = 0.771 and accuracy of 0.75\n",
      "Iteration 8958: with minibatch training loss = 1.02 and accuracy of 0.73\n",
      "Iteration 8959: with minibatch training loss = 0.84 and accuracy of 0.8\n",
      "Iteration 8960: with minibatch training loss = 0.837 and accuracy of 0.75\n",
      "Iteration 8961: with minibatch training loss = 0.878 and accuracy of 0.77\n",
      "Iteration 8962: with minibatch training loss = 0.476 and accuracy of 0.89\n",
      "Iteration 8963: with minibatch training loss = 0.698 and accuracy of 0.8\n",
      "Iteration 8964: with minibatch training loss = 0.716 and accuracy of 0.78\n",
      "Iteration 8965: with minibatch training loss = 0.805 and accuracy of 0.78\n",
      "Iteration 8966: with minibatch training loss = 0.834 and accuracy of 0.77\n",
      "Iteration 8967: with minibatch training loss = 0.689 and accuracy of 0.81\n",
      "Iteration 8968: with minibatch training loss = 0.603 and accuracy of 0.83\n",
      "Iteration 8969: with minibatch training loss = 0.64 and accuracy of 0.8\n",
      "Iteration 8970: with minibatch training loss = 0.686 and accuracy of 0.78\n",
      "Iteration 8971: with minibatch training loss = 1.01 and accuracy of 0.69\n",
      "Iteration 8972: with minibatch training loss = 0.756 and accuracy of 0.83\n",
      "Iteration 8973: with minibatch training loss = 0.649 and accuracy of 0.81\n",
      "Iteration 8974: with minibatch training loss = 0.716 and accuracy of 0.8\n",
      "Iteration 8975: with minibatch training loss = 0.545 and accuracy of 0.86\n",
      "Iteration 8976: with minibatch training loss = 0.894 and accuracy of 0.72\n",
      "Iteration 8977: with minibatch training loss = 0.768 and accuracy of 0.78\n",
      "Iteration 8978: with minibatch training loss = 0.988 and accuracy of 0.72\n",
      "Iteration 8979: with minibatch training loss = 0.789 and accuracy of 0.77\n",
      "Iteration 8980: with minibatch training loss = 0.8 and accuracy of 0.77\n",
      "Iteration 8981: with minibatch training loss = 1.15 and accuracy of 0.7\n",
      "Iteration 8982: with minibatch training loss = 0.805 and accuracy of 0.81\n",
      "Iteration 8983: with minibatch training loss = 0.601 and accuracy of 0.81\n",
      "Iteration 8984: with minibatch training loss = 0.948 and accuracy of 0.73\n",
      "Iteration 8985: with minibatch training loss = 0.584 and accuracy of 0.84\n",
      "Iteration 8986: with minibatch training loss = 0.791 and accuracy of 0.77\n",
      "Iteration 8987: with minibatch training loss = 0.523 and accuracy of 0.83\n",
      "Iteration 8988: with minibatch training loss = 0.863 and accuracy of 0.77\n",
      "Iteration 8989: with minibatch training loss = 0.588 and accuracy of 0.81\n",
      "Iteration 8990: with minibatch training loss = 0.727 and accuracy of 0.8\n",
      "Iteration 8991: with minibatch training loss = 0.687 and accuracy of 0.83\n",
      "Iteration 8992: with minibatch training loss = 0.687 and accuracy of 0.8\n",
      "Iteration 8993: with minibatch training loss = 0.884 and accuracy of 0.77\n",
      "Iteration 8994: with minibatch training loss = 1.01 and accuracy of 0.72\n",
      "Iteration 8995: with minibatch training loss = 0.534 and accuracy of 0.84\n",
      "Iteration 8996: with minibatch training loss = 0.971 and accuracy of 0.72\n",
      "Iteration 8997: with minibatch training loss = 0.978 and accuracy of 0.72\n",
      "Iteration 8998: with minibatch training loss = 0.8 and accuracy of 0.77\n",
      "Iteration 8999: with minibatch training loss = 0.776 and accuracy of 0.78\n",
      "Iteration 9000: with minibatch training loss = 0.805 and accuracy of 0.77\n",
      "Iteration 9001: with minibatch training loss = 0.838 and accuracy of 0.72\n",
      "Iteration 9002: with minibatch training loss = 0.881 and accuracy of 0.73\n",
      "Iteration 9003: with minibatch training loss = 0.688 and accuracy of 0.8\n",
      "Iteration 9004: with minibatch training loss = 0.836 and accuracy of 0.77\n",
      "Iteration 9005: with minibatch training loss = 0.616 and accuracy of 0.84\n",
      "Iteration 9006: with minibatch training loss = 0.887 and accuracy of 0.75\n",
      "Iteration 9007: with minibatch training loss = 0.975 and accuracy of 0.73\n",
      "Iteration 9008: with minibatch training loss = 0.685 and accuracy of 0.8\n",
      "Iteration 9009: with minibatch training loss = 0.902 and accuracy of 0.77\n",
      "Iteration 9010: with minibatch training loss = 1.02 and accuracy of 0.69\n",
      "Iteration 9011: with minibatch training loss = 0.599 and accuracy of 0.88\n",
      "Iteration 9012: with minibatch training loss = 1.06 and accuracy of 0.69\n",
      "Iteration 9013: with minibatch training loss = 0.478 and accuracy of 0.86\n",
      "Iteration 9014: with minibatch training loss = 0.422 and accuracy of 0.86\n",
      "Iteration 9015: with minibatch training loss = 0.636 and accuracy of 0.81\n",
      "Iteration 9016: with minibatch training loss = 0.711 and accuracy of 0.8\n",
      "Iteration 9017: with minibatch training loss = 0.636 and accuracy of 0.81\n",
      "Iteration 9018: with minibatch training loss = 0.647 and accuracy of 0.81\n",
      "Iteration 9019: with minibatch training loss = 0.589 and accuracy of 0.86\n",
      "Iteration 9020: with minibatch training loss = 0.947 and accuracy of 0.7\n",
      "Iteration 9021: with minibatch training loss = 1.03 and accuracy of 0.72\n",
      "Iteration 9022: with minibatch training loss = 0.831 and accuracy of 0.75\n",
      "Iteration 9023: with minibatch training loss = 0.942 and accuracy of 0.75\n",
      "Iteration 9024: with minibatch training loss = 0.826 and accuracy of 0.77\n",
      "Iteration 9025: with minibatch training loss = 0.752 and accuracy of 0.78\n",
      "Iteration 9026: with minibatch training loss = 0.906 and accuracy of 0.72\n",
      "Iteration 9027: with minibatch training loss = 0.854 and accuracy of 0.77\n",
      "Iteration 9028: with minibatch training loss = 0.84 and accuracy of 0.75\n",
      "Iteration 9029: with minibatch training loss = 0.654 and accuracy of 0.8\n",
      "Iteration 9030: with minibatch training loss = 0.702 and accuracy of 0.8\n",
      "Iteration 9031: with minibatch training loss = 0.727 and accuracy of 0.8\n",
      "Iteration 9032: with minibatch training loss = 0.933 and accuracy of 0.73\n",
      "Iteration 9033: with minibatch training loss = 0.915 and accuracy of 0.73\n",
      "Iteration 9034: with minibatch training loss = 0.597 and accuracy of 0.84\n",
      "Iteration 9035: with minibatch training loss = 0.746 and accuracy of 0.75\n",
      "Iteration 9036: with minibatch training loss = 0.968 and accuracy of 0.77\n",
      "Iteration 9037: with minibatch training loss = 1.05 and accuracy of 0.69\n",
      "Iteration 9038: with minibatch training loss = 0.721 and accuracy of 0.77\n",
      "Iteration 9039: with minibatch training loss = 0.794 and accuracy of 0.78\n",
      "Iteration 9040: with minibatch training loss = 0.391 and accuracy of 0.88\n",
      "Iteration 9041: with minibatch training loss = 1.07 and accuracy of 0.67\n",
      "Iteration 9042: with minibatch training loss = 0.935 and accuracy of 0.72\n",
      "Iteration 9043: with minibatch training loss = 0.905 and accuracy of 0.72\n",
      "Iteration 9044: with minibatch training loss = 0.794 and accuracy of 0.73\n",
      "Iteration 9045: with minibatch training loss = 0.746 and accuracy of 0.8\n",
      "Iteration 9046: with minibatch training loss = 0.714 and accuracy of 0.81\n",
      "Iteration 9047: with minibatch training loss = 0.752 and accuracy of 0.83\n",
      "Iteration 9048: with minibatch training loss = 0.855 and accuracy of 0.73\n",
      "Iteration 9049: with minibatch training loss = 0.743 and accuracy of 0.81\n",
      "Iteration 9050: with minibatch training loss = 0.63 and accuracy of 0.8\n",
      "Iteration 9051: with minibatch training loss = 0.89 and accuracy of 0.75\n",
      "Iteration 9052: with minibatch training loss = 0.702 and accuracy of 0.83\n",
      "Iteration 9053: with minibatch training loss = 0.711 and accuracy of 0.8\n",
      "Iteration 9054: with minibatch training loss = 0.58 and accuracy of 0.86\n",
      "Iteration 9055: with minibatch training loss = 0.625 and accuracy of 0.83\n",
      "Iteration 9056: with minibatch training loss = 0.665 and accuracy of 0.81\n",
      "Iteration 9057: with minibatch training loss = 0.725 and accuracy of 0.73\n",
      "Iteration 9058: with minibatch training loss = 0.908 and accuracy of 0.75\n",
      "Iteration 9059: with minibatch training loss = 0.654 and accuracy of 0.81\n",
      "Iteration 9060: with minibatch training loss = 0.781 and accuracy of 0.77\n",
      "Iteration 9061: with minibatch training loss = 0.466 and accuracy of 0.91\n",
      "Iteration 9062: with minibatch training loss = 1.02 and accuracy of 0.67\n",
      "Iteration 9063: with minibatch training loss = 0.781 and accuracy of 0.77\n",
      "Iteration 9064: with minibatch training loss = 0.927 and accuracy of 0.73\n",
      "Iteration 9065: with minibatch training loss = 0.771 and accuracy of 0.8\n",
      "Iteration 9066: with minibatch training loss = 0.89 and accuracy of 0.77\n",
      "Iteration 9067: with minibatch training loss = 0.991 and accuracy of 0.72\n",
      "Iteration 9068: with minibatch training loss = 0.758 and accuracy of 0.8\n",
      "Iteration 9069: with minibatch training loss = 0.765 and accuracy of 0.77\n",
      "Iteration 9070: with minibatch training loss = 0.646 and accuracy of 0.81\n",
      "Iteration 9071: with minibatch training loss = 0.797 and accuracy of 0.75\n",
      "Iteration 9072: with minibatch training loss = 0.77 and accuracy of 0.77\n",
      "Iteration 9073: with minibatch training loss = 0.924 and accuracy of 0.73\n",
      "Iteration 9074: with minibatch training loss = 0.801 and accuracy of 0.77\n",
      "Iteration 9075: with minibatch training loss = 0.741 and accuracy of 0.8\n",
      "Iteration 9076: with minibatch training loss = 0.774 and accuracy of 0.81\n",
      "Iteration 9077: with minibatch training loss = 0.814 and accuracy of 0.77\n",
      "Iteration 9078: with minibatch training loss = 0.705 and accuracy of 0.81\n",
      "Iteration 9079: with minibatch training loss = 0.926 and accuracy of 0.7\n",
      "Iteration 9080: with minibatch training loss = 0.683 and accuracy of 0.83\n",
      "Iteration 9081: with minibatch training loss = 0.8 and accuracy of 0.75\n",
      "Iteration 9082: with minibatch training loss = 0.899 and accuracy of 0.72\n",
      "Iteration 9083: with minibatch training loss = 0.938 and accuracy of 0.7\n",
      "Iteration 9084: with minibatch training loss = 0.713 and accuracy of 0.78\n",
      "Iteration 9085: with minibatch training loss = 0.992 and accuracy of 0.7\n",
      "Iteration 9086: with minibatch training loss = 0.924 and accuracy of 0.75\n",
      "Iteration 9087: with minibatch training loss = 0.645 and accuracy of 0.81\n",
      "Iteration 9088: with minibatch training loss = 1.09 and accuracy of 0.69\n",
      "Iteration 9089: with minibatch training loss = 0.745 and accuracy of 0.78\n",
      "Iteration 9090: with minibatch training loss = 0.848 and accuracy of 0.75\n",
      "Iteration 9091: with minibatch training loss = 0.919 and accuracy of 0.7\n",
      "Iteration 9092: with minibatch training loss = 0.564 and accuracy of 0.86\n",
      "Iteration 9093: with minibatch training loss = 0.77 and accuracy of 0.8\n",
      "Iteration 9094: with minibatch training loss = 0.865 and accuracy of 0.75\n",
      "Iteration 9095: with minibatch training loss = 0.586 and accuracy of 0.84\n",
      "Iteration 9096: with minibatch training loss = 1.02 and accuracy of 0.67\n",
      "Iteration 9097: with minibatch training loss = 0.644 and accuracy of 0.81\n",
      "Iteration 9098: with minibatch training loss = 0.984 and accuracy of 0.7\n",
      "Iteration 9099: with minibatch training loss = 0.688 and accuracy of 0.84\n",
      "Iteration 9100: with minibatch training loss = 0.628 and accuracy of 0.86\n",
      "Iteration 9101: with minibatch training loss = 1.13 and accuracy of 0.69\n",
      "Iteration 9102: with minibatch training loss = 0.848 and accuracy of 0.72\n",
      "Iteration 9103: with minibatch training loss = 0.877 and accuracy of 0.73\n",
      "Iteration 9104: with minibatch training loss = 0.642 and accuracy of 0.86\n",
      "Iteration 9105: with minibatch training loss = 1.02 and accuracy of 0.67\n",
      "Iteration 9106: with minibatch training loss = 0.97 and accuracy of 0.72\n",
      "Iteration 9107: with minibatch training loss = 0.872 and accuracy of 0.73\n",
      "Iteration 9108: with minibatch training loss = 0.577 and accuracy of 0.84\n",
      "Iteration 9109: with minibatch training loss = 0.982 and accuracy of 0.72\n",
      "Iteration 9110: with minibatch training loss = 0.643 and accuracy of 0.81\n",
      "Iteration 9111: with minibatch training loss = 0.801 and accuracy of 0.75\n",
      "Iteration 9112: with minibatch training loss = 0.733 and accuracy of 0.77\n",
      "Iteration 9113: with minibatch training loss = 0.821 and accuracy of 0.77\n",
      "Iteration 9114: with minibatch training loss = 0.963 and accuracy of 0.72\n",
      "Iteration 9115: with minibatch training loss = 0.738 and accuracy of 0.78\n",
      "Iteration 9116: with minibatch training loss = 0.913 and accuracy of 0.78\n",
      "Iteration 9117: with minibatch training loss = 0.825 and accuracy of 0.73\n",
      "Iteration 9118: with minibatch training loss = 0.781 and accuracy of 0.77\n",
      "Iteration 9119: with minibatch training loss = 0.69 and accuracy of 0.78\n",
      "Iteration 9120: with minibatch training loss = 0.772 and accuracy of 0.78\n",
      "Iteration 9121: with minibatch training loss = 0.504 and accuracy of 0.84\n",
      "Iteration 9122: with minibatch training loss = 0.474 and accuracy of 0.84\n",
      "Iteration 9123: with minibatch training loss = 0.954 and accuracy of 0.73\n",
      "Iteration 9124: with minibatch training loss = 0.839 and accuracy of 0.77\n",
      "Iteration 9125: with minibatch training loss = 1.14 and accuracy of 0.64\n",
      "Iteration 9126: with minibatch training loss = 0.909 and accuracy of 0.73\n",
      "Iteration 9127: with minibatch training loss = 1.31 and accuracy of 0.61\n",
      "Iteration 9128: with minibatch training loss = 0.615 and accuracy of 0.81\n",
      "Iteration 9129: with minibatch training loss = 0.855 and accuracy of 0.77\n",
      "Iteration 9130: with minibatch training loss = 0.581 and accuracy of 0.81\n",
      "Iteration 9131: with minibatch training loss = 0.759 and accuracy of 0.81\n",
      "Iteration 9132: with minibatch training loss = 0.745 and accuracy of 0.78\n",
      "Iteration 9133: with minibatch training loss = 0.619 and accuracy of 0.83\n",
      "Iteration 9134: with minibatch training loss = 1.23 and accuracy of 0.62\n",
      "Iteration 9135: with minibatch training loss = 1.14 and accuracy of 0.62\n",
      "Iteration 9136: with minibatch training loss = 0.958 and accuracy of 0.75\n",
      "Iteration 9137: with minibatch training loss = 0.777 and accuracy of 0.77\n",
      "Iteration 9138: with minibatch training loss = 0.947 and accuracy of 0.72\n",
      "Iteration 9139: with minibatch training loss = 1.07 and accuracy of 0.66\n",
      "Iteration 9140: with minibatch training loss = 0.84 and accuracy of 0.77\n",
      "Iteration 9141: with minibatch training loss = 0.442 and accuracy of 0.89\n",
      "Iteration 9142: with minibatch training loss = 0.613 and accuracy of 0.83\n",
      "Iteration 9143: with minibatch training loss = 0.778 and accuracy of 0.77\n",
      "Iteration 9144: with minibatch training loss = 0.704 and accuracy of 0.78\n",
      "Iteration 9145: with minibatch training loss = 0.596 and accuracy of 0.83\n",
      "Iteration 9146: with minibatch training loss = 0.637 and accuracy of 0.83\n",
      "Iteration 9147: with minibatch training loss = 0.565 and accuracy of 0.84\n",
      "Iteration 9148: with minibatch training loss = 0.865 and accuracy of 0.78\n",
      "Iteration 9149: with minibatch training loss = 0.872 and accuracy of 0.73\n",
      "Iteration 9150: with minibatch training loss = 0.744 and accuracy of 0.8\n",
      "Iteration 9151: with minibatch training loss = 0.946 and accuracy of 0.73\n",
      "Iteration 9152: with minibatch training loss = 0.491 and accuracy of 0.86\n",
      "Iteration 9153: with minibatch training loss = 0.788 and accuracy of 0.77\n",
      "Iteration 9154: with minibatch training loss = 0.649 and accuracy of 0.81\n",
      "Iteration 9155: with minibatch training loss = 0.609 and accuracy of 0.83\n",
      "Iteration 9156: with minibatch training loss = 0.689 and accuracy of 0.8\n",
      "Iteration 9157: with minibatch training loss = 0.811 and accuracy of 0.77\n",
      "Iteration 9158: with minibatch training loss = 0.61 and accuracy of 0.84\n",
      "Iteration 9159: with minibatch training loss = 0.662 and accuracy of 0.8\n",
      "Iteration 9160: with minibatch training loss = 1.14 and accuracy of 0.61\n",
      "Iteration 9161: with minibatch training loss = 1.03 and accuracy of 0.69\n",
      "Iteration 9162: with minibatch training loss = 1.01 and accuracy of 0.67\n",
      "Iteration 9163: with minibatch training loss = 0.442 and accuracy of 0.88\n",
      "Iteration 9164: with minibatch training loss = 0.71 and accuracy of 0.81\n",
      "Iteration 9165: with minibatch training loss = 0.813 and accuracy of 0.75\n",
      "Iteration 9166: with minibatch training loss = 1.02 and accuracy of 0.73\n",
      "Iteration 9167: with minibatch training loss = 0.676 and accuracy of 0.78\n",
      "Iteration 9168: with minibatch training loss = 0.92 and accuracy of 0.72\n",
      "Iteration 9169: with minibatch training loss = 0.841 and accuracy of 0.75\n",
      "Iteration 9170: with minibatch training loss = 0.535 and accuracy of 0.86\n",
      "Iteration 9171: with minibatch training loss = 0.696 and accuracy of 0.78\n",
      "Iteration 9172: with minibatch training loss = 0.858 and accuracy of 0.73\n",
      "Iteration 9173: with minibatch training loss = 0.695 and accuracy of 0.78\n",
      "Iteration 9174: with minibatch training loss = 0.637 and accuracy of 0.81\n",
      "Iteration 9175: with minibatch training loss = 0.891 and accuracy of 0.77\n",
      "Iteration 9176: with minibatch training loss = 0.939 and accuracy of 0.72\n",
      "Iteration 9177: with minibatch training loss = 0.598 and accuracy of 0.83\n",
      "Iteration 9178: with minibatch training loss = 0.897 and accuracy of 0.72\n",
      "Iteration 9179: with minibatch training loss = 0.821 and accuracy of 0.77\n",
      "Iteration 9180: with minibatch training loss = 0.702 and accuracy of 0.8\n",
      "Iteration 9181: with minibatch training loss = 0.79 and accuracy of 0.77\n",
      "Iteration 9182: with minibatch training loss = 0.445 and accuracy of 0.88\n",
      "Iteration 9183: with minibatch training loss = 0.796 and accuracy of 0.75\n",
      "Iteration 9184: with minibatch training loss = 1.17 and accuracy of 0.67\n",
      "Iteration 9185: with minibatch training loss = 0.846 and accuracy of 0.75\n",
      "Iteration 9186: with minibatch training loss = 0.876 and accuracy of 0.72\n",
      "Iteration 9187: with minibatch training loss = 0.571 and accuracy of 0.84\n",
      "Iteration 9188: with minibatch training loss = 0.735 and accuracy of 0.83\n",
      "Iteration 9189: with minibatch training loss = 0.739 and accuracy of 0.8\n",
      "Iteration 9190: with minibatch training loss = 1.02 and accuracy of 0.69\n",
      "Iteration 9191: with minibatch training loss = 0.885 and accuracy of 0.73\n",
      "Iteration 9192: with minibatch training loss = 0.86 and accuracy of 0.77\n",
      "Iteration 9193: with minibatch training loss = 0.642 and accuracy of 0.83\n",
      "Iteration 9194: with minibatch training loss = 0.683 and accuracy of 0.8\n",
      "Iteration 9195: with minibatch training loss = 0.7 and accuracy of 0.78\n",
      "Iteration 9196: with minibatch training loss = 0.881 and accuracy of 0.7\n",
      "Iteration 9197: with minibatch training loss = 0.564 and accuracy of 0.88\n",
      "Iteration 9198: with minibatch training loss = 0.742 and accuracy of 0.78\n",
      "Iteration 9199: with minibatch training loss = 0.894 and accuracy of 0.77\n",
      "Iteration 9200: with minibatch training loss = 0.858 and accuracy of 0.73\n",
      "Iteration 9201: with minibatch training loss = 0.667 and accuracy of 0.83\n",
      "Iteration 9202: with minibatch training loss = 0.863 and accuracy of 0.77\n",
      "Iteration 9203: with minibatch training loss = 0.632 and accuracy of 0.83\n",
      "Iteration 9204: with minibatch training loss = 0.698 and accuracy of 0.78\n",
      "Iteration 9205: with minibatch training loss = 1.01 and accuracy of 0.69\n",
      "Iteration 9206: with minibatch training loss = 0.979 and accuracy of 0.73\n",
      "Iteration 9207: with minibatch training loss = 0.715 and accuracy of 0.78\n",
      "Iteration 9208: with minibatch training loss = 0.856 and accuracy of 0.75\n",
      "Iteration 9209: with minibatch training loss = 0.966 and accuracy of 0.72\n",
      "Iteration 9210: with minibatch training loss = 0.74 and accuracy of 0.78\n",
      "Iteration 9211: with minibatch training loss = 0.778 and accuracy of 0.77\n",
      "Iteration 9212: with minibatch training loss = 0.881 and accuracy of 0.78\n",
      "Iteration 9213: with minibatch training loss = 0.549 and accuracy of 0.84\n",
      "Iteration 9214: with minibatch training loss = 0.584 and accuracy of 0.84\n",
      "Iteration 9215: with minibatch training loss = 0.7 and accuracy of 0.83\n",
      "Iteration 9216: with minibatch training loss = 0.503 and accuracy of 0.86\n",
      "Iteration 9217: with minibatch training loss = 0.679 and accuracy of 0.78\n",
      "Iteration 9218: with minibatch training loss = 0.633 and accuracy of 0.83\n",
      "Iteration 9219: with minibatch training loss = 0.817 and accuracy of 0.78\n",
      "Iteration 9220: with minibatch training loss = 0.691 and accuracy of 0.78\n",
      "Iteration 9221: with minibatch training loss = 0.915 and accuracy of 0.72\n",
      "Iteration 9222: with minibatch training loss = 1.05 and accuracy of 0.66\n",
      "Iteration 9223: with minibatch training loss = 0.642 and accuracy of 0.81\n",
      "Iteration 9224: with minibatch training loss = 0.81 and accuracy of 0.77\n",
      "Iteration 9225: with minibatch training loss = 0.843 and accuracy of 0.78\n",
      "Iteration 9226: with minibatch training loss = 1.02 and accuracy of 0.7\n",
      "Iteration 9227: with minibatch training loss = 0.852 and accuracy of 0.75\n",
      "Iteration 9228: with minibatch training loss = 0.997 and accuracy of 0.72\n",
      "Iteration 9229: with minibatch training loss = 0.858 and accuracy of 0.75\n",
      "Iteration 9230: with minibatch training loss = 0.738 and accuracy of 0.78\n",
      "Iteration 9231: with minibatch training loss = 0.628 and accuracy of 0.83\n",
      "Iteration 9232: with minibatch training loss = 0.921 and accuracy of 0.75\n",
      "Iteration 9233: with minibatch training loss = 0.76 and accuracy of 0.77\n",
      "Iteration 9234: with minibatch training loss = 0.884 and accuracy of 0.7\n",
      "Iteration 9235: with minibatch training loss = 0.679 and accuracy of 0.78\n",
      "Iteration 9236: with minibatch training loss = 0.388 and accuracy of 0.91\n",
      "Iteration 9237: with minibatch training loss = 1.07 and accuracy of 0.7\n",
      "Iteration 9238: with minibatch training loss = 0.764 and accuracy of 0.78\n",
      "Iteration 9239: with minibatch training loss = 0.896 and accuracy of 0.75\n",
      "Iteration 9240: with minibatch training loss = 0.647 and accuracy of 0.83\n",
      "Iteration 9241: with minibatch training loss = 0.718 and accuracy of 0.8\n",
      "Iteration 9242: with minibatch training loss = 0.658 and accuracy of 0.81\n",
      "Iteration 9243: with minibatch training loss = 0.775 and accuracy of 0.75\n",
      "Iteration 9244: with minibatch training loss = 0.681 and accuracy of 0.81\n",
      "Iteration 9245: with minibatch training loss = 0.705 and accuracy of 0.8\n",
      "Iteration 9246: with minibatch training loss = 0.657 and accuracy of 0.8\n",
      "Iteration 9247: with minibatch training loss = 0.823 and accuracy of 0.77\n",
      "Iteration 9248: with minibatch training loss = 0.577 and accuracy of 0.86\n",
      "Iteration 9249: with minibatch training loss = 0.619 and accuracy of 0.81\n",
      "Iteration 9250: with minibatch training loss = 0.808 and accuracy of 0.77\n",
      "Iteration 9251: with minibatch training loss = 1.02 and accuracy of 0.72\n",
      "Iteration 9252: with minibatch training loss = 0.552 and accuracy of 0.84\n",
      "Iteration 9253: with minibatch training loss = 0.974 and accuracy of 0.7\n",
      "Iteration 9254: with minibatch training loss = 0.792 and accuracy of 0.75\n",
      "Iteration 9255: with minibatch training loss = 0.668 and accuracy of 0.81\n",
      "Iteration 9256: with minibatch training loss = 0.806 and accuracy of 0.75\n",
      "Iteration 9257: with minibatch training loss = 0.486 and accuracy of 0.88\n",
      "Iteration 9258: with minibatch training loss = 0.627 and accuracy of 0.81\n",
      "Iteration 9259: with minibatch training loss = 0.619 and accuracy of 0.83\n",
      "Iteration 9260: with minibatch training loss = 0.767 and accuracy of 0.77\n",
      "Iteration 9261: with minibatch training loss = 0.682 and accuracy of 0.83\n",
      "Iteration 9262: with minibatch training loss = 0.594 and accuracy of 0.81\n",
      "Iteration 9263: with minibatch training loss = 0.887 and accuracy of 0.73\n",
      "Iteration 9264: with minibatch training loss = 0.813 and accuracy of 0.75\n",
      "Iteration 9265: with minibatch training loss = 1.03 and accuracy of 0.7\n",
      "Iteration 9266: with minibatch training loss = 0.961 and accuracy of 0.73\n",
      "Iteration 9267: with minibatch training loss = 0.625 and accuracy of 0.81\n",
      "Iteration 9268: with minibatch training loss = 0.503 and accuracy of 0.88\n",
      "Iteration 9269: with minibatch training loss = 0.561 and accuracy of 0.88\n",
      "Iteration 9270: with minibatch training loss = 0.936 and accuracy of 0.73\n",
      "Iteration 9271: with minibatch training loss = 0.704 and accuracy of 0.8\n",
      "Iteration 9272: with minibatch training loss = 1.13 and accuracy of 0.66\n",
      "Iteration 9273: with minibatch training loss = 1.07 and accuracy of 0.7\n",
      "Iteration 9274: with minibatch training loss = 0.882 and accuracy of 0.77\n",
      "Iteration 9275: with minibatch training loss = 0.735 and accuracy of 0.78\n",
      "Iteration 9276: with minibatch training loss = 0.688 and accuracy of 0.81\n",
      "Iteration 9277: with minibatch training loss = 0.666 and accuracy of 0.8\n",
      "Iteration 9278: with minibatch training loss = 0.864 and accuracy of 0.77\n",
      "Iteration 9279: with minibatch training loss = 0.652 and accuracy of 0.81\n",
      "Iteration 9280: with minibatch training loss = 0.733 and accuracy of 0.78\n",
      "Iteration 9281: with minibatch training loss = 0.827 and accuracy of 0.73\n",
      "Iteration 9282: with minibatch training loss = 0.862 and accuracy of 0.77\n",
      "Iteration 9283: with minibatch training loss = 1.17 and accuracy of 0.64\n",
      "Iteration 9284: with minibatch training loss = 0.949 and accuracy of 0.72\n",
      "Iteration 9285: with minibatch training loss = 0.848 and accuracy of 0.75\n",
      "Iteration 9286: with minibatch training loss = 0.71 and accuracy of 0.83\n",
      "Iteration 9287: with minibatch training loss = 0.887 and accuracy of 0.75\n",
      "Iteration 9288: with minibatch training loss = 0.789 and accuracy of 0.8\n",
      "Iteration 9289: with minibatch training loss = 0.83 and accuracy of 0.75\n",
      "Iteration 9290: with minibatch training loss = 0.758 and accuracy of 0.75\n",
      "Iteration 9291: with minibatch training loss = 0.796 and accuracy of 0.8\n",
      "Iteration 9292: with minibatch training loss = 1.12 and accuracy of 0.66\n",
      "Iteration 9293: with minibatch training loss = 0.785 and accuracy of 0.78\n",
      "Iteration 9294: with minibatch training loss = 0.813 and accuracy of 0.75\n",
      "Iteration 9295: with minibatch training loss = 0.96 and accuracy of 0.72\n",
      "Iteration 9296: with minibatch training loss = 1.03 and accuracy of 0.69\n",
      "Iteration 9297: with minibatch training loss = 1.03 and accuracy of 0.7\n",
      "Iteration 9298: with minibatch training loss = 0.65 and accuracy of 0.8\n",
      "Iteration 9299: with minibatch training loss = 0.92 and accuracy of 0.72\n",
      "Iteration 9300: with minibatch training loss = 0.739 and accuracy of 0.78\n",
      "Iteration 9301: with minibatch training loss = 0.825 and accuracy of 0.77\n",
      "Iteration 9302: with minibatch training loss = 1.12 and accuracy of 0.67\n",
      "Iteration 9303: with minibatch training loss = 0.73 and accuracy of 0.78\n",
      "Iteration 9304: with minibatch training loss = 0.664 and accuracy of 0.83\n",
      "Iteration 9305: with minibatch training loss = 0.692 and accuracy of 0.77\n",
      "Iteration 9306: with minibatch training loss = 0.842 and accuracy of 0.77\n",
      "Iteration 9307: with minibatch training loss = 0.855 and accuracy of 0.75\n",
      "Iteration 9308: with minibatch training loss = 0.759 and accuracy of 0.8\n",
      "Iteration 9309: with minibatch training loss = 0.829 and accuracy of 0.75\n",
      "Iteration 9310: with minibatch training loss = 0.756 and accuracy of 0.77\n",
      "Iteration 9311: with minibatch training loss = 1.02 and accuracy of 0.73\n",
      "Iteration 9312: with minibatch training loss = 0.871 and accuracy of 0.77\n",
      "Iteration 9313: with minibatch training loss = 0.689 and accuracy of 0.83\n",
      "Iteration 9314: with minibatch training loss = 0.823 and accuracy of 0.78\n",
      "Iteration 9315: with minibatch training loss = 0.702 and accuracy of 0.77\n",
      "Iteration 9316: with minibatch training loss = 0.941 and accuracy of 0.72\n",
      "Iteration 9317: with minibatch training loss = 0.859 and accuracy of 0.78\n",
      "Iteration 9318: with minibatch training loss = 0.903 and accuracy of 0.77\n",
      "Iteration 9319: with minibatch training loss = 0.975 and accuracy of 0.73\n",
      "Iteration 9320: with minibatch training loss = 0.788 and accuracy of 0.8\n",
      "Iteration 9321: with minibatch training loss = 0.744 and accuracy of 0.78\n",
      "Iteration 9322: with minibatch training loss = 1.08 and accuracy of 0.67\n",
      "Iteration 9323: with minibatch training loss = 0.856 and accuracy of 0.72\n",
      "Iteration 9324: with minibatch training loss = 0.759 and accuracy of 0.77\n",
      "Iteration 9325: with minibatch training loss = 0.905 and accuracy of 0.73\n",
      "Iteration 9326: with minibatch training loss = 0.94 and accuracy of 0.77\n",
      "Iteration 9327: with minibatch training loss = 0.804 and accuracy of 0.77\n",
      "Iteration 9328: with minibatch training loss = 0.796 and accuracy of 0.81\n",
      "Iteration 9329: with minibatch training loss = 0.716 and accuracy of 0.78\n",
      "Iteration 9330: with minibatch training loss = 0.808 and accuracy of 0.78\n",
      "Iteration 9331: with minibatch training loss = 0.752 and accuracy of 0.77\n",
      "Iteration 9332: with minibatch training loss = 0.677 and accuracy of 0.8\n",
      "Iteration 9333: with minibatch training loss = 0.763 and accuracy of 0.75\n",
      "Iteration 9334: with minibatch training loss = 0.728 and accuracy of 0.78\n",
      "Iteration 9335: with minibatch training loss = 0.761 and accuracy of 0.73\n",
      "Iteration 9336: with minibatch training loss = 1.01 and accuracy of 0.77\n",
      "Iteration 9337: with minibatch training loss = 0.848 and accuracy of 0.73\n",
      "Iteration 9338: with minibatch training loss = 0.87 and accuracy of 0.72\n",
      "Iteration 9339: with minibatch training loss = 0.412 and accuracy of 0.89\n",
      "Iteration 9340: with minibatch training loss = 0.576 and accuracy of 0.84\n",
      "Iteration 9341: with minibatch training loss = 0.853 and accuracy of 0.75\n",
      "Iteration 9342: with minibatch training loss = 0.62 and accuracy of 0.83\n",
      "Iteration 9343: with minibatch training loss = 1.05 and accuracy of 0.7\n",
      "Iteration 9344: with minibatch training loss = 0.667 and accuracy of 0.81\n",
      "Iteration 9345: with minibatch training loss = 0.708 and accuracy of 0.78\n",
      "Iteration 9346: with minibatch training loss = 0.524 and accuracy of 0.84\n",
      "Iteration 9347: with minibatch training loss = 0.751 and accuracy of 0.77\n",
      "Iteration 9348: with minibatch training loss = 0.396 and accuracy of 0.92\n",
      "Iteration 9349: with minibatch training loss = 0.927 and accuracy of 0.73\n",
      "Iteration 9350: with minibatch training loss = 0.812 and accuracy of 0.78\n",
      "Iteration 9351: with minibatch training loss = 0.841 and accuracy of 0.73\n",
      "Iteration 9352: with minibatch training loss = 0.8 and accuracy of 0.75\n",
      "Iteration 9353: with minibatch training loss = 0.977 and accuracy of 0.72\n",
      "Iteration 9354: with minibatch training loss = 0.81 and accuracy of 0.78\n",
      "Iteration 9355: with minibatch training loss = 0.837 and accuracy of 0.73\n",
      "Iteration 9356: with minibatch training loss = 0.785 and accuracy of 0.78\n",
      "Iteration 9357: with minibatch training loss = 0.981 and accuracy of 0.72\n",
      "Iteration 9358: with minibatch training loss = 0.801 and accuracy of 0.77\n",
      "Iteration 9359: with minibatch training loss = 1.08 and accuracy of 0.69\n",
      "Iteration 9360: with minibatch training loss = 0.563 and accuracy of 0.84\n",
      "Iteration 9361: with minibatch training loss = 1.04 and accuracy of 0.69\n",
      "Iteration 9362: with minibatch training loss = 0.521 and accuracy of 0.86\n",
      "Iteration 9363: with minibatch training loss = 0.689 and accuracy of 0.8\n",
      "Iteration 9364: with minibatch training loss = 0.892 and accuracy of 0.75\n",
      "Iteration 9365: with minibatch training loss = 0.77 and accuracy of 0.78\n",
      "Iteration 9366: with minibatch training loss = 0.749 and accuracy of 0.78\n",
      "Iteration 9367: with minibatch training loss = 0.909 and accuracy of 0.77\n",
      "Iteration 9368: with minibatch training loss = 1.11 and accuracy of 0.64\n",
      "Iteration 9369: with minibatch training loss = 0.785 and accuracy of 0.77\n",
      "Iteration 9370: with minibatch training loss = 0.946 and accuracy of 0.72\n",
      "Iteration 9371: with minibatch training loss = 0.919 and accuracy of 0.73\n",
      "Iteration 9372: with minibatch training loss = 0.712 and accuracy of 0.8\n",
      "Iteration 9373: with minibatch training loss = 0.624 and accuracy of 0.83\n",
      "Iteration 9374: with minibatch training loss = 0.931 and accuracy of 0.77\n",
      "Iteration 9375: with minibatch training loss = 0.558 and accuracy of 0.86\n",
      "Iteration 9376: with minibatch training loss = 0.82 and accuracy of 0.78\n",
      "Iteration 9377: with minibatch training loss = 0.827 and accuracy of 0.78\n",
      "Iteration 9378: with minibatch training loss = 0.985 and accuracy of 0.7\n",
      "Iteration 9379: with minibatch training loss = 0.686 and accuracy of 0.83\n",
      "Iteration 9380: with minibatch training loss = 0.691 and accuracy of 0.83\n",
      "Iteration 9381: with minibatch training loss = 0.886 and accuracy of 0.7\n",
      "Iteration 9382: with minibatch training loss = 0.943 and accuracy of 0.75\n",
      "Iteration 9383: with minibatch training loss = 0.582 and accuracy of 0.83\n",
      "Iteration 9384: with minibatch training loss = 0.524 and accuracy of 0.86\n",
      "Iteration 9385: with minibatch training loss = 0.942 and accuracy of 0.72\n",
      "Iteration 9386: with minibatch training loss = 0.801 and accuracy of 0.75\n",
      "Iteration 9387: with minibatch training loss = 0.702 and accuracy of 0.77\n",
      "Iteration 9388: with minibatch training loss = 0.81 and accuracy of 0.77\n",
      "Iteration 9389: with minibatch training loss = 0.657 and accuracy of 0.81\n",
      "Iteration 9390: with minibatch training loss = 0.804 and accuracy of 0.75\n",
      "Iteration 9391: with minibatch training loss = 0.901 and accuracy of 0.73\n",
      "Iteration 9392: with minibatch training loss = 0.608 and accuracy of 0.83\n",
      "Iteration 9393: with minibatch training loss = 0.916 and accuracy of 0.77\n",
      "Iteration 9394: with minibatch training loss = 0.74 and accuracy of 0.78\n",
      "Iteration 9395: with minibatch training loss = 0.798 and accuracy of 0.75\n",
      "Iteration 9396: with minibatch training loss = 1 and accuracy of 0.69\n",
      "Iteration 9397: with minibatch training loss = 0.804 and accuracy of 0.73\n",
      "Iteration 9398: with minibatch training loss = 0.752 and accuracy of 0.77\n",
      "Iteration 9399: with minibatch training loss = 0.654 and accuracy of 0.83\n",
      "Iteration 9400: with minibatch training loss = 0.624 and accuracy of 0.81\n",
      "Iteration 9401: with minibatch training loss = 0.786 and accuracy of 0.77\n",
      "Iteration 9402: with minibatch training loss = 0.697 and accuracy of 0.8\n",
      "Iteration 9403: with minibatch training loss = 0.758 and accuracy of 0.78\n",
      "Iteration 9404: with minibatch training loss = 0.747 and accuracy of 0.75\n",
      "Iteration 9405: with minibatch training loss = 0.883 and accuracy of 0.72\n",
      "Iteration 9406: with minibatch training loss = 0.964 and accuracy of 0.73\n",
      "Iteration 9407: with minibatch training loss = 0.618 and accuracy of 0.86\n",
      "Iteration 9408: with minibatch training loss = 0.712 and accuracy of 0.78\n",
      "Iteration 9409: with minibatch training loss = 0.796 and accuracy of 0.77\n",
      "Iteration 9410: with minibatch training loss = 0.838 and accuracy of 0.75\n",
      "Iteration 9411: with minibatch training loss = 0.771 and accuracy of 0.75\n",
      "Iteration 9412: with minibatch training loss = 0.924 and accuracy of 0.73\n",
      "Iteration 9413: with minibatch training loss = 0.647 and accuracy of 0.8\n",
      "Iteration 9414: with minibatch training loss = 0.765 and accuracy of 0.77\n",
      "Iteration 9415: with minibatch training loss = 0.604 and accuracy of 0.86\n",
      "Iteration 9416: with minibatch training loss = 0.805 and accuracy of 0.77\n",
      "Iteration 9417: with minibatch training loss = 0.785 and accuracy of 0.78\n",
      "Iteration 9418: with minibatch training loss = 0.987 and accuracy of 0.69\n",
      "Iteration 9419: with minibatch training loss = 0.528 and accuracy of 0.88\n",
      "Iteration 9420: with minibatch training loss = 1 and accuracy of 0.69\n",
      "Iteration 9421: with minibatch training loss = 1.03 and accuracy of 0.72\n",
      "Iteration 9422: with minibatch training loss = 0.818 and accuracy of 0.77\n",
      "Iteration 9423: with minibatch training loss = 0.954 and accuracy of 0.72\n",
      "Iteration 9424: with minibatch training loss = 0.608 and accuracy of 0.83\n",
      "Iteration 9425: with minibatch training loss = 0.748 and accuracy of 0.77\n",
      "Iteration 9426: with minibatch training loss = 0.884 and accuracy of 0.73\n",
      "Iteration 9427: with minibatch training loss = 1.03 and accuracy of 0.69\n",
      "Iteration 9428: with minibatch training loss = 0.648 and accuracy of 0.83\n",
      "Iteration 9429: with minibatch training loss = 0.751 and accuracy of 0.77\n",
      "Iteration 9430: with minibatch training loss = 0.83 and accuracy of 0.8\n",
      "Iteration 9431: with minibatch training loss = 0.626 and accuracy of 0.83\n",
      "Iteration 9432: with minibatch training loss = 0.966 and accuracy of 0.73\n",
      "Iteration 9433: with minibatch training loss = 0.514 and accuracy of 0.84\n",
      "Iteration 9434: with minibatch training loss = 0.909 and accuracy of 0.73\n",
      "Iteration 9435: with minibatch training loss = 0.796 and accuracy of 0.77\n",
      "Iteration 9436: with minibatch training loss = 0.603 and accuracy of 0.83\n",
      "Iteration 9437: with minibatch training loss = 0.949 and accuracy of 0.7\n",
      "Iteration 9438: with minibatch training loss = 0.443 and accuracy of 0.89\n",
      "Iteration 9439: with minibatch training loss = 0.502 and accuracy of 0.88\n",
      "Iteration 9440: with minibatch training loss = 1.01 and accuracy of 0.7\n",
      "Iteration 9441: with minibatch training loss = 0.57 and accuracy of 0.84\n",
      "Iteration 9442: with minibatch training loss = 0.778 and accuracy of 0.77\n",
      "Iteration 9443: with minibatch training loss = 0.552 and accuracy of 0.86\n",
      "Iteration 9444: with minibatch training loss = 0.886 and accuracy of 0.73\n",
      "Iteration 9445: with minibatch training loss = 0.915 and accuracy of 0.72\n",
      "Iteration 9446: with minibatch training loss = 0.679 and accuracy of 0.81\n",
      "Iteration 9447: with minibatch training loss = 0.504 and accuracy of 0.84\n",
      "Iteration 9448: with minibatch training loss = 0.575 and accuracy of 0.84\n",
      "Iteration 9449: with minibatch training loss = 0.575 and accuracy of 0.84\n",
      "Iteration 9450: with minibatch training loss = 0.64 and accuracy of 0.83\n",
      "Iteration 9451: with minibatch training loss = 0.729 and accuracy of 0.77\n",
      "Iteration 9452: with minibatch training loss = 0.858 and accuracy of 0.77\n",
      "Iteration 9453: with minibatch training loss = 0.746 and accuracy of 0.8\n",
      "Iteration 9454: with minibatch training loss = 0.812 and accuracy of 0.77\n",
      "Iteration 9455: with minibatch training loss = 0.803 and accuracy of 0.75\n",
      "Iteration 9456: with minibatch training loss = 0.893 and accuracy of 0.72\n",
      "Iteration 9457: with minibatch training loss = 0.665 and accuracy of 0.81\n",
      "Iteration 9458: with minibatch training loss = 0.857 and accuracy of 0.73\n",
      "Iteration 9459: with minibatch training loss = 0.59 and accuracy of 0.84\n",
      "Iteration 9460: with minibatch training loss = 0.63 and accuracy of 0.86\n",
      "Iteration 9461: with minibatch training loss = 1.03 and accuracy of 0.7\n",
      "Iteration 9462: with minibatch training loss = 0.83 and accuracy of 0.77\n",
      "Iteration 9463: with minibatch training loss = 0.773 and accuracy of 0.78\n",
      "Iteration 9464: with minibatch training loss = 0.815 and accuracy of 0.75\n",
      "Iteration 9465: with minibatch training loss = 0.781 and accuracy of 0.77\n",
      "Iteration 9466: with minibatch training loss = 0.753 and accuracy of 0.8\n",
      "Iteration 9467: with minibatch training loss = 0.78 and accuracy of 0.73\n",
      "Iteration 9468: with minibatch training loss = 0.589 and accuracy of 0.83\n",
      "Iteration 9469: with minibatch training loss = 0.668 and accuracy of 0.81\n",
      "Iteration 9470: with minibatch training loss = 0.681 and accuracy of 0.78\n",
      "Iteration 9471: with minibatch training loss = 0.831 and accuracy of 0.75\n",
      "Iteration 9472: with minibatch training loss = 0.685 and accuracy of 0.81\n",
      "Iteration 9473: with minibatch training loss = 0.764 and accuracy of 0.77\n",
      "Iteration 9474: with minibatch training loss = 0.596 and accuracy of 0.81\n",
      "Iteration 9475: with minibatch training loss = 0.632 and accuracy of 0.81\n",
      "Iteration 9476: with minibatch training loss = 0.883 and accuracy of 0.7\n",
      "Iteration 9477: with minibatch training loss = 0.608 and accuracy of 0.81\n",
      "Iteration 9478: with minibatch training loss = 0.631 and accuracy of 0.83\n",
      "Iteration 9479: with minibatch training loss = 1.01 and accuracy of 0.72\n",
      "Iteration 9480: with minibatch training loss = 0.635 and accuracy of 0.81\n",
      "Iteration 9481: with minibatch training loss = 0.829 and accuracy of 0.77\n",
      "Iteration 9482: with minibatch training loss = 0.572 and accuracy of 0.84\n",
      "Iteration 9483: with minibatch training loss = 0.776 and accuracy of 0.75\n",
      "Iteration 9484: with minibatch training loss = 1.2 and accuracy of 0.67\n",
      "Iteration 9485: with minibatch training loss = 0.984 and accuracy of 0.73\n",
      "Iteration 9486: with minibatch training loss = 0.524 and accuracy of 0.84\n",
      "Iteration 9487: with minibatch training loss = 0.618 and accuracy of 0.83\n",
      "Iteration 9488: with minibatch training loss = 0.434 and accuracy of 0.91\n",
      "Iteration 9489: with minibatch training loss = 0.77 and accuracy of 0.78\n",
      "Iteration 9490: with minibatch training loss = 0.7 and accuracy of 0.83\n",
      "Iteration 9491: with minibatch training loss = 0.728 and accuracy of 0.81\n",
      "Iteration 9492: with minibatch training loss = 0.88 and accuracy of 0.75\n",
      "Iteration 9493: with minibatch training loss = 0.74 and accuracy of 0.8\n",
      "Iteration 9494: with minibatch training loss = 0.83 and accuracy of 0.78\n",
      "Iteration 9495: with minibatch training loss = 0.948 and accuracy of 0.72\n",
      "Iteration 9496: with minibatch training loss = 0.649 and accuracy of 0.81\n",
      "Iteration 9497: with minibatch training loss = 0.807 and accuracy of 0.75\n",
      "Iteration 9498: with minibatch training loss = 0.772 and accuracy of 0.8\n",
      "Iteration 9499: with minibatch training loss = 0.977 and accuracy of 0.7\n",
      "Iteration 9500: with minibatch training loss = 0.477 and accuracy of 0.86\n",
      "Iteration 9501: with minibatch training loss = 0.529 and accuracy of 0.86\n",
      "Iteration 9502: with minibatch training loss = 0.63 and accuracy of 0.86\n",
      "Iteration 9503: with minibatch training loss = 0.844 and accuracy of 0.78\n",
      "Iteration 9504: with minibatch training loss = 0.997 and accuracy of 0.69\n",
      "Iteration 9505: with minibatch training loss = 1.18 and accuracy of 0.61\n",
      "Iteration 9506: with minibatch training loss = 0.962 and accuracy of 0.73\n",
      "Iteration 9507: with minibatch training loss = 0.527 and accuracy of 0.86\n",
      "Iteration 9508: with minibatch training loss = 0.558 and accuracy of 0.84\n",
      "Iteration 9509: with minibatch training loss = 0.534 and accuracy of 0.86\n",
      "Iteration 9510: with minibatch training loss = 0.752 and accuracy of 0.8\n",
      "Iteration 9511: with minibatch training loss = 0.934 and accuracy of 0.73\n",
      "Iteration 9512: with minibatch training loss = 0.539 and accuracy of 0.84\n",
      "Iteration 9513: with minibatch training loss = 0.87 and accuracy of 0.77\n",
      "Iteration 9514: with minibatch training loss = 0.773 and accuracy of 0.8\n",
      "Iteration 9515: with minibatch training loss = 0.802 and accuracy of 0.78\n",
      "Iteration 9516: with minibatch training loss = 0.404 and accuracy of 0.92\n",
      "Iteration 9517: with minibatch training loss = 0.806 and accuracy of 0.77\n",
      "Iteration 9518: with minibatch training loss = 0.918 and accuracy of 0.73\n",
      "Iteration 9519: with minibatch training loss = 0.715 and accuracy of 0.8\n",
      "Iteration 9520: with minibatch training loss = 0.859 and accuracy of 0.73\n",
      "Iteration 9521: with minibatch training loss = 0.625 and accuracy of 0.83\n",
      "Iteration 9522: with minibatch training loss = 0.831 and accuracy of 0.77\n",
      "Iteration 9523: with minibatch training loss = 0.625 and accuracy of 0.8\n",
      "Iteration 9524: with minibatch training loss = 0.611 and accuracy of 0.83\n",
      "Iteration 9525: with minibatch training loss = 0.507 and accuracy of 0.86\n",
      "Iteration 9526: with minibatch training loss = 0.532 and accuracy of 0.88\n",
      "Iteration 9527: with minibatch training loss = 0.854 and accuracy of 0.75\n",
      "Iteration 9528: with minibatch training loss = 0.802 and accuracy of 0.78\n",
      "Iteration 9529: with minibatch training loss = 0.631 and accuracy of 0.84\n",
      "Iteration 9530: with minibatch training loss = 0.467 and accuracy of 0.91\n",
      "Iteration 9531: with minibatch training loss = 0.741 and accuracy of 0.83\n",
      "Iteration 9532: with minibatch training loss = 0.704 and accuracy of 0.8\n",
      "Iteration 9533: with minibatch training loss = 0.573 and accuracy of 0.83\n",
      "Iteration 9534: with minibatch training loss = 0.86 and accuracy of 0.73\n",
      "Iteration 9535: with minibatch training loss = 0.753 and accuracy of 0.78\n",
      "Iteration 9536: with minibatch training loss = 1.29 and accuracy of 0.58\n",
      "Iteration 9537: with minibatch training loss = 0.731 and accuracy of 0.8\n",
      "Iteration 9538: with minibatch training loss = 0.589 and accuracy of 0.83\n",
      "Iteration 9539: with minibatch training loss = 0.602 and accuracy of 0.83\n",
      "Iteration 9540: with minibatch training loss = 1.19 and accuracy of 0.67\n",
      "Iteration 9541: with minibatch training loss = 0.855 and accuracy of 0.77\n",
      "Iteration 9542: with minibatch training loss = 0.796 and accuracy of 0.77\n",
      "Iteration 9543: with minibatch training loss = 0.884 and accuracy of 0.75\n",
      "Iteration 9544: with minibatch training loss = 0.807 and accuracy of 0.81\n",
      "Iteration 9545: with minibatch training loss = 0.655 and accuracy of 0.8\n",
      "Iteration 9546: with minibatch training loss = 0.921 and accuracy of 0.75\n",
      "Iteration 9547: with minibatch training loss = 0.86 and accuracy of 0.78\n",
      "Iteration 9548: with minibatch training loss = 1 and accuracy of 0.72\n",
      "Iteration 9549: with minibatch training loss = 0.745 and accuracy of 0.81\n",
      "Iteration 9550: with minibatch training loss = 0.597 and accuracy of 0.83\n",
      "Iteration 9551: with minibatch training loss = 0.8 and accuracy of 0.78\n",
      "Iteration 9552: with minibatch training loss = 0.801 and accuracy of 0.78\n",
      "Iteration 9553: with minibatch training loss = 1.11 and accuracy of 0.66\n",
      "Iteration 9554: with minibatch training loss = 0.993 and accuracy of 0.75\n",
      "Iteration 9555: with minibatch training loss = 0.566 and accuracy of 0.83\n",
      "Iteration 9556: with minibatch training loss = 0.621 and accuracy of 0.83\n",
      "Iteration 9557: with minibatch training loss = 0.768 and accuracy of 0.77\n",
      "Iteration 9558: with minibatch training loss = 0.828 and accuracy of 0.75\n",
      "Iteration 9559: with minibatch training loss = 0.798 and accuracy of 0.78\n",
      "Iteration 9560: with minibatch training loss = 0.827 and accuracy of 0.72\n",
      "Iteration 9561: with minibatch training loss = 0.565 and accuracy of 0.86\n",
      "Iteration 9562: with minibatch training loss = 0.864 and accuracy of 0.72\n",
      "Iteration 9563: with minibatch training loss = 1.15 and accuracy of 0.67\n",
      "Iteration 9564: with minibatch training loss = 0.941 and accuracy of 0.72\n",
      "Iteration 9565: with minibatch training loss = 0.7 and accuracy of 0.84\n",
      "Iteration 9566: with minibatch training loss = 0.74 and accuracy of 0.78\n",
      "Iteration 9567: with minibatch training loss = 0.697 and accuracy of 0.8\n",
      "Iteration 9568: with minibatch training loss = 0.842 and accuracy of 0.78\n",
      "Iteration 9569: with minibatch training loss = 0.633 and accuracy of 0.83\n",
      "Iteration 9570: with minibatch training loss = 0.985 and accuracy of 0.7\n",
      "Iteration 9571: with minibatch training loss = 0.741 and accuracy of 0.78\n",
      "Iteration 9572: with minibatch training loss = 0.427 and accuracy of 0.89\n",
      "Iteration 9573: with minibatch training loss = 0.712 and accuracy of 0.81\n",
      "Iteration 9574: with minibatch training loss = 0.638 and accuracy of 0.83\n",
      "Iteration 9575: with minibatch training loss = 0.733 and accuracy of 0.8\n",
      "Iteration 9576: with minibatch training loss = 0.636 and accuracy of 0.81\n",
      "Iteration 9577: with minibatch training loss = 0.689 and accuracy of 0.8\n",
      "Iteration 9578: with minibatch training loss = 0.673 and accuracy of 0.81\n",
      "Iteration 9579: with minibatch training loss = 0.813 and accuracy of 0.78\n",
      "Iteration 9580: with minibatch training loss = 0.88 and accuracy of 0.73\n",
      "Iteration 9581: with minibatch training loss = 0.68 and accuracy of 0.83\n",
      "Iteration 9582: with minibatch training loss = 0.741 and accuracy of 0.78\n",
      "Iteration 9583: with minibatch training loss = 0.835 and accuracy of 0.75\n",
      "Iteration 9584: with minibatch training loss = 0.739 and accuracy of 0.78\n",
      "Iteration 9585: with minibatch training loss = 0.902 and accuracy of 0.73\n",
      "Iteration 9586: with minibatch training loss = 0.835 and accuracy of 0.72\n",
      "Iteration 9587: with minibatch training loss = 1 and accuracy of 0.73\n",
      "Iteration 9588: with minibatch training loss = 0.677 and accuracy of 0.78\n",
      "Iteration 9589: with minibatch training loss = 0.714 and accuracy of 0.78\n",
      "Iteration 9590: with minibatch training loss = 0.856 and accuracy of 0.75\n",
      "Iteration 9591: with minibatch training loss = 0.853 and accuracy of 0.73\n",
      "Iteration 9592: with minibatch training loss = 0.644 and accuracy of 0.83\n",
      "Iteration 9593: with minibatch training loss = 0.834 and accuracy of 0.77\n",
      "Iteration 9594: with minibatch training loss = 0.817 and accuracy of 0.77\n",
      "Iteration 9595: with minibatch training loss = 0.911 and accuracy of 0.7\n",
      "Iteration 9596: with minibatch training loss = 0.794 and accuracy of 0.73\n",
      "Iteration 9597: with minibatch training loss = 0.872 and accuracy of 0.75\n",
      "Iteration 9598: with minibatch training loss = 0.775 and accuracy of 0.8\n",
      "Iteration 9599: with minibatch training loss = 0.761 and accuracy of 0.77\n",
      "Iteration 9600: with minibatch training loss = 0.671 and accuracy of 0.83\n",
      "Iteration 9601: with minibatch training loss = 0.863 and accuracy of 0.73\n",
      "Iteration 9602: with minibatch training loss = 0.939 and accuracy of 0.72\n",
      "Iteration 9603: with minibatch training loss = 0.81 and accuracy of 0.78\n",
      "Iteration 9604: with minibatch training loss = 0.876 and accuracy of 0.72\n",
      "Iteration 9605: with minibatch training loss = 0.677 and accuracy of 0.81\n",
      "Iteration 9606: with minibatch training loss = 0.641 and accuracy of 0.83\n",
      "Iteration 9607: with minibatch training loss = 0.932 and accuracy of 0.72\n",
      "Iteration 9608: with minibatch training loss = 0.636 and accuracy of 0.8\n",
      "Iteration 9609: with minibatch training loss = 0.691 and accuracy of 0.81\n",
      "Iteration 9610: with minibatch training loss = 0.79 and accuracy of 0.72\n",
      "Iteration 9611: with minibatch training loss = 0.516 and accuracy of 0.83\n",
      "Iteration 9612: with minibatch training loss = 0.799 and accuracy of 0.78\n",
      "Iteration 9613: with minibatch training loss = 0.816 and accuracy of 0.75\n",
      "Iteration 9614: with minibatch training loss = 0.981 and accuracy of 0.73\n",
      "Iteration 9615: with minibatch training loss = 0.717 and accuracy of 0.81\n",
      "Iteration 9616: with minibatch training loss = 0.785 and accuracy of 0.77\n",
      "Iteration 9617: with minibatch training loss = 0.769 and accuracy of 0.78\n",
      "Iteration 9618: with minibatch training loss = 0.572 and accuracy of 0.83\n",
      "Iteration 9619: with minibatch training loss = 0.823 and accuracy of 0.78\n",
      "Iteration 9620: with minibatch training loss = 0.871 and accuracy of 0.75\n",
      "Iteration 9621: with minibatch training loss = 0.963 and accuracy of 0.75\n",
      "Iteration 9622: with minibatch training loss = 0.598 and accuracy of 0.88\n",
      "Iteration 9623: with minibatch training loss = 0.738 and accuracy of 0.8\n",
      "Iteration 9624: with minibatch training loss = 0.734 and accuracy of 0.81\n",
      "Iteration 9625: with minibatch training loss = 0.826 and accuracy of 0.8\n",
      "Iteration 9626: with minibatch training loss = 0.789 and accuracy of 0.77\n",
      "Iteration 9627: with minibatch training loss = 0.754 and accuracy of 0.78\n",
      "Iteration 9628: with minibatch training loss = 0.342 and accuracy of 0.91\n",
      "Iteration 9629: with minibatch training loss = 0.672 and accuracy of 0.77\n",
      "Iteration 9630: with minibatch training loss = 0.651 and accuracy of 0.81\n",
      "Iteration 9631: with minibatch training loss = 0.88 and accuracy of 0.73\n",
      "Iteration 9632: with minibatch training loss = 0.555 and accuracy of 0.86\n",
      "Iteration 9633: with minibatch training loss = 0.871 and accuracy of 0.75\n",
      "Iteration 9634: with minibatch training loss = 0.787 and accuracy of 0.8\n",
      "Iteration 9635: with minibatch training loss = 0.871 and accuracy of 0.77\n",
      "Iteration 9636: with minibatch training loss = 0.663 and accuracy of 0.83\n",
      "Iteration 9637: with minibatch training loss = 0.582 and accuracy of 0.86\n",
      "Iteration 9638: with minibatch training loss = 0.802 and accuracy of 0.78\n",
      "Iteration 9639: with minibatch training loss = 0.729 and accuracy of 0.8\n",
      "Iteration 9640: with minibatch training loss = 0.593 and accuracy of 0.84\n",
      "Iteration 9641: with minibatch training loss = 0.819 and accuracy of 0.78\n",
      "Iteration 9642: with minibatch training loss = 0.382 and accuracy of 0.89\n",
      "Iteration 9643: with minibatch training loss = 0.863 and accuracy of 0.77\n",
      "Iteration 9644: with minibatch training loss = 0.789 and accuracy of 0.75\n",
      "Iteration 9645: with minibatch training loss = 1.09 and accuracy of 0.72\n",
      "Iteration 9646: with minibatch training loss = 0.579 and accuracy of 0.84\n",
      "Iteration 9647: with minibatch training loss = 0.599 and accuracy of 0.84\n",
      "Iteration 9648: with minibatch training loss = 0.811 and accuracy of 0.73\n",
      "Iteration 9649: with minibatch training loss = 0.774 and accuracy of 0.77\n",
      "Iteration 9650: with minibatch training loss = 0.91 and accuracy of 0.73\n",
      "Iteration 9651: with minibatch training loss = 0.684 and accuracy of 0.78\n",
      "Iteration 9652: with minibatch training loss = 0.887 and accuracy of 0.73\n",
      "Iteration 9653: with minibatch training loss = 0.947 and accuracy of 0.7\n",
      "Iteration 9654: with minibatch training loss = 0.732 and accuracy of 0.78\n",
      "Iteration 9655: with minibatch training loss = 0.777 and accuracy of 0.77\n",
      "Iteration 9656: with minibatch training loss = 0.843 and accuracy of 0.77\n",
      "Iteration 9657: with minibatch training loss = 0.732 and accuracy of 0.78\n",
      "Iteration 9658: with minibatch training loss = 0.825 and accuracy of 0.81\n",
      "Iteration 9659: with minibatch training loss = 0.757 and accuracy of 0.77\n",
      "Iteration 9660: with minibatch training loss = 0.548 and accuracy of 0.84\n",
      "Iteration 9661: with minibatch training loss = 0.703 and accuracy of 0.8\n",
      "Iteration 9662: with minibatch training loss = 0.75 and accuracy of 0.84\n",
      "Iteration 9663: with minibatch training loss = 0.526 and accuracy of 0.81\n",
      "Iteration 9664: with minibatch training loss = 1.15 and accuracy of 0.66\n",
      "Iteration 9665: with minibatch training loss = 0.547 and accuracy of 0.88\n",
      "Iteration 9666: with minibatch training loss = 0.69 and accuracy of 0.8\n",
      "Iteration 9667: with minibatch training loss = 0.778 and accuracy of 0.75\n",
      "Iteration 9668: with minibatch training loss = 0.69 and accuracy of 0.81\n",
      "Iteration 9669: with minibatch training loss = 0.954 and accuracy of 0.69\n",
      "Iteration 9670: with minibatch training loss = 0.764 and accuracy of 0.8\n",
      "Iteration 9671: with minibatch training loss = 0.99 and accuracy of 0.73\n",
      "Iteration 9672: with minibatch training loss = 0.824 and accuracy of 0.81\n",
      "Iteration 9673: with minibatch training loss = 0.814 and accuracy of 0.75\n",
      "Iteration 9674: with minibatch training loss = 0.961 and accuracy of 0.73\n",
      "Iteration 9675: with minibatch training loss = 0.667 and accuracy of 0.8\n",
      "Iteration 9676: with minibatch training loss = 0.833 and accuracy of 0.73\n",
      "Iteration 9677: with minibatch training loss = 1.02 and accuracy of 0.69\n",
      "Iteration 9678: with minibatch training loss = 0.835 and accuracy of 0.77\n",
      "Iteration 9679: with minibatch training loss = 0.946 and accuracy of 0.7\n",
      "Iteration 9680: with minibatch training loss = 0.837 and accuracy of 0.75\n",
      "Iteration 9681: with minibatch training loss = 0.55 and accuracy of 0.84\n",
      "Iteration 9682: with minibatch training loss = 0.798 and accuracy of 0.78\n",
      "Iteration 9683: with minibatch training loss = 0.865 and accuracy of 0.77\n",
      "Iteration 9684: with minibatch training loss = 0.749 and accuracy of 0.77\n",
      "Iteration 9685: with minibatch training loss = 1.04 and accuracy of 0.67\n",
      "Iteration 9686: with minibatch training loss = 0.983 and accuracy of 0.73\n",
      "Iteration 9687: with minibatch training loss = 0.8 and accuracy of 0.77\n",
      "Iteration 9688: with minibatch training loss = 1.01 and accuracy of 0.66\n",
      "Iteration 9689: with minibatch training loss = 0.91 and accuracy of 0.72\n",
      "Iteration 9690: with minibatch training loss = 0.42 and accuracy of 0.89\n",
      "Iteration 9691: with minibatch training loss = 0.679 and accuracy of 0.8\n",
      "Iteration 9692: with minibatch training loss = 0.906 and accuracy of 0.75\n",
      "Iteration 9693: with minibatch training loss = 0.946 and accuracy of 0.77\n",
      "Iteration 9694: with minibatch training loss = 0.559 and accuracy of 0.84\n",
      "Iteration 9695: with minibatch training loss = 1.01 and accuracy of 0.73\n",
      "Iteration 9696: with minibatch training loss = 0.85 and accuracy of 0.73\n",
      "Iteration 9697: with minibatch training loss = 0.942 and accuracy of 0.7\n",
      "Iteration 9698: with minibatch training loss = 0.866 and accuracy of 0.75\n",
      "Iteration 9699: with minibatch training loss = 0.682 and accuracy of 0.78\n",
      "Iteration 9700: with minibatch training loss = 0.687 and accuracy of 0.78\n",
      "Iteration 9701: with minibatch training loss = 0.844 and accuracy of 0.77\n",
      "Iteration 9702: with minibatch training loss = 0.888 and accuracy of 0.73\n",
      "Iteration 9703: with minibatch training loss = 0.61 and accuracy of 0.81\n",
      "Iteration 9704: with minibatch training loss = 0.841 and accuracy of 0.77\n",
      "Iteration 9705: with minibatch training loss = 0.899 and accuracy of 0.73\n",
      "Iteration 9706: with minibatch training loss = 0.636 and accuracy of 0.78\n",
      "Iteration 9707: with minibatch training loss = 0.868 and accuracy of 0.77\n",
      "Iteration 9708: with minibatch training loss = 0.945 and accuracy of 0.73\n",
      "Iteration 9709: with minibatch training loss = 0.781 and accuracy of 0.75\n",
      "Iteration 9710: with minibatch training loss = 0.89 and accuracy of 0.73\n",
      "Iteration 9711: with minibatch training loss = 0.743 and accuracy of 0.78\n",
      "Iteration 9712: with minibatch training loss = 1.12 and accuracy of 0.66\n",
      "Iteration 9713: with minibatch training loss = 0.609 and accuracy of 0.81\n",
      "Iteration 9714: with minibatch training loss = 0.839 and accuracy of 0.77\n",
      "Validation loss: 0.38525695\n",
      "Epoch 7, Overall loss = 0.79 and accuracy of 0.772\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFoKUUqq5LAQppVRzWQhSSqnm/gf92hl7lBV66gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x121066550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9716: with minibatch training loss = 0.695 and accuracy of 0.81\n",
      "Iteration 9717: with minibatch training loss = 1.13 and accuracy of 0.66\n",
      "Iteration 9718: with minibatch training loss = 0.578 and accuracy of 0.84\n",
      "Iteration 9719: with minibatch training loss = 0.872 and accuracy of 0.73\n",
      "Iteration 9720: with minibatch training loss = 0.777 and accuracy of 0.8\n",
      "Iteration 9721: with minibatch training loss = 1.03 and accuracy of 0.7\n",
      "Iteration 9722: with minibatch training loss = 0.726 and accuracy of 0.77\n",
      "Iteration 9723: with minibatch training loss = 0.896 and accuracy of 0.73\n",
      "Iteration 9724: with minibatch training loss = 0.624 and accuracy of 0.81\n",
      "Iteration 9725: with minibatch training loss = 0.823 and accuracy of 0.73\n",
      "Iteration 9726: with minibatch training loss = 0.693 and accuracy of 0.8\n",
      "Iteration 9727: with minibatch training loss = 0.983 and accuracy of 0.7\n",
      "Iteration 9728: with minibatch training loss = 0.736 and accuracy of 0.78\n",
      "Iteration 9729: with minibatch training loss = 0.974 and accuracy of 0.69\n",
      "Iteration 9730: with minibatch training loss = 0.739 and accuracy of 0.8\n",
      "Iteration 9731: with minibatch training loss = 0.662 and accuracy of 0.84\n",
      "Iteration 9732: with minibatch training loss = 0.936 and accuracy of 0.69\n",
      "Iteration 9733: with minibatch training loss = 0.775 and accuracy of 0.78\n",
      "Iteration 9734: with minibatch training loss = 0.813 and accuracy of 0.77\n",
      "Iteration 9735: with minibatch training loss = 0.559 and accuracy of 0.84\n",
      "Iteration 9736: with minibatch training loss = 0.831 and accuracy of 0.78\n",
      "Iteration 9737: with minibatch training loss = 0.799 and accuracy of 0.75\n",
      "Iteration 9738: with minibatch training loss = 0.926 and accuracy of 0.69\n",
      "Iteration 9739: with minibatch training loss = 0.902 and accuracy of 0.77\n",
      "Iteration 9740: with minibatch training loss = 0.841 and accuracy of 0.73\n",
      "Iteration 9741: with minibatch training loss = 0.65 and accuracy of 0.83\n",
      "Iteration 9742: with minibatch training loss = 0.974 and accuracy of 0.72\n",
      "Iteration 9743: with minibatch training loss = 0.851 and accuracy of 0.75\n",
      "Iteration 9744: with minibatch training loss = 1.02 and accuracy of 0.67\n",
      "Iteration 9745: with minibatch training loss = 0.773 and accuracy of 0.78\n",
      "Iteration 9746: with minibatch training loss = 0.827 and accuracy of 0.8\n",
      "Iteration 9747: with minibatch training loss = 0.933 and accuracy of 0.72\n",
      "Iteration 9748: with minibatch training loss = 0.74 and accuracy of 0.8\n",
      "Iteration 9749: with minibatch training loss = 0.694 and accuracy of 0.78\n",
      "Iteration 9750: with minibatch training loss = 0.872 and accuracy of 0.77\n",
      "Iteration 9751: with minibatch training loss = 0.681 and accuracy of 0.81\n",
      "Iteration 9752: with minibatch training loss = 0.805 and accuracy of 0.73\n",
      "Iteration 9753: with minibatch training loss = 0.887 and accuracy of 0.78\n",
      "Iteration 9754: with minibatch training loss = 0.795 and accuracy of 0.81\n",
      "Iteration 9755: with minibatch training loss = 1.15 and accuracy of 0.7\n",
      "Iteration 9756: with minibatch training loss = 1.05 and accuracy of 0.7\n",
      "Iteration 9757: with minibatch training loss = 0.446 and accuracy of 0.88\n",
      "Iteration 9758: with minibatch training loss = 0.835 and accuracy of 0.73\n",
      "Iteration 9759: with minibatch training loss = 0.958 and accuracy of 0.73\n",
      "Iteration 9760: with minibatch training loss = 0.545 and accuracy of 0.86\n",
      "Iteration 9761: with minibatch training loss = 0.905 and accuracy of 0.77\n",
      "Iteration 9762: with minibatch training loss = 0.664 and accuracy of 0.78\n",
      "Iteration 9763: with minibatch training loss = 0.582 and accuracy of 0.83\n",
      "Iteration 9764: with minibatch training loss = 0.586 and accuracy of 0.84\n",
      "Iteration 9765: with minibatch training loss = 0.934 and accuracy of 0.75\n",
      "Iteration 9766: with minibatch training loss = 0.758 and accuracy of 0.78\n",
      "Iteration 9767: with minibatch training loss = 0.713 and accuracy of 0.8\n",
      "Iteration 9768: with minibatch training loss = 0.561 and accuracy of 0.83\n",
      "Iteration 9769: with minibatch training loss = 0.804 and accuracy of 0.75\n",
      "Iteration 9770: with minibatch training loss = 0.797 and accuracy of 0.75\n",
      "Iteration 9771: with minibatch training loss = 0.699 and accuracy of 0.81\n",
      "Iteration 9772: with minibatch training loss = 0.899 and accuracy of 0.72\n",
      "Iteration 9773: with minibatch training loss = 0.731 and accuracy of 0.77\n",
      "Iteration 9774: with minibatch training loss = 0.726 and accuracy of 0.78\n",
      "Iteration 9775: with minibatch training loss = 1.08 and accuracy of 0.67\n",
      "Iteration 9776: with minibatch training loss = 0.817 and accuracy of 0.77\n",
      "Iteration 9777: with minibatch training loss = 0.68 and accuracy of 0.8\n",
      "Iteration 9778: with minibatch training loss = 1.1 and accuracy of 0.72\n",
      "Iteration 9779: with minibatch training loss = 0.48 and accuracy of 0.86\n",
      "Iteration 9780: with minibatch training loss = 0.745 and accuracy of 0.77\n",
      "Iteration 9781: with minibatch training loss = 0.941 and accuracy of 0.7\n",
      "Iteration 9782: with minibatch training loss = 1.01 and accuracy of 0.72\n",
      "Iteration 9783: with minibatch training loss = 0.581 and accuracy of 0.84\n",
      "Iteration 9784: with minibatch training loss = 0.952 and accuracy of 0.69\n",
      "Iteration 9785: with minibatch training loss = 0.814 and accuracy of 0.72\n",
      "Iteration 9786: with minibatch training loss = 0.642 and accuracy of 0.84\n",
      "Iteration 9787: with minibatch training loss = 0.738 and accuracy of 0.8\n",
      "Iteration 9788: with minibatch training loss = 1.01 and accuracy of 0.73\n",
      "Iteration 9789: with minibatch training loss = 0.829 and accuracy of 0.77\n",
      "Iteration 9790: with minibatch training loss = 0.624 and accuracy of 0.81\n",
      "Iteration 9791: with minibatch training loss = 0.584 and accuracy of 0.84\n",
      "Iteration 9792: with minibatch training loss = 0.803 and accuracy of 0.75\n",
      "Iteration 9793: with minibatch training loss = 0.521 and accuracy of 0.84\n",
      "Iteration 9794: with minibatch training loss = 1.11 and accuracy of 0.69\n",
      "Iteration 9795: with minibatch training loss = 0.815 and accuracy of 0.77\n",
      "Iteration 9796: with minibatch training loss = 0.679 and accuracy of 0.81\n",
      "Iteration 9797: with minibatch training loss = 0.698 and accuracy of 0.81\n",
      "Iteration 9798: with minibatch training loss = 0.958 and accuracy of 0.7\n",
      "Iteration 9799: with minibatch training loss = 0.765 and accuracy of 0.77\n",
      "Iteration 9800: with minibatch training loss = 0.876 and accuracy of 0.73\n",
      "Iteration 9801: with minibatch training loss = 0.827 and accuracy of 0.75\n",
      "Iteration 9802: with minibatch training loss = 0.906 and accuracy of 0.72\n",
      "Iteration 9803: with minibatch training loss = 0.529 and accuracy of 0.86\n",
      "Iteration 9804: with minibatch training loss = 0.628 and accuracy of 0.86\n",
      "Iteration 9805: with minibatch training loss = 0.412 and accuracy of 0.88\n",
      "Iteration 9806: with minibatch training loss = 0.982 and accuracy of 0.7\n",
      "Iteration 9807: with minibatch training loss = 0.811 and accuracy of 0.75\n",
      "Iteration 9808: with minibatch training loss = 0.715 and accuracy of 0.84\n",
      "Iteration 9809: with minibatch training loss = 0.999 and accuracy of 0.67\n",
      "Iteration 9810: with minibatch training loss = 0.88 and accuracy of 0.75\n",
      "Iteration 9811: with minibatch training loss = 0.869 and accuracy of 0.72\n",
      "Iteration 9812: with minibatch training loss = 0.857 and accuracy of 0.72\n",
      "Iteration 9813: with minibatch training loss = 0.647 and accuracy of 0.78\n",
      "Iteration 9814: with minibatch training loss = 0.678 and accuracy of 0.83\n",
      "Iteration 9815: with minibatch training loss = 0.996 and accuracy of 0.67\n",
      "Iteration 9816: with minibatch training loss = 0.947 and accuracy of 0.75\n",
      "Iteration 9817: with minibatch training loss = 0.736 and accuracy of 0.78\n",
      "Iteration 9818: with minibatch training loss = 0.754 and accuracy of 0.78\n",
      "Iteration 9819: with minibatch training loss = 1.03 and accuracy of 0.73\n",
      "Iteration 9820: with minibatch training loss = 0.517 and accuracy of 0.84\n",
      "Iteration 9821: with minibatch training loss = 0.673 and accuracy of 0.81\n",
      "Iteration 9822: with minibatch training loss = 0.523 and accuracy of 0.86\n",
      "Iteration 9823: with minibatch training loss = 0.724 and accuracy of 0.77\n",
      "Iteration 9824: with minibatch training loss = 0.775 and accuracy of 0.75\n",
      "Iteration 9825: with minibatch training loss = 0.803 and accuracy of 0.77\n",
      "Iteration 9826: with minibatch training loss = 0.83 and accuracy of 0.73\n",
      "Iteration 9827: with minibatch training loss = 1.09 and accuracy of 0.66\n",
      "Iteration 9828: with minibatch training loss = 0.703 and accuracy of 0.81\n",
      "Iteration 9829: with minibatch training loss = 0.707 and accuracy of 0.8\n",
      "Iteration 9830: with minibatch training loss = 0.806 and accuracy of 0.75\n",
      "Iteration 9831: with minibatch training loss = 0.739 and accuracy of 0.83\n",
      "Iteration 9832: with minibatch training loss = 1.01 and accuracy of 0.7\n",
      "Iteration 9833: with minibatch training loss = 1.06 and accuracy of 0.7\n",
      "Iteration 9834: with minibatch training loss = 0.639 and accuracy of 0.81\n",
      "Iteration 9835: with minibatch training loss = 1.06 and accuracy of 0.67\n",
      "Iteration 9836: with minibatch training loss = 0.702 and accuracy of 0.81\n",
      "Iteration 9837: with minibatch training loss = 0.644 and accuracy of 0.83\n",
      "Iteration 9838: with minibatch training loss = 0.731 and accuracy of 0.78\n",
      "Iteration 9839: with minibatch training loss = 0.936 and accuracy of 0.73\n",
      "Iteration 9840: with minibatch training loss = 0.617 and accuracy of 0.84\n",
      "Iteration 9841: with minibatch training loss = 0.752 and accuracy of 0.8\n",
      "Iteration 9842: with minibatch training loss = 0.593 and accuracy of 0.81\n",
      "Iteration 9843: with minibatch training loss = 0.667 and accuracy of 0.83\n",
      "Iteration 9844: with minibatch training loss = 0.686 and accuracy of 0.78\n",
      "Iteration 9845: with minibatch training loss = 0.712 and accuracy of 0.8\n",
      "Iteration 9846: with minibatch training loss = 0.847 and accuracy of 0.75\n",
      "Iteration 9847: with minibatch training loss = 0.783 and accuracy of 0.77\n",
      "Iteration 9848: with minibatch training loss = 0.893 and accuracy of 0.72\n",
      "Iteration 9849: with minibatch training loss = 0.946 and accuracy of 0.73\n",
      "Iteration 9850: with minibatch training loss = 0.551 and accuracy of 0.88\n",
      "Iteration 9851: with minibatch training loss = 0.522 and accuracy of 0.86\n",
      "Iteration 9852: with minibatch training loss = 1.05 and accuracy of 0.66\n",
      "Iteration 9853: with minibatch training loss = 0.626 and accuracy of 0.83\n",
      "Iteration 9854: with minibatch training loss = 0.878 and accuracy of 0.73\n",
      "Iteration 9855: with minibatch training loss = 0.968 and accuracy of 0.67\n",
      "Iteration 9856: with minibatch training loss = 0.851 and accuracy of 0.72\n",
      "Iteration 9857: with minibatch training loss = 0.796 and accuracy of 0.77\n",
      "Iteration 9858: with minibatch training loss = 0.854 and accuracy of 0.77\n",
      "Iteration 9859: with minibatch training loss = 0.759 and accuracy of 0.78\n",
      "Iteration 9860: with minibatch training loss = 1.01 and accuracy of 0.67\n",
      "Iteration 9861: with minibatch training loss = 0.624 and accuracy of 0.83\n",
      "Iteration 9862: with minibatch training loss = 0.577 and accuracy of 0.84\n",
      "Iteration 9863: with minibatch training loss = 0.556 and accuracy of 0.84\n",
      "Iteration 9864: with minibatch training loss = 0.859 and accuracy of 0.77\n",
      "Iteration 9865: with minibatch training loss = 0.807 and accuracy of 0.73\n",
      "Iteration 9866: with minibatch training loss = 0.754 and accuracy of 0.75\n",
      "Iteration 9867: with minibatch training loss = 0.564 and accuracy of 0.84\n",
      "Iteration 9868: with minibatch training loss = 0.638 and accuracy of 0.8\n",
      "Iteration 9869: with minibatch training loss = 0.86 and accuracy of 0.75\n",
      "Iteration 9870: with minibatch training loss = 0.692 and accuracy of 0.8\n",
      "Iteration 9871: with minibatch training loss = 0.97 and accuracy of 0.69\n",
      "Iteration 9872: with minibatch training loss = 0.586 and accuracy of 0.84\n",
      "Iteration 9873: with minibatch training loss = 0.716 and accuracy of 0.81\n",
      "Iteration 9874: with minibatch training loss = 0.623 and accuracy of 0.84\n",
      "Iteration 9875: with minibatch training loss = 0.695 and accuracy of 0.78\n",
      "Iteration 9876: with minibatch training loss = 0.789 and accuracy of 0.75\n",
      "Iteration 9877: with minibatch training loss = 0.585 and accuracy of 0.84\n",
      "Iteration 9878: with minibatch training loss = 0.718 and accuracy of 0.81\n",
      "Iteration 9879: with minibatch training loss = 0.842 and accuracy of 0.78\n",
      "Iteration 9880: with minibatch training loss = 0.737 and accuracy of 0.78\n",
      "Iteration 9881: with minibatch training loss = 0.935 and accuracy of 0.75\n",
      "Iteration 9882: with minibatch training loss = 0.682 and accuracy of 0.83\n",
      "Iteration 9883: with minibatch training loss = 0.583 and accuracy of 0.84\n",
      "Iteration 9884: with minibatch training loss = 0.839 and accuracy of 0.73\n",
      "Iteration 9885: with minibatch training loss = 0.791 and accuracy of 0.75\n",
      "Iteration 9886: with minibatch training loss = 0.735 and accuracy of 0.81\n",
      "Iteration 9887: with minibatch training loss = 0.871 and accuracy of 0.75\n",
      "Iteration 9888: with minibatch training loss = 0.745 and accuracy of 0.84\n",
      "Iteration 9889: with minibatch training loss = 0.703 and accuracy of 0.83\n",
      "Iteration 9890: with minibatch training loss = 0.626 and accuracy of 0.84\n",
      "Iteration 9891: with minibatch training loss = 0.564 and accuracy of 0.86\n",
      "Iteration 9892: with minibatch training loss = 0.627 and accuracy of 0.81\n",
      "Iteration 9893: with minibatch training loss = 0.79 and accuracy of 0.78\n",
      "Iteration 9894: with minibatch training loss = 0.789 and accuracy of 0.78\n",
      "Iteration 9895: with minibatch training loss = 1.14 and accuracy of 0.64\n",
      "Iteration 9896: with minibatch training loss = 0.823 and accuracy of 0.78\n",
      "Iteration 9897: with minibatch training loss = 1.11 and accuracy of 0.69\n",
      "Iteration 9898: with minibatch training loss = 0.736 and accuracy of 0.77\n",
      "Iteration 9899: with minibatch training loss = 0.815 and accuracy of 0.77\n",
      "Iteration 9900: with minibatch training loss = 0.915 and accuracy of 0.73\n",
      "Iteration 9901: with minibatch training loss = 0.673 and accuracy of 0.83\n",
      "Iteration 9902: with minibatch training loss = 0.771 and accuracy of 0.78\n",
      "Iteration 9903: with minibatch training loss = 0.563 and accuracy of 0.83\n",
      "Iteration 9904: with minibatch training loss = 0.962 and accuracy of 0.77\n",
      "Iteration 9905: with minibatch training loss = 0.867 and accuracy of 0.73\n",
      "Iteration 9906: with minibatch training loss = 0.753 and accuracy of 0.8\n",
      "Iteration 9907: with minibatch training loss = 0.684 and accuracy of 0.81\n",
      "Iteration 9908: with minibatch training loss = 0.868 and accuracy of 0.75\n",
      "Iteration 9909: with minibatch training loss = 0.747 and accuracy of 0.8\n",
      "Iteration 9910: with minibatch training loss = 0.51 and accuracy of 0.86\n",
      "Iteration 9911: with minibatch training loss = 0.751 and accuracy of 0.75\n",
      "Iteration 9912: with minibatch training loss = 0.838 and accuracy of 0.73\n",
      "Iteration 9913: with minibatch training loss = 0.928 and accuracy of 0.73\n",
      "Iteration 9914: with minibatch training loss = 0.608 and accuracy of 0.86\n",
      "Iteration 9915: with minibatch training loss = 0.643 and accuracy of 0.8\n",
      "Iteration 9916: with minibatch training loss = 0.776 and accuracy of 0.8\n",
      "Iteration 9917: with minibatch training loss = 0.78 and accuracy of 0.8\n",
      "Iteration 9918: with minibatch training loss = 0.723 and accuracy of 0.81\n",
      "Iteration 9919: with minibatch training loss = 0.792 and accuracy of 0.8\n",
      "Iteration 9920: with minibatch training loss = 0.76 and accuracy of 0.77\n",
      "Iteration 9921: with minibatch training loss = 0.952 and accuracy of 0.69\n",
      "Iteration 9922: with minibatch training loss = 0.689 and accuracy of 0.8\n",
      "Iteration 9923: with minibatch training loss = 0.788 and accuracy of 0.75\n",
      "Iteration 9924: with minibatch training loss = 0.873 and accuracy of 0.78\n",
      "Iteration 9925: with minibatch training loss = 0.783 and accuracy of 0.78\n",
      "Iteration 9926: with minibatch training loss = 0.771 and accuracy of 0.75\n",
      "Iteration 9927: with minibatch training loss = 0.822 and accuracy of 0.77\n",
      "Iteration 9928: with minibatch training loss = 1.08 and accuracy of 0.7\n",
      "Iteration 9929: with minibatch training loss = 0.893 and accuracy of 0.75\n",
      "Iteration 9930: with minibatch training loss = 0.787 and accuracy of 0.73\n",
      "Iteration 9931: with minibatch training loss = 0.511 and accuracy of 0.88\n",
      "Iteration 9932: with minibatch training loss = 1.01 and accuracy of 0.7\n",
      "Iteration 9933: with minibatch training loss = 0.747 and accuracy of 0.78\n",
      "Iteration 9934: with minibatch training loss = 0.741 and accuracy of 0.77\n",
      "Iteration 9935: with minibatch training loss = 0.734 and accuracy of 0.77\n",
      "Iteration 9936: with minibatch training loss = 0.888 and accuracy of 0.73\n",
      "Iteration 9937: with minibatch training loss = 0.855 and accuracy of 0.78\n",
      "Iteration 9938: with minibatch training loss = 0.874 and accuracy of 0.77\n",
      "Iteration 9939: with minibatch training loss = 1.16 and accuracy of 0.67\n",
      "Iteration 9940: with minibatch training loss = 0.747 and accuracy of 0.81\n",
      "Iteration 9941: with minibatch training loss = 0.834 and accuracy of 0.72\n",
      "Iteration 9942: with minibatch training loss = 0.703 and accuracy of 0.78\n",
      "Iteration 9943: with minibatch training loss = 0.876 and accuracy of 0.72\n",
      "Iteration 9944: with minibatch training loss = 0.96 and accuracy of 0.72\n",
      "Iteration 9945: with minibatch training loss = 0.717 and accuracy of 0.8\n",
      "Iteration 9946: with minibatch training loss = 0.615 and accuracy of 0.81\n",
      "Iteration 9947: with minibatch training loss = 0.524 and accuracy of 0.84\n",
      "Iteration 9948: with minibatch training loss = 1.01 and accuracy of 0.69\n",
      "Iteration 9949: with minibatch training loss = 0.88 and accuracy of 0.72\n",
      "Iteration 9950: with minibatch training loss = 0.926 and accuracy of 0.69\n",
      "Iteration 9951: with minibatch training loss = 1.03 and accuracy of 0.7\n",
      "Iteration 9952: with minibatch training loss = 0.995 and accuracy of 0.67\n",
      "Iteration 9953: with minibatch training loss = 0.61 and accuracy of 0.83\n",
      "Iteration 9954: with minibatch training loss = 0.876 and accuracy of 0.72\n",
      "Iteration 9955: with minibatch training loss = 0.584 and accuracy of 0.83\n",
      "Iteration 9956: with minibatch training loss = 0.63 and accuracy of 0.84\n",
      "Iteration 9957: with minibatch training loss = 0.525 and accuracy of 0.86\n",
      "Iteration 9958: with minibatch training loss = 0.628 and accuracy of 0.81\n",
      "Iteration 9959: with minibatch training loss = 0.965 and accuracy of 0.7\n",
      "Iteration 9960: with minibatch training loss = 0.894 and accuracy of 0.72\n",
      "Iteration 9961: with minibatch training loss = 1.04 and accuracy of 0.66\n",
      "Iteration 9962: with minibatch training loss = 0.621 and accuracy of 0.81\n",
      "Iteration 9963: with minibatch training loss = 0.699 and accuracy of 0.8\n",
      "Iteration 9964: with minibatch training loss = 0.712 and accuracy of 0.78\n",
      "Iteration 9965: with minibatch training loss = 0.691 and accuracy of 0.83\n",
      "Iteration 9966: with minibatch training loss = 0.834 and accuracy of 0.77\n",
      "Iteration 9967: with minibatch training loss = 0.644 and accuracy of 0.84\n",
      "Iteration 9968: with minibatch training loss = 0.607 and accuracy of 0.81\n",
      "Iteration 9969: with minibatch training loss = 0.67 and accuracy of 0.81\n",
      "Iteration 9970: with minibatch training loss = 0.73 and accuracy of 0.78\n",
      "Iteration 9971: with minibatch training loss = 0.805 and accuracy of 0.77\n",
      "Iteration 9972: with minibatch training loss = 0.734 and accuracy of 0.78\n",
      "Iteration 9973: with minibatch training loss = 0.774 and accuracy of 0.78\n",
      "Iteration 9974: with minibatch training loss = 0.601 and accuracy of 0.81\n",
      "Iteration 9975: with minibatch training loss = 0.823 and accuracy of 0.75\n",
      "Iteration 9976: with minibatch training loss = 0.807 and accuracy of 0.77\n",
      "Iteration 9977: with minibatch training loss = 1.02 and accuracy of 0.72\n",
      "Iteration 9978: with minibatch training loss = 0.534 and accuracy of 0.83\n",
      "Iteration 9979: with minibatch training loss = 0.835 and accuracy of 0.75\n",
      "Iteration 9980: with minibatch training loss = 1.05 and accuracy of 0.69\n",
      "Iteration 9981: with minibatch training loss = 0.833 and accuracy of 0.8\n",
      "Iteration 9982: with minibatch training loss = 0.765 and accuracy of 0.8\n",
      "Iteration 9983: with minibatch training loss = 0.777 and accuracy of 0.77\n",
      "Iteration 9984: with minibatch training loss = 0.882 and accuracy of 0.72\n",
      "Iteration 9985: with minibatch training loss = 0.893 and accuracy of 0.77\n",
      "Iteration 9986: with minibatch training loss = 0.902 and accuracy of 0.73\n",
      "Iteration 9987: with minibatch training loss = 0.715 and accuracy of 0.83\n",
      "Iteration 9988: with minibatch training loss = 0.708 and accuracy of 0.8\n",
      "Iteration 9989: with minibatch training loss = 0.978 and accuracy of 0.7\n",
      "Iteration 9990: with minibatch training loss = 0.638 and accuracy of 0.83\n",
      "Iteration 9991: with minibatch training loss = 0.741 and accuracy of 0.8\n",
      "Iteration 9992: with minibatch training loss = 0.91 and accuracy of 0.75\n",
      "Iteration 9993: with minibatch training loss = 0.443 and accuracy of 0.88\n",
      "Iteration 9994: with minibatch training loss = 0.62 and accuracy of 0.84\n",
      "Iteration 9995: with minibatch training loss = 0.673 and accuracy of 0.8\n",
      "Iteration 9996: with minibatch training loss = 0.671 and accuracy of 0.83\n",
      "Iteration 9997: with minibatch training loss = 0.732 and accuracy of 0.77\n",
      "Iteration 9998: with minibatch training loss = 0.686 and accuracy of 0.81\n",
      "Iteration 9999: with minibatch training loss = 0.909 and accuracy of 0.7\n",
      "Iteration 10000: with minibatch training loss = 0.52 and accuracy of 0.86\n",
      "Iteration 10001: with minibatch training loss = 0.548 and accuracy of 0.81\n",
      "Iteration 10002: with minibatch training loss = 0.552 and accuracy of 0.86\n",
      "Iteration 10003: with minibatch training loss = 0.68 and accuracy of 0.8\n",
      "Iteration 10004: with minibatch training loss = 0.722 and accuracy of 0.8\n",
      "Iteration 10005: with minibatch training loss = 0.751 and accuracy of 0.8\n",
      "Iteration 10006: with minibatch training loss = 0.641 and accuracy of 0.78\n",
      "Iteration 10007: with minibatch training loss = 0.787 and accuracy of 0.8\n",
      "Iteration 10008: with minibatch training loss = 0.643 and accuracy of 0.81\n",
      "Iteration 10009: with minibatch training loss = 0.871 and accuracy of 0.78\n",
      "Iteration 10010: with minibatch training loss = 0.793 and accuracy of 0.75\n",
      "Iteration 10011: with minibatch training loss = 0.697 and accuracy of 0.81\n",
      "Iteration 10012: with minibatch training loss = 0.714 and accuracy of 0.81\n",
      "Iteration 10013: with minibatch training loss = 0.936 and accuracy of 0.72\n",
      "Iteration 10014: with minibatch training loss = 0.575 and accuracy of 0.84\n",
      "Iteration 10015: with minibatch training loss = 0.635 and accuracy of 0.84\n",
      "Iteration 10016: with minibatch training loss = 0.734 and accuracy of 0.78\n",
      "Iteration 10017: with minibatch training loss = 0.73 and accuracy of 0.8\n",
      "Iteration 10018: with minibatch training loss = 0.69 and accuracy of 0.8\n",
      "Iteration 10019: with minibatch training loss = 1.11 and accuracy of 0.62\n",
      "Iteration 10020: with minibatch training loss = 0.584 and accuracy of 0.83\n",
      "Iteration 10021: with minibatch training loss = 1.03 and accuracy of 0.7\n",
      "Iteration 10022: with minibatch training loss = 0.694 and accuracy of 0.78\n",
      "Iteration 10023: with minibatch training loss = 0.677 and accuracy of 0.77\n",
      "Iteration 10024: with minibatch training loss = 0.981 and accuracy of 0.67\n",
      "Iteration 10025: with minibatch training loss = 0.865 and accuracy of 0.72\n",
      "Iteration 10026: with minibatch training loss = 0.839 and accuracy of 0.73\n",
      "Iteration 10027: with minibatch training loss = 0.832 and accuracy of 0.75\n",
      "Iteration 10028: with minibatch training loss = 0.658 and accuracy of 0.81\n",
      "Iteration 10029: with minibatch training loss = 0.714 and accuracy of 0.8\n",
      "Iteration 10030: with minibatch training loss = 0.812 and accuracy of 0.77\n",
      "Iteration 10031: with minibatch training loss = 0.789 and accuracy of 0.8\n",
      "Iteration 10032: with minibatch training loss = 0.72 and accuracy of 0.77\n",
      "Iteration 10033: with minibatch training loss = 0.993 and accuracy of 0.75\n",
      "Iteration 10034: with minibatch training loss = 0.689 and accuracy of 0.83\n",
      "Iteration 10035: with minibatch training loss = 0.882 and accuracy of 0.75\n",
      "Iteration 10036: with minibatch training loss = 0.931 and accuracy of 0.73\n",
      "Iteration 10037: with minibatch training loss = 0.976 and accuracy of 0.72\n",
      "Iteration 10038: with minibatch training loss = 0.85 and accuracy of 0.75\n",
      "Iteration 10039: with minibatch training loss = 0.56 and accuracy of 0.83\n",
      "Iteration 10040: with minibatch training loss = 0.845 and accuracy of 0.73\n",
      "Iteration 10041: with minibatch training loss = 0.778 and accuracy of 0.8\n",
      "Iteration 10042: with minibatch training loss = 0.568 and accuracy of 0.86\n",
      "Iteration 10043: with minibatch training loss = 0.7 and accuracy of 0.81\n",
      "Iteration 10044: with minibatch training loss = 0.707 and accuracy of 0.77\n",
      "Iteration 10045: with minibatch training loss = 0.575 and accuracy of 0.81\n",
      "Iteration 10046: with minibatch training loss = 1.03 and accuracy of 0.66\n",
      "Iteration 10047: with minibatch training loss = 0.555 and accuracy of 0.84\n",
      "Iteration 10048: with minibatch training loss = 0.759 and accuracy of 0.77\n",
      "Iteration 10049: with minibatch training loss = 0.855 and accuracy of 0.75\n",
      "Iteration 10050: with minibatch training loss = 0.721 and accuracy of 0.8\n",
      "Iteration 10051: with minibatch training loss = 1.25 and accuracy of 0.62\n",
      "Iteration 10052: with minibatch training loss = 0.868 and accuracy of 0.73\n",
      "Iteration 10053: with minibatch training loss = 0.81 and accuracy of 0.77\n",
      "Iteration 10054: with minibatch training loss = 0.704 and accuracy of 0.81\n",
      "Iteration 10055: with minibatch training loss = 0.828 and accuracy of 0.8\n",
      "Iteration 10056: with minibatch training loss = 0.811 and accuracy of 0.8\n",
      "Iteration 10057: with minibatch training loss = 0.919 and accuracy of 0.73\n",
      "Iteration 10058: with minibatch training loss = 0.835 and accuracy of 0.73\n",
      "Iteration 10059: with minibatch training loss = 0.51 and accuracy of 0.84\n",
      "Iteration 10060: with minibatch training loss = 0.74 and accuracy of 0.81\n",
      "Iteration 10061: with minibatch training loss = 0.575 and accuracy of 0.83\n",
      "Iteration 10062: with minibatch training loss = 0.816 and accuracy of 0.77\n",
      "Iteration 10063: with minibatch training loss = 0.508 and accuracy of 0.86\n",
      "Iteration 10064: with minibatch training loss = 0.891 and accuracy of 0.73\n",
      "Iteration 10065: with minibatch training loss = 0.525 and accuracy of 0.83\n",
      "Iteration 10066: with minibatch training loss = 0.757 and accuracy of 0.8\n",
      "Iteration 10067: with minibatch training loss = 0.669 and accuracy of 0.84\n",
      "Iteration 10068: with minibatch training loss = 0.64 and accuracy of 0.83\n",
      "Iteration 10069: with minibatch training loss = 0.864 and accuracy of 0.75\n",
      "Iteration 10070: with minibatch training loss = 1.04 and accuracy of 0.69\n",
      "Iteration 10071: with minibatch training loss = 1.01 and accuracy of 0.69\n",
      "Iteration 10072: with minibatch training loss = 0.911 and accuracy of 0.73\n",
      "Iteration 10073: with minibatch training loss = 0.812 and accuracy of 0.78\n",
      "Iteration 10074: with minibatch training loss = 0.661 and accuracy of 0.81\n",
      "Iteration 10075: with minibatch training loss = 0.65 and accuracy of 0.83\n",
      "Iteration 10076: with minibatch training loss = 0.7 and accuracy of 0.81\n",
      "Iteration 10077: with minibatch training loss = 0.673 and accuracy of 0.83\n",
      "Iteration 10078: with minibatch training loss = 0.708 and accuracy of 0.78\n",
      "Iteration 10079: with minibatch training loss = 0.935 and accuracy of 0.7\n",
      "Iteration 10080: with minibatch training loss = 0.589 and accuracy of 0.84\n",
      "Iteration 10081: with minibatch training loss = 0.786 and accuracy of 0.78\n",
      "Iteration 10082: with minibatch training loss = 0.765 and accuracy of 0.78\n",
      "Iteration 10083: with minibatch training loss = 1.15 and accuracy of 0.64\n",
      "Iteration 10084: with minibatch training loss = 0.817 and accuracy of 0.78\n",
      "Iteration 10085: with minibatch training loss = 0.587 and accuracy of 0.81\n",
      "Iteration 10086: with minibatch training loss = 0.749 and accuracy of 0.77\n",
      "Iteration 10087: with minibatch training loss = 0.77 and accuracy of 0.78\n",
      "Iteration 10088: with minibatch training loss = 0.737 and accuracy of 0.8\n",
      "Iteration 10089: with minibatch training loss = 1.03 and accuracy of 0.7\n",
      "Iteration 10090: with minibatch training loss = 0.667 and accuracy of 0.78\n",
      "Iteration 10091: with minibatch training loss = 0.843 and accuracy of 0.73\n",
      "Iteration 10092: with minibatch training loss = 0.54 and accuracy of 0.86\n",
      "Iteration 10093: with minibatch training loss = 0.768 and accuracy of 0.78\n",
      "Iteration 10094: with minibatch training loss = 0.939 and accuracy of 0.72\n",
      "Iteration 10095: with minibatch training loss = 0.79 and accuracy of 0.78\n",
      "Iteration 10096: with minibatch training loss = 0.911 and accuracy of 0.73\n",
      "Iteration 10097: with minibatch training loss = 0.857 and accuracy of 0.75\n",
      "Iteration 10098: with minibatch training loss = 1.22 and accuracy of 0.64\n",
      "Iteration 10099: with minibatch training loss = 1.16 and accuracy of 0.61\n",
      "Iteration 10100: with minibatch training loss = 0.601 and accuracy of 0.81\n",
      "Iteration 10101: with minibatch training loss = 0.64 and accuracy of 0.8\n",
      "Iteration 10102: with minibatch training loss = 0.67 and accuracy of 0.8\n",
      "Iteration 10103: with minibatch training loss = 0.631 and accuracy of 0.81\n",
      "Iteration 10104: with minibatch training loss = 0.88 and accuracy of 0.73\n",
      "Iteration 10105: with minibatch training loss = 1.06 and accuracy of 0.69\n",
      "Iteration 10106: with minibatch training loss = 0.939 and accuracy of 0.73\n",
      "Iteration 10107: with minibatch training loss = 0.972 and accuracy of 0.73\n",
      "Iteration 10108: with minibatch training loss = 0.603 and accuracy of 0.81\n",
      "Iteration 10109: with minibatch training loss = 0.862 and accuracy of 0.72\n",
      "Iteration 10110: with minibatch training loss = 1.07 and accuracy of 0.69\n",
      "Iteration 10111: with minibatch training loss = 0.807 and accuracy of 0.77\n",
      "Iteration 10112: with minibatch training loss = 0.686 and accuracy of 0.78\n",
      "Iteration 10113: with minibatch training loss = 0.813 and accuracy of 0.78\n",
      "Iteration 10114: with minibatch training loss = 0.87 and accuracy of 0.7\n",
      "Iteration 10115: with minibatch training loss = 0.695 and accuracy of 0.81\n",
      "Iteration 10116: with minibatch training loss = 0.8 and accuracy of 0.77\n",
      "Iteration 10117: with minibatch training loss = 0.918 and accuracy of 0.73\n",
      "Iteration 10118: with minibatch training loss = 1.06 and accuracy of 0.67\n",
      "Iteration 10119: with minibatch training loss = 0.759 and accuracy of 0.78\n",
      "Iteration 10120: with minibatch training loss = 0.608 and accuracy of 0.84\n",
      "Iteration 10121: with minibatch training loss = 0.853 and accuracy of 0.72\n",
      "Iteration 10122: with minibatch training loss = 0.591 and accuracy of 0.84\n",
      "Iteration 10123: with minibatch training loss = 0.868 and accuracy of 0.73\n",
      "Iteration 10124: with minibatch training loss = 0.755 and accuracy of 0.78\n",
      "Iteration 10125: with minibatch training loss = 0.961 and accuracy of 0.72\n",
      "Iteration 10126: with minibatch training loss = 0.762 and accuracy of 0.8\n",
      "Iteration 10127: with minibatch training loss = 0.807 and accuracy of 0.78\n",
      "Iteration 10128: with minibatch training loss = 0.774 and accuracy of 0.77\n",
      "Iteration 10129: with minibatch training loss = 0.622 and accuracy of 0.83\n",
      "Iteration 10130: with minibatch training loss = 0.847 and accuracy of 0.77\n",
      "Iteration 10131: with minibatch training loss = 0.455 and accuracy of 0.91\n",
      "Iteration 10132: with minibatch training loss = 0.829 and accuracy of 0.75\n",
      "Iteration 10133: with minibatch training loss = 0.694 and accuracy of 0.81\n",
      "Iteration 10134: with minibatch training loss = 0.891 and accuracy of 0.73\n",
      "Iteration 10135: with minibatch training loss = 0.674 and accuracy of 0.84\n",
      "Iteration 10136: with minibatch training loss = 0.513 and accuracy of 0.84\n",
      "Iteration 10137: with minibatch training loss = 0.787 and accuracy of 0.77\n",
      "Iteration 10138: with minibatch training loss = 0.872 and accuracy of 0.73\n",
      "Iteration 10139: with minibatch training loss = 0.584 and accuracy of 0.8\n",
      "Iteration 10140: with minibatch training loss = 0.84 and accuracy of 0.73\n",
      "Iteration 10141: with minibatch training loss = 0.64 and accuracy of 0.83\n",
      "Iteration 10142: with minibatch training loss = 1.04 and accuracy of 0.72\n",
      "Iteration 10143: with minibatch training loss = 0.674 and accuracy of 0.8\n",
      "Iteration 10144: with minibatch training loss = 0.734 and accuracy of 0.78\n",
      "Iteration 10145: with minibatch training loss = 0.704 and accuracy of 0.78\n",
      "Iteration 10146: with minibatch training loss = 0.614 and accuracy of 0.81\n",
      "Iteration 10147: with minibatch training loss = 1.1 and accuracy of 0.67\n",
      "Iteration 10148: with minibatch training loss = 1.05 and accuracy of 0.69\n",
      "Iteration 10149: with minibatch training loss = 1.18 and accuracy of 0.64\n",
      "Iteration 10150: with minibatch training loss = 0.57 and accuracy of 0.83\n",
      "Iteration 10151: with minibatch training loss = 0.647 and accuracy of 0.81\n",
      "Iteration 10152: with minibatch training loss = 1.1 and accuracy of 0.69\n",
      "Iteration 10153: with minibatch training loss = 0.909 and accuracy of 0.73\n",
      "Iteration 10154: with minibatch training loss = 1.04 and accuracy of 0.72\n",
      "Iteration 10155: with minibatch training loss = 0.715 and accuracy of 0.8\n",
      "Iteration 10156: with minibatch training loss = 0.861 and accuracy of 0.78\n",
      "Iteration 10157: with minibatch training loss = 0.648 and accuracy of 0.8\n",
      "Iteration 10158: with minibatch training loss = 0.736 and accuracy of 0.8\n",
      "Iteration 10159: with minibatch training loss = 0.641 and accuracy of 0.8\n",
      "Iteration 10160: with minibatch training loss = 0.938 and accuracy of 0.77\n",
      "Iteration 10161: with minibatch training loss = 0.981 and accuracy of 0.7\n",
      "Iteration 10162: with minibatch training loss = 0.925 and accuracy of 0.72\n",
      "Iteration 10163: with minibatch training loss = 0.712 and accuracy of 0.8\n",
      "Iteration 10164: with minibatch training loss = 0.642 and accuracy of 0.8\n",
      "Iteration 10165: with minibatch training loss = 0.661 and accuracy of 0.83\n",
      "Iteration 10166: with minibatch training loss = 0.701 and accuracy of 0.78\n",
      "Iteration 10167: with minibatch training loss = 0.732 and accuracy of 0.8\n",
      "Iteration 10168: with minibatch training loss = 0.608 and accuracy of 0.83\n",
      "Iteration 10169: with minibatch training loss = 1 and accuracy of 0.67\n",
      "Iteration 10170: with minibatch training loss = 0.705 and accuracy of 0.81\n",
      "Iteration 10171: with minibatch training loss = 0.663 and accuracy of 0.81\n",
      "Iteration 10172: with minibatch training loss = 0.822 and accuracy of 0.75\n",
      "Iteration 10173: with minibatch training loss = 0.551 and accuracy of 0.84\n",
      "Iteration 10174: with minibatch training loss = 0.73 and accuracy of 0.8\n",
      "Iteration 10175: with minibatch training loss = 0.636 and accuracy of 0.8\n",
      "Iteration 10176: with minibatch training loss = 0.671 and accuracy of 0.81\n",
      "Iteration 10177: with minibatch training loss = 1.04 and accuracy of 0.73\n",
      "Iteration 10178: with minibatch training loss = 0.42 and accuracy of 0.89\n",
      "Iteration 10179: with minibatch training loss = 0.949 and accuracy of 0.72\n",
      "Iteration 10180: with minibatch training loss = 0.764 and accuracy of 0.78\n",
      "Iteration 10181: with minibatch training loss = 0.79 and accuracy of 0.75\n",
      "Iteration 10182: with minibatch training loss = 0.58 and accuracy of 0.83\n",
      "Iteration 10183: with minibatch training loss = 0.982 and accuracy of 0.69\n",
      "Iteration 10184: with minibatch training loss = 0.732 and accuracy of 0.77\n",
      "Iteration 10185: with minibatch training loss = 0.693 and accuracy of 0.81\n",
      "Iteration 10186: with minibatch training loss = 0.72 and accuracy of 0.78\n",
      "Iteration 10187: with minibatch training loss = 1 and accuracy of 0.69\n",
      "Iteration 10188: with minibatch training loss = 0.948 and accuracy of 0.72\n",
      "Iteration 10189: with minibatch training loss = 0.724 and accuracy of 0.8\n",
      "Iteration 10190: with minibatch training loss = 0.984 and accuracy of 0.72\n",
      "Iteration 10191: with minibatch training loss = 0.746 and accuracy of 0.8\n",
      "Iteration 10192: with minibatch training loss = 0.526 and accuracy of 0.81\n",
      "Iteration 10193: with minibatch training loss = 0.816 and accuracy of 0.75\n",
      "Iteration 10194: with minibatch training loss = 0.362 and accuracy of 0.94\n",
      "Iteration 10195: with minibatch training loss = 0.904 and accuracy of 0.77\n",
      "Iteration 10196: with minibatch training loss = 0.813 and accuracy of 0.77\n",
      "Iteration 10197: with minibatch training loss = 0.919 and accuracy of 0.73\n",
      "Iteration 10198: with minibatch training loss = 0.84 and accuracy of 0.8\n",
      "Iteration 10199: with minibatch training loss = 0.701 and accuracy of 0.8\n",
      "Iteration 10200: with minibatch training loss = 0.77 and accuracy of 0.83\n",
      "Iteration 10201: with minibatch training loss = 1.04 and accuracy of 0.66\n",
      "Iteration 10202: with minibatch training loss = 0.654 and accuracy of 0.84\n",
      "Iteration 10203: with minibatch training loss = 0.652 and accuracy of 0.81\n",
      "Iteration 10204: with minibatch training loss = 0.547 and accuracy of 0.86\n",
      "Iteration 10205: with minibatch training loss = 0.664 and accuracy of 0.83\n",
      "Iteration 10206: with minibatch training loss = 0.801 and accuracy of 0.77\n",
      "Iteration 10207: with minibatch training loss = 0.759 and accuracy of 0.78\n",
      "Iteration 10208: with minibatch training loss = 0.415 and accuracy of 0.89\n",
      "Iteration 10209: with minibatch training loss = 0.652 and accuracy of 0.8\n",
      "Iteration 10210: with minibatch training loss = 0.527 and accuracy of 0.86\n",
      "Iteration 10211: with minibatch training loss = 0.965 and accuracy of 0.7\n",
      "Iteration 10212: with minibatch training loss = 0.588 and accuracy of 0.83\n",
      "Iteration 10213: with minibatch training loss = 0.596 and accuracy of 0.86\n",
      "Iteration 10214: with minibatch training loss = 0.84 and accuracy of 0.73\n",
      "Iteration 10215: with minibatch training loss = 0.971 and accuracy of 0.73\n",
      "Iteration 10216: with minibatch training loss = 0.791 and accuracy of 0.77\n",
      "Iteration 10217: with minibatch training loss = 0.604 and accuracy of 0.84\n",
      "Iteration 10218: with minibatch training loss = 0.622 and accuracy of 0.83\n",
      "Iteration 10219: with minibatch training loss = 0.736 and accuracy of 0.78\n",
      "Iteration 10220: with minibatch training loss = 1.08 and accuracy of 0.72\n",
      "Iteration 10221: with minibatch training loss = 0.683 and accuracy of 0.78\n",
      "Iteration 10222: with minibatch training loss = 1.16 and accuracy of 0.64\n",
      "Iteration 10223: with minibatch training loss = 0.735 and accuracy of 0.8\n",
      "Iteration 10224: with minibatch training loss = 0.95 and accuracy of 0.73\n",
      "Iteration 10225: with minibatch training loss = 0.8 and accuracy of 0.77\n",
      "Iteration 10226: with minibatch training loss = 0.647 and accuracy of 0.81\n",
      "Iteration 10227: with minibatch training loss = 0.811 and accuracy of 0.77\n",
      "Iteration 10228: with minibatch training loss = 0.992 and accuracy of 0.67\n",
      "Iteration 10229: with minibatch training loss = 0.658 and accuracy of 0.81\n",
      "Iteration 10230: with minibatch training loss = 0.925 and accuracy of 0.72\n",
      "Iteration 10231: with minibatch training loss = 0.82 and accuracy of 0.73\n",
      "Iteration 10232: with minibatch training loss = 0.772 and accuracy of 0.81\n",
      "Iteration 10233: with minibatch training loss = 0.763 and accuracy of 0.8\n",
      "Iteration 10234: with minibatch training loss = 0.883 and accuracy of 0.73\n",
      "Iteration 10235: with minibatch training loss = 0.851 and accuracy of 0.75\n",
      "Iteration 10236: with minibatch training loss = 0.749 and accuracy of 0.75\n",
      "Iteration 10237: with minibatch training loss = 0.838 and accuracy of 0.78\n",
      "Iteration 10238: with minibatch training loss = 0.779 and accuracy of 0.78\n",
      "Iteration 10239: with minibatch training loss = 0.787 and accuracy of 0.78\n",
      "Iteration 10240: with minibatch training loss = 0.743 and accuracy of 0.78\n",
      "Iteration 10241: with minibatch training loss = 0.701 and accuracy of 0.83\n",
      "Iteration 10242: with minibatch training loss = 0.832 and accuracy of 0.73\n",
      "Iteration 10243: with minibatch training loss = 0.761 and accuracy of 0.8\n",
      "Iteration 10244: with minibatch training loss = 0.614 and accuracy of 0.83\n",
      "Iteration 10245: with minibatch training loss = 0.973 and accuracy of 0.72\n",
      "Iteration 10246: with minibatch training loss = 0.897 and accuracy of 0.75\n",
      "Iteration 10247: with minibatch training loss = 0.767 and accuracy of 0.8\n",
      "Iteration 10248: with minibatch training loss = 0.576 and accuracy of 0.83\n",
      "Iteration 10249: with minibatch training loss = 0.505 and accuracy of 0.84\n",
      "Iteration 10250: with minibatch training loss = 0.677 and accuracy of 0.8\n",
      "Iteration 10251: with minibatch training loss = 0.635 and accuracy of 0.86\n",
      "Iteration 10252: with minibatch training loss = 0.77 and accuracy of 0.81\n",
      "Iteration 10253: with minibatch training loss = 0.637 and accuracy of 0.84\n",
      "Iteration 10254: with minibatch training loss = 0.651 and accuracy of 0.81\n",
      "Iteration 10255: with minibatch training loss = 0.689 and accuracy of 0.83\n",
      "Iteration 10256: with minibatch training loss = 0.749 and accuracy of 0.77\n",
      "Iteration 10257: with minibatch training loss = 0.525 and accuracy of 0.88\n",
      "Iteration 10258: with minibatch training loss = 0.758 and accuracy of 0.78\n",
      "Iteration 10259: with minibatch training loss = 0.623 and accuracy of 0.78\n",
      "Iteration 10260: with minibatch training loss = 0.74 and accuracy of 0.84\n",
      "Iteration 10261: with minibatch training loss = 0.476 and accuracy of 0.89\n",
      "Iteration 10262: with minibatch training loss = 0.738 and accuracy of 0.78\n",
      "Iteration 10263: with minibatch training loss = 0.927 and accuracy of 0.73\n",
      "Iteration 10264: with minibatch training loss = 1.13 and accuracy of 0.69\n",
      "Iteration 10265: with minibatch training loss = 0.767 and accuracy of 0.78\n",
      "Iteration 10266: with minibatch training loss = 0.604 and accuracy of 0.81\n",
      "Iteration 10267: with minibatch training loss = 0.873 and accuracy of 0.75\n",
      "Iteration 10268: with minibatch training loss = 0.751 and accuracy of 0.8\n",
      "Iteration 10269: with minibatch training loss = 0.913 and accuracy of 0.73\n",
      "Iteration 10270: with minibatch training loss = 0.882 and accuracy of 0.75\n",
      "Iteration 10271: with minibatch training loss = 0.923 and accuracy of 0.7\n",
      "Iteration 10272: with minibatch training loss = 0.761 and accuracy of 0.75\n",
      "Iteration 10273: with minibatch training loss = 0.958 and accuracy of 0.72\n",
      "Iteration 10274: with minibatch training loss = 0.811 and accuracy of 0.77\n",
      "Iteration 10275: with minibatch training loss = 0.826 and accuracy of 0.75\n",
      "Iteration 10276: with minibatch training loss = 0.595 and accuracy of 0.84\n",
      "Iteration 10277: with minibatch training loss = 0.647 and accuracy of 0.83\n",
      "Iteration 10278: with minibatch training loss = 0.701 and accuracy of 0.8\n",
      "Iteration 10279: with minibatch training loss = 0.694 and accuracy of 0.78\n",
      "Iteration 10280: with minibatch training loss = 0.656 and accuracy of 0.78\n",
      "Iteration 10281: with minibatch training loss = 0.748 and accuracy of 0.78\n",
      "Iteration 10282: with minibatch training loss = 0.837 and accuracy of 0.77\n",
      "Iteration 10283: with minibatch training loss = 0.634 and accuracy of 0.83\n",
      "Iteration 10284: with minibatch training loss = 0.909 and accuracy of 0.72\n",
      "Iteration 10285: with minibatch training loss = 0.727 and accuracy of 0.8\n",
      "Iteration 10286: with minibatch training loss = 1.01 and accuracy of 0.69\n",
      "Iteration 10287: with minibatch training loss = 0.658 and accuracy of 0.8\n",
      "Iteration 10288: with minibatch training loss = 0.752 and accuracy of 0.78\n",
      "Iteration 10289: with minibatch training loss = 0.889 and accuracy of 0.8\n",
      "Iteration 10290: with minibatch training loss = 0.768 and accuracy of 0.8\n",
      "Iteration 10291: with minibatch training loss = 0.704 and accuracy of 0.8\n",
      "Iteration 10292: with minibatch training loss = 0.656 and accuracy of 0.78\n",
      "Iteration 10293: with minibatch training loss = 0.741 and accuracy of 0.8\n",
      "Iteration 10294: with minibatch training loss = 0.846 and accuracy of 0.73\n",
      "Iteration 10295: with minibatch training loss = 0.669 and accuracy of 0.81\n",
      "Iteration 10296: with minibatch training loss = 0.73 and accuracy of 0.75\n",
      "Iteration 10297: with minibatch training loss = 0.521 and accuracy of 0.89\n",
      "Iteration 10298: with minibatch training loss = 0.762 and accuracy of 0.8\n",
      "Iteration 10299: with minibatch training loss = 0.587 and accuracy of 0.84\n",
      "Iteration 10300: with minibatch training loss = 0.782 and accuracy of 0.77\n",
      "Iteration 10301: with minibatch training loss = 0.777 and accuracy of 0.75\n",
      "Iteration 10302: with minibatch training loss = 0.537 and accuracy of 0.86\n",
      "Iteration 10303: with minibatch training loss = 1.02 and accuracy of 0.73\n",
      "Iteration 10304: with minibatch training loss = 0.928 and accuracy of 0.69\n",
      "Iteration 10305: with minibatch training loss = 0.554 and accuracy of 0.84\n",
      "Iteration 10306: with minibatch training loss = 0.632 and accuracy of 0.81\n",
      "Iteration 10307: with minibatch training loss = 0.534 and accuracy of 0.86\n",
      "Iteration 10308: with minibatch training loss = 0.617 and accuracy of 0.83\n",
      "Iteration 10309: with minibatch training loss = 0.831 and accuracy of 0.75\n",
      "Iteration 10310: with minibatch training loss = 0.586 and accuracy of 0.84\n",
      "Iteration 10311: with minibatch training loss = 1.13 and accuracy of 0.69\n",
      "Iteration 10312: with minibatch training loss = 0.882 and accuracy of 0.77\n",
      "Iteration 10313: with minibatch training loss = 0.786 and accuracy of 0.77\n",
      "Iteration 10314: with minibatch training loss = 0.911 and accuracy of 0.73\n",
      "Iteration 10315: with minibatch training loss = 1.03 and accuracy of 0.69\n",
      "Iteration 10316: with minibatch training loss = 0.825 and accuracy of 0.75\n",
      "Iteration 10317: with minibatch training loss = 0.685 and accuracy of 0.81\n",
      "Iteration 10318: with minibatch training loss = 1.03 and accuracy of 0.67\n",
      "Iteration 10319: with minibatch training loss = 0.677 and accuracy of 0.83\n",
      "Iteration 10320: with minibatch training loss = 0.515 and accuracy of 0.86\n",
      "Iteration 10321: with minibatch training loss = 0.85 and accuracy of 0.77\n",
      "Iteration 10322: with minibatch training loss = 0.515 and accuracy of 0.86\n",
      "Iteration 10323: with minibatch training loss = 0.91 and accuracy of 0.75\n",
      "Iteration 10324: with minibatch training loss = 0.744 and accuracy of 0.8\n",
      "Iteration 10325: with minibatch training loss = 0.572 and accuracy of 0.83\n",
      "Iteration 10326: with minibatch training loss = 0.735 and accuracy of 0.77\n",
      "Iteration 10327: with minibatch training loss = 0.721 and accuracy of 0.8\n",
      "Iteration 10328: with minibatch training loss = 0.652 and accuracy of 0.8\n",
      "Iteration 10329: with minibatch training loss = 0.975 and accuracy of 0.72\n",
      "Iteration 10330: with minibatch training loss = 0.946 and accuracy of 0.75\n",
      "Iteration 10331: with minibatch training loss = 0.945 and accuracy of 0.72\n",
      "Iteration 10332: with minibatch training loss = 0.951 and accuracy of 0.75\n",
      "Iteration 10333: with minibatch training loss = 0.599 and accuracy of 0.84\n",
      "Iteration 10334: with minibatch training loss = 0.716 and accuracy of 0.81\n",
      "Iteration 10335: with minibatch training loss = 0.711 and accuracy of 0.8\n",
      "Iteration 10336: with minibatch training loss = 0.688 and accuracy of 0.81\n",
      "Iteration 10337: with minibatch training loss = 0.851 and accuracy of 0.75\n",
      "Iteration 10338: with minibatch training loss = 0.594 and accuracy of 0.83\n",
      "Iteration 10339: with minibatch training loss = 0.802 and accuracy of 0.78\n",
      "Iteration 10340: with minibatch training loss = 1.02 and accuracy of 0.66\n",
      "Iteration 10341: with minibatch training loss = 0.958 and accuracy of 0.75\n",
      "Iteration 10342: with minibatch training loss = 0.591 and accuracy of 0.83\n",
      "Iteration 10343: with minibatch training loss = 0.68 and accuracy of 0.81\n",
      "Iteration 10344: with minibatch training loss = 1.04 and accuracy of 0.69\n",
      "Iteration 10345: with minibatch training loss = 0.74 and accuracy of 0.78\n",
      "Iteration 10346: with minibatch training loss = 0.556 and accuracy of 0.81\n",
      "Iteration 10347: with minibatch training loss = 0.779 and accuracy of 0.77\n",
      "Iteration 10348: with minibatch training loss = 0.601 and accuracy of 0.83\n",
      "Iteration 10349: with minibatch training loss = 0.757 and accuracy of 0.8\n",
      "Iteration 10350: with minibatch training loss = 0.957 and accuracy of 0.72\n",
      "Iteration 10351: with minibatch training loss = 0.974 and accuracy of 0.72\n",
      "Iteration 10352: with minibatch training loss = 0.612 and accuracy of 0.81\n",
      "Iteration 10353: with minibatch training loss = 0.735 and accuracy of 0.77\n",
      "Iteration 10354: with minibatch training loss = 0.709 and accuracy of 0.78\n",
      "Iteration 10355: with minibatch training loss = 0.634 and accuracy of 0.8\n",
      "Iteration 10356: with minibatch training loss = 0.858 and accuracy of 0.75\n",
      "Iteration 10357: with minibatch training loss = 0.879 and accuracy of 0.73\n",
      "Iteration 10358: with minibatch training loss = 0.7 and accuracy of 0.78\n",
      "Iteration 10359: with minibatch training loss = 0.747 and accuracy of 0.78\n",
      "Iteration 10360: with minibatch training loss = 0.947 and accuracy of 0.75\n",
      "Iteration 10361: with minibatch training loss = 1.04 and accuracy of 0.69\n",
      "Iteration 10362: with minibatch training loss = 0.63 and accuracy of 0.83\n",
      "Iteration 10363: with minibatch training loss = 0.799 and accuracy of 0.77\n",
      "Iteration 10364: with minibatch training loss = 0.77 and accuracy of 0.75\n",
      "Iteration 10365: with minibatch training loss = 0.808 and accuracy of 0.78\n",
      "Iteration 10366: with minibatch training loss = 0.665 and accuracy of 0.8\n",
      "Iteration 10367: with minibatch training loss = 0.566 and accuracy of 0.86\n",
      "Iteration 10368: with minibatch training loss = 0.88 and accuracy of 0.73\n",
      "Iteration 10369: with minibatch training loss = 1.01 and accuracy of 0.72\n",
      "Iteration 10370: with minibatch training loss = 0.578 and accuracy of 0.86\n",
      "Iteration 10371: with minibatch training loss = 0.866 and accuracy of 0.69\n",
      "Iteration 10372: with minibatch training loss = 1.12 and accuracy of 0.69\n",
      "Iteration 10373: with minibatch training loss = 0.58 and accuracy of 0.83\n",
      "Iteration 10374: with minibatch training loss = 0.55 and accuracy of 0.86\n",
      "Iteration 10375: with minibatch training loss = 0.81 and accuracy of 0.75\n",
      "Iteration 10376: with minibatch training loss = 0.76 and accuracy of 0.8\n",
      "Iteration 10377: with minibatch training loss = 0.814 and accuracy of 0.75\n",
      "Iteration 10378: with minibatch training loss = 0.44 and accuracy of 0.89\n",
      "Iteration 10379: with minibatch training loss = 0.716 and accuracy of 0.77\n",
      "Iteration 10380: with minibatch training loss = 0.715 and accuracy of 0.8\n",
      "Iteration 10381: with minibatch training loss = 0.78 and accuracy of 0.78\n",
      "Iteration 10382: with minibatch training loss = 0.558 and accuracy of 0.84\n",
      "Iteration 10383: with minibatch training loss = 0.561 and accuracy of 0.84\n",
      "Iteration 10384: with minibatch training loss = 0.936 and accuracy of 0.75\n",
      "Iteration 10385: with minibatch training loss = 0.738 and accuracy of 0.8\n",
      "Iteration 10386: with minibatch training loss = 0.923 and accuracy of 0.72\n",
      "Iteration 10387: with minibatch training loss = 0.725 and accuracy of 0.8\n",
      "Iteration 10388: with minibatch training loss = 0.725 and accuracy of 0.8\n",
      "Iteration 10389: with minibatch training loss = 0.74 and accuracy of 0.77\n",
      "Iteration 10390: with minibatch training loss = 0.577 and accuracy of 0.81\n",
      "Iteration 10391: with minibatch training loss = 0.798 and accuracy of 0.78\n",
      "Iteration 10392: with minibatch training loss = 1.02 and accuracy of 0.69\n",
      "Iteration 10393: with minibatch training loss = 0.89 and accuracy of 0.8\n",
      "Iteration 10394: with minibatch training loss = 0.651 and accuracy of 0.83\n",
      "Iteration 10395: with minibatch training loss = 0.772 and accuracy of 0.78\n",
      "Iteration 10396: with minibatch training loss = 0.676 and accuracy of 0.8\n",
      "Iteration 10397: with minibatch training loss = 0.675 and accuracy of 0.8\n",
      "Iteration 10398: with minibatch training loss = 0.536 and accuracy of 0.84\n",
      "Iteration 10399: with minibatch training loss = 0.615 and accuracy of 0.84\n",
      "Iteration 10400: with minibatch training loss = 0.9 and accuracy of 0.72\n",
      "Iteration 10401: with minibatch training loss = 0.897 and accuracy of 0.73\n",
      "Iteration 10402: with minibatch training loss = 0.692 and accuracy of 0.78\n",
      "Iteration 10403: with minibatch training loss = 0.959 and accuracy of 0.69\n",
      "Iteration 10404: with minibatch training loss = 1.11 and accuracy of 0.66\n",
      "Iteration 10405: with minibatch training loss = 1.39 and accuracy of 0.59\n",
      "Iteration 10406: with minibatch training loss = 0.654 and accuracy of 0.81\n",
      "Iteration 10407: with minibatch training loss = 0.86 and accuracy of 0.77\n",
      "Iteration 10408: with minibatch training loss = 0.665 and accuracy of 0.81\n",
      "Iteration 10409: with minibatch training loss = 0.912 and accuracy of 0.75\n",
      "Iteration 10410: with minibatch training loss = 0.972 and accuracy of 0.69\n",
      "Iteration 10411: with minibatch training loss = 0.936 and accuracy of 0.73\n",
      "Iteration 10412: with minibatch training loss = 0.654 and accuracy of 0.81\n",
      "Iteration 10413: with minibatch training loss = 0.798 and accuracy of 0.77\n",
      "Iteration 10414: with minibatch training loss = 0.704 and accuracy of 0.8\n",
      "Iteration 10415: with minibatch training loss = 0.837 and accuracy of 0.75\n",
      "Iteration 10416: with minibatch training loss = 0.917 and accuracy of 0.73\n",
      "Iteration 10417: with minibatch training loss = 0.609 and accuracy of 0.83\n",
      "Iteration 10418: with minibatch training loss = 0.766 and accuracy of 0.73\n",
      "Iteration 10419: with minibatch training loss = 0.798 and accuracy of 0.78\n",
      "Iteration 10420: with minibatch training loss = 0.72 and accuracy of 0.83\n",
      "Iteration 10421: with minibatch training loss = 0.741 and accuracy of 0.81\n",
      "Iteration 10422: with minibatch training loss = 0.558 and accuracy of 0.83\n",
      "Iteration 10423: with minibatch training loss = 0.548 and accuracy of 0.81\n",
      "Iteration 10424: with minibatch training loss = 0.952 and accuracy of 0.72\n",
      "Iteration 10425: with minibatch training loss = 0.903 and accuracy of 0.73\n",
      "Iteration 10426: with minibatch training loss = 1.01 and accuracy of 0.69\n",
      "Iteration 10427: with minibatch training loss = 0.801 and accuracy of 0.72\n",
      "Iteration 10428: with minibatch training loss = 0.815 and accuracy of 0.75\n",
      "Iteration 10429: with minibatch training loss = 1.13 and accuracy of 0.67\n",
      "Iteration 10430: with minibatch training loss = 0.819 and accuracy of 0.77\n",
      "Iteration 10431: with minibatch training loss = 0.378 and accuracy of 0.92\n",
      "Iteration 10432: with minibatch training loss = 0.721 and accuracy of 0.8\n",
      "Iteration 10433: with minibatch training loss = 0.644 and accuracy of 0.83\n",
      "Iteration 10434: with minibatch training loss = 0.876 and accuracy of 0.75\n",
      "Iteration 10435: with minibatch training loss = 0.852 and accuracy of 0.8\n",
      "Iteration 10436: with minibatch training loss = 0.654 and accuracy of 0.8\n",
      "Iteration 10437: with minibatch training loss = 0.675 and accuracy of 0.8\n",
      "Iteration 10438: with minibatch training loss = 0.697 and accuracy of 0.77\n",
      "Iteration 10439: with minibatch training loss = 0.769 and accuracy of 0.77\n",
      "Iteration 10440: with minibatch training loss = 0.738 and accuracy of 0.78\n",
      "Iteration 10441: with minibatch training loss = 0.721 and accuracy of 0.81\n",
      "Iteration 10442: with minibatch training loss = 0.761 and accuracy of 0.8\n",
      "Iteration 10443: with minibatch training loss = 0.848 and accuracy of 0.73\n",
      "Iteration 10444: with minibatch training loss = 0.463 and accuracy of 0.86\n",
      "Iteration 10445: with minibatch training loss = 0.886 and accuracy of 0.75\n",
      "Iteration 10446: with minibatch training loss = 1.06 and accuracy of 0.67\n",
      "Iteration 10447: with minibatch training loss = 0.713 and accuracy of 0.8\n",
      "Iteration 10448: with minibatch training loss = 0.684 and accuracy of 0.8\n",
      "Iteration 10449: with minibatch training loss = 0.724 and accuracy of 0.78\n",
      "Iteration 10450: with minibatch training loss = 1.1 and accuracy of 0.67\n",
      "Iteration 10451: with minibatch training loss = 0.805 and accuracy of 0.77\n",
      "Iteration 10452: with minibatch training loss = 0.67 and accuracy of 0.78\n",
      "Iteration 10453: with minibatch training loss = 0.948 and accuracy of 0.7\n",
      "Iteration 10454: with minibatch training loss = 0.494 and accuracy of 0.83\n",
      "Iteration 10455: with minibatch training loss = 0.714 and accuracy of 0.81\n",
      "Iteration 10456: with minibatch training loss = 0.663 and accuracy of 0.8\n",
      "Iteration 10457: with minibatch training loss = 0.463 and accuracy of 0.86\n",
      "Iteration 10458: with minibatch training loss = 0.858 and accuracy of 0.72\n",
      "Iteration 10459: with minibatch training loss = 0.727 and accuracy of 0.81\n",
      "Iteration 10460: with minibatch training loss = 0.511 and accuracy of 0.86\n",
      "Iteration 10461: with minibatch training loss = 0.888 and accuracy of 0.77\n",
      "Iteration 10462: with minibatch training loss = 0.628 and accuracy of 0.81\n",
      "Iteration 10463: with minibatch training loss = 0.995 and accuracy of 0.72\n",
      "Iteration 10464: with minibatch training loss = 0.95 and accuracy of 0.73\n",
      "Iteration 10465: with minibatch training loss = 0.869 and accuracy of 0.73\n",
      "Iteration 10466: with minibatch training loss = 0.697 and accuracy of 0.81\n",
      "Iteration 10467: with minibatch training loss = 0.635 and accuracy of 0.81\n",
      "Iteration 10468: with minibatch training loss = 0.745 and accuracy of 0.77\n",
      "Iteration 10469: with minibatch training loss = 0.672 and accuracy of 0.77\n",
      "Iteration 10470: with minibatch training loss = 0.653 and accuracy of 0.83\n",
      "Iteration 10471: with minibatch training loss = 0.793 and accuracy of 0.77\n",
      "Iteration 10472: with minibatch training loss = 0.783 and accuracy of 0.78\n",
      "Iteration 10473: with minibatch training loss = 0.791 and accuracy of 0.75\n",
      "Iteration 10474: with minibatch training loss = 0.644 and accuracy of 0.84\n",
      "Iteration 10475: with minibatch training loss = 0.829 and accuracy of 0.75\n",
      "Iteration 10476: with minibatch training loss = 0.934 and accuracy of 0.75\n",
      "Iteration 10477: with minibatch training loss = 0.6 and accuracy of 0.81\n",
      "Iteration 10478: with minibatch training loss = 0.497 and accuracy of 0.88\n",
      "Iteration 10479: with minibatch training loss = 0.717 and accuracy of 0.78\n",
      "Iteration 10480: with minibatch training loss = 0.671 and accuracy of 0.83\n",
      "Iteration 10481: with minibatch training loss = 0.734 and accuracy of 0.8\n",
      "Iteration 10482: with minibatch training loss = 0.619 and accuracy of 0.84\n",
      "Iteration 10483: with minibatch training loss = 0.64 and accuracy of 0.83\n",
      "Iteration 10484: with minibatch training loss = 0.908 and accuracy of 0.75\n",
      "Iteration 10485: with minibatch training loss = 0.735 and accuracy of 0.81\n",
      "Iteration 10486: with minibatch training loss = 1 and accuracy of 0.7\n",
      "Iteration 10487: with minibatch training loss = 0.708 and accuracy of 0.8\n",
      "Iteration 10488: with minibatch training loss = 0.88 and accuracy of 0.73\n",
      "Iteration 10489: with minibatch training loss = 0.724 and accuracy of 0.81\n",
      "Iteration 10490: with minibatch training loss = 0.916 and accuracy of 0.72\n",
      "Iteration 10491: with minibatch training loss = 0.638 and accuracy of 0.8\n",
      "Iteration 10492: with minibatch training loss = 0.926 and accuracy of 0.73\n",
      "Iteration 10493: with minibatch training loss = 0.707 and accuracy of 0.81\n",
      "Iteration 10494: with minibatch training loss = 1.25 and accuracy of 0.61\n",
      "Iteration 10495: with minibatch training loss = 0.743 and accuracy of 0.77\n",
      "Iteration 10496: with minibatch training loss = 1.11 and accuracy of 0.66\n",
      "Iteration 10497: with minibatch training loss = 0.683 and accuracy of 0.81\n",
      "Iteration 10498: with minibatch training loss = 0.743 and accuracy of 0.77\n",
      "Iteration 10499: with minibatch training loss = 0.637 and accuracy of 0.78\n",
      "Iteration 10500: with minibatch training loss = 0.876 and accuracy of 0.72\n",
      "Iteration 10501: with minibatch training loss = 0.834 and accuracy of 0.77\n",
      "Iteration 10502: with minibatch training loss = 0.961 and accuracy of 0.69\n",
      "Iteration 10503: with minibatch training loss = 0.908 and accuracy of 0.7\n",
      "Iteration 10504: with minibatch training loss = 0.925 and accuracy of 0.72\n",
      "Iteration 10505: with minibatch training loss = 0.79 and accuracy of 0.75\n",
      "Iteration 10506: with minibatch training loss = 0.829 and accuracy of 0.77\n",
      "Iteration 10507: with minibatch training loss = 0.639 and accuracy of 0.81\n",
      "Iteration 10508: with minibatch training loss = 0.858 and accuracy of 0.73\n",
      "Iteration 10509: with minibatch training loss = 0.729 and accuracy of 0.75\n",
      "Iteration 10510: with minibatch training loss = 0.662 and accuracy of 0.81\n",
      "Iteration 10511: with minibatch training loss = 0.862 and accuracy of 0.75\n",
      "Iteration 10512: with minibatch training loss = 1.05 and accuracy of 0.69\n",
      "Iteration 10513: with minibatch training loss = 0.515 and accuracy of 0.84\n",
      "Iteration 10514: with minibatch training loss = 0.711 and accuracy of 0.8\n",
      "Iteration 10515: with minibatch training loss = 0.71 and accuracy of 0.8\n",
      "Iteration 10516: with minibatch training loss = 0.621 and accuracy of 0.81\n",
      "Iteration 10517: with minibatch training loss = 0.618 and accuracy of 0.84\n",
      "Iteration 10518: with minibatch training loss = 0.695 and accuracy of 0.8\n",
      "Iteration 10519: with minibatch training loss = 0.658 and accuracy of 0.84\n",
      "Iteration 10520: with minibatch training loss = 1.02 and accuracy of 0.69\n",
      "Iteration 10521: with minibatch training loss = 1.07 and accuracy of 0.69\n",
      "Iteration 10522: with minibatch training loss = 0.871 and accuracy of 0.75\n",
      "Iteration 10523: with minibatch training loss = 0.639 and accuracy of 0.75\n",
      "Iteration 10524: with minibatch training loss = 0.752 and accuracy of 0.78\n",
      "Iteration 10525: with minibatch training loss = 0.86 and accuracy of 0.77\n",
      "Iteration 10526: with minibatch training loss = 0.96 and accuracy of 0.72\n",
      "Iteration 10527: with minibatch training loss = 0.846 and accuracy of 0.77\n",
      "Iteration 10528: with minibatch training loss = 0.604 and accuracy of 0.81\n",
      "Iteration 10529: with minibatch training loss = 0.742 and accuracy of 0.78\n",
      "Iteration 10530: with minibatch training loss = 0.63 and accuracy of 0.8\n",
      "Iteration 10531: with minibatch training loss = 0.682 and accuracy of 0.78\n",
      "Iteration 10532: with minibatch training loss = 0.774 and accuracy of 0.77\n",
      "Iteration 10533: with minibatch training loss = 0.685 and accuracy of 0.81\n",
      "Iteration 10534: with minibatch training loss = 0.689 and accuracy of 0.78\n",
      "Iteration 10535: with minibatch training loss = 0.75 and accuracy of 0.77\n",
      "Iteration 10536: with minibatch training loss = 0.727 and accuracy of 0.8\n",
      "Iteration 10537: with minibatch training loss = 0.794 and accuracy of 0.77\n",
      "Iteration 10538: with minibatch training loss = 0.924 and accuracy of 0.7\n",
      "Iteration 10539: with minibatch training loss = 0.556 and accuracy of 0.88\n",
      "Iteration 10540: with minibatch training loss = 0.824 and accuracy of 0.78\n",
      "Iteration 10541: with minibatch training loss = 0.848 and accuracy of 0.8\n",
      "Iteration 10542: with minibatch training loss = 0.643 and accuracy of 0.78\n",
      "Iteration 10543: with minibatch training loss = 0.806 and accuracy of 0.78\n",
      "Iteration 10544: with minibatch training loss = 1.01 and accuracy of 0.66\n",
      "Iteration 10545: with minibatch training loss = 0.597 and accuracy of 0.81\n",
      "Iteration 10546: with minibatch training loss = 0.7 and accuracy of 0.8\n",
      "Iteration 10547: with minibatch training loss = 0.786 and accuracy of 0.77\n",
      "Iteration 10548: with minibatch training loss = 0.86 and accuracy of 0.77\n",
      "Iteration 10549: with minibatch training loss = 0.931 and accuracy of 0.77\n",
      "Iteration 10550: with minibatch training loss = 1.01 and accuracy of 0.69\n",
      "Iteration 10551: with minibatch training loss = 0.709 and accuracy of 0.78\n",
      "Iteration 10552: with minibatch training loss = 0.987 and accuracy of 0.69\n",
      "Iteration 10553: with minibatch training loss = 0.873 and accuracy of 0.77\n",
      "Iteration 10554: with minibatch training loss = 1.04 and accuracy of 0.67\n",
      "Iteration 10555: with minibatch training loss = 0.868 and accuracy of 0.75\n",
      "Iteration 10556: with minibatch training loss = 0.856 and accuracy of 0.72\n",
      "Iteration 10557: with minibatch training loss = 0.864 and accuracy of 0.77\n",
      "Iteration 10558: with minibatch training loss = 0.966 and accuracy of 0.72\n",
      "Iteration 10559: with minibatch training loss = 0.634 and accuracy of 0.81\n",
      "Iteration 10560: with minibatch training loss = 0.576 and accuracy of 0.83\n",
      "Iteration 10561: with minibatch training loss = 0.937 and accuracy of 0.7\n",
      "Iteration 10562: with minibatch training loss = 0.614 and accuracy of 0.81\n",
      "Iteration 10563: with minibatch training loss = 0.796 and accuracy of 0.8\n",
      "Iteration 10564: with minibatch training loss = 0.513 and accuracy of 0.91\n",
      "Iteration 10565: with minibatch training loss = 0.59 and accuracy of 0.84\n",
      "Iteration 10566: with minibatch training loss = 0.733 and accuracy of 0.78\n",
      "Iteration 10567: with minibatch training loss = 0.58 and accuracy of 0.81\n",
      "Iteration 10568: with minibatch training loss = 0.921 and accuracy of 0.75\n",
      "Iteration 10569: with minibatch training loss = 0.658 and accuracy of 0.81\n",
      "Iteration 10570: with minibatch training loss = 0.802 and accuracy of 0.78\n",
      "Iteration 10571: with minibatch training loss = 0.973 and accuracy of 0.7\n",
      "Iteration 10572: with minibatch training loss = 1.06 and accuracy of 0.73\n",
      "Iteration 10573: with minibatch training loss = 0.593 and accuracy of 0.84\n",
      "Iteration 10574: with minibatch training loss = 0.721 and accuracy of 0.78\n",
      "Iteration 10575: with minibatch training loss = 0.9 and accuracy of 0.75\n",
      "Iteration 10576: with minibatch training loss = 0.901 and accuracy of 0.75\n",
      "Iteration 10577: with minibatch training loss = 1.04 and accuracy of 0.7\n",
      "Iteration 10578: with minibatch training loss = 0.746 and accuracy of 0.78\n",
      "Iteration 10579: with minibatch training loss = 0.478 and accuracy of 0.88\n",
      "Iteration 10580: with minibatch training loss = 0.986 and accuracy of 0.7\n",
      "Iteration 10581: with minibatch training loss = 0.76 and accuracy of 0.77\n",
      "Iteration 10582: with minibatch training loss = 0.557 and accuracy of 0.83\n",
      "Iteration 10583: with minibatch training loss = 0.769 and accuracy of 0.78\n",
      "Iteration 10584: with minibatch training loss = 0.829 and accuracy of 0.75\n",
      "Iteration 10585: with minibatch training loss = 0.68 and accuracy of 0.8\n",
      "Iteration 10586: with minibatch training loss = 0.5 and accuracy of 0.86\n",
      "Iteration 10587: with minibatch training loss = 0.743 and accuracy of 0.78\n",
      "Iteration 10588: with minibatch training loss = 0.438 and accuracy of 0.91\n",
      "Iteration 10589: with minibatch training loss = 0.768 and accuracy of 0.78\n",
      "Iteration 10590: with minibatch training loss = 0.408 and accuracy of 0.89\n",
      "Iteration 10591: with minibatch training loss = 0.68 and accuracy of 0.81\n",
      "Iteration 10592: with minibatch training loss = 1.2 and accuracy of 0.66\n",
      "Iteration 10593: with minibatch training loss = 0.622 and accuracy of 0.84\n",
      "Iteration 10594: with minibatch training loss = 0.564 and accuracy of 0.83\n",
      "Iteration 10595: with minibatch training loss = 0.708 and accuracy of 0.8\n",
      "Iteration 10596: with minibatch training loss = 0.904 and accuracy of 0.72\n",
      "Iteration 10597: with minibatch training loss = 0.957 and accuracy of 0.75\n",
      "Iteration 10598: with minibatch training loss = 0.761 and accuracy of 0.77\n",
      "Iteration 10599: with minibatch training loss = 0.691 and accuracy of 0.8\n",
      "Iteration 10600: with minibatch training loss = 0.821 and accuracy of 0.78\n",
      "Iteration 10601: with minibatch training loss = 0.501 and accuracy of 0.88\n",
      "Iteration 10602: with minibatch training loss = 0.539 and accuracy of 0.83\n",
      "Iteration 10603: with minibatch training loss = 0.793 and accuracy of 0.77\n",
      "Iteration 10604: with minibatch training loss = 0.458 and accuracy of 0.86\n",
      "Iteration 10605: with minibatch training loss = 1.01 and accuracy of 0.72\n",
      "Iteration 10606: with minibatch training loss = 0.893 and accuracy of 0.73\n",
      "Iteration 10607: with minibatch training loss = 0.792 and accuracy of 0.78\n",
      "Iteration 10608: with minibatch training loss = 1.02 and accuracy of 0.69\n",
      "Iteration 10609: with minibatch training loss = 0.734 and accuracy of 0.78\n",
      "Iteration 10610: with minibatch training loss = 0.527 and accuracy of 0.86\n",
      "Iteration 10611: with minibatch training loss = 0.821 and accuracy of 0.78\n",
      "Iteration 10612: with minibatch training loss = 0.584 and accuracy of 0.84\n",
      "Iteration 10613: with minibatch training loss = 0.75 and accuracy of 0.8\n",
      "Iteration 10614: with minibatch training loss = 0.594 and accuracy of 0.83\n",
      "Iteration 10615: with minibatch training loss = 0.371 and accuracy of 0.91\n",
      "Iteration 10616: with minibatch training loss = 0.388 and accuracy of 0.89\n",
      "Iteration 10617: with minibatch training loss = 0.737 and accuracy of 0.8\n",
      "Iteration 10618: with minibatch training loss = 0.728 and accuracy of 0.78\n",
      "Iteration 10619: with minibatch training loss = 0.574 and accuracy of 0.83\n",
      "Iteration 10620: with minibatch training loss = 0.703 and accuracy of 0.77\n",
      "Iteration 10621: with minibatch training loss = 0.736 and accuracy of 0.8\n",
      "Iteration 10622: with minibatch training loss = 1.03 and accuracy of 0.73\n",
      "Iteration 10623: with minibatch training loss = 0.527 and accuracy of 0.83\n",
      "Iteration 10624: with minibatch training loss = 0.656 and accuracy of 0.81\n",
      "Iteration 10625: with minibatch training loss = 0.781 and accuracy of 0.77\n",
      "Iteration 10626: with minibatch training loss = 1.03 and accuracy of 0.67\n",
      "Iteration 10627: with minibatch training loss = 0.695 and accuracy of 0.8\n",
      "Iteration 10628: with minibatch training loss = 0.584 and accuracy of 0.83\n",
      "Iteration 10629: with minibatch training loss = 0.676 and accuracy of 0.8\n",
      "Iteration 10630: with minibatch training loss = 0.868 and accuracy of 0.75\n",
      "Iteration 10631: with minibatch training loss = 0.472 and accuracy of 0.89\n",
      "Iteration 10632: with minibatch training loss = 1.19 and accuracy of 0.62\n",
      "Iteration 10633: with minibatch training loss = 0.702 and accuracy of 0.78\n",
      "Iteration 10634: with minibatch training loss = 0.643 and accuracy of 0.86\n",
      "Iteration 10635: with minibatch training loss = 0.693 and accuracy of 0.8\n",
      "Iteration 10636: with minibatch training loss = 0.784 and accuracy of 0.77\n",
      "Iteration 10637: with minibatch training loss = 0.54 and accuracy of 0.83\n",
      "Iteration 10638: with minibatch training loss = 0.818 and accuracy of 0.77\n",
      "Iteration 10639: with minibatch training loss = 0.638 and accuracy of 0.81\n",
      "Iteration 10640: with minibatch training loss = 0.934 and accuracy of 0.75\n",
      "Iteration 10641: with minibatch training loss = 1.06 and accuracy of 0.7\n",
      "Iteration 10642: with minibatch training loss = 0.7 and accuracy of 0.8\n",
      "Iteration 10643: with minibatch training loss = 0.602 and accuracy of 0.83\n",
      "Iteration 10644: with minibatch training loss = 0.876 and accuracy of 0.7\n",
      "Iteration 10645: with minibatch training loss = 0.62 and accuracy of 0.84\n",
      "Iteration 10646: with minibatch training loss = 0.661 and accuracy of 0.8\n",
      "Iteration 10647: with minibatch training loss = 0.711 and accuracy of 0.78\n",
      "Iteration 10648: with minibatch training loss = 0.772 and accuracy of 0.77\n",
      "Iteration 10649: with minibatch training loss = 1.03 and accuracy of 0.67\n",
      "Iteration 10650: with minibatch training loss = 0.662 and accuracy of 0.81\n",
      "Iteration 10651: with minibatch training loss = 0.669 and accuracy of 0.84\n",
      "Iteration 10652: with minibatch training loss = 1.02 and accuracy of 0.69\n",
      "Iteration 10653: with minibatch training loss = 0.729 and accuracy of 0.78\n",
      "Iteration 10654: with minibatch training loss = 0.501 and accuracy of 0.86\n",
      "Iteration 10655: with minibatch training loss = 0.702 and accuracy of 0.8\n",
      "Iteration 10656: with minibatch training loss = 0.554 and accuracy of 0.86\n",
      "Iteration 10657: with minibatch training loss = 0.576 and accuracy of 0.86\n",
      "Iteration 10658: with minibatch training loss = 0.969 and accuracy of 0.7\n",
      "Iteration 10659: with minibatch training loss = 0.546 and accuracy of 0.84\n",
      "Iteration 10660: with minibatch training loss = 0.878 and accuracy of 0.78\n",
      "Iteration 10661: with minibatch training loss = 0.869 and accuracy of 0.73\n",
      "Iteration 10662: with minibatch training loss = 1.11 and accuracy of 0.66\n",
      "Iteration 10663: with minibatch training loss = 0.663 and accuracy of 0.81\n",
      "Iteration 10664: with minibatch training loss = 0.962 and accuracy of 0.7\n",
      "Iteration 10665: with minibatch training loss = 0.613 and accuracy of 0.8\n",
      "Iteration 10666: with minibatch training loss = 0.861 and accuracy of 0.75\n",
      "Iteration 10667: with minibatch training loss = 0.732 and accuracy of 0.8\n",
      "Iteration 10668: with minibatch training loss = 0.43 and accuracy of 0.88\n",
      "Iteration 10669: with minibatch training loss = 0.527 and accuracy of 0.84\n",
      "Iteration 10670: with minibatch training loss = 1.26 and accuracy of 0.66\n",
      "Iteration 10671: with minibatch training loss = 0.936 and accuracy of 0.72\n",
      "Iteration 10672: with minibatch training loss = 0.859 and accuracy of 0.7\n",
      "Iteration 10673: with minibatch training loss = 0.771 and accuracy of 0.77\n",
      "Iteration 10674: with minibatch training loss = 0.814 and accuracy of 0.73\n",
      "Iteration 10675: with minibatch training loss = 0.562 and accuracy of 0.86\n",
      "Iteration 10676: with minibatch training loss = 0.742 and accuracy of 0.8\n",
      "Iteration 10677: with minibatch training loss = 0.499 and accuracy of 0.88\n",
      "Iteration 10678: with minibatch training loss = 0.786 and accuracy of 0.77\n",
      "Iteration 10679: with minibatch training loss = 0.838 and accuracy of 0.75\n",
      "Iteration 10680: with minibatch training loss = 0.915 and accuracy of 0.72\n",
      "Iteration 10681: with minibatch training loss = 0.682 and accuracy of 0.81\n",
      "Iteration 10682: with minibatch training loss = 0.824 and accuracy of 0.73\n",
      "Iteration 10683: with minibatch training loss = 0.886 and accuracy of 0.73\n",
      "Iteration 10684: with minibatch training loss = 0.978 and accuracy of 0.69\n",
      "Iteration 10685: with minibatch training loss = 0.82 and accuracy of 0.75\n",
      "Iteration 10686: with minibatch training loss = 0.665 and accuracy of 0.81\n",
      "Iteration 10687: with minibatch training loss = 0.853 and accuracy of 0.72\n",
      "Iteration 10688: with minibatch training loss = 0.742 and accuracy of 0.78\n",
      "Iteration 10689: with minibatch training loss = 0.466 and accuracy of 0.91\n",
      "Iteration 10690: with minibatch training loss = 0.771 and accuracy of 0.78\n",
      "Iteration 10691: with minibatch training loss = 0.784 and accuracy of 0.73\n",
      "Iteration 10692: with minibatch training loss = 0.574 and accuracy of 0.86\n",
      "Iteration 10693: with minibatch training loss = 0.58 and accuracy of 0.8\n",
      "Iteration 10694: with minibatch training loss = 0.765 and accuracy of 0.77\n",
      "Iteration 10695: with minibatch training loss = 0.865 and accuracy of 0.77\n",
      "Iteration 10696: with minibatch training loss = 1.25 and accuracy of 0.62\n",
      "Iteration 10697: with minibatch training loss = 0.592 and accuracy of 0.83\n",
      "Iteration 10698: with minibatch training loss = 0.787 and accuracy of 0.77\n",
      "Iteration 10699: with minibatch training loss = 0.985 and accuracy of 0.7\n",
      "Iteration 10700: with minibatch training loss = 0.802 and accuracy of 0.8\n",
      "Iteration 10701: with minibatch training loss = 0.787 and accuracy of 0.8\n",
      "Iteration 10702: with minibatch training loss = 0.686 and accuracy of 0.81\n",
      "Iteration 10703: with minibatch training loss = 0.886 and accuracy of 0.73\n",
      "Iteration 10704: with minibatch training loss = 0.884 and accuracy of 0.75\n",
      "Iteration 10705: with minibatch training loss = 0.708 and accuracy of 0.81\n",
      "Iteration 10706: with minibatch training loss = 0.794 and accuracy of 0.78\n",
      "Iteration 10707: with minibatch training loss = 0.665 and accuracy of 0.81\n",
      "Iteration 10708: with minibatch training loss = 0.586 and accuracy of 0.83\n",
      "Iteration 10709: with minibatch training loss = 0.705 and accuracy of 0.81\n",
      "Iteration 10710: with minibatch training loss = 0.72 and accuracy of 0.8\n",
      "Iteration 10711: with minibatch training loss = 0.594 and accuracy of 0.83\n",
      "Iteration 10712: with minibatch training loss = 1.02 and accuracy of 0.66\n",
      "Iteration 10713: with minibatch training loss = 0.808 and accuracy of 0.77\n",
      "Iteration 10714: with minibatch training loss = 0.971 and accuracy of 0.7\n",
      "Iteration 10715: with minibatch training loss = 0.836 and accuracy of 0.75\n",
      "Iteration 10716: with minibatch training loss = 1.08 and accuracy of 0.7\n",
      "Iteration 10717: with minibatch training loss = 0.507 and accuracy of 0.86\n",
      "Iteration 10718: with minibatch training loss = 0.831 and accuracy of 0.73\n",
      "Iteration 10719: with minibatch training loss = 0.713 and accuracy of 0.81\n",
      "Iteration 10720: with minibatch training loss = 0.865 and accuracy of 0.73\n",
      "Iteration 10721: with minibatch training loss = 0.714 and accuracy of 0.78\n",
      "Iteration 10722: with minibatch training loss = 0.599 and accuracy of 0.83\n",
      "Iteration 10723: with minibatch training loss = 1 and accuracy of 0.67\n",
      "Iteration 10724: with minibatch training loss = 1.04 and accuracy of 0.66\n",
      "Iteration 10725: with minibatch training loss = 0.563 and accuracy of 0.84\n",
      "Iteration 10726: with minibatch training loss = 0.521 and accuracy of 0.84\n",
      "Iteration 10727: with minibatch training loss = 0.667 and accuracy of 0.81\n",
      "Iteration 10728: with minibatch training loss = 0.801 and accuracy of 0.8\n",
      "Iteration 10729: with minibatch training loss = 0.667 and accuracy of 0.83\n",
      "Iteration 10730: with minibatch training loss = 0.786 and accuracy of 0.77\n",
      "Iteration 10731: with minibatch training loss = 0.817 and accuracy of 0.75\n",
      "Iteration 10732: with minibatch training loss = 0.89 and accuracy of 0.72\n",
      "Iteration 10733: with minibatch training loss = 0.692 and accuracy of 0.78\n",
      "Iteration 10734: with minibatch training loss = 0.611 and accuracy of 0.83\n",
      "Iteration 10735: with minibatch training loss = 0.846 and accuracy of 0.75\n",
      "Iteration 10736: with minibatch training loss = 0.621 and accuracy of 0.83\n",
      "Iteration 10737: with minibatch training loss = 0.838 and accuracy of 0.75\n",
      "Iteration 10738: with minibatch training loss = 0.692 and accuracy of 0.81\n",
      "Iteration 10739: with minibatch training loss = 0.678 and accuracy of 0.8\n",
      "Iteration 10740: with minibatch training loss = 0.567 and accuracy of 0.88\n",
      "Iteration 10741: with minibatch training loss = 0.759 and accuracy of 0.78\n",
      "Iteration 10742: with minibatch training loss = 0.787 and accuracy of 0.78\n",
      "Iteration 10743: with minibatch training loss = 0.678 and accuracy of 0.81\n",
      "Iteration 10744: with minibatch training loss = 0.989 and accuracy of 0.72\n",
      "Iteration 10745: with minibatch training loss = 0.827 and accuracy of 0.77\n",
      "Iteration 10746: with minibatch training loss = 0.735 and accuracy of 0.78\n",
      "Iteration 10747: with minibatch training loss = 1.06 and accuracy of 0.69\n",
      "Iteration 10748: with minibatch training loss = 0.622 and accuracy of 0.83\n",
      "Iteration 10749: with minibatch training loss = 0.951 and accuracy of 0.72\n",
      "Iteration 10750: with minibatch training loss = 0.785 and accuracy of 0.75\n",
      "Iteration 10751: with minibatch training loss = 0.49 and accuracy of 0.86\n",
      "Iteration 10752: with minibatch training loss = 0.751 and accuracy of 0.8\n",
      "Iteration 10753: with minibatch training loss = 0.71 and accuracy of 0.77\n",
      "Iteration 10754: with minibatch training loss = 0.968 and accuracy of 0.69\n",
      "Iteration 10755: with minibatch training loss = 0.67 and accuracy of 0.81\n",
      "Iteration 10756: with minibatch training loss = 0.838 and accuracy of 0.8\n",
      "Iteration 10757: with minibatch training loss = 0.465 and accuracy of 0.91\n",
      "Iteration 10758: with minibatch training loss = 0.79 and accuracy of 0.78\n",
      "Iteration 10759: with minibatch training loss = 0.794 and accuracy of 0.78\n",
      "Iteration 10760: with minibatch training loss = 0.789 and accuracy of 0.78\n",
      "Iteration 10761: with minibatch training loss = 0.827 and accuracy of 0.77\n",
      "Iteration 10762: with minibatch training loss = 0.442 and accuracy of 0.89\n",
      "Iteration 10763: with minibatch training loss = 0.876 and accuracy of 0.75\n",
      "Iteration 10764: with minibatch training loss = 0.447 and accuracy of 0.89\n",
      "Iteration 10765: with minibatch training loss = 0.442 and accuracy of 0.91\n",
      "Iteration 10766: with minibatch training loss = 0.924 and accuracy of 0.73\n",
      "Iteration 10767: with minibatch training loss = 0.827 and accuracy of 0.77\n",
      "Iteration 10768: with minibatch training loss = 0.932 and accuracy of 0.7\n",
      "Iteration 10769: with minibatch training loss = 0.669 and accuracy of 0.81\n",
      "Iteration 10770: with minibatch training loss = 0.452 and accuracy of 0.89\n",
      "Iteration 10771: with minibatch training loss = 0.635 and accuracy of 0.81\n",
      "Iteration 10772: with minibatch training loss = 0.899 and accuracy of 0.73\n",
      "Iteration 10773: with minibatch training loss = 0.895 and accuracy of 0.75\n",
      "Iteration 10774: with minibatch training loss = 0.871 and accuracy of 0.75\n",
      "Iteration 10775: with minibatch training loss = 0.724 and accuracy of 0.77\n",
      "Iteration 10776: with minibatch training loss = 0.485 and accuracy of 0.89\n",
      "Iteration 10777: with minibatch training loss = 0.461 and accuracy of 0.88\n",
      "Iteration 10778: with minibatch training loss = 0.65 and accuracy of 0.81\n",
      "Iteration 10779: with minibatch training loss = 0.766 and accuracy of 0.75\n",
      "Iteration 10780: with minibatch training loss = 0.532 and accuracy of 0.89\n",
      "Iteration 10781: with minibatch training loss = 0.814 and accuracy of 0.77\n",
      "Iteration 10782: with minibatch training loss = 0.724 and accuracy of 0.81\n",
      "Iteration 10783: with minibatch training loss = 0.753 and accuracy of 0.75\n",
      "Iteration 10784: with minibatch training loss = 0.786 and accuracy of 0.77\n",
      "Iteration 10785: with minibatch training loss = 0.974 and accuracy of 0.72\n",
      "Iteration 10786: with minibatch training loss = 0.653 and accuracy of 0.8\n",
      "Iteration 10787: with minibatch training loss = 0.648 and accuracy of 0.84\n",
      "Iteration 10788: with minibatch training loss = 0.681 and accuracy of 0.78\n",
      "Iteration 10789: with minibatch training loss = 0.664 and accuracy of 0.78\n",
      "Iteration 10790: with minibatch training loss = 0.926 and accuracy of 0.75\n",
      "Iteration 10791: with minibatch training loss = 0.66 and accuracy of 0.8\n",
      "Iteration 10792: with minibatch training loss = 0.744 and accuracy of 0.78\n",
      "Iteration 10793: with minibatch training loss = 0.773 and accuracy of 0.78\n",
      "Iteration 10794: with minibatch training loss = 0.706 and accuracy of 0.83\n",
      "Iteration 10795: with minibatch training loss = 0.678 and accuracy of 0.8\n",
      "Iteration 10796: with minibatch training loss = 0.59 and accuracy of 0.81\n",
      "Iteration 10797: with minibatch training loss = 0.885 and accuracy of 0.73\n",
      "Iteration 10798: with minibatch training loss = 0.604 and accuracy of 0.84\n",
      "Iteration 10799: with minibatch training loss = 0.759 and accuracy of 0.75\n",
      "Iteration 10800: with minibatch training loss = 0.837 and accuracy of 0.77\n",
      "Iteration 10801: with minibatch training loss = 0.729 and accuracy of 0.8\n",
      "Iteration 10802: with minibatch training loss = 0.715 and accuracy of 0.78\n",
      "Iteration 10803: with minibatch training loss = 0.939 and accuracy of 0.73\n",
      "Iteration 10804: with minibatch training loss = 0.938 and accuracy of 0.69\n",
      "Iteration 10805: with minibatch training loss = 0.897 and accuracy of 0.72\n",
      "Iteration 10806: with minibatch training loss = 0.665 and accuracy of 0.8\n",
      "Iteration 10807: with minibatch training loss = 0.702 and accuracy of 0.81\n",
      "Iteration 10808: with minibatch training loss = 0.962 and accuracy of 0.72\n",
      "Iteration 10809: with minibatch training loss = 0.682 and accuracy of 0.8\n",
      "Iteration 10810: with minibatch training loss = 0.724 and accuracy of 0.8\n",
      "Iteration 10811: with minibatch training loss = 0.768 and accuracy of 0.77\n",
      "Iteration 10812: with minibatch training loss = 0.876 and accuracy of 0.73\n",
      "Iteration 10813: with minibatch training loss = 0.565 and accuracy of 0.83\n",
      "Iteration 10814: with minibatch training loss = 0.596 and accuracy of 0.83\n",
      "Iteration 10815: with minibatch training loss = 0.606 and accuracy of 0.81\n",
      "Iteration 10816: with minibatch training loss = 0.991 and accuracy of 0.7\n",
      "Iteration 10817: with minibatch training loss = 0.963 and accuracy of 0.73\n",
      "Iteration 10818: with minibatch training loss = 0.769 and accuracy of 0.8\n",
      "Iteration 10819: with minibatch training loss = 0.867 and accuracy of 0.75\n",
      "Iteration 10820: with minibatch training loss = 1.03 and accuracy of 0.72\n",
      "Iteration 10821: with minibatch training loss = 0.646 and accuracy of 0.8\n",
      "Iteration 10822: with minibatch training loss = 0.982 and accuracy of 0.7\n",
      "Iteration 10823: with minibatch training loss = 0.663 and accuracy of 0.8\n",
      "Iteration 10824: with minibatch training loss = 0.608 and accuracy of 0.8\n",
      "Iteration 10825: with minibatch training loss = 0.72 and accuracy of 0.81\n",
      "Iteration 10826: with minibatch training loss = 0.975 and accuracy of 0.72\n",
      "Iteration 10827: with minibatch training loss = 0.72 and accuracy of 0.8\n",
      "Iteration 10828: with minibatch training loss = 1.14 and accuracy of 0.66\n",
      "Iteration 10829: with minibatch training loss = 0.699 and accuracy of 0.83\n",
      "Iteration 10830: with minibatch training loss = 0.687 and accuracy of 0.83\n",
      "Iteration 10831: with minibatch training loss = 0.52 and accuracy of 0.86\n",
      "Iteration 10832: with minibatch training loss = 0.883 and accuracy of 0.73\n",
      "Iteration 10833: with minibatch training loss = 0.778 and accuracy of 0.77\n",
      "Iteration 10834: with minibatch training loss = 0.764 and accuracy of 0.75\n",
      "Iteration 10835: with minibatch training loss = 0.888 and accuracy of 0.72\n",
      "Iteration 10836: with minibatch training loss = 0.947 and accuracy of 0.7\n",
      "Iteration 10837: with minibatch training loss = 0.843 and accuracy of 0.77\n",
      "Iteration 10838: with minibatch training loss = 0.579 and accuracy of 0.81\n",
      "Iteration 10839: with minibatch training loss = 0.629 and accuracy of 0.81\n",
      "Iteration 10840: with minibatch training loss = 0.773 and accuracy of 0.78\n",
      "Iteration 10841: with minibatch training loss = 1.1 and accuracy of 0.64\n",
      "Iteration 10842: with minibatch training loss = 0.646 and accuracy of 0.8\n",
      "Iteration 10843: with minibatch training loss = 0.948 and accuracy of 0.69\n",
      "Iteration 10844: with minibatch training loss = 0.74 and accuracy of 0.77\n",
      "Iteration 10845: with minibatch training loss = 0.854 and accuracy of 0.73\n",
      "Iteration 10846: with minibatch training loss = 0.652 and accuracy of 0.8\n",
      "Iteration 10847: with minibatch training loss = 0.555 and accuracy of 0.84\n",
      "Iteration 10848: with minibatch training loss = 0.477 and accuracy of 0.89\n",
      "Iteration 10849: with minibatch training loss = 0.747 and accuracy of 0.77\n",
      "Iteration 10850: with minibatch training loss = 0.6 and accuracy of 0.84\n",
      "Iteration 10851: with minibatch training loss = 0.801 and accuracy of 0.77\n",
      "Iteration 10852: with minibatch training loss = 0.472 and accuracy of 0.88\n",
      "Iteration 10853: with minibatch training loss = 0.696 and accuracy of 0.8\n",
      "Iteration 10854: with minibatch training loss = 0.69 and accuracy of 0.8\n",
      "Iteration 10855: with minibatch training loss = 0.765 and accuracy of 0.77\n",
      "Iteration 10856: with minibatch training loss = 0.76 and accuracy of 0.77\n",
      "Iteration 10857: with minibatch training loss = 0.774 and accuracy of 0.78\n",
      "Iteration 10858: with minibatch training loss = 0.779 and accuracy of 0.78\n",
      "Iteration 10859: with minibatch training loss = 1 and accuracy of 0.7\n",
      "Iteration 10860: with minibatch training loss = 0.558 and accuracy of 0.83\n",
      "Iteration 10861: with minibatch training loss = 0.655 and accuracy of 0.81\n",
      "Iteration 10862: with minibatch training loss = 0.496 and accuracy of 0.84\n",
      "Iteration 10863: with minibatch training loss = 0.623 and accuracy of 0.8\n",
      "Iteration 10864: with minibatch training loss = 0.878 and accuracy of 0.72\n",
      "Iteration 10865: with minibatch training loss = 0.589 and accuracy of 0.83\n",
      "Iteration 10866: with minibatch training loss = 0.81 and accuracy of 0.75\n",
      "Iteration 10867: with minibatch training loss = 0.815 and accuracy of 0.77\n",
      "Iteration 10868: with minibatch training loss = 0.547 and accuracy of 0.84\n",
      "Iteration 10869: with minibatch training loss = 0.781 and accuracy of 0.77\n",
      "Iteration 10870: with minibatch training loss = 0.783 and accuracy of 0.77\n",
      "Iteration 10871: with minibatch training loss = 0.596 and accuracy of 0.81\n",
      "Iteration 10872: with minibatch training loss = 0.709 and accuracy of 0.8\n",
      "Iteration 10873: with minibatch training loss = 1 and accuracy of 0.72\n",
      "Iteration 10874: with minibatch training loss = 0.765 and accuracy of 0.8\n",
      "Iteration 10875: with minibatch training loss = 0.816 and accuracy of 0.77\n",
      "Iteration 10876: with minibatch training loss = 0.542 and accuracy of 0.86\n",
      "Iteration 10877: with minibatch training loss = 0.989 and accuracy of 0.72\n",
      "Iteration 10878: with minibatch training loss = 0.827 and accuracy of 0.77\n",
      "Iteration 10879: with minibatch training loss = 0.798 and accuracy of 0.78\n",
      "Iteration 10880: with minibatch training loss = 0.762 and accuracy of 0.75\n",
      "Iteration 10881: with minibatch training loss = 0.759 and accuracy of 0.75\n",
      "Iteration 10882: with minibatch training loss = 1.13 and accuracy of 0.66\n",
      "Iteration 10883: with minibatch training loss = 0.887 and accuracy of 0.75\n",
      "Iteration 10884: with minibatch training loss = 0.443 and accuracy of 0.88\n",
      "Iteration 10885: with minibatch training loss = 0.881 and accuracy of 0.78\n",
      "Iteration 10886: with minibatch training loss = 0.712 and accuracy of 0.83\n",
      "Iteration 10887: with minibatch training loss = 0.928 and accuracy of 0.7\n",
      "Iteration 10888: with minibatch training loss = 0.78 and accuracy of 0.75\n",
      "Iteration 10889: with minibatch training loss = 0.598 and accuracy of 0.83\n",
      "Iteration 10890: with minibatch training loss = 0.668 and accuracy of 0.81\n",
      "Iteration 10891: with minibatch training loss = 1.1 and accuracy of 0.64\n",
      "Iteration 10892: with minibatch training loss = 1.04 and accuracy of 0.72\n",
      "Iteration 10893: with minibatch training loss = 0.742 and accuracy of 0.8\n",
      "Iteration 10894: with minibatch training loss = 0.844 and accuracy of 0.75\n",
      "Iteration 10895: with minibatch training loss = 0.853 and accuracy of 0.77\n",
      "Iteration 10896: with minibatch training loss = 0.979 and accuracy of 0.69\n",
      "Iteration 10897: with minibatch training loss = 0.661 and accuracy of 0.81\n",
      "Iteration 10898: with minibatch training loss = 0.936 and accuracy of 0.7\n",
      "Iteration 10899: with minibatch training loss = 0.791 and accuracy of 0.75\n",
      "Iteration 10900: with minibatch training loss = 0.838 and accuracy of 0.73\n",
      "Iteration 10901: with minibatch training loss = 0.747 and accuracy of 0.8\n",
      "Iteration 10902: with minibatch training loss = 0.711 and accuracy of 0.81\n",
      "Iteration 10903: with minibatch training loss = 0.873 and accuracy of 0.75\n",
      "Iteration 10904: with minibatch training loss = 0.659 and accuracy of 0.81\n",
      "Iteration 10905: with minibatch training loss = 0.669 and accuracy of 0.8\n",
      "Iteration 10906: with minibatch training loss = 0.742 and accuracy of 0.81\n",
      "Iteration 10907: with minibatch training loss = 0.652 and accuracy of 0.81\n",
      "Iteration 10908: with minibatch training loss = 0.865 and accuracy of 0.78\n",
      "Iteration 10909: with minibatch training loss = 0.58 and accuracy of 0.81\n",
      "Iteration 10910: with minibatch training loss = 0.948 and accuracy of 0.72\n",
      "Iteration 10911: with minibatch training loss = 0.682 and accuracy of 0.78\n",
      "Iteration 10912: with minibatch training loss = 0.669 and accuracy of 0.8\n",
      "Iteration 10913: with minibatch training loss = 0.617 and accuracy of 0.81\n",
      "Iteration 10914: with minibatch training loss = 0.854 and accuracy of 0.72\n",
      "Iteration 10915: with minibatch training loss = 0.873 and accuracy of 0.72\n",
      "Iteration 10916: with minibatch training loss = 0.843 and accuracy of 0.75\n",
      "Iteration 10917: with minibatch training loss = 0.652 and accuracy of 0.8\n",
      "Iteration 10918: with minibatch training loss = 0.813 and accuracy of 0.77\n",
      "Iteration 10919: with minibatch training loss = 0.757 and accuracy of 0.81\n",
      "Iteration 10920: with minibatch training loss = 0.649 and accuracy of 0.78\n",
      "Iteration 10921: with minibatch training loss = 0.601 and accuracy of 0.81\n",
      "Iteration 10922: with minibatch training loss = 0.75 and accuracy of 0.8\n",
      "Iteration 10923: with minibatch training loss = 0.794 and accuracy of 0.78\n",
      "Iteration 10924: with minibatch training loss = 0.833 and accuracy of 0.75\n",
      "Iteration 10925: with minibatch training loss = 0.757 and accuracy of 0.78\n",
      "Iteration 10926: with minibatch training loss = 0.64 and accuracy of 0.8\n",
      "Iteration 10927: with minibatch training loss = 0.737 and accuracy of 0.8\n",
      "Iteration 10928: with minibatch training loss = 0.828 and accuracy of 0.77\n",
      "Iteration 10929: with minibatch training loss = 0.982 and accuracy of 0.72\n",
      "Iteration 10930: with minibatch training loss = 0.872 and accuracy of 0.75\n",
      "Iteration 10931: with minibatch training loss = 0.603 and accuracy of 0.83\n",
      "Iteration 10932: with minibatch training loss = 0.812 and accuracy of 0.78\n",
      "Iteration 10933: with minibatch training loss = 0.354 and accuracy of 0.92\n",
      "Iteration 10934: with minibatch training loss = 0.758 and accuracy of 0.77\n",
      "Iteration 10935: with minibatch training loss = 0.472 and accuracy of 0.89\n",
      "Iteration 10936: with minibatch training loss = 0.701 and accuracy of 0.78\n",
      "Iteration 10937: with minibatch training loss = 0.775 and accuracy of 0.78\n",
      "Iteration 10938: with minibatch training loss = 0.923 and accuracy of 0.73\n",
      "Iteration 10939: with minibatch training loss = 0.736 and accuracy of 0.81\n",
      "Iteration 10940: with minibatch training loss = 0.642 and accuracy of 0.81\n",
      "Iteration 10941: with minibatch training loss = 0.972 and accuracy of 0.7\n",
      "Iteration 10942: with minibatch training loss = 0.981 and accuracy of 0.75\n",
      "Iteration 10943: with minibatch training loss = 0.581 and accuracy of 0.83\n",
      "Iteration 10944: with minibatch training loss = 0.835 and accuracy of 0.81\n",
      "Iteration 10945: with minibatch training loss = 0.709 and accuracy of 0.78\n",
      "Iteration 10946: with minibatch training loss = 0.64 and accuracy of 0.83\n",
      "Iteration 10947: with minibatch training loss = 0.97 and accuracy of 0.78\n",
      "Iteration 10948: with minibatch training loss = 0.639 and accuracy of 0.81\n",
      "Iteration 10949: with minibatch training loss = 0.734 and accuracy of 0.77\n",
      "Iteration 10950: with minibatch training loss = 0.669 and accuracy of 0.83\n",
      "Iteration 10951: with minibatch training loss = 0.528 and accuracy of 0.86\n",
      "Iteration 10952: with minibatch training loss = 0.724 and accuracy of 0.77\n",
      "Iteration 10953: with minibatch training loss = 0.858 and accuracy of 0.77\n",
      "Iteration 10954: with minibatch training loss = 0.752 and accuracy of 0.77\n",
      "Iteration 10955: with minibatch training loss = 0.568 and accuracy of 0.83\n",
      "Iteration 10956: with minibatch training loss = 0.726 and accuracy of 0.81\n",
      "Iteration 10957: with minibatch training loss = 0.749 and accuracy of 0.77\n",
      "Iteration 10958: with minibatch training loss = 1.02 and accuracy of 0.72\n",
      "Iteration 10959: with minibatch training loss = 0.672 and accuracy of 0.78\n",
      "Iteration 10960: with minibatch training loss = 0.546 and accuracy of 0.83\n",
      "Iteration 10961: with minibatch training loss = 0.836 and accuracy of 0.78\n",
      "Iteration 10962: with minibatch training loss = 0.489 and accuracy of 0.88\n",
      "Iteration 10963: with minibatch training loss = 0.575 and accuracy of 0.81\n",
      "Iteration 10964: with minibatch training loss = 0.702 and accuracy of 0.81\n",
      "Iteration 10965: with minibatch training loss = 0.717 and accuracy of 0.78\n",
      "Iteration 10966: with minibatch training loss = 0.925 and accuracy of 0.73\n",
      "Iteration 10967: with minibatch training loss = 0.966 and accuracy of 0.75\n",
      "Iteration 10968: with minibatch training loss = 0.783 and accuracy of 0.77\n",
      "Iteration 10969: with minibatch training loss = 0.699 and accuracy of 0.83\n",
      "Iteration 10970: with minibatch training loss = 0.639 and accuracy of 0.8\n",
      "Iteration 10971: with minibatch training loss = 0.651 and accuracy of 0.81\n",
      "Iteration 10972: with minibatch training loss = 0.761 and accuracy of 0.8\n",
      "Iteration 10973: with minibatch training loss = 0.959 and accuracy of 0.7\n",
      "Iteration 10974: with minibatch training loss = 0.882 and accuracy of 0.72\n",
      "Iteration 10975: with minibatch training loss = 0.544 and accuracy of 0.86\n",
      "Iteration 10976: with minibatch training loss = 0.742 and accuracy of 0.78\n",
      "Iteration 10977: with minibatch training loss = 0.538 and accuracy of 0.86\n",
      "Iteration 10978: with minibatch training loss = 0.794 and accuracy of 0.77\n",
      "Iteration 10979: with minibatch training loss = 1.05 and accuracy of 0.66\n",
      "Iteration 10980: with minibatch training loss = 0.551 and accuracy of 0.86\n",
      "Iteration 10981: with minibatch training loss = 0.749 and accuracy of 0.81\n",
      "Iteration 10982: with minibatch training loss = 0.904 and accuracy of 0.73\n",
      "Iteration 10983: with minibatch training loss = 0.636 and accuracy of 0.81\n",
      "Iteration 10984: with minibatch training loss = 1.13 and accuracy of 0.67\n",
      "Iteration 10985: with minibatch training loss = 0.872 and accuracy of 0.73\n",
      "Iteration 10986: with minibatch training loss = 0.753 and accuracy of 0.8\n",
      "Iteration 10987: with minibatch training loss = 1.1 and accuracy of 0.66\n",
      "Iteration 10988: with minibatch training loss = 0.72 and accuracy of 0.81\n",
      "Iteration 10989: with minibatch training loss = 1.08 and accuracy of 0.7\n",
      "Iteration 10990: with minibatch training loss = 0.78 and accuracy of 0.77\n",
      "Iteration 10991: with minibatch training loss = 0.744 and accuracy of 0.8\n",
      "Iteration 10992: with minibatch training loss = 0.685 and accuracy of 0.78\n",
      "Iteration 10993: with minibatch training loss = 0.614 and accuracy of 0.81\n",
      "Iteration 10994: with minibatch training loss = 0.827 and accuracy of 0.75\n",
      "Iteration 10995: with minibatch training loss = 0.889 and accuracy of 0.73\n",
      "Iteration 10996: with minibatch training loss = 0.604 and accuracy of 0.8\n",
      "Iteration 10997: with minibatch training loss = 0.759 and accuracy of 0.75\n",
      "Iteration 10998: with minibatch training loss = 0.625 and accuracy of 0.8\n",
      "Iteration 10999: with minibatch training loss = 0.571 and accuracy of 0.81\n",
      "Iteration 11000: with minibatch training loss = 0.829 and accuracy of 0.72\n",
      "Iteration 11001: with minibatch training loss = 0.847 and accuracy of 0.75\n",
      "Iteration 11002: with minibatch training loss = 0.802 and accuracy of 0.75\n",
      "Iteration 11003: with minibatch training loss = 0.652 and accuracy of 0.81\n",
      "Iteration 11004: with minibatch training loss = 0.864 and accuracy of 0.73\n",
      "Iteration 11005: with minibatch training loss = 0.729 and accuracy of 0.78\n",
      "Iteration 11006: with minibatch training loss = 0.917 and accuracy of 0.72\n",
      "Iteration 11007: with minibatch training loss = 0.786 and accuracy of 0.77\n",
      "Iteration 11008: with minibatch training loss = 0.613 and accuracy of 0.83\n",
      "Iteration 11009: with minibatch training loss = 0.916 and accuracy of 0.75\n",
      "Iteration 11010: with minibatch training loss = 0.723 and accuracy of 0.78\n",
      "Iteration 11011: with minibatch training loss = 0.622 and accuracy of 0.83\n",
      "Iteration 11012: with minibatch training loss = 0.896 and accuracy of 0.73\n",
      "Iteration 11013: with minibatch training loss = 0.98 and accuracy of 0.77\n",
      "Iteration 11014: with minibatch training loss = 0.757 and accuracy of 0.77\n",
      "Iteration 11015: with minibatch training loss = 0.643 and accuracy of 0.81\n",
      "Iteration 11016: with minibatch training loss = 0.788 and accuracy of 0.77\n",
      "Iteration 11017: with minibatch training loss = 0.94 and accuracy of 0.72\n",
      "Iteration 11018: with minibatch training loss = 0.923 and accuracy of 0.73\n",
      "Iteration 11019: with minibatch training loss = 0.783 and accuracy of 0.77\n",
      "Iteration 11020: with minibatch training loss = 0.96 and accuracy of 0.69\n",
      "Iteration 11021: with minibatch training loss = 1.08 and accuracy of 0.66\n",
      "Iteration 11022: with minibatch training loss = 0.555 and accuracy of 0.84\n",
      "Iteration 11023: with minibatch training loss = 0.916 and accuracy of 0.72\n",
      "Iteration 11024: with minibatch training loss = 0.858 and accuracy of 0.77\n",
      "Iteration 11025: with minibatch training loss = 0.744 and accuracy of 0.78\n",
      "Iteration 11026: with minibatch training loss = 0.653 and accuracy of 0.83\n",
      "Iteration 11027: with minibatch training loss = 0.679 and accuracy of 0.78\n",
      "Iteration 11028: with minibatch training loss = 0.491 and accuracy of 0.86\n",
      "Iteration 11029: with minibatch training loss = 0.781 and accuracy of 0.8\n",
      "Iteration 11030: with minibatch training loss = 0.625 and accuracy of 0.81\n",
      "Iteration 11031: with minibatch training loss = 0.939 and accuracy of 0.7\n",
      "Iteration 11032: with minibatch training loss = 0.927 and accuracy of 0.73\n",
      "Iteration 11033: with minibatch training loss = 1.05 and accuracy of 0.7\n",
      "Iteration 11034: with minibatch training loss = 0.502 and accuracy of 0.86\n",
      "Iteration 11035: with minibatch training loss = 1.2 and accuracy of 0.64\n",
      "Iteration 11036: with minibatch training loss = 0.714 and accuracy of 0.78\n",
      "Iteration 11037: with minibatch training loss = 0.773 and accuracy of 0.73\n",
      "Iteration 11038: with minibatch training loss = 0.677 and accuracy of 0.81\n",
      "Iteration 11039: with minibatch training loss = 0.76 and accuracy of 0.77\n",
      "Iteration 11040: with minibatch training loss = 0.875 and accuracy of 0.72\n",
      "Iteration 11041: with minibatch training loss = 0.784 and accuracy of 0.78\n",
      "Iteration 11042: with minibatch training loss = 0.688 and accuracy of 0.83\n",
      "Iteration 11043: with minibatch training loss = 0.864 and accuracy of 0.72\n",
      "Iteration 11044: with minibatch training loss = 0.653 and accuracy of 0.8\n",
      "Iteration 11045: with minibatch training loss = 0.835 and accuracy of 0.75\n",
      "Iteration 11046: with minibatch training loss = 0.705 and accuracy of 0.83\n",
      "Iteration 11047: with minibatch training loss = 0.649 and accuracy of 0.81\n",
      "Iteration 11048: with minibatch training loss = 0.821 and accuracy of 0.73\n",
      "Iteration 11049: with minibatch training loss = 0.719 and accuracy of 0.81\n",
      "Iteration 11050: with minibatch training loss = 0.815 and accuracy of 0.81\n",
      "Iteration 11051: with minibatch training loss = 0.812 and accuracy of 0.77\n",
      "Iteration 11052: with minibatch training loss = 0.775 and accuracy of 0.78\n",
      "Iteration 11053: with minibatch training loss = 0.583 and accuracy of 0.83\n",
      "Iteration 11054: with minibatch training loss = 0.608 and accuracy of 0.83\n",
      "Iteration 11055: with minibatch training loss = 0.902 and accuracy of 0.73\n",
      "Iteration 11056: with minibatch training loss = 0.624 and accuracy of 0.8\n",
      "Iteration 11057: with minibatch training loss = 0.778 and accuracy of 0.78\n",
      "Iteration 11058: with minibatch training loss = 0.673 and accuracy of 0.83\n",
      "Iteration 11059: with minibatch training loss = 0.789 and accuracy of 0.75\n",
      "Iteration 11060: with minibatch training loss = 0.6 and accuracy of 0.83\n",
      "Iteration 11061: with minibatch training loss = 0.927 and accuracy of 0.72\n",
      "Iteration 11062: with minibatch training loss = 0.735 and accuracy of 0.8\n",
      "Iteration 11063: with minibatch training loss = 0.669 and accuracy of 0.77\n",
      "Iteration 11064: with minibatch training loss = 0.791 and accuracy of 0.77\n",
      "Iteration 11065: with minibatch training loss = 0.821 and accuracy of 0.73\n",
      "Iteration 11066: with minibatch training loss = 0.793 and accuracy of 0.8\n",
      "Iteration 11067: with minibatch training loss = 0.536 and accuracy of 0.84\n",
      "Iteration 11068: with minibatch training loss = 0.89 and accuracy of 0.75\n",
      "Iteration 11069: with minibatch training loss = 0.531 and accuracy of 0.84\n",
      "Iteration 11070: with minibatch training loss = 0.643 and accuracy of 0.81\n",
      "Iteration 11071: with minibatch training loss = 0.653 and accuracy of 0.81\n",
      "Iteration 11072: with minibatch training loss = 0.812 and accuracy of 0.78\n",
      "Iteration 11073: with minibatch training loss = 0.55 and accuracy of 0.86\n",
      "Iteration 11074: with minibatch training loss = 1.14 and accuracy of 0.62\n",
      "Iteration 11075: with minibatch training loss = 0.871 and accuracy of 0.73\n",
      "Iteration 11076: with minibatch training loss = 0.48 and accuracy of 0.84\n",
      "Iteration 11077: with minibatch training loss = 0.611 and accuracy of 0.81\n",
      "Iteration 11078: with minibatch training loss = 0.774 and accuracy of 0.77\n",
      "Iteration 11079: with minibatch training loss = 0.973 and accuracy of 0.7\n",
      "Iteration 11080: with minibatch training loss = 0.652 and accuracy of 0.81\n",
      "Iteration 11081: with minibatch training loss = 0.775 and accuracy of 0.78\n",
      "Iteration 11082: with minibatch training loss = 0.636 and accuracy of 0.81\n",
      "Iteration 11083: with minibatch training loss = 0.792 and accuracy of 0.75\n",
      "Iteration 11084: with minibatch training loss = 0.63 and accuracy of 0.81\n",
      "Iteration 11085: with minibatch training loss = 0.693 and accuracy of 0.8\n",
      "Iteration 11086: with minibatch training loss = 0.386 and accuracy of 0.91\n",
      "Iteration 11087: with minibatch training loss = 0.537 and accuracy of 0.83\n",
      "Iteration 11088: with minibatch training loss = 0.785 and accuracy of 0.77\n",
      "Iteration 11089: with minibatch training loss = 0.803 and accuracy of 0.75\n",
      "Iteration 11090: with minibatch training loss = 0.804 and accuracy of 0.73\n",
      "Iteration 11091: with minibatch training loss = 0.983 and accuracy of 0.7\n",
      "Iteration 11092: with minibatch training loss = 0.594 and accuracy of 0.83\n",
      "Iteration 11093: with minibatch training loss = 0.85 and accuracy of 0.77\n",
      "Iteration 11094: with minibatch training loss = 0.859 and accuracy of 0.77\n",
      "Iteration 11095: with minibatch training loss = 0.436 and accuracy of 0.88\n",
      "Iteration 11096: with minibatch training loss = 0.77 and accuracy of 0.77\n",
      "Iteration 11097: with minibatch training loss = 0.719 and accuracy of 0.8\n",
      "Iteration 11098: with minibatch training loss = 0.846 and accuracy of 0.78\n",
      "Iteration 11099: with minibatch training loss = 0.874 and accuracy of 0.72\n",
      "Iteration 11100: with minibatch training loss = 1.03 and accuracy of 0.7\n",
      "Iteration 11101: with minibatch training loss = 0.695 and accuracy of 0.83\n",
      "Iteration 11102: with minibatch training loss = 0.847 and accuracy of 0.72\n",
      "Validation loss: 0.19859605\n",
      "Model's weights saved at /Users/nhat/Documents/Projects/LetterClassifier/weights/model_se.ckpt\n",
      "Epoch 8, Overall loss = 0.768 and accuracy of 0.776\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwMys5lwIzMxqzoXAzKzmfgFhkyvDy7AaogAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12107a860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11104: with minibatch training loss = 0.863 and accuracy of 0.72\n",
      "Iteration 11105: with minibatch training loss = 0.69 and accuracy of 0.81\n",
      "Iteration 11106: with minibatch training loss = 0.757 and accuracy of 0.8\n",
      "Iteration 11107: with minibatch training loss = 0.845 and accuracy of 0.75\n",
      "Iteration 11108: with minibatch training loss = 0.493 and accuracy of 0.84\n",
      "Iteration 11109: with minibatch training loss = 0.589 and accuracy of 0.84\n",
      "Iteration 11110: with minibatch training loss = 0.652 and accuracy of 0.81\n",
      "Iteration 11111: with minibatch training loss = 0.661 and accuracy of 0.78\n",
      "Iteration 11112: with minibatch training loss = 0.762 and accuracy of 0.75\n",
      "Iteration 11113: with minibatch training loss = 0.583 and accuracy of 0.84\n",
      "Iteration 11114: with minibatch training loss = 0.987 and accuracy of 0.7\n",
      "Iteration 11115: with minibatch training loss = 0.476 and accuracy of 0.89\n",
      "Iteration 11116: with minibatch training loss = 0.749 and accuracy of 0.77\n",
      "Iteration 11117: with minibatch training loss = 0.79 and accuracy of 0.77\n",
      "Iteration 11118: with minibatch training loss = 0.777 and accuracy of 0.78\n",
      "Iteration 11119: with minibatch training loss = 0.633 and accuracy of 0.83\n",
      "Iteration 11120: with minibatch training loss = 1.11 and accuracy of 0.67\n",
      "Iteration 11121: with minibatch training loss = 0.782 and accuracy of 0.77\n",
      "Iteration 11122: with minibatch training loss = 0.449 and accuracy of 0.88\n",
      "Iteration 11123: with minibatch training loss = 0.391 and accuracy of 0.89\n",
      "Iteration 11124: with minibatch training loss = 0.51 and accuracy of 0.84\n",
      "Iteration 11125: with minibatch training loss = 0.812 and accuracy of 0.75\n",
      "Iteration 11126: with minibatch training loss = 0.591 and accuracy of 0.83\n",
      "Iteration 11127: with minibatch training loss = 0.664 and accuracy of 0.78\n",
      "Iteration 11128: with minibatch training loss = 0.681 and accuracy of 0.78\n",
      "Iteration 11129: with minibatch training loss = 0.601 and accuracy of 0.83\n",
      "Iteration 11130: with minibatch training loss = 0.638 and accuracy of 0.83\n",
      "Iteration 11131: with minibatch training loss = 0.755 and accuracy of 0.77\n",
      "Iteration 11132: with minibatch training loss = 0.585 and accuracy of 0.83\n",
      "Iteration 11133: with minibatch training loss = 0.745 and accuracy of 0.77\n",
      "Iteration 11134: with minibatch training loss = 0.865 and accuracy of 0.78\n",
      "Iteration 11135: with minibatch training loss = 0.811 and accuracy of 0.77\n",
      "Iteration 11136: with minibatch training loss = 0.687 and accuracy of 0.8\n",
      "Iteration 11137: with minibatch training loss = 0.718 and accuracy of 0.78\n",
      "Iteration 11138: with minibatch training loss = 0.67 and accuracy of 0.83\n",
      "Iteration 11139: with minibatch training loss = 0.775 and accuracy of 0.75\n",
      "Iteration 11140: with minibatch training loss = 1.02 and accuracy of 0.7\n",
      "Iteration 11141: with minibatch training loss = 0.633 and accuracy of 0.83\n",
      "Iteration 11142: with minibatch training loss = 0.759 and accuracy of 0.78\n",
      "Iteration 11143: with minibatch training loss = 0.575 and accuracy of 0.81\n",
      "Iteration 11144: with minibatch training loss = 0.813 and accuracy of 0.78\n",
      "Iteration 11145: with minibatch training loss = 0.76 and accuracy of 0.8\n",
      "Iteration 11146: with minibatch training loss = 0.712 and accuracy of 0.78\n",
      "Iteration 11147: with minibatch training loss = 1 and accuracy of 0.7\n",
      "Iteration 11148: with minibatch training loss = 0.699 and accuracy of 0.78\n",
      "Iteration 11149: with minibatch training loss = 0.609 and accuracy of 0.84\n",
      "Iteration 11150: with minibatch training loss = 0.514 and accuracy of 0.86\n",
      "Iteration 11151: with minibatch training loss = 0.893 and accuracy of 0.72\n",
      "Iteration 11152: with minibatch training loss = 0.721 and accuracy of 0.78\n",
      "Iteration 11153: with minibatch training loss = 0.587 and accuracy of 0.83\n",
      "Iteration 11154: with minibatch training loss = 0.746 and accuracy of 0.8\n",
      "Iteration 11155: with minibatch training loss = 1.09 and accuracy of 0.67\n",
      "Iteration 11156: with minibatch training loss = 0.868 and accuracy of 0.72\n",
      "Iteration 11157: with minibatch training loss = 0.641 and accuracy of 0.8\n",
      "Iteration 11158: with minibatch training loss = 0.77 and accuracy of 0.77\n",
      "Iteration 11159: with minibatch training loss = 0.871 and accuracy of 0.73\n",
      "Iteration 11160: with minibatch training loss = 0.689 and accuracy of 0.81\n",
      "Iteration 11161: with minibatch training loss = 0.746 and accuracy of 0.78\n",
      "Iteration 11162: with minibatch training loss = 0.649 and accuracy of 0.84\n",
      "Iteration 11163: with minibatch training loss = 0.675 and accuracy of 0.8\n",
      "Iteration 11164: with minibatch training loss = 1.04 and accuracy of 0.69\n",
      "Iteration 11165: with minibatch training loss = 0.802 and accuracy of 0.75\n",
      "Iteration 11166: with minibatch training loss = 0.723 and accuracy of 0.77\n",
      "Iteration 11167: with minibatch training loss = 0.604 and accuracy of 0.83\n",
      "Iteration 11168: with minibatch training loss = 0.65 and accuracy of 0.78\n",
      "Iteration 11169: with minibatch training loss = 0.764 and accuracy of 0.8\n",
      "Iteration 11170: with minibatch training loss = 0.94 and accuracy of 0.7\n",
      "Iteration 11171: with minibatch training loss = 0.675 and accuracy of 0.8\n",
      "Iteration 11172: with minibatch training loss = 0.687 and accuracy of 0.8\n",
      "Iteration 11173: with minibatch training loss = 0.777 and accuracy of 0.77\n",
      "Iteration 11174: with minibatch training loss = 0.636 and accuracy of 0.8\n",
      "Iteration 11175: with minibatch training loss = 0.817 and accuracy of 0.75\n",
      "Iteration 11176: with minibatch training loss = 0.665 and accuracy of 0.83\n",
      "Iteration 11177: with minibatch training loss = 0.811 and accuracy of 0.75\n",
      "Iteration 11178: with minibatch training loss = 0.756 and accuracy of 0.75\n",
      "Iteration 11179: with minibatch training loss = 0.607 and accuracy of 0.81\n",
      "Iteration 11180: with minibatch training loss = 0.606 and accuracy of 0.83\n",
      "Iteration 11181: with minibatch training loss = 0.726 and accuracy of 0.81\n",
      "Iteration 11182: with minibatch training loss = 0.848 and accuracy of 0.77\n",
      "Iteration 11183: with minibatch training loss = 0.807 and accuracy of 0.75\n",
      "Iteration 11184: with minibatch training loss = 0.764 and accuracy of 0.78\n",
      "Iteration 11185: with minibatch training loss = 0.677 and accuracy of 0.8\n",
      "Iteration 11186: with minibatch training loss = 0.729 and accuracy of 0.78\n",
      "Iteration 11187: with minibatch training loss = 0.797 and accuracy of 0.72\n",
      "Iteration 11188: with minibatch training loss = 0.575 and accuracy of 0.83\n",
      "Iteration 11189: with minibatch training loss = 0.651 and accuracy of 0.8\n",
      "Iteration 11190: with minibatch training loss = 0.731 and accuracy of 0.78\n",
      "Iteration 11191: with minibatch training loss = 0.692 and accuracy of 0.83\n",
      "Iteration 11192: with minibatch training loss = 0.62 and accuracy of 0.81\n",
      "Iteration 11193: with minibatch training loss = 0.724 and accuracy of 0.78\n",
      "Iteration 11194: with minibatch training loss = 0.546 and accuracy of 0.84\n",
      "Iteration 11195: with minibatch training loss = 0.923 and accuracy of 0.7\n",
      "Iteration 11196: with minibatch training loss = 0.577 and accuracy of 0.84\n",
      "Iteration 11197: with minibatch training loss = 0.836 and accuracy of 0.75\n",
      "Iteration 11198: with minibatch training loss = 0.908 and accuracy of 0.73\n",
      "Iteration 11199: with minibatch training loss = 0.676 and accuracy of 0.8\n",
      "Iteration 11200: with minibatch training loss = 0.867 and accuracy of 0.73\n",
      "Iteration 11201: with minibatch training loss = 0.361 and accuracy of 0.91\n",
      "Iteration 11202: with minibatch training loss = 0.739 and accuracy of 0.78\n",
      "Iteration 11203: with minibatch training loss = 0.501 and accuracy of 0.88\n",
      "Iteration 11204: with minibatch training loss = 0.706 and accuracy of 0.77\n",
      "Iteration 11205: with minibatch training loss = 0.762 and accuracy of 0.78\n",
      "Iteration 11206: with minibatch training loss = 0.716 and accuracy of 0.8\n",
      "Iteration 11207: with minibatch training loss = 0.641 and accuracy of 0.81\n",
      "Iteration 11208: with minibatch training loss = 0.902 and accuracy of 0.75\n",
      "Iteration 11209: with minibatch training loss = 0.905 and accuracy of 0.73\n",
      "Iteration 11210: with minibatch training loss = 0.796 and accuracy of 0.75\n",
      "Iteration 11211: with minibatch training loss = 0.626 and accuracy of 0.78\n",
      "Iteration 11212: with minibatch training loss = 0.642 and accuracy of 0.83\n",
      "Iteration 11213: with minibatch training loss = 0.561 and accuracy of 0.84\n",
      "Iteration 11214: with minibatch training loss = 0.749 and accuracy of 0.77\n",
      "Iteration 11215: with minibatch training loss = 0.739 and accuracy of 0.78\n",
      "Iteration 11216: with minibatch training loss = 0.865 and accuracy of 0.75\n",
      "Iteration 11217: with minibatch training loss = 0.8 and accuracy of 0.77\n",
      "Iteration 11218: with minibatch training loss = 0.77 and accuracy of 0.78\n",
      "Iteration 11219: with minibatch training loss = 0.656 and accuracy of 0.8\n",
      "Iteration 11220: with minibatch training loss = 0.966 and accuracy of 0.69\n",
      "Iteration 11221: with minibatch training loss = 1.06 and accuracy of 0.72\n",
      "Iteration 11222: with minibatch training loss = 0.81 and accuracy of 0.75\n",
      "Iteration 11223: with minibatch training loss = 0.668 and accuracy of 0.8\n",
      "Iteration 11224: with minibatch training loss = 0.846 and accuracy of 0.75\n",
      "Iteration 11225: with minibatch training loss = 0.667 and accuracy of 0.81\n",
      "Iteration 11226: with minibatch training loss = 0.753 and accuracy of 0.75\n",
      "Iteration 11227: with minibatch training loss = 0.557 and accuracy of 0.83\n",
      "Iteration 11228: with minibatch training loss = 0.733 and accuracy of 0.8\n",
      "Iteration 11229: with minibatch training loss = 0.557 and accuracy of 0.83\n",
      "Iteration 11230: with minibatch training loss = 0.576 and accuracy of 0.84\n",
      "Iteration 11231: with minibatch training loss = 0.696 and accuracy of 0.8\n",
      "Iteration 11232: with minibatch training loss = 0.469 and accuracy of 0.88\n",
      "Iteration 11233: with minibatch training loss = 0.683 and accuracy of 0.81\n",
      "Iteration 11234: with minibatch training loss = 0.626 and accuracy of 0.81\n",
      "Iteration 11235: with minibatch training loss = 0.638 and accuracy of 0.8\n",
      "Iteration 11236: with minibatch training loss = 0.559 and accuracy of 0.84\n",
      "Iteration 11237: with minibatch training loss = 0.792 and accuracy of 0.8\n",
      "Iteration 11238: with minibatch training loss = 0.751 and accuracy of 0.78\n",
      "Iteration 11239: with minibatch training loss = 0.473 and accuracy of 0.86\n",
      "Iteration 11240: with minibatch training loss = 0.835 and accuracy of 0.73\n",
      "Iteration 11241: with minibatch training loss = 0.746 and accuracy of 0.77\n",
      "Iteration 11242: with minibatch training loss = 0.868 and accuracy of 0.73\n",
      "Iteration 11243: with minibatch training loss = 0.689 and accuracy of 0.77\n",
      "Iteration 11244: with minibatch training loss = 0.58 and accuracy of 0.83\n",
      "Iteration 11245: with minibatch training loss = 0.613 and accuracy of 0.8\n",
      "Iteration 11246: with minibatch training loss = 0.802 and accuracy of 0.8\n",
      "Iteration 11247: with minibatch training loss = 0.63 and accuracy of 0.8\n",
      "Iteration 11248: with minibatch training loss = 1.07 and accuracy of 0.67\n",
      "Iteration 11249: with minibatch training loss = 1.01 and accuracy of 0.67\n",
      "Iteration 11250: with minibatch training loss = 0.744 and accuracy of 0.8\n",
      "Iteration 11251: with minibatch training loss = 0.827 and accuracy of 0.75\n",
      "Iteration 11252: with minibatch training loss = 0.82 and accuracy of 0.77\n",
      "Iteration 11253: with minibatch training loss = 0.777 and accuracy of 0.77\n",
      "Iteration 11254: with minibatch training loss = 0.89 and accuracy of 0.75\n",
      "Iteration 11255: with minibatch training loss = 0.672 and accuracy of 0.81\n",
      "Iteration 11256: with minibatch training loss = 0.703 and accuracy of 0.8\n",
      "Iteration 11257: with minibatch training loss = 0.786 and accuracy of 0.73\n",
      "Iteration 11258: with minibatch training loss = 0.903 and accuracy of 0.75\n",
      "Iteration 11259: with minibatch training loss = 0.637 and accuracy of 0.8\n",
      "Iteration 11260: with minibatch training loss = 0.597 and accuracy of 0.81\n",
      "Iteration 11261: with minibatch training loss = 0.77 and accuracy of 0.77\n",
      "Iteration 11262: with minibatch training loss = 0.686 and accuracy of 0.8\n",
      "Iteration 11263: with minibatch training loss = 0.545 and accuracy of 0.84\n",
      "Iteration 11264: with minibatch training loss = 0.809 and accuracy of 0.73\n",
      "Iteration 11265: with minibatch training loss = 0.667 and accuracy of 0.81\n",
      "Iteration 11266: with minibatch training loss = 0.804 and accuracy of 0.78\n",
      "Iteration 11267: with minibatch training loss = 0.507 and accuracy of 0.86\n",
      "Iteration 11268: with minibatch training loss = 0.467 and accuracy of 0.86\n",
      "Iteration 11269: with minibatch training loss = 0.642 and accuracy of 0.8\n",
      "Iteration 11270: with minibatch training loss = 0.535 and accuracy of 0.86\n",
      "Iteration 11271: with minibatch training loss = 0.606 and accuracy of 0.81\n",
      "Iteration 11272: with minibatch training loss = 0.402 and accuracy of 0.88\n",
      "Iteration 11273: with minibatch training loss = 0.709 and accuracy of 0.81\n",
      "Iteration 11274: with minibatch training loss = 0.844 and accuracy of 0.77\n",
      "Iteration 11275: with minibatch training loss = 0.57 and accuracy of 0.84\n",
      "Iteration 11276: with minibatch training loss = 0.483 and accuracy of 0.89\n",
      "Iteration 11277: with minibatch training loss = 0.691 and accuracy of 0.8\n",
      "Iteration 11278: with minibatch training loss = 0.675 and accuracy of 0.78\n",
      "Iteration 11279: with minibatch training loss = 1.1 and accuracy of 0.66\n",
      "Iteration 11280: with minibatch training loss = 0.768 and accuracy of 0.77\n",
      "Iteration 11281: with minibatch training loss = 0.642 and accuracy of 0.8\n",
      "Iteration 11282: with minibatch training loss = 0.563 and accuracy of 0.86\n",
      "Iteration 11283: with minibatch training loss = 1.02 and accuracy of 0.75\n",
      "Iteration 11284: with minibatch training loss = 0.775 and accuracy of 0.78\n",
      "Iteration 11285: with minibatch training loss = 0.834 and accuracy of 0.78\n",
      "Iteration 11286: with minibatch training loss = 0.426 and accuracy of 0.88\n",
      "Iteration 11287: with minibatch training loss = 0.913 and accuracy of 0.75\n",
      "Iteration 11288: with minibatch training loss = 0.67 and accuracy of 0.8\n",
      "Iteration 11289: with minibatch training loss = 0.648 and accuracy of 0.83\n",
      "Iteration 11290: with minibatch training loss = 0.686 and accuracy of 0.81\n",
      "Iteration 11291: with minibatch training loss = 0.692 and accuracy of 0.81\n",
      "Iteration 11292: with minibatch training loss = 0.914 and accuracy of 0.75\n",
      "Iteration 11293: with minibatch training loss = 0.655 and accuracy of 0.83\n",
      "Iteration 11294: with minibatch training loss = 0.775 and accuracy of 0.8\n",
      "Iteration 11295: with minibatch training loss = 0.662 and accuracy of 0.8\n",
      "Iteration 11296: with minibatch training loss = 0.955 and accuracy of 0.73\n",
      "Iteration 11297: with minibatch training loss = 0.649 and accuracy of 0.83\n",
      "Iteration 11298: with minibatch training loss = 0.738 and accuracy of 0.78\n",
      "Iteration 11299: with minibatch training loss = 0.63 and accuracy of 0.86\n",
      "Iteration 11300: with minibatch training loss = 0.635 and accuracy of 0.8\n",
      "Iteration 11301: with minibatch training loss = 0.738 and accuracy of 0.78\n",
      "Iteration 11302: with minibatch training loss = 0.658 and accuracy of 0.81\n",
      "Iteration 11303: with minibatch training loss = 0.432 and accuracy of 0.89\n",
      "Iteration 11304: with minibatch training loss = 0.47 and accuracy of 0.86\n",
      "Iteration 11305: with minibatch training loss = 0.822 and accuracy of 0.77\n",
      "Iteration 11306: with minibatch training loss = 0.545 and accuracy of 0.86\n",
      "Iteration 11307: with minibatch training loss = 0.722 and accuracy of 0.78\n",
      "Iteration 11308: with minibatch training loss = 0.654 and accuracy of 0.78\n",
      "Iteration 11309: with minibatch training loss = 0.928 and accuracy of 0.7\n",
      "Iteration 11310: with minibatch training loss = 0.741 and accuracy of 0.8\n",
      "Iteration 11311: with minibatch training loss = 0.465 and accuracy of 0.89\n",
      "Iteration 11312: with minibatch training loss = 0.767 and accuracy of 0.78\n",
      "Iteration 11313: with minibatch training loss = 0.893 and accuracy of 0.73\n",
      "Iteration 11314: with minibatch training loss = 0.815 and accuracy of 0.73\n",
      "Iteration 11315: with minibatch training loss = 0.882 and accuracy of 0.73\n",
      "Iteration 11316: with minibatch training loss = 0.85 and accuracy of 0.78\n",
      "Iteration 11317: with minibatch training loss = 0.83 and accuracy of 0.7\n",
      "Iteration 11318: with minibatch training loss = 0.637 and accuracy of 0.81\n",
      "Iteration 11319: with minibatch training loss = 0.747 and accuracy of 0.78\n",
      "Iteration 11320: with minibatch training loss = 0.808 and accuracy of 0.81\n",
      "Iteration 11321: with minibatch training loss = 0.59 and accuracy of 0.81\n",
      "Iteration 11322: with minibatch training loss = 0.53 and accuracy of 0.86\n",
      "Iteration 11323: with minibatch training loss = 0.82 and accuracy of 0.77\n",
      "Iteration 11324: with minibatch training loss = 0.775 and accuracy of 0.8\n",
      "Iteration 11325: with minibatch training loss = 0.73 and accuracy of 0.83\n",
      "Iteration 11326: with minibatch training loss = 0.606 and accuracy of 0.86\n",
      "Iteration 11327: with minibatch training loss = 0.563 and accuracy of 0.83\n",
      "Iteration 11328: with minibatch training loss = 0.561 and accuracy of 0.86\n",
      "Iteration 11329: with minibatch training loss = 0.578 and accuracy of 0.8\n",
      "Iteration 11330: with minibatch training loss = 0.391 and accuracy of 0.88\n",
      "Iteration 11331: with minibatch training loss = 0.882 and accuracy of 0.73\n",
      "Iteration 11332: with minibatch training loss = 0.717 and accuracy of 0.8\n",
      "Iteration 11333: with minibatch training loss = 0.783 and accuracy of 0.77\n",
      "Iteration 11334: with minibatch training loss = 0.593 and accuracy of 0.83\n",
      "Iteration 11335: with minibatch training loss = 0.949 and accuracy of 0.7\n",
      "Iteration 11336: with minibatch training loss = 0.704 and accuracy of 0.78\n",
      "Iteration 11337: with minibatch training loss = 0.74 and accuracy of 0.8\n",
      "Iteration 11338: with minibatch training loss = 0.818 and accuracy of 0.78\n",
      "Iteration 11339: with minibatch training loss = 0.68 and accuracy of 0.8\n",
      "Iteration 11340: with minibatch training loss = 0.731 and accuracy of 0.78\n",
      "Iteration 11341: with minibatch training loss = 0.78 and accuracy of 0.77\n",
      "Iteration 11342: with minibatch training loss = 0.884 and accuracy of 0.72\n",
      "Iteration 11343: with minibatch training loss = 0.907 and accuracy of 0.7\n",
      "Iteration 11344: with minibatch training loss = 0.649 and accuracy of 0.77\n",
      "Iteration 11345: with minibatch training loss = 0.789 and accuracy of 0.75\n",
      "Iteration 11346: with minibatch training loss = 0.624 and accuracy of 0.83\n",
      "Iteration 11347: with minibatch training loss = 0.529 and accuracy of 0.88\n",
      "Iteration 11348: with minibatch training loss = 0.69 and accuracy of 0.8\n",
      "Iteration 11349: with minibatch training loss = 0.771 and accuracy of 0.78\n",
      "Iteration 11350: with minibatch training loss = 0.686 and accuracy of 0.81\n",
      "Iteration 11351: with minibatch training loss = 0.649 and accuracy of 0.83\n",
      "Iteration 11352: with minibatch training loss = 0.709 and accuracy of 0.78\n",
      "Iteration 11353: with minibatch training loss = 0.616 and accuracy of 0.81\n",
      "Iteration 11354: with minibatch training loss = 0.614 and accuracy of 0.84\n",
      "Iteration 11355: with minibatch training loss = 0.452 and accuracy of 0.88\n",
      "Iteration 11356: with minibatch training loss = 0.624 and accuracy of 0.78\n",
      "Iteration 11357: with minibatch training loss = 0.822 and accuracy of 0.75\n",
      "Iteration 11358: with minibatch training loss = 0.689 and accuracy of 0.78\n",
      "Iteration 11359: with minibatch training loss = 0.904 and accuracy of 0.73\n",
      "Iteration 11360: with minibatch training loss = 0.685 and accuracy of 0.78\n",
      "Iteration 11361: with minibatch training loss = 0.839 and accuracy of 0.78\n",
      "Iteration 11362: with minibatch training loss = 0.578 and accuracy of 0.83\n",
      "Iteration 11363: with minibatch training loss = 0.668 and accuracy of 0.81\n",
      "Iteration 11364: with minibatch training loss = 0.497 and accuracy of 0.86\n",
      "Iteration 11365: with minibatch training loss = 0.958 and accuracy of 0.73\n",
      "Iteration 11366: with minibatch training loss = 0.808 and accuracy of 0.75\n",
      "Iteration 11367: with minibatch training loss = 0.636 and accuracy of 0.81\n",
      "Iteration 11368: with minibatch training loss = 0.62 and accuracy of 0.81\n",
      "Iteration 11369: with minibatch training loss = 0.713 and accuracy of 0.77\n",
      "Iteration 11370: with minibatch training loss = 0.648 and accuracy of 0.81\n",
      "Iteration 11371: with minibatch training loss = 0.888 and accuracy of 0.75\n",
      "Iteration 11372: with minibatch training loss = 0.45 and accuracy of 0.84\n",
      "Iteration 11373: with minibatch training loss = 0.765 and accuracy of 0.8\n",
      "Iteration 11374: with minibatch training loss = 0.558 and accuracy of 0.83\n",
      "Iteration 11375: with minibatch training loss = 0.581 and accuracy of 0.81\n",
      "Iteration 11376: with minibatch training loss = 0.689 and accuracy of 0.81\n",
      "Iteration 11377: with minibatch training loss = 0.635 and accuracy of 0.8\n",
      "Iteration 11378: with minibatch training loss = 0.514 and accuracy of 0.84\n",
      "Iteration 11379: with minibatch training loss = 0.524 and accuracy of 0.84\n",
      "Iteration 11380: with minibatch training loss = 0.733 and accuracy of 0.75\n",
      "Iteration 11381: with minibatch training loss = 0.747 and accuracy of 0.77\n",
      "Iteration 11382: with minibatch training loss = 0.8 and accuracy of 0.77\n",
      "Iteration 11383: with minibatch training loss = 0.401 and accuracy of 0.89\n",
      "Iteration 11384: with minibatch training loss = 0.762 and accuracy of 0.77\n",
      "Iteration 11385: with minibatch training loss = 0.739 and accuracy of 0.81\n",
      "Iteration 11386: with minibatch training loss = 0.691 and accuracy of 0.78\n",
      "Iteration 11387: with minibatch training loss = 0.805 and accuracy of 0.75\n",
      "Iteration 11388: with minibatch training loss = 0.89 and accuracy of 0.75\n",
      "Iteration 11389: with minibatch training loss = 0.52 and accuracy of 0.86\n",
      "Iteration 11390: with minibatch training loss = 0.847 and accuracy of 0.75\n",
      "Iteration 11391: with minibatch training loss = 0.56 and accuracy of 0.84\n",
      "Iteration 11392: with minibatch training loss = 0.737 and accuracy of 0.78\n",
      "Iteration 11393: with minibatch training loss = 0.741 and accuracy of 0.77\n",
      "Iteration 11394: with minibatch training loss = 0.883 and accuracy of 0.72\n",
      "Iteration 11395: with minibatch training loss = 0.758 and accuracy of 0.78\n",
      "Iteration 11396: with minibatch training loss = 0.671 and accuracy of 0.81\n",
      "Iteration 11397: with minibatch training loss = 0.921 and accuracy of 0.75\n",
      "Iteration 11398: with minibatch training loss = 0.858 and accuracy of 0.8\n",
      "Iteration 11399: with minibatch training loss = 0.806 and accuracy of 0.75\n",
      "Iteration 11400: with minibatch training loss = 0.945 and accuracy of 0.72\n",
      "Iteration 11401: with minibatch training loss = 0.819 and accuracy of 0.73\n",
      "Iteration 11402: with minibatch training loss = 0.692 and accuracy of 0.81\n",
      "Iteration 11403: with minibatch training loss = 0.904 and accuracy of 0.75\n",
      "Iteration 11404: with minibatch training loss = 0.673 and accuracy of 0.81\n",
      "Iteration 11405: with minibatch training loss = 0.825 and accuracy of 0.77\n",
      "Iteration 11406: with minibatch training loss = 0.897 and accuracy of 0.69\n",
      "Iteration 11407: with minibatch training loss = 0.664 and accuracy of 0.78\n",
      "Iteration 11408: with minibatch training loss = 0.755 and accuracy of 0.78\n",
      "Iteration 11409: with minibatch training loss = 0.801 and accuracy of 0.77\n",
      "Iteration 11410: with minibatch training loss = 0.533 and accuracy of 0.84\n",
      "Iteration 11411: with minibatch training loss = 0.792 and accuracy of 0.75\n",
      "Iteration 11412: with minibatch training loss = 0.668 and accuracy of 0.81\n",
      "Iteration 11413: with minibatch training loss = 0.647 and accuracy of 0.77\n",
      "Iteration 11414: with minibatch training loss = 0.725 and accuracy of 0.77\n",
      "Iteration 11415: with minibatch training loss = 0.914 and accuracy of 0.67\n",
      "Iteration 11416: with minibatch training loss = 0.451 and accuracy of 0.84\n",
      "Iteration 11417: with minibatch training loss = 0.697 and accuracy of 0.8\n",
      "Iteration 11418: with minibatch training loss = 0.617 and accuracy of 0.81\n",
      "Iteration 11419: with minibatch training loss = 0.623 and accuracy of 0.83\n",
      "Iteration 11420: with minibatch training loss = 0.665 and accuracy of 0.78\n",
      "Iteration 11421: with minibatch training loss = 0.777 and accuracy of 0.78\n",
      "Iteration 11422: with minibatch training loss = 1.02 and accuracy of 0.7\n",
      "Iteration 11423: with minibatch training loss = 0.773 and accuracy of 0.75\n",
      "Iteration 11424: with minibatch training loss = 1.07 and accuracy of 0.67\n",
      "Iteration 11425: with minibatch training loss = 0.65 and accuracy of 0.81\n",
      "Iteration 11426: with minibatch training loss = 0.761 and accuracy of 0.77\n",
      "Iteration 11427: with minibatch training loss = 0.514 and accuracy of 0.84\n",
      "Iteration 11428: with minibatch training loss = 0.665 and accuracy of 0.8\n",
      "Iteration 11429: with minibatch training loss = 0.736 and accuracy of 0.81\n",
      "Iteration 11430: with minibatch training loss = 0.762 and accuracy of 0.77\n",
      "Iteration 11431: with minibatch training loss = 0.631 and accuracy of 0.81\n",
      "Iteration 11432: with minibatch training loss = 0.69 and accuracy of 0.78\n",
      "Iteration 11433: with minibatch training loss = 0.663 and accuracy of 0.8\n",
      "Iteration 11434: with minibatch training loss = 0.676 and accuracy of 0.83\n",
      "Iteration 11435: with minibatch training loss = 0.682 and accuracy of 0.8\n",
      "Iteration 11436: with minibatch training loss = 0.657 and accuracy of 0.83\n",
      "Iteration 11437: with minibatch training loss = 0.747 and accuracy of 0.81\n",
      "Iteration 11438: with minibatch training loss = 0.697 and accuracy of 0.8\n",
      "Iteration 11439: with minibatch training loss = 0.748 and accuracy of 0.77\n",
      "Iteration 11440: with minibatch training loss = 0.614 and accuracy of 0.84\n",
      "Iteration 11441: with minibatch training loss = 0.627 and accuracy of 0.81\n",
      "Iteration 11442: with minibatch training loss = 0.978 and accuracy of 0.69\n",
      "Iteration 11443: with minibatch training loss = 0.564 and accuracy of 0.84\n",
      "Iteration 11444: with minibatch training loss = 0.809 and accuracy of 0.78\n",
      "Iteration 11445: with minibatch training loss = 0.629 and accuracy of 0.81\n",
      "Iteration 11446: with minibatch training loss = 0.47 and accuracy of 0.86\n",
      "Iteration 11447: with minibatch training loss = 0.682 and accuracy of 0.81\n",
      "Iteration 11448: with minibatch training loss = 0.681 and accuracy of 0.8\n",
      "Iteration 11449: with minibatch training loss = 0.813 and accuracy of 0.77\n",
      "Iteration 11450: with minibatch training loss = 0.664 and accuracy of 0.8\n",
      "Iteration 11451: with minibatch training loss = 0.607 and accuracy of 0.83\n",
      "Iteration 11452: with minibatch training loss = 0.759 and accuracy of 0.77\n",
      "Iteration 11453: with minibatch training loss = 0.689 and accuracy of 0.8\n",
      "Iteration 11454: with minibatch training loss = 0.755 and accuracy of 0.78\n",
      "Iteration 11455: with minibatch training loss = 0.808 and accuracy of 0.78\n",
      "Iteration 11456: with minibatch training loss = 0.554 and accuracy of 0.83\n",
      "Iteration 11457: with minibatch training loss = 0.567 and accuracy of 0.84\n",
      "Iteration 11458: with minibatch training loss = 0.767 and accuracy of 0.78\n",
      "Iteration 11459: with minibatch training loss = 0.877 and accuracy of 0.72\n",
      "Iteration 11460: with minibatch training loss = 0.565 and accuracy of 0.83\n",
      "Iteration 11461: with minibatch training loss = 1.01 and accuracy of 0.7\n",
      "Iteration 11462: with minibatch training loss = 0.583 and accuracy of 0.84\n",
      "Iteration 11463: with minibatch training loss = 0.646 and accuracy of 0.81\n",
      "Iteration 11464: with minibatch training loss = 1.12 and accuracy of 0.69\n",
      "Iteration 11465: with minibatch training loss = 0.748 and accuracy of 0.8\n",
      "Iteration 11466: with minibatch training loss = 0.562 and accuracy of 0.84\n",
      "Iteration 11467: with minibatch training loss = 0.725 and accuracy of 0.81\n",
      "Iteration 11468: with minibatch training loss = 0.627 and accuracy of 0.83\n",
      "Iteration 11469: with minibatch training loss = 0.933 and accuracy of 0.75\n",
      "Iteration 11470: with minibatch training loss = 0.981 and accuracy of 0.69\n",
      "Iteration 11471: with minibatch training loss = 0.818 and accuracy of 0.78\n",
      "Iteration 11472: with minibatch training loss = 0.593 and accuracy of 0.83\n",
      "Iteration 11473: with minibatch training loss = 0.478 and accuracy of 0.86\n",
      "Iteration 11474: with minibatch training loss = 0.71 and accuracy of 0.8\n",
      "Iteration 11475: with minibatch training loss = 0.865 and accuracy of 0.73\n",
      "Iteration 11476: with minibatch training loss = 0.65 and accuracy of 0.81\n",
      "Iteration 11477: with minibatch training loss = 0.572 and accuracy of 0.83\n",
      "Iteration 11478: with minibatch training loss = 0.77 and accuracy of 0.8\n",
      "Iteration 11479: with minibatch training loss = 0.648 and accuracy of 0.81\n",
      "Iteration 11480: with minibatch training loss = 0.645 and accuracy of 0.78\n",
      "Iteration 11481: with minibatch training loss = 0.796 and accuracy of 0.77\n",
      "Iteration 11482: with minibatch training loss = 0.521 and accuracy of 0.86\n",
      "Iteration 11483: with minibatch training loss = 0.467 and accuracy of 0.84\n",
      "Iteration 11484: with minibatch training loss = 0.543 and accuracy of 0.86\n",
      "Iteration 11485: with minibatch training loss = 0.952 and accuracy of 0.72\n",
      "Iteration 11486: with minibatch training loss = 0.701 and accuracy of 0.83\n",
      "Iteration 11487: with minibatch training loss = 0.558 and accuracy of 0.84\n",
      "Iteration 11488: with minibatch training loss = 0.652 and accuracy of 0.78\n",
      "Iteration 11489: with minibatch training loss = 0.909 and accuracy of 0.72\n",
      "Iteration 11490: with minibatch training loss = 0.632 and accuracy of 0.83\n",
      "Iteration 11491: with minibatch training loss = 0.483 and accuracy of 0.88\n",
      "Iteration 11492: with minibatch training loss = 0.572 and accuracy of 0.83\n",
      "Iteration 11493: with minibatch training loss = 0.657 and accuracy of 0.8\n",
      "Iteration 11494: with minibatch training loss = 0.781 and accuracy of 0.77\n",
      "Iteration 11495: with minibatch training loss = 0.873 and accuracy of 0.78\n",
      "Iteration 11496: with minibatch training loss = 0.857 and accuracy of 0.73\n",
      "Iteration 11497: with minibatch training loss = 0.889 and accuracy of 0.75\n",
      "Iteration 11498: with minibatch training loss = 0.692 and accuracy of 0.83\n",
      "Iteration 11499: with minibatch training loss = 0.781 and accuracy of 0.77\n",
      "Iteration 11500: with minibatch training loss = 0.634 and accuracy of 0.78\n",
      "Iteration 11501: with minibatch training loss = 0.734 and accuracy of 0.8\n",
      "Iteration 11502: with minibatch training loss = 0.576 and accuracy of 0.83\n",
      "Iteration 11503: with minibatch training loss = 0.605 and accuracy of 0.83\n",
      "Iteration 11504: with minibatch training loss = 0.713 and accuracy of 0.78\n",
      "Iteration 11505: with minibatch training loss = 0.866 and accuracy of 0.73\n",
      "Iteration 11506: with minibatch training loss = 0.8 and accuracy of 0.75\n",
      "Iteration 11507: with minibatch training loss = 0.664 and accuracy of 0.78\n",
      "Iteration 11508: with minibatch training loss = 0.863 and accuracy of 0.73\n",
      "Iteration 11509: with minibatch training loss = 0.909 and accuracy of 0.75\n",
      "Iteration 11510: with minibatch training loss = 0.763 and accuracy of 0.77\n",
      "Iteration 11511: with minibatch training loss = 0.886 and accuracy of 0.77\n",
      "Iteration 11512: with minibatch training loss = 0.596 and accuracy of 0.84\n",
      "Iteration 11513: with minibatch training loss = 0.933 and accuracy of 0.77\n",
      "Iteration 11514: with minibatch training loss = 0.66 and accuracy of 0.81\n",
      "Iteration 11515: with minibatch training loss = 1.03 and accuracy of 0.7\n",
      "Iteration 11516: with minibatch training loss = 0.355 and accuracy of 0.89\n",
      "Iteration 11517: with minibatch training loss = 0.496 and accuracy of 0.86\n",
      "Iteration 11518: with minibatch training loss = 0.611 and accuracy of 0.84\n",
      "Iteration 11519: with minibatch training loss = 0.632 and accuracy of 0.83\n",
      "Iteration 11520: with minibatch training loss = 1.02 and accuracy of 0.72\n",
      "Iteration 11521: with minibatch training loss = 0.74 and accuracy of 0.83\n",
      "Iteration 11522: with minibatch training loss = 0.546 and accuracy of 0.84\n",
      "Iteration 11523: with minibatch training loss = 0.692 and accuracy of 0.77\n",
      "Iteration 11524: with minibatch training loss = 0.849 and accuracy of 0.75\n",
      "Iteration 11525: with minibatch training loss = 1.11 and accuracy of 0.64\n",
      "Iteration 11526: with minibatch training loss = 0.639 and accuracy of 0.83\n",
      "Iteration 11527: with minibatch training loss = 0.729 and accuracy of 0.78\n",
      "Iteration 11528: with minibatch training loss = 1.02 and accuracy of 0.72\n",
      "Iteration 11529: with minibatch training loss = 0.672 and accuracy of 0.81\n",
      "Iteration 11530: with minibatch training loss = 1.05 and accuracy of 0.7\n",
      "Iteration 11531: with minibatch training loss = 0.55 and accuracy of 0.81\n",
      "Iteration 11532: with minibatch training loss = 0.68 and accuracy of 0.78\n",
      "Iteration 11533: with minibatch training loss = 0.767 and accuracy of 0.75\n",
      "Iteration 11534: with minibatch training loss = 0.942 and accuracy of 0.72\n",
      "Iteration 11535: with minibatch training loss = 0.599 and accuracy of 0.83\n",
      "Iteration 11536: with minibatch training loss = 0.487 and accuracy of 0.86\n",
      "Iteration 11537: with minibatch training loss = 0.758 and accuracy of 0.77\n",
      "Iteration 11538: with minibatch training loss = 0.672 and accuracy of 0.8\n",
      "Iteration 11539: with minibatch training loss = 0.777 and accuracy of 0.78\n",
      "Iteration 11540: with minibatch training loss = 0.858 and accuracy of 0.73\n",
      "Iteration 11541: with minibatch training loss = 0.693 and accuracy of 0.8\n",
      "Iteration 11542: with minibatch training loss = 0.611 and accuracy of 0.83\n",
      "Iteration 11543: with minibatch training loss = 0.593 and accuracy of 0.8\n",
      "Iteration 11544: with minibatch training loss = 0.973 and accuracy of 0.73\n",
      "Iteration 11545: with minibatch training loss = 0.685 and accuracy of 0.8\n",
      "Iteration 11546: with minibatch training loss = 0.746 and accuracy of 0.81\n",
      "Iteration 11547: with minibatch training loss = 0.432 and accuracy of 0.91\n",
      "Iteration 11548: with minibatch training loss = 1.11 and accuracy of 0.7\n",
      "Iteration 11549: with minibatch training loss = 1.01 and accuracy of 0.72\n",
      "Iteration 11550: with minibatch training loss = 0.518 and accuracy of 0.88\n",
      "Iteration 11551: with minibatch training loss = 0.589 and accuracy of 0.84\n",
      "Iteration 11552: with minibatch training loss = 0.703 and accuracy of 0.81\n",
      "Iteration 11553: with minibatch training loss = 0.822 and accuracy of 0.77\n",
      "Iteration 11554: with minibatch training loss = 1.06 and accuracy of 0.67\n",
      "Iteration 11555: with minibatch training loss = 0.572 and accuracy of 0.84\n",
      "Iteration 11556: with minibatch training loss = 0.798 and accuracy of 0.77\n",
      "Iteration 11557: with minibatch training loss = 0.833 and accuracy of 0.73\n",
      "Iteration 11558: with minibatch training loss = 0.881 and accuracy of 0.72\n",
      "Iteration 11559: with minibatch training loss = 0.697 and accuracy of 0.8\n",
      "Iteration 11560: with minibatch training loss = 0.905 and accuracy of 0.7\n",
      "Iteration 11561: with minibatch training loss = 0.414 and accuracy of 0.88\n",
      "Iteration 11562: with minibatch training loss = 1.07 and accuracy of 0.72\n",
      "Iteration 11563: with minibatch training loss = 0.807 and accuracy of 0.72\n",
      "Iteration 11564: with minibatch training loss = 0.708 and accuracy of 0.78\n",
      "Iteration 11565: with minibatch training loss = 0.71 and accuracy of 0.8\n",
      "Iteration 11566: with minibatch training loss = 0.764 and accuracy of 0.78\n",
      "Iteration 11567: with minibatch training loss = 0.864 and accuracy of 0.73\n",
      "Iteration 11568: with minibatch training loss = 0.403 and accuracy of 0.89\n",
      "Iteration 11569: with minibatch training loss = 0.854 and accuracy of 0.75\n",
      "Iteration 11570: with minibatch training loss = 0.668 and accuracy of 0.8\n",
      "Iteration 11571: with minibatch training loss = 0.767 and accuracy of 0.75\n",
      "Iteration 11572: with minibatch training loss = 0.718 and accuracy of 0.78\n",
      "Iteration 11573: with minibatch training loss = 0.714 and accuracy of 0.8\n",
      "Iteration 11574: with minibatch training loss = 0.478 and accuracy of 0.89\n",
      "Iteration 11575: with minibatch training loss = 0.694 and accuracy of 0.8\n",
      "Iteration 11576: with minibatch training loss = 0.813 and accuracy of 0.75\n",
      "Iteration 11577: with minibatch training loss = 0.718 and accuracy of 0.8\n",
      "Iteration 11578: with minibatch training loss = 1.02 and accuracy of 0.7\n",
      "Iteration 11579: with minibatch training loss = 0.736 and accuracy of 0.78\n",
      "Iteration 11580: with minibatch training loss = 0.925 and accuracy of 0.73\n",
      "Iteration 11581: with minibatch training loss = 0.915 and accuracy of 0.72\n",
      "Iteration 11582: with minibatch training loss = 0.754 and accuracy of 0.77\n",
      "Iteration 11583: with minibatch training loss = 0.668 and accuracy of 0.84\n",
      "Iteration 11584: with minibatch training loss = 0.811 and accuracy of 0.77\n",
      "Iteration 11585: with minibatch training loss = 0.769 and accuracy of 0.75\n",
      "Iteration 11586: with minibatch training loss = 0.692 and accuracy of 0.77\n",
      "Iteration 11587: with minibatch training loss = 0.596 and accuracy of 0.81\n",
      "Iteration 11588: with minibatch training loss = 0.977 and accuracy of 0.77\n",
      "Iteration 11589: with minibatch training loss = 0.607 and accuracy of 0.81\n",
      "Iteration 11590: with minibatch training loss = 0.604 and accuracy of 0.84\n",
      "Iteration 11591: with minibatch training loss = 0.688 and accuracy of 0.83\n",
      "Iteration 11592: with minibatch training loss = 0.798 and accuracy of 0.8\n",
      "Iteration 11593: with minibatch training loss = 0.645 and accuracy of 0.81\n",
      "Iteration 11594: with minibatch training loss = 0.645 and accuracy of 0.81\n",
      "Iteration 11595: with minibatch training loss = 0.606 and accuracy of 0.83\n",
      "Iteration 11596: with minibatch training loss = 0.554 and accuracy of 0.86\n",
      "Iteration 11597: with minibatch training loss = 0.772 and accuracy of 0.73\n",
      "Iteration 11598: with minibatch training loss = 0.941 and accuracy of 0.72\n",
      "Iteration 11599: with minibatch training loss = 0.79 and accuracy of 0.73\n",
      "Iteration 11600: with minibatch training loss = 0.682 and accuracy of 0.8\n",
      "Iteration 11601: with minibatch training loss = 0.502 and accuracy of 0.86\n",
      "Iteration 11602: with minibatch training loss = 0.594 and accuracy of 0.8\n",
      "Iteration 11603: with minibatch training loss = 0.664 and accuracy of 0.78\n",
      "Iteration 11604: with minibatch training loss = 1.01 and accuracy of 0.67\n",
      "Iteration 11605: with minibatch training loss = 0.549 and accuracy of 0.86\n",
      "Iteration 11606: with minibatch training loss = 0.807 and accuracy of 0.73\n",
      "Iteration 11607: with minibatch training loss = 0.83 and accuracy of 0.77\n",
      "Iteration 11608: with minibatch training loss = 0.636 and accuracy of 0.81\n",
      "Iteration 11609: with minibatch training loss = 0.768 and accuracy of 0.72\n",
      "Iteration 11610: with minibatch training loss = 0.923 and accuracy of 0.67\n",
      "Iteration 11611: with minibatch training loss = 0.628 and accuracy of 0.81\n",
      "Iteration 11612: with minibatch training loss = 0.957 and accuracy of 0.73\n",
      "Iteration 11613: with minibatch training loss = 0.406 and accuracy of 0.88\n",
      "Iteration 11614: with minibatch training loss = 0.687 and accuracy of 0.8\n",
      "Iteration 11615: with minibatch training loss = 0.488 and accuracy of 0.86\n",
      "Iteration 11616: with minibatch training loss = 0.979 and accuracy of 0.72\n",
      "Iteration 11617: with minibatch training loss = 0.898 and accuracy of 0.72\n",
      "Iteration 11618: with minibatch training loss = 0.643 and accuracy of 0.81\n",
      "Iteration 11619: with minibatch training loss = 0.875 and accuracy of 0.72\n",
      "Iteration 11620: with minibatch training loss = 0.433 and accuracy of 0.89\n",
      "Iteration 11621: with minibatch training loss = 0.745 and accuracy of 0.81\n",
      "Iteration 11622: with minibatch training loss = 0.792 and accuracy of 0.78\n",
      "Iteration 11623: with minibatch training loss = 0.759 and accuracy of 0.78\n",
      "Iteration 11624: with minibatch training loss = 0.894 and accuracy of 0.72\n",
      "Iteration 11625: with minibatch training loss = 0.664 and accuracy of 0.81\n",
      "Iteration 11626: with minibatch training loss = 0.793 and accuracy of 0.77\n",
      "Iteration 11627: with minibatch training loss = 0.749 and accuracy of 0.77\n",
      "Iteration 11628: with minibatch training loss = 0.461 and accuracy of 0.89\n",
      "Iteration 11629: with minibatch training loss = 0.879 and accuracy of 0.73\n",
      "Iteration 11630: with minibatch training loss = 0.765 and accuracy of 0.73\n",
      "Iteration 11631: with minibatch training loss = 0.611 and accuracy of 0.83\n",
      "Iteration 11632: with minibatch training loss = 0.817 and accuracy of 0.78\n",
      "Iteration 11633: with minibatch training loss = 0.752 and accuracy of 0.78\n",
      "Iteration 11634: with minibatch training loss = 0.701 and accuracy of 0.81\n",
      "Iteration 11635: with minibatch training loss = 0.528 and accuracy of 0.86\n",
      "Iteration 11636: with minibatch training loss = 0.57 and accuracy of 0.83\n",
      "Iteration 11637: with minibatch training loss = 0.709 and accuracy of 0.81\n",
      "Iteration 11638: with minibatch training loss = 0.632 and accuracy of 0.78\n",
      "Iteration 11639: with minibatch training loss = 0.752 and accuracy of 0.77\n",
      "Iteration 11640: with minibatch training loss = 0.743 and accuracy of 0.81\n",
      "Iteration 11641: with minibatch training loss = 0.93 and accuracy of 0.75\n",
      "Iteration 11642: with minibatch training loss = 0.745 and accuracy of 0.8\n",
      "Iteration 11643: with minibatch training loss = 0.696 and accuracy of 0.78\n",
      "Iteration 11644: with minibatch training loss = 0.805 and accuracy of 0.77\n",
      "Iteration 11645: with minibatch training loss = 0.58 and accuracy of 0.86\n",
      "Iteration 11646: with minibatch training loss = 0.629 and accuracy of 0.84\n",
      "Iteration 11647: with minibatch training loss = 0.776 and accuracy of 0.77\n",
      "Iteration 11648: with minibatch training loss = 0.817 and accuracy of 0.77\n",
      "Iteration 11649: with minibatch training loss = 0.748 and accuracy of 0.78\n",
      "Iteration 11650: with minibatch training loss = 0.624 and accuracy of 0.8\n",
      "Iteration 11651: with minibatch training loss = 0.858 and accuracy of 0.78\n",
      "Iteration 11652: with minibatch training loss = 0.739 and accuracy of 0.8\n",
      "Iteration 11653: with minibatch training loss = 0.653 and accuracy of 0.81\n",
      "Iteration 11654: with minibatch training loss = 0.846 and accuracy of 0.72\n",
      "Iteration 11655: with minibatch training loss = 0.788 and accuracy of 0.8\n",
      "Iteration 11656: with minibatch training loss = 0.776 and accuracy of 0.78\n",
      "Iteration 11657: with minibatch training loss = 0.509 and accuracy of 0.86\n",
      "Iteration 11658: with minibatch training loss = 0.563 and accuracy of 0.86\n",
      "Iteration 11659: with minibatch training loss = 0.611 and accuracy of 0.83\n",
      "Iteration 11660: with minibatch training loss = 0.584 and accuracy of 0.81\n",
      "Iteration 11661: with minibatch training loss = 0.796 and accuracy of 0.77\n",
      "Iteration 11662: with minibatch training loss = 0.679 and accuracy of 0.8\n",
      "Iteration 11663: with minibatch training loss = 0.6 and accuracy of 0.81\n",
      "Iteration 11664: with minibatch training loss = 1.01 and accuracy of 0.69\n",
      "Iteration 11665: with minibatch training loss = 0.458 and accuracy of 0.88\n",
      "Iteration 11666: with minibatch training loss = 0.834 and accuracy of 0.75\n",
      "Iteration 11667: with minibatch training loss = 0.528 and accuracy of 0.86\n",
      "Iteration 11668: with minibatch training loss = 0.594 and accuracy of 0.83\n",
      "Iteration 11669: with minibatch training loss = 0.852 and accuracy of 0.73\n",
      "Iteration 11670: with minibatch training loss = 0.938 and accuracy of 0.7\n",
      "Iteration 11671: with minibatch training loss = 0.601 and accuracy of 0.81\n",
      "Iteration 11672: with minibatch training loss = 0.762 and accuracy of 0.72\n",
      "Iteration 11673: with minibatch training loss = 0.695 and accuracy of 0.8\n",
      "Iteration 11674: with minibatch training loss = 0.78 and accuracy of 0.75\n",
      "Iteration 11675: with minibatch training loss = 0.74 and accuracy of 0.77\n",
      "Iteration 11676: with minibatch training loss = 0.76 and accuracy of 0.77\n",
      "Iteration 11677: with minibatch training loss = 0.86 and accuracy of 0.77\n",
      "Iteration 11678: with minibatch training loss = 0.737 and accuracy of 0.81\n",
      "Iteration 11679: with minibatch training loss = 0.591 and accuracy of 0.84\n",
      "Iteration 11680: with minibatch training loss = 0.682 and accuracy of 0.77\n",
      "Iteration 11681: with minibatch training loss = 0.854 and accuracy of 0.73\n",
      "Iteration 11682: with minibatch training loss = 0.734 and accuracy of 0.78\n",
      "Iteration 11683: with minibatch training loss = 0.812 and accuracy of 0.72\n",
      "Iteration 11684: with minibatch training loss = 0.706 and accuracy of 0.78\n",
      "Iteration 11685: with minibatch training loss = 0.725 and accuracy of 0.8\n",
      "Iteration 11686: with minibatch training loss = 0.787 and accuracy of 0.8\n",
      "Iteration 11687: with minibatch training loss = 0.753 and accuracy of 0.77\n",
      "Iteration 11688: with minibatch training loss = 0.773 and accuracy of 0.75\n",
      "Iteration 11689: with minibatch training loss = 0.705 and accuracy of 0.8\n",
      "Iteration 11690: with minibatch training loss = 0.702 and accuracy of 0.8\n",
      "Iteration 11691: with minibatch training loss = 0.843 and accuracy of 0.78\n",
      "Iteration 11692: with minibatch training loss = 0.707 and accuracy of 0.78\n",
      "Iteration 11693: with minibatch training loss = 1.05 and accuracy of 0.72\n",
      "Iteration 11694: with minibatch training loss = 0.57 and accuracy of 0.83\n",
      "Iteration 11695: with minibatch training loss = 0.551 and accuracy of 0.84\n",
      "Iteration 11696: with minibatch training loss = 0.732 and accuracy of 0.8\n",
      "Iteration 11697: with minibatch training loss = 0.682 and accuracy of 0.78\n",
      "Iteration 11698: with minibatch training loss = 0.923 and accuracy of 0.72\n",
      "Iteration 11699: with minibatch training loss = 1.11 and accuracy of 0.67\n",
      "Iteration 11700: with minibatch training loss = 0.629 and accuracy of 0.81\n",
      "Iteration 11701: with minibatch training loss = 0.711 and accuracy of 0.8\n",
      "Iteration 11702: with minibatch training loss = 0.561 and accuracy of 0.83\n",
      "Iteration 11703: with minibatch training loss = 0.897 and accuracy of 0.75\n",
      "Iteration 11704: with minibatch training loss = 0.537 and accuracy of 0.84\n",
      "Iteration 11705: with minibatch training loss = 0.556 and accuracy of 0.83\n",
      "Iteration 11706: with minibatch training loss = 0.472 and accuracy of 0.88\n",
      "Iteration 11707: with minibatch training loss = 0.539 and accuracy of 0.86\n",
      "Iteration 11708: with minibatch training loss = 0.763 and accuracy of 0.77\n",
      "Iteration 11709: with minibatch training loss = 0.649 and accuracy of 0.81\n",
      "Iteration 11710: with minibatch training loss = 0.563 and accuracy of 0.83\n",
      "Iteration 11711: with minibatch training loss = 0.538 and accuracy of 0.86\n",
      "Iteration 11712: with minibatch training loss = 0.483 and accuracy of 0.86\n",
      "Iteration 11713: with minibatch training loss = 0.638 and accuracy of 0.84\n",
      "Iteration 11714: with minibatch training loss = 0.691 and accuracy of 0.81\n",
      "Iteration 11715: with minibatch training loss = 0.777 and accuracy of 0.75\n",
      "Iteration 11716: with minibatch training loss = 0.609 and accuracy of 0.81\n",
      "Iteration 11717: with minibatch training loss = 1.11 and accuracy of 0.64\n",
      "Iteration 11718: with minibatch training loss = 0.818 and accuracy of 0.77\n",
      "Iteration 11719: with minibatch training loss = 1.02 and accuracy of 0.66\n",
      "Iteration 11720: with minibatch training loss = 0.555 and accuracy of 0.84\n",
      "Iteration 11721: with minibatch training loss = 0.589 and accuracy of 0.84\n",
      "Iteration 11722: with minibatch training loss = 0.64 and accuracy of 0.78\n",
      "Iteration 11723: with minibatch training loss = 0.769 and accuracy of 0.8\n",
      "Iteration 11724: with minibatch training loss = 0.959 and accuracy of 0.72\n",
      "Iteration 11725: with minibatch training loss = 0.708 and accuracy of 0.78\n",
      "Iteration 11726: with minibatch training loss = 0.676 and accuracy of 0.81\n",
      "Iteration 11727: with minibatch training loss = 0.791 and accuracy of 0.77\n",
      "Iteration 11728: with minibatch training loss = 0.762 and accuracy of 0.77\n",
      "Iteration 11729: with minibatch training loss = 0.856 and accuracy of 0.77\n",
      "Iteration 11730: with minibatch training loss = 0.454 and accuracy of 0.88\n",
      "Iteration 11731: with minibatch training loss = 0.65 and accuracy of 0.81\n",
      "Iteration 11732: with minibatch training loss = 0.558 and accuracy of 0.84\n",
      "Iteration 11733: with minibatch training loss = 0.792 and accuracy of 0.73\n",
      "Iteration 11734: with minibatch training loss = 0.866 and accuracy of 0.77\n",
      "Iteration 11735: with minibatch training loss = 0.761 and accuracy of 0.78\n",
      "Iteration 11736: with minibatch training loss = 0.836 and accuracy of 0.75\n",
      "Iteration 11737: with minibatch training loss = 0.853 and accuracy of 0.75\n",
      "Iteration 11738: with minibatch training loss = 0.707 and accuracy of 0.8\n",
      "Iteration 11739: with minibatch training loss = 0.877 and accuracy of 0.72\n",
      "Iteration 11740: with minibatch training loss = 0.639 and accuracy of 0.78\n",
      "Iteration 11741: with minibatch training loss = 0.855 and accuracy of 0.73\n",
      "Iteration 11742: with minibatch training loss = 0.885 and accuracy of 0.75\n",
      "Iteration 11743: with minibatch training loss = 0.737 and accuracy of 0.78\n",
      "Iteration 11744: with minibatch training loss = 0.737 and accuracy of 0.77\n",
      "Iteration 11745: with minibatch training loss = 0.623 and accuracy of 0.8\n",
      "Iteration 11746: with minibatch training loss = 0.787 and accuracy of 0.72\n",
      "Iteration 11747: with minibatch training loss = 0.748 and accuracy of 0.78\n",
      "Iteration 11748: with minibatch training loss = 0.747 and accuracy of 0.75\n",
      "Iteration 11749: with minibatch training loss = 0.569 and accuracy of 0.83\n",
      "Iteration 11750: with minibatch training loss = 0.712 and accuracy of 0.78\n",
      "Iteration 11751: with minibatch training loss = 0.643 and accuracy of 0.8\n",
      "Iteration 11752: with minibatch training loss = 0.703 and accuracy of 0.78\n",
      "Iteration 11753: with minibatch training loss = 0.812 and accuracy of 0.78\n",
      "Iteration 11754: with minibatch training loss = 0.52 and accuracy of 0.88\n",
      "Iteration 11755: with minibatch training loss = 0.731 and accuracy of 0.8\n",
      "Iteration 11756: with minibatch training loss = 0.83 and accuracy of 0.73\n",
      "Iteration 11757: with minibatch training loss = 0.773 and accuracy of 0.77\n",
      "Iteration 11758: with minibatch training loss = 0.764 and accuracy of 0.8\n",
      "Iteration 11759: with minibatch training loss = 0.724 and accuracy of 0.8\n",
      "Iteration 11760: with minibatch training loss = 0.82 and accuracy of 0.77\n",
      "Iteration 11761: with minibatch training loss = 0.51 and accuracy of 0.83\n",
      "Iteration 11762: with minibatch training loss = 0.552 and accuracy of 0.86\n",
      "Iteration 11763: with minibatch training loss = 0.636 and accuracy of 0.81\n",
      "Iteration 11764: with minibatch training loss = 0.924 and accuracy of 0.72\n",
      "Iteration 11765: with minibatch training loss = 0.631 and accuracy of 0.81\n",
      "Iteration 11766: with minibatch training loss = 0.78 and accuracy of 0.77\n",
      "Iteration 11767: with minibatch training loss = 0.561 and accuracy of 0.86\n",
      "Iteration 11768: with minibatch training loss = 0.825 and accuracy of 0.75\n",
      "Iteration 11769: with minibatch training loss = 0.663 and accuracy of 0.81\n",
      "Iteration 11770: with minibatch training loss = 0.596 and accuracy of 0.83\n",
      "Iteration 11771: with minibatch training loss = 0.715 and accuracy of 0.8\n",
      "Iteration 11772: with minibatch training loss = 1.18 and accuracy of 0.67\n",
      "Iteration 11773: with minibatch training loss = 0.697 and accuracy of 0.81\n",
      "Iteration 11774: with minibatch training loss = 0.621 and accuracy of 0.81\n",
      "Iteration 11775: with minibatch training loss = 0.759 and accuracy of 0.81\n",
      "Iteration 11776: with minibatch training loss = 0.603 and accuracy of 0.81\n",
      "Iteration 11777: with minibatch training loss = 0.804 and accuracy of 0.75\n",
      "Iteration 11778: with minibatch training loss = 0.64 and accuracy of 0.8\n",
      "Iteration 11779: with minibatch training loss = 0.451 and accuracy of 0.89\n",
      "Iteration 11780: with minibatch training loss = 0.849 and accuracy of 0.73\n",
      "Iteration 11781: with minibatch training loss = 0.848 and accuracy of 0.77\n",
      "Iteration 11782: with minibatch training loss = 0.643 and accuracy of 0.81\n",
      "Iteration 11783: with minibatch training loss = 0.675 and accuracy of 0.83\n",
      "Iteration 11784: with minibatch training loss = 0.529 and accuracy of 0.84\n",
      "Iteration 11785: with minibatch training loss = 0.977 and accuracy of 0.75\n",
      "Iteration 11786: with minibatch training loss = 0.754 and accuracy of 0.78\n",
      "Iteration 11787: with minibatch training loss = 1.03 and accuracy of 0.7\n",
      "Iteration 11788: with minibatch training loss = 0.73 and accuracy of 0.8\n",
      "Iteration 11789: with minibatch training loss = 0.453 and accuracy of 0.89\n",
      "Iteration 11790: with minibatch training loss = 0.665 and accuracy of 0.81\n",
      "Iteration 11791: with minibatch training loss = 0.62 and accuracy of 0.83\n",
      "Iteration 11792: with minibatch training loss = 0.626 and accuracy of 0.83\n",
      "Iteration 11793: with minibatch training loss = 0.921 and accuracy of 0.7\n",
      "Iteration 11794: with minibatch training loss = 0.605 and accuracy of 0.84\n",
      "Iteration 11795: with minibatch training loss = 0.601 and accuracy of 0.83\n",
      "Iteration 11796: with minibatch training loss = 0.652 and accuracy of 0.77\n",
      "Iteration 11797: with minibatch training loss = 0.751 and accuracy of 0.77\n",
      "Iteration 11798: with minibatch training loss = 0.761 and accuracy of 0.77\n",
      "Iteration 11799: with minibatch training loss = 0.505 and accuracy of 0.89\n",
      "Iteration 11800: with minibatch training loss = 0.822 and accuracy of 0.73\n",
      "Iteration 11801: with minibatch training loss = 0.962 and accuracy of 0.72\n",
      "Iteration 11802: with minibatch training loss = 1.13 and accuracy of 0.69\n",
      "Iteration 11803: with minibatch training loss = 0.457 and accuracy of 0.86\n",
      "Iteration 11804: with minibatch training loss = 0.727 and accuracy of 0.78\n",
      "Iteration 11805: with minibatch training loss = 0.616 and accuracy of 0.81\n",
      "Iteration 11806: with minibatch training loss = 0.647 and accuracy of 0.81\n",
      "Iteration 11807: with minibatch training loss = 0.549 and accuracy of 0.83\n",
      "Iteration 11808: with minibatch training loss = 0.735 and accuracy of 0.83\n",
      "Iteration 11809: with minibatch training loss = 0.977 and accuracy of 0.69\n",
      "Iteration 11810: with minibatch training loss = 0.748 and accuracy of 0.77\n",
      "Iteration 11811: with minibatch training loss = 0.703 and accuracy of 0.8\n",
      "Iteration 11812: with minibatch training loss = 0.897 and accuracy of 0.75\n",
      "Iteration 11813: with minibatch training loss = 0.529 and accuracy of 0.86\n",
      "Iteration 11814: with minibatch training loss = 0.655 and accuracy of 0.83\n",
      "Iteration 11815: with minibatch training loss = 0.581 and accuracy of 0.83\n",
      "Iteration 11816: with minibatch training loss = 0.588 and accuracy of 0.81\n",
      "Iteration 11817: with minibatch training loss = 0.559 and accuracy of 0.83\n",
      "Iteration 11818: with minibatch training loss = 0.5 and accuracy of 0.88\n",
      "Iteration 11819: with minibatch training loss = 0.588 and accuracy of 0.84\n",
      "Iteration 11820: with minibatch training loss = 0.706 and accuracy of 0.8\n",
      "Iteration 11821: with minibatch training loss = 0.797 and accuracy of 0.8\n",
      "Iteration 11822: with minibatch training loss = 0.756 and accuracy of 0.77\n",
      "Iteration 11823: with minibatch training loss = 0.773 and accuracy of 0.8\n",
      "Iteration 11824: with minibatch training loss = 0.758 and accuracy of 0.77\n",
      "Iteration 11825: with minibatch training loss = 0.524 and accuracy of 0.88\n",
      "Iteration 11826: with minibatch training loss = 0.715 and accuracy of 0.8\n",
      "Iteration 11827: with minibatch training loss = 0.481 and accuracy of 0.86\n",
      "Iteration 11828: with minibatch training loss = 0.789 and accuracy of 0.73\n",
      "Iteration 11829: with minibatch training loss = 0.92 and accuracy of 0.77\n",
      "Iteration 11830: with minibatch training loss = 0.618 and accuracy of 0.81\n",
      "Iteration 11831: with minibatch training loss = 0.913 and accuracy of 0.7\n",
      "Iteration 11832: with minibatch training loss = 0.91 and accuracy of 0.72\n",
      "Iteration 11833: with minibatch training loss = 0.869 and accuracy of 0.72\n",
      "Iteration 11834: with minibatch training loss = 0.681 and accuracy of 0.84\n",
      "Iteration 11835: with minibatch training loss = 0.445 and accuracy of 0.86\n",
      "Iteration 11836: with minibatch training loss = 0.769 and accuracy of 0.78\n",
      "Iteration 11837: with minibatch training loss = 0.751 and accuracy of 0.75\n",
      "Iteration 11838: with minibatch training loss = 0.76 and accuracy of 0.75\n",
      "Iteration 11839: with minibatch training loss = 0.773 and accuracy of 0.75\n",
      "Iteration 11840: with minibatch training loss = 0.609 and accuracy of 0.8\n",
      "Iteration 11841: with minibatch training loss = 0.692 and accuracy of 0.78\n",
      "Iteration 11842: with minibatch training loss = 1.11 and accuracy of 0.7\n",
      "Iteration 11843: with minibatch training loss = 0.652 and accuracy of 0.81\n",
      "Iteration 11844: with minibatch training loss = 0.625 and accuracy of 0.83\n",
      "Iteration 11845: with minibatch training loss = 0.708 and accuracy of 0.78\n",
      "Iteration 11846: with minibatch training loss = 0.761 and accuracy of 0.77\n",
      "Iteration 11847: with minibatch training loss = 0.699 and accuracy of 0.81\n",
      "Iteration 11848: with minibatch training loss = 0.627 and accuracy of 0.81\n",
      "Iteration 11849: with minibatch training loss = 0.587 and accuracy of 0.84\n",
      "Iteration 11850: with minibatch training loss = 0.599 and accuracy of 0.83\n",
      "Iteration 11851: with minibatch training loss = 0.807 and accuracy of 0.78\n",
      "Iteration 11852: with minibatch training loss = 0.971 and accuracy of 0.75\n",
      "Iteration 11853: with minibatch training loss = 0.596 and accuracy of 0.83\n",
      "Iteration 11854: with minibatch training loss = 0.584 and accuracy of 0.84\n",
      "Iteration 11855: with minibatch training loss = 0.433 and accuracy of 0.88\n",
      "Iteration 11856: with minibatch training loss = 1.07 and accuracy of 0.67\n",
      "Iteration 11857: with minibatch training loss = 0.831 and accuracy of 0.8\n",
      "Iteration 11858: with minibatch training loss = 0.545 and accuracy of 0.84\n",
      "Iteration 11859: with minibatch training loss = 0.974 and accuracy of 0.7\n",
      "Iteration 11860: with minibatch training loss = 0.709 and accuracy of 0.78\n",
      "Iteration 11861: with minibatch training loss = 0.878 and accuracy of 0.75\n",
      "Iteration 11862: with minibatch training loss = 0.627 and accuracy of 0.81\n",
      "Iteration 11863: with minibatch training loss = 0.854 and accuracy of 0.75\n",
      "Iteration 11864: with minibatch training loss = 0.797 and accuracy of 0.77\n",
      "Iteration 11865: with minibatch training loss = 0.653 and accuracy of 0.8\n",
      "Iteration 11866: with minibatch training loss = 0.787 and accuracy of 0.78\n",
      "Iteration 11867: with minibatch training loss = 0.651 and accuracy of 0.8\n",
      "Iteration 11868: with minibatch training loss = 0.83 and accuracy of 0.77\n",
      "Iteration 11869: with minibatch training loss = 0.671 and accuracy of 0.78\n",
      "Iteration 11870: with minibatch training loss = 1.01 and accuracy of 0.67\n",
      "Iteration 11871: with minibatch training loss = 1.01 and accuracy of 0.69\n",
      "Iteration 11872: with minibatch training loss = 0.777 and accuracy of 0.8\n",
      "Iteration 11873: with minibatch training loss = 0.865 and accuracy of 0.73\n",
      "Iteration 11874: with minibatch training loss = 0.858 and accuracy of 0.73\n",
      "Iteration 11875: with minibatch training loss = 0.696 and accuracy of 0.78\n",
      "Iteration 11876: with minibatch training loss = 0.736 and accuracy of 0.8\n",
      "Iteration 11877: with minibatch training loss = 0.576 and accuracy of 0.83\n",
      "Iteration 11878: with minibatch training loss = 0.81 and accuracy of 0.8\n",
      "Iteration 11879: with minibatch training loss = 0.632 and accuracy of 0.8\n",
      "Iteration 11880: with minibatch training loss = 0.599 and accuracy of 0.83\n",
      "Iteration 11881: with minibatch training loss = 0.794 and accuracy of 0.77\n",
      "Iteration 11882: with minibatch training loss = 0.688 and accuracy of 0.8\n",
      "Iteration 11883: with minibatch training loss = 0.673 and accuracy of 0.78\n",
      "Iteration 11884: with minibatch training loss = 0.577 and accuracy of 0.84\n",
      "Iteration 11885: with minibatch training loss = 0.749 and accuracy of 0.77\n",
      "Iteration 11886: with minibatch training loss = 0.897 and accuracy of 0.7\n",
      "Iteration 11887: with minibatch training loss = 0.876 and accuracy of 0.75\n",
      "Iteration 11888: with minibatch training loss = 0.695 and accuracy of 0.81\n",
      "Iteration 11889: with minibatch training loss = 0.752 and accuracy of 0.75\n",
      "Iteration 11890: with minibatch training loss = 0.819 and accuracy of 0.73\n",
      "Iteration 11891: with minibatch training loss = 0.82 and accuracy of 0.73\n",
      "Iteration 11892: with minibatch training loss = 0.683 and accuracy of 0.8\n",
      "Iteration 11893: with minibatch training loss = 0.6 and accuracy of 0.81\n",
      "Iteration 11894: with minibatch training loss = 0.713 and accuracy of 0.8\n",
      "Iteration 11895: with minibatch training loss = 0.565 and accuracy of 0.83\n",
      "Iteration 11896: with minibatch training loss = 0.758 and accuracy of 0.78\n",
      "Iteration 11897: with minibatch training loss = 0.693 and accuracy of 0.81\n",
      "Iteration 11898: with minibatch training loss = 0.762 and accuracy of 0.77\n",
      "Iteration 11899: with minibatch training loss = 0.527 and accuracy of 0.83\n",
      "Iteration 11900: with minibatch training loss = 0.792 and accuracy of 0.73\n",
      "Iteration 11901: with minibatch training loss = 0.929 and accuracy of 0.72\n",
      "Iteration 11902: with minibatch training loss = 0.944 and accuracy of 0.73\n",
      "Iteration 11903: with minibatch training loss = 0.776 and accuracy of 0.78\n",
      "Iteration 11904: with minibatch training loss = 0.557 and accuracy of 0.81\n",
      "Iteration 11905: with minibatch training loss = 0.837 and accuracy of 0.73\n",
      "Iteration 11906: with minibatch training loss = 0.609 and accuracy of 0.8\n",
      "Iteration 11907: with minibatch training loss = 0.729 and accuracy of 0.77\n",
      "Iteration 11908: with minibatch training loss = 0.944 and accuracy of 0.7\n",
      "Iteration 11909: with minibatch training loss = 0.77 and accuracy of 0.78\n",
      "Iteration 11910: with minibatch training loss = 0.589 and accuracy of 0.78\n",
      "Iteration 11911: with minibatch training loss = 0.401 and accuracy of 0.89\n",
      "Iteration 11912: with minibatch training loss = 0.644 and accuracy of 0.84\n",
      "Iteration 11913: with minibatch training loss = 0.982 and accuracy of 0.69\n",
      "Iteration 11914: with minibatch training loss = 1.06 and accuracy of 0.69\n",
      "Iteration 11915: with minibatch training loss = 0.599 and accuracy of 0.83\n",
      "Iteration 11916: with minibatch training loss = 0.47 and accuracy of 0.88\n",
      "Iteration 11917: with minibatch training loss = 0.572 and accuracy of 0.86\n",
      "Iteration 11918: with minibatch training loss = 0.724 and accuracy of 0.78\n",
      "Iteration 11919: with minibatch training loss = 0.734 and accuracy of 0.77\n",
      "Iteration 11920: with minibatch training loss = 0.604 and accuracy of 0.84\n",
      "Iteration 11921: with minibatch training loss = 0.781 and accuracy of 0.78\n",
      "Iteration 11922: with minibatch training loss = 0.723 and accuracy of 0.77\n",
      "Iteration 11923: with minibatch training loss = 0.783 and accuracy of 0.77\n",
      "Iteration 11924: with minibatch training loss = 0.85 and accuracy of 0.75\n",
      "Iteration 11925: with minibatch training loss = 0.788 and accuracy of 0.78\n",
      "Iteration 11926: with minibatch training loss = 0.658 and accuracy of 0.81\n",
      "Iteration 11927: with minibatch training loss = 0.778 and accuracy of 0.8\n",
      "Iteration 11928: with minibatch training loss = 0.637 and accuracy of 0.81\n",
      "Iteration 11929: with minibatch training loss = 0.568 and accuracy of 0.84\n",
      "Iteration 11930: with minibatch training loss = 0.886 and accuracy of 0.73\n",
      "Iteration 11931: with minibatch training loss = 0.719 and accuracy of 0.83\n",
      "Iteration 11932: with minibatch training loss = 0.631 and accuracy of 0.8\n",
      "Iteration 11933: with minibatch training loss = 0.86 and accuracy of 0.72\n",
      "Iteration 11934: with minibatch training loss = 0.625 and accuracy of 0.81\n",
      "Iteration 11935: with minibatch training loss = 0.592 and accuracy of 0.86\n",
      "Iteration 11936: with minibatch training loss = 0.923 and accuracy of 0.72\n",
      "Iteration 11937: with minibatch training loss = 0.832 and accuracy of 0.83\n",
      "Iteration 11938: with minibatch training loss = 0.908 and accuracy of 0.75\n",
      "Iteration 11939: with minibatch training loss = 0.425 and accuracy of 0.88\n",
      "Iteration 11940: with minibatch training loss = 0.678 and accuracy of 0.78\n",
      "Iteration 11941: with minibatch training loss = 0.514 and accuracy of 0.88\n",
      "Iteration 11942: with minibatch training loss = 0.794 and accuracy of 0.78\n",
      "Iteration 11943: with minibatch training loss = 0.629 and accuracy of 0.81\n",
      "Iteration 11944: with minibatch training loss = 0.748 and accuracy of 0.73\n",
      "Iteration 11945: with minibatch training loss = 0.867 and accuracy of 0.73\n",
      "Iteration 11946: with minibatch training loss = 0.572 and accuracy of 0.83\n",
      "Iteration 11947: with minibatch training loss = 0.461 and accuracy of 0.86\n",
      "Iteration 11948: with minibatch training loss = 0.69 and accuracy of 0.78\n",
      "Iteration 11949: with minibatch training loss = 0.703 and accuracy of 0.8\n",
      "Iteration 11950: with minibatch training loss = 0.599 and accuracy of 0.8\n",
      "Iteration 11951: with minibatch training loss = 0.614 and accuracy of 0.81\n",
      "Iteration 11952: with minibatch training loss = 0.83 and accuracy of 0.77\n",
      "Iteration 11953: with minibatch training loss = 0.741 and accuracy of 0.8\n",
      "Iteration 11954: with minibatch training loss = 0.998 and accuracy of 0.7\n",
      "Iteration 11955: with minibatch training loss = 0.635 and accuracy of 0.84\n",
      "Iteration 11956: with minibatch training loss = 0.578 and accuracy of 0.86\n",
      "Iteration 11957: with minibatch training loss = 0.641 and accuracy of 0.81\n",
      "Iteration 11958: with minibatch training loss = 0.614 and accuracy of 0.81\n",
      "Iteration 11959: with minibatch training loss = 0.712 and accuracy of 0.77\n",
      "Iteration 11960: with minibatch training loss = 1 and accuracy of 0.7\n",
      "Iteration 11961: with minibatch training loss = 0.85 and accuracy of 0.73\n",
      "Iteration 11962: with minibatch training loss = 0.729 and accuracy of 0.77\n",
      "Iteration 11963: with minibatch training loss = 0.878 and accuracy of 0.75\n",
      "Iteration 11964: with minibatch training loss = 0.722 and accuracy of 0.81\n",
      "Iteration 11965: with minibatch training loss = 0.605 and accuracy of 0.81\n",
      "Iteration 11966: with minibatch training loss = 0.977 and accuracy of 0.72\n",
      "Iteration 11967: with minibatch training loss = 0.708 and accuracy of 0.78\n",
      "Iteration 11968: with minibatch training loss = 0.461 and accuracy of 0.88\n",
      "Iteration 11969: with minibatch training loss = 0.713 and accuracy of 0.78\n",
      "Iteration 11970: with minibatch training loss = 0.759 and accuracy of 0.77\n",
      "Iteration 11971: with minibatch training loss = 0.56 and accuracy of 0.88\n",
      "Iteration 11972: with minibatch training loss = 0.711 and accuracy of 0.77\n",
      "Iteration 11973: with minibatch training loss = 0.674 and accuracy of 0.81\n",
      "Iteration 11974: with minibatch training loss = 0.821 and accuracy of 0.77\n",
      "Iteration 11975: with minibatch training loss = 0.519 and accuracy of 0.89\n",
      "Iteration 11976: with minibatch training loss = 0.786 and accuracy of 0.75\n",
      "Iteration 11977: with minibatch training loss = 0.413 and accuracy of 0.89\n",
      "Iteration 11978: with minibatch training loss = 0.658 and accuracy of 0.8\n",
      "Iteration 11979: with minibatch training loss = 0.782 and accuracy of 0.75\n",
      "Iteration 11980: with minibatch training loss = 0.742 and accuracy of 0.77\n",
      "Iteration 11981: with minibatch training loss = 0.801 and accuracy of 0.77\n",
      "Iteration 11982: with minibatch training loss = 0.77 and accuracy of 0.77\n",
      "Iteration 11983: with minibatch training loss = 0.823 and accuracy of 0.73\n",
      "Iteration 11984: with minibatch training loss = 0.669 and accuracy of 0.8\n",
      "Iteration 11985: with minibatch training loss = 0.846 and accuracy of 0.75\n",
      "Iteration 11986: with minibatch training loss = 0.646 and accuracy of 0.81\n",
      "Iteration 11987: with minibatch training loss = 0.718 and accuracy of 0.78\n",
      "Iteration 11988: with minibatch training loss = 0.546 and accuracy of 0.84\n",
      "Iteration 11989: with minibatch training loss = 0.659 and accuracy of 0.8\n",
      "Iteration 11990: with minibatch training loss = 0.891 and accuracy of 0.72\n",
      "Iteration 11991: with minibatch training loss = 0.945 and accuracy of 0.72\n",
      "Iteration 11992: with minibatch training loss = 0.526 and accuracy of 0.84\n",
      "Iteration 11993: with minibatch training loss = 0.665 and accuracy of 0.83\n",
      "Iteration 11994: with minibatch training loss = 1.14 and accuracy of 0.7\n",
      "Iteration 11995: with minibatch training loss = 0.854 and accuracy of 0.73\n",
      "Iteration 11996: with minibatch training loss = 0.642 and accuracy of 0.8\n",
      "Iteration 11997: with minibatch training loss = 0.689 and accuracy of 0.81\n",
      "Iteration 11998: with minibatch training loss = 0.659 and accuracy of 0.83\n",
      "Iteration 11999: with minibatch training loss = 0.721 and accuracy of 0.81\n",
      "Iteration 12000: with minibatch training loss = 0.563 and accuracy of 0.81\n",
      "Iteration 12001: with minibatch training loss = 0.636 and accuracy of 0.83\n",
      "Iteration 12002: with minibatch training loss = 0.738 and accuracy of 0.8\n",
      "Iteration 12003: with minibatch training loss = 0.68 and accuracy of 0.78\n",
      "Iteration 12004: with minibatch training loss = 0.565 and accuracy of 0.83\n",
      "Iteration 12005: with minibatch training loss = 0.846 and accuracy of 0.73\n",
      "Iteration 12006: with minibatch training loss = 0.591 and accuracy of 0.83\n",
      "Iteration 12007: with minibatch training loss = 0.708 and accuracy of 0.8\n",
      "Iteration 12008: with minibatch training loss = 0.678 and accuracy of 0.81\n",
      "Iteration 12009: with minibatch training loss = 0.736 and accuracy of 0.78\n",
      "Iteration 12010: with minibatch training loss = 0.774 and accuracy of 0.77\n",
      "Iteration 12011: with minibatch training loss = 0.497 and accuracy of 0.84\n",
      "Iteration 12012: with minibatch training loss = 0.952 and accuracy of 0.73\n",
      "Iteration 12013: with minibatch training loss = 0.979 and accuracy of 0.7\n",
      "Iteration 12014: with minibatch training loss = 0.783 and accuracy of 0.78\n",
      "Iteration 12015: with minibatch training loss = 0.621 and accuracy of 0.83\n",
      "Iteration 12016: with minibatch training loss = 0.843 and accuracy of 0.75\n",
      "Iteration 12017: with minibatch training loss = 0.526 and accuracy of 0.84\n",
      "Iteration 12018: with minibatch training loss = 0.925 and accuracy of 0.72\n",
      "Iteration 12019: with minibatch training loss = 0.939 and accuracy of 0.72\n",
      "Iteration 12020: with minibatch training loss = 0.568 and accuracy of 0.83\n",
      "Iteration 12021: with minibatch training loss = 0.585 and accuracy of 0.83\n",
      "Iteration 12022: with minibatch training loss = 0.594 and accuracy of 0.81\n",
      "Iteration 12023: with minibatch training loss = 0.706 and accuracy of 0.78\n",
      "Iteration 12024: with minibatch training loss = 0.485 and accuracy of 0.88\n",
      "Iteration 12025: with minibatch training loss = 0.916 and accuracy of 0.7\n",
      "Iteration 12026: with minibatch training loss = 0.832 and accuracy of 0.77\n",
      "Iteration 12027: with minibatch training loss = 0.601 and accuracy of 0.81\n",
      "Iteration 12028: with minibatch training loss = 0.651 and accuracy of 0.81\n",
      "Iteration 12029: with minibatch training loss = 0.57 and accuracy of 0.83\n",
      "Iteration 12030: with minibatch training loss = 0.615 and accuracy of 0.81\n",
      "Iteration 12031: with minibatch training loss = 0.82 and accuracy of 0.75\n",
      "Iteration 12032: with minibatch training loss = 0.685 and accuracy of 0.78\n",
      "Iteration 12033: with minibatch training loss = 0.553 and accuracy of 0.83\n",
      "Iteration 12034: with minibatch training loss = 0.914 and accuracy of 0.69\n",
      "Iteration 12035: with minibatch training loss = 0.615 and accuracy of 0.83\n",
      "Iteration 12036: with minibatch training loss = 0.926 and accuracy of 0.7\n",
      "Iteration 12037: with minibatch training loss = 0.516 and accuracy of 0.83\n",
      "Iteration 12038: with minibatch training loss = 0.638 and accuracy of 0.83\n",
      "Iteration 12039: with minibatch training loss = 0.734 and accuracy of 0.78\n",
      "Iteration 12040: with minibatch training loss = 0.879 and accuracy of 0.73\n",
      "Iteration 12041: with minibatch training loss = 0.796 and accuracy of 0.77\n",
      "Iteration 12042: with minibatch training loss = 0.67 and accuracy of 0.83\n",
      "Iteration 12043: with minibatch training loss = 0.682 and accuracy of 0.8\n",
      "Iteration 12044: with minibatch training loss = 0.519 and accuracy of 0.86\n",
      "Iteration 12045: with minibatch training loss = 0.615 and accuracy of 0.83\n",
      "Iteration 12046: with minibatch training loss = 0.547 and accuracy of 0.86\n",
      "Iteration 12047: with minibatch training loss = 0.924 and accuracy of 0.73\n",
      "Iteration 12048: with minibatch training loss = 1.13 and accuracy of 0.73\n",
      "Iteration 12049: with minibatch training loss = 0.658 and accuracy of 0.83\n",
      "Iteration 12050: with minibatch training loss = 0.988 and accuracy of 0.72\n",
      "Iteration 12051: with minibatch training loss = 0.652 and accuracy of 0.8\n",
      "Iteration 12052: with minibatch training loss = 0.816 and accuracy of 0.77\n",
      "Iteration 12053: with minibatch training loss = 0.71 and accuracy of 0.77\n",
      "Iteration 12054: with minibatch training loss = 0.863 and accuracy of 0.75\n",
      "Iteration 12055: with minibatch training loss = 0.692 and accuracy of 0.77\n",
      "Iteration 12056: with minibatch training loss = 0.636 and accuracy of 0.78\n",
      "Iteration 12057: with minibatch training loss = 0.491 and accuracy of 0.84\n",
      "Iteration 12058: with minibatch training loss = 0.95 and accuracy of 0.69\n",
      "Iteration 12059: with minibatch training loss = 0.57 and accuracy of 0.83\n",
      "Iteration 12060: with minibatch training loss = 0.833 and accuracy of 0.73\n",
      "Iteration 12061: with minibatch training loss = 0.78 and accuracy of 0.77\n",
      "Iteration 12062: with minibatch training loss = 0.976 and accuracy of 0.73\n",
      "Iteration 12063: with minibatch training loss = 0.985 and accuracy of 0.72\n",
      "Iteration 12064: with minibatch training loss = 0.673 and accuracy of 0.8\n",
      "Iteration 12065: with minibatch training loss = 0.714 and accuracy of 0.78\n",
      "Iteration 12066: with minibatch training loss = 0.602 and accuracy of 0.83\n",
      "Iteration 12067: with minibatch training loss = 0.838 and accuracy of 0.78\n",
      "Iteration 12068: with minibatch training loss = 0.575 and accuracy of 0.83\n",
      "Iteration 12069: with minibatch training loss = 0.892 and accuracy of 0.75\n",
      "Iteration 12070: with minibatch training loss = 0.42 and accuracy of 0.88\n",
      "Iteration 12071: with minibatch training loss = 0.871 and accuracy of 0.73\n",
      "Iteration 12072: with minibatch training loss = 0.881 and accuracy of 0.72\n",
      "Iteration 12073: with minibatch training loss = 0.791 and accuracy of 0.75\n",
      "Iteration 12074: with minibatch training loss = 0.782 and accuracy of 0.77\n",
      "Iteration 12075: with minibatch training loss = 0.657 and accuracy of 0.8\n",
      "Iteration 12076: with minibatch training loss = 0.654 and accuracy of 0.8\n",
      "Iteration 12077: with minibatch training loss = 0.962 and accuracy of 0.72\n",
      "Iteration 12078: with minibatch training loss = 0.711 and accuracy of 0.8\n",
      "Iteration 12079: with minibatch training loss = 0.761 and accuracy of 0.77\n",
      "Iteration 12080: with minibatch training loss = 0.755 and accuracy of 0.8\n",
      "Iteration 12081: with minibatch training loss = 0.416 and accuracy of 0.88\n",
      "Iteration 12082: with minibatch training loss = 0.589 and accuracy of 0.83\n",
      "Iteration 12083: with minibatch training loss = 0.689 and accuracy of 0.77\n",
      "Iteration 12084: with minibatch training loss = 0.859 and accuracy of 0.73\n",
      "Iteration 12085: with minibatch training loss = 0.912 and accuracy of 0.75\n",
      "Iteration 12086: with minibatch training loss = 0.82 and accuracy of 0.72\n",
      "Iteration 12087: with minibatch training loss = 0.665 and accuracy of 0.81\n",
      "Iteration 12088: with minibatch training loss = 0.887 and accuracy of 0.77\n",
      "Iteration 12089: with minibatch training loss = 0.845 and accuracy of 0.75\n",
      "Iteration 12090: with minibatch training loss = 0.729 and accuracy of 0.8\n",
      "Iteration 12091: with minibatch training loss = 1.03 and accuracy of 0.69\n",
      "Iteration 12092: with minibatch training loss = 0.657 and accuracy of 0.8\n",
      "Iteration 12093: with minibatch training loss = 0.739 and accuracy of 0.81\n",
      "Iteration 12094: with minibatch training loss = 0.668 and accuracy of 0.8\n",
      "Iteration 12095: with minibatch training loss = 0.651 and accuracy of 0.8\n",
      "Iteration 12096: with minibatch training loss = 0.836 and accuracy of 0.78\n",
      "Iteration 12097: with minibatch training loss = 0.71 and accuracy of 0.78\n",
      "Iteration 12098: with minibatch training loss = 0.971 and accuracy of 0.75\n",
      "Iteration 12099: with minibatch training loss = 0.447 and accuracy of 0.89\n",
      "Iteration 12100: with minibatch training loss = 0.599 and accuracy of 0.81\n",
      "Iteration 12101: with minibatch training loss = 0.67 and accuracy of 0.81\n",
      "Iteration 12102: with minibatch training loss = 0.732 and accuracy of 0.78\n",
      "Iteration 12103: with minibatch training loss = 0.767 and accuracy of 0.81\n",
      "Iteration 12104: with minibatch training loss = 0.791 and accuracy of 0.8\n",
      "Iteration 12105: with minibatch training loss = 0.763 and accuracy of 0.78\n",
      "Iteration 12106: with minibatch training loss = 0.706 and accuracy of 0.78\n",
      "Iteration 12107: with minibatch training loss = 0.7 and accuracy of 0.78\n",
      "Iteration 12108: with minibatch training loss = 0.837 and accuracy of 0.77\n",
      "Iteration 12109: with minibatch training loss = 0.529 and accuracy of 0.83\n",
      "Iteration 12110: with minibatch training loss = 0.585 and accuracy of 0.81\n",
      "Iteration 12111: with minibatch training loss = 0.634 and accuracy of 0.8\n",
      "Iteration 12112: with minibatch training loss = 0.906 and accuracy of 0.72\n",
      "Iteration 12113: with minibatch training loss = 1.05 and accuracy of 0.7\n",
      "Iteration 12114: with minibatch training loss = 0.578 and accuracy of 0.81\n",
      "Iteration 12115: with minibatch training loss = 0.545 and accuracy of 0.83\n",
      "Iteration 12116: with minibatch training loss = 0.788 and accuracy of 0.77\n",
      "Iteration 12117: with minibatch training loss = 0.625 and accuracy of 0.83\n",
      "Iteration 12118: with minibatch training loss = 0.785 and accuracy of 0.75\n",
      "Iteration 12119: with minibatch training loss = 0.816 and accuracy of 0.77\n",
      "Iteration 12120: with minibatch training loss = 0.516 and accuracy of 0.84\n",
      "Iteration 12121: with minibatch training loss = 1.04 and accuracy of 0.67\n",
      "Iteration 12122: with minibatch training loss = 0.622 and accuracy of 0.83\n",
      "Iteration 12123: with minibatch training loss = 0.671 and accuracy of 0.78\n",
      "Iteration 12124: with minibatch training loss = 0.714 and accuracy of 0.8\n",
      "Iteration 12125: with minibatch training loss = 0.705 and accuracy of 0.81\n",
      "Iteration 12126: with minibatch training loss = 0.735 and accuracy of 0.78\n",
      "Iteration 12127: with minibatch training loss = 0.796 and accuracy of 0.8\n",
      "Iteration 12128: with minibatch training loss = 0.573 and accuracy of 0.86\n",
      "Iteration 12129: with minibatch training loss = 0.74 and accuracy of 0.8\n",
      "Iteration 12130: with minibatch training loss = 0.523 and accuracy of 0.84\n",
      "Iteration 12131: with minibatch training loss = 0.627 and accuracy of 0.8\n",
      "Iteration 12132: with minibatch training loss = 0.853 and accuracy of 0.73\n",
      "Iteration 12133: with minibatch training loss = 0.437 and accuracy of 0.89\n",
      "Iteration 12134: with minibatch training loss = 0.805 and accuracy of 0.78\n",
      "Iteration 12135: with minibatch training loss = 0.681 and accuracy of 0.8\n",
      "Iteration 12136: with minibatch training loss = 0.896 and accuracy of 0.73\n",
      "Iteration 12137: with minibatch training loss = 0.884 and accuracy of 0.73\n",
      "Iteration 12138: with minibatch training loss = 0.75 and accuracy of 0.78\n",
      "Iteration 12139: with minibatch training loss = 0.498 and accuracy of 0.88\n",
      "Iteration 12140: with minibatch training loss = 0.715 and accuracy of 0.81\n",
      "Iteration 12141: with minibatch training loss = 0.817 and accuracy of 0.75\n",
      "Iteration 12142: with minibatch training loss = 1.05 and accuracy of 0.67\n",
      "Iteration 12143: with minibatch training loss = 0.672 and accuracy of 0.78\n",
      "Iteration 12144: with minibatch training loss = 0.7 and accuracy of 0.8\n",
      "Iteration 12145: with minibatch training loss = 0.582 and accuracy of 0.83\n",
      "Iteration 12146: with minibatch training loss = 0.652 and accuracy of 0.83\n",
      "Iteration 12147: with minibatch training loss = 1.03 and accuracy of 0.7\n",
      "Iteration 12148: with minibatch training loss = 0.747 and accuracy of 0.78\n",
      "Iteration 12149: with minibatch training loss = 0.618 and accuracy of 0.84\n",
      "Iteration 12150: with minibatch training loss = 0.62 and accuracy of 0.81\n",
      "Iteration 12151: with minibatch training loss = 0.723 and accuracy of 0.8\n",
      "Iteration 12152: with minibatch training loss = 0.739 and accuracy of 0.75\n",
      "Iteration 12153: with minibatch training loss = 0.667 and accuracy of 0.8\n",
      "Iteration 12154: with minibatch training loss = 0.846 and accuracy of 0.77\n",
      "Iteration 12155: with minibatch training loss = 0.539 and accuracy of 0.83\n",
      "Iteration 12156: with minibatch training loss = 0.62 and accuracy of 0.83\n",
      "Iteration 12157: with minibatch training loss = 0.561 and accuracy of 0.83\n",
      "Iteration 12158: with minibatch training loss = 0.656 and accuracy of 0.8\n",
      "Iteration 12159: with minibatch training loss = 0.6 and accuracy of 0.81\n",
      "Iteration 12160: with minibatch training loss = 0.759 and accuracy of 0.77\n",
      "Iteration 12161: with minibatch training loss = 0.474 and accuracy of 0.86\n",
      "Iteration 12162: with minibatch training loss = 0.59 and accuracy of 0.83\n",
      "Iteration 12163: with minibatch training loss = 0.667 and accuracy of 0.8\n",
      "Iteration 12164: with minibatch training loss = 0.908 and accuracy of 0.77\n",
      "Iteration 12165: with minibatch training loss = 0.608 and accuracy of 0.86\n",
      "Iteration 12166: with minibatch training loss = 0.647 and accuracy of 0.8\n",
      "Iteration 12167: with minibatch training loss = 0.867 and accuracy of 0.75\n",
      "Iteration 12168: with minibatch training loss = 0.959 and accuracy of 0.72\n",
      "Iteration 12169: with minibatch training loss = 0.908 and accuracy of 0.72\n",
      "Iteration 12170: with minibatch training loss = 0.8 and accuracy of 0.75\n",
      "Iteration 12171: with minibatch training loss = 0.875 and accuracy of 0.73\n",
      "Iteration 12172: with minibatch training loss = 1.02 and accuracy of 0.67\n",
      "Iteration 12173: with minibatch training loss = 0.957 and accuracy of 0.7\n",
      "Iteration 12174: with minibatch training loss = 0.781 and accuracy of 0.77\n",
      "Iteration 12175: with minibatch training loss = 0.883 and accuracy of 0.75\n",
      "Iteration 12176: with minibatch training loss = 0.672 and accuracy of 0.81\n",
      "Iteration 12177: with minibatch training loss = 0.528 and accuracy of 0.84\n",
      "Iteration 12178: with minibatch training loss = 0.922 and accuracy of 0.72\n",
      "Iteration 12179: with minibatch training loss = 0.905 and accuracy of 0.72\n",
      "Iteration 12180: with minibatch training loss = 0.849 and accuracy of 0.77\n",
      "Iteration 12181: with minibatch training loss = 0.474 and accuracy of 0.84\n",
      "Iteration 12182: with minibatch training loss = 0.834 and accuracy of 0.72\n",
      "Iteration 12183: with minibatch training loss = 0.683 and accuracy of 0.83\n",
      "Iteration 12184: with minibatch training loss = 0.824 and accuracy of 0.75\n",
      "Iteration 12185: with minibatch training loss = 0.722 and accuracy of 0.8\n",
      "Iteration 12186: with minibatch training loss = 0.582 and accuracy of 0.84\n",
      "Iteration 12187: with minibatch training loss = 0.686 and accuracy of 0.77\n",
      "Iteration 12188: with minibatch training loss = 0.509 and accuracy of 0.86\n",
      "Iteration 12189: with minibatch training loss = 0.565 and accuracy of 0.83\n",
      "Iteration 12190: with minibatch training loss = 0.563 and accuracy of 0.84\n",
      "Iteration 12191: with minibatch training loss = 0.767 and accuracy of 0.8\n",
      "Iteration 12192: with minibatch training loss = 0.522 and accuracy of 0.84\n",
      "Iteration 12193: with minibatch training loss = 0.53 and accuracy of 0.86\n",
      "Iteration 12194: with minibatch training loss = 0.906 and accuracy of 0.72\n",
      "Iteration 12195: with minibatch training loss = 0.547 and accuracy of 0.84\n",
      "Iteration 12196: with minibatch training loss = 0.742 and accuracy of 0.8\n",
      "Iteration 12197: with minibatch training loss = 0.952 and accuracy of 0.72\n",
      "Iteration 12198: with minibatch training loss = 0.672 and accuracy of 0.81\n",
      "Iteration 12199: with minibatch training loss = 0.708 and accuracy of 0.8\n",
      "Iteration 12200: with minibatch training loss = 0.974 and accuracy of 0.67\n",
      "Iteration 12201: with minibatch training loss = 0.835 and accuracy of 0.75\n",
      "Iteration 12202: with minibatch training loss = 0.747 and accuracy of 0.77\n",
      "Iteration 12203: with minibatch training loss = 0.699 and accuracy of 0.81\n",
      "Iteration 12204: with minibatch training loss = 0.744 and accuracy of 0.78\n",
      "Iteration 12205: with minibatch training loss = 0.655 and accuracy of 0.84\n",
      "Iteration 12206: with minibatch training loss = 0.798 and accuracy of 0.81\n",
      "Iteration 12207: with minibatch training loss = 0.62 and accuracy of 0.81\n",
      "Iteration 12208: with minibatch training loss = 0.562 and accuracy of 0.81\n",
      "Iteration 12209: with minibatch training loss = 0.674 and accuracy of 0.78\n",
      "Iteration 12210: with minibatch training loss = 0.824 and accuracy of 0.77\n",
      "Iteration 12211: with minibatch training loss = 0.434 and accuracy of 0.89\n",
      "Iteration 12212: with minibatch training loss = 0.851 and accuracy of 0.73\n",
      "Iteration 12213: with minibatch training loss = 0.703 and accuracy of 0.78\n",
      "Iteration 12214: with minibatch training loss = 0.726 and accuracy of 0.78\n",
      "Iteration 12215: with minibatch training loss = 0.607 and accuracy of 0.81\n",
      "Iteration 12216: with minibatch training loss = 0.731 and accuracy of 0.8\n",
      "Iteration 12217: with minibatch training loss = 0.605 and accuracy of 0.84\n",
      "Iteration 12218: with minibatch training loss = 0.843 and accuracy of 0.72\n",
      "Iteration 12219: with minibatch training loss = 0.919 and accuracy of 0.72\n",
      "Iteration 12220: with minibatch training loss = 0.962 and accuracy of 0.77\n",
      "Iteration 12221: with minibatch training loss = 0.789 and accuracy of 0.75\n",
      "Iteration 12222: with minibatch training loss = 0.515 and accuracy of 0.86\n",
      "Iteration 12223: with minibatch training loss = 0.35 and accuracy of 0.89\n",
      "Iteration 12224: with minibatch training loss = 0.614 and accuracy of 0.84\n",
      "Iteration 12225: with minibatch training loss = 0.81 and accuracy of 0.77\n",
      "Iteration 12226: with minibatch training loss = 0.85 and accuracy of 0.73\n",
      "Iteration 12227: with minibatch training loss = 0.66 and accuracy of 0.81\n",
      "Iteration 12228: with minibatch training loss = 0.576 and accuracy of 0.83\n",
      "Iteration 12229: with minibatch training loss = 0.806 and accuracy of 0.75\n",
      "Iteration 12230: with minibatch training loss = 0.42 and accuracy of 0.89\n",
      "Iteration 12231: with minibatch training loss = 0.786 and accuracy of 0.72\n",
      "Iteration 12232: with minibatch training loss = 0.744 and accuracy of 0.77\n",
      "Iteration 12233: with minibatch training loss = 0.535 and accuracy of 0.84\n",
      "Iteration 12234: with minibatch training loss = 0.673 and accuracy of 0.8\n",
      "Iteration 12235: with minibatch training loss = 0.908 and accuracy of 0.77\n",
      "Iteration 12236: with minibatch training loss = 0.932 and accuracy of 0.73\n",
      "Iteration 12237: with minibatch training loss = 0.537 and accuracy of 0.83\n",
      "Iteration 12238: with minibatch training loss = 0.525 and accuracy of 0.84\n",
      "Iteration 12239: with minibatch training loss = 0.962 and accuracy of 0.7\n",
      "Iteration 12240: with minibatch training loss = 0.395 and accuracy of 0.88\n",
      "Iteration 12241: with minibatch training loss = 0.656 and accuracy of 0.81\n",
      "Iteration 12242: with minibatch training loss = 0.783 and accuracy of 0.78\n",
      "Iteration 12243: with minibatch training loss = 0.981 and accuracy of 0.7\n",
      "Iteration 12244: with minibatch training loss = 0.599 and accuracy of 0.8\n",
      "Iteration 12245: with minibatch training loss = 0.927 and accuracy of 0.72\n",
      "Iteration 12246: with minibatch training loss = 0.673 and accuracy of 0.75\n",
      "Iteration 12247: with minibatch training loss = 0.817 and accuracy of 0.73\n",
      "Iteration 12248: with minibatch training loss = 0.676 and accuracy of 0.8\n",
      "Iteration 12249: with minibatch training loss = 0.941 and accuracy of 0.72\n",
      "Iteration 12250: with minibatch training loss = 0.599 and accuracy of 0.83\n",
      "Iteration 12251: with minibatch training loss = 0.774 and accuracy of 0.8\n",
      "Iteration 12252: with minibatch training loss = 0.823 and accuracy of 0.7\n",
      "Iteration 12253: with minibatch training loss = 0.756 and accuracy of 0.77\n",
      "Iteration 12254: with minibatch training loss = 0.59 and accuracy of 0.83\n",
      "Iteration 12255: with minibatch training loss = 0.619 and accuracy of 0.81\n",
      "Iteration 12256: with minibatch training loss = 0.938 and accuracy of 0.69\n",
      "Iteration 12257: with minibatch training loss = 1.12 and accuracy of 0.59\n",
      "Iteration 12258: with minibatch training loss = 0.482 and accuracy of 0.88\n",
      "Iteration 12259: with minibatch training loss = 0.651 and accuracy of 0.8\n",
      "Iteration 12260: with minibatch training loss = 1.07 and accuracy of 0.67\n",
      "Iteration 12261: with minibatch training loss = 0.453 and accuracy of 0.86\n",
      "Iteration 12262: with minibatch training loss = 0.674 and accuracy of 0.81\n",
      "Iteration 12263: with minibatch training loss = 0.768 and accuracy of 0.8\n",
      "Iteration 12264: with minibatch training loss = 0.423 and accuracy of 0.88\n",
      "Iteration 12265: with minibatch training loss = 0.67 and accuracy of 0.81\n",
      "Iteration 12266: with minibatch training loss = 0.779 and accuracy of 0.8\n",
      "Iteration 12267: with minibatch training loss = 0.914 and accuracy of 0.72\n",
      "Iteration 12268: with minibatch training loss = 0.703 and accuracy of 0.8\n",
      "Iteration 12269: with minibatch training loss = 1.16 and accuracy of 0.69\n",
      "Iteration 12270: with minibatch training loss = 0.67 and accuracy of 0.84\n",
      "Iteration 12271: with minibatch training loss = 0.649 and accuracy of 0.84\n",
      "Iteration 12272: with minibatch training loss = 0.76 and accuracy of 0.77\n",
      "Iteration 12273: with minibatch training loss = 0.821 and accuracy of 0.78\n",
      "Iteration 12274: with minibatch training loss = 0.843 and accuracy of 0.72\n",
      "Iteration 12275: with minibatch training loss = 0.885 and accuracy of 0.77\n",
      "Iteration 12276: with minibatch training loss = 0.458 and accuracy of 0.86\n",
      "Iteration 12277: with minibatch training loss = 0.734 and accuracy of 0.77\n",
      "Iteration 12278: with minibatch training loss = 0.6 and accuracy of 0.84\n",
      "Iteration 12279: with minibatch training loss = 0.73 and accuracy of 0.8\n",
      "Iteration 12280: with minibatch training loss = 0.816 and accuracy of 0.73\n",
      "Iteration 12281: with minibatch training loss = 0.703 and accuracy of 0.77\n",
      "Iteration 12282: with minibatch training loss = 0.74 and accuracy of 0.77\n",
      "Iteration 12283: with minibatch training loss = 0.842 and accuracy of 0.73\n",
      "Iteration 12284: with minibatch training loss = 0.591 and accuracy of 0.81\n",
      "Iteration 12285: with minibatch training loss = 0.677 and accuracy of 0.8\n",
      "Iteration 12286: with minibatch training loss = 0.847 and accuracy of 0.7\n",
      "Iteration 12287: with minibatch training loss = 0.442 and accuracy of 0.86\n",
      "Iteration 12288: with minibatch training loss = 0.517 and accuracy of 0.88\n",
      "Iteration 12289: with minibatch training loss = 0.804 and accuracy of 0.78\n",
      "Iteration 12290: with minibatch training loss = 0.627 and accuracy of 0.86\n",
      "Iteration 12291: with minibatch training loss = 1.01 and accuracy of 0.72\n",
      "Iteration 12292: with minibatch training loss = 0.805 and accuracy of 0.78\n",
      "Iteration 12293: with minibatch training loss = 0.697 and accuracy of 0.8\n",
      "Iteration 12294: with minibatch training loss = 0.819 and accuracy of 0.78\n",
      "Iteration 12295: with minibatch training loss = 0.979 and accuracy of 0.72\n",
      "Iteration 12296: with minibatch training loss = 0.623 and accuracy of 0.8\n",
      "Iteration 12297: with minibatch training loss = 0.55 and accuracy of 0.84\n",
      "Iteration 12298: with minibatch training loss = 0.897 and accuracy of 0.73\n",
      "Iteration 12299: with minibatch training loss = 0.755 and accuracy of 0.78\n",
      "Iteration 12300: with minibatch training loss = 0.618 and accuracy of 0.84\n",
      "Iteration 12301: with minibatch training loss = 0.8 and accuracy of 0.75\n",
      "Iteration 12302: with minibatch training loss = 0.816 and accuracy of 0.73\n",
      "Iteration 12303: with minibatch training loss = 0.6 and accuracy of 0.83\n",
      "Iteration 12304: with minibatch training loss = 0.722 and accuracy of 0.8\n",
      "Iteration 12305: with minibatch training loss = 0.542 and accuracy of 0.83\n",
      "Iteration 12306: with minibatch training loss = 0.969 and accuracy of 0.72\n",
      "Iteration 12307: with minibatch training loss = 0.713 and accuracy of 0.78\n",
      "Iteration 12308: with minibatch training loss = 0.567 and accuracy of 0.83\n",
      "Iteration 12309: with minibatch training loss = 0.77 and accuracy of 0.75\n",
      "Iteration 12310: with minibatch training loss = 0.526 and accuracy of 0.88\n",
      "Iteration 12311: with minibatch training loss = 0.626 and accuracy of 0.8\n",
      "Iteration 12312: with minibatch training loss = 0.736 and accuracy of 0.81\n",
      "Iteration 12313: with minibatch training loss = 0.653 and accuracy of 0.81\n",
      "Iteration 12314: with minibatch training loss = 0.722 and accuracy of 0.8\n",
      "Iteration 12315: with minibatch training loss = 0.944 and accuracy of 0.72\n",
      "Iteration 12316: with minibatch training loss = 0.65 and accuracy of 0.83\n",
      "Iteration 12317: with minibatch training loss = 0.628 and accuracy of 0.84\n",
      "Iteration 12318: with minibatch training loss = 0.822 and accuracy of 0.73\n",
      "Iteration 12319: with minibatch training loss = 0.848 and accuracy of 0.73\n",
      "Iteration 12320: with minibatch training loss = 0.737 and accuracy of 0.83\n",
      "Iteration 12321: with minibatch training loss = 0.492 and accuracy of 0.84\n",
      "Iteration 12322: with minibatch training loss = 0.692 and accuracy of 0.78\n",
      "Iteration 12323: with minibatch training loss = 0.701 and accuracy of 0.81\n",
      "Iteration 12324: with minibatch training loss = 0.813 and accuracy of 0.78\n",
      "Iteration 12325: with minibatch training loss = 0.693 and accuracy of 0.83\n",
      "Iteration 12326: with minibatch training loss = 0.726 and accuracy of 0.77\n",
      "Iteration 12327: with minibatch training loss = 1.16 and accuracy of 0.64\n",
      "Iteration 12328: with minibatch training loss = 0.747 and accuracy of 0.8\n",
      "Iteration 12329: with minibatch training loss = 0.711 and accuracy of 0.78\n",
      "Iteration 12330: with minibatch training loss = 0.556 and accuracy of 0.84\n",
      "Iteration 12331: with minibatch training loss = 0.621 and accuracy of 0.81\n",
      "Iteration 12332: with minibatch training loss = 0.801 and accuracy of 0.78\n",
      "Iteration 12333: with minibatch training loss = 0.81 and accuracy of 0.77\n",
      "Iteration 12334: with minibatch training loss = 0.579 and accuracy of 0.84\n",
      "Iteration 12335: with minibatch training loss = 0.481 and accuracy of 0.86\n",
      "Iteration 12336: with minibatch training loss = 0.613 and accuracy of 0.8\n",
      "Iteration 12337: with minibatch training loss = 0.716 and accuracy of 0.78\n",
      "Iteration 12338: with minibatch training loss = 0.552 and accuracy of 0.83\n",
      "Iteration 12339: with minibatch training loss = 0.554 and accuracy of 0.81\n",
      "Iteration 12340: with minibatch training loss = 0.679 and accuracy of 0.8\n",
      "Iteration 12341: with minibatch training loss = 0.683 and accuracy of 0.81\n",
      "Iteration 12342: with minibatch training loss = 0.676 and accuracy of 0.81\n",
      "Iteration 12343: with minibatch training loss = 0.849 and accuracy of 0.73\n",
      "Iteration 12344: with minibatch training loss = 0.685 and accuracy of 0.8\n",
      "Iteration 12345: with minibatch training loss = 0.587 and accuracy of 0.83\n",
      "Iteration 12346: with minibatch training loss = 0.579 and accuracy of 0.81\n",
      "Iteration 12347: with minibatch training loss = 0.58 and accuracy of 0.83\n",
      "Iteration 12348: with minibatch training loss = 0.84 and accuracy of 0.73\n",
      "Iteration 12349: with minibatch training loss = 0.959 and accuracy of 0.69\n",
      "Iteration 12350: with minibatch training loss = 0.668 and accuracy of 0.78\n",
      "Iteration 12351: with minibatch training loss = 0.648 and accuracy of 0.8\n",
      "Iteration 12352: with minibatch training loss = 0.789 and accuracy of 0.77\n",
      "Iteration 12353: with minibatch training loss = 0.634 and accuracy of 0.83\n",
      "Iteration 12354: with minibatch training loss = 0.759 and accuracy of 0.8\n",
      "Iteration 12355: with minibatch training loss = 0.922 and accuracy of 0.73\n",
      "Iteration 12356: with minibatch training loss = 0.575 and accuracy of 0.81\n",
      "Iteration 12357: with minibatch training loss = 0.725 and accuracy of 0.77\n",
      "Iteration 12358: with minibatch training loss = 0.717 and accuracy of 0.77\n",
      "Iteration 12359: with minibatch training loss = 0.602 and accuracy of 0.83\n",
      "Iteration 12360: with minibatch training loss = 0.74 and accuracy of 0.78\n",
      "Iteration 12361: with minibatch training loss = 0.743 and accuracy of 0.77\n",
      "Iteration 12362: with minibatch training loss = 0.788 and accuracy of 0.78\n",
      "Iteration 12363: with minibatch training loss = 0.667 and accuracy of 0.83\n",
      "Iteration 12364: with minibatch training loss = 0.908 and accuracy of 0.72\n",
      "Iteration 12365: with minibatch training loss = 0.615 and accuracy of 0.81\n",
      "Iteration 12366: with minibatch training loss = 0.664 and accuracy of 0.84\n",
      "Iteration 12367: with minibatch training loss = 0.672 and accuracy of 0.84\n",
      "Iteration 12368: with minibatch training loss = 0.606 and accuracy of 0.83\n",
      "Iteration 12369: with minibatch training loss = 0.851 and accuracy of 0.72\n",
      "Iteration 12370: with minibatch training loss = 0.7 and accuracy of 0.78\n",
      "Iteration 12371: with minibatch training loss = 0.619 and accuracy of 0.81\n",
      "Iteration 12372: with minibatch training loss = 0.583 and accuracy of 0.83\n",
      "Iteration 12373: with minibatch training loss = 0.936 and accuracy of 0.75\n",
      "Iteration 12374: with minibatch training loss = 0.728 and accuracy of 0.77\n",
      "Iteration 12375: with minibatch training loss = 0.904 and accuracy of 0.7\n",
      "Iteration 12376: with minibatch training loss = 0.603 and accuracy of 0.86\n",
      "Iteration 12377: with minibatch training loss = 0.912 and accuracy of 0.7\n",
      "Iteration 12378: with minibatch training loss = 0.844 and accuracy of 0.72\n",
      "Iteration 12379: with minibatch training loss = 0.621 and accuracy of 0.81\n",
      "Iteration 12380: with minibatch training loss = 0.804 and accuracy of 0.73\n",
      "Iteration 12381: with minibatch training loss = 0.987 and accuracy of 0.7\n",
      "Iteration 12382: with minibatch training loss = 0.69 and accuracy of 0.78\n",
      "Iteration 12383: with minibatch training loss = 0.905 and accuracy of 0.72\n",
      "Iteration 12384: with minibatch training loss = 0.58 and accuracy of 0.8\n",
      "Iteration 12385: with minibatch training loss = 0.703 and accuracy of 0.78\n",
      "Iteration 12386: with minibatch training loss = 0.615 and accuracy of 0.83\n",
      "Iteration 12387: with minibatch training loss = 0.737 and accuracy of 0.81\n",
      "Iteration 12388: with minibatch training loss = 0.883 and accuracy of 0.73\n",
      "Iteration 12389: with minibatch training loss = 1.04 and accuracy of 0.67\n",
      "Iteration 12390: with minibatch training loss = 0.798 and accuracy of 0.77\n",
      "Iteration 12391: with minibatch training loss = 0.819 and accuracy of 0.77\n",
      "Iteration 12392: with minibatch training loss = 0.453 and accuracy of 0.88\n",
      "Iteration 12393: with minibatch training loss = 0.654 and accuracy of 0.8\n",
      "Iteration 12394: with minibatch training loss = 0.849 and accuracy of 0.73\n",
      "Iteration 12395: with minibatch training loss = 0.743 and accuracy of 0.8\n",
      "Iteration 12396: with minibatch training loss = 0.775 and accuracy of 0.78\n",
      "Iteration 12397: with minibatch training loss = 0.729 and accuracy of 0.78\n",
      "Iteration 12398: with minibatch training loss = 0.674 and accuracy of 0.83\n",
      "Iteration 12399: with minibatch training loss = 0.829 and accuracy of 0.75\n",
      "Iteration 12400: with minibatch training loss = 0.787 and accuracy of 0.77\n",
      "Iteration 12401: with minibatch training loss = 0.796 and accuracy of 0.77\n",
      "Iteration 12402: with minibatch training loss = 0.585 and accuracy of 0.81\n",
      "Iteration 12403: with minibatch training loss = 0.679 and accuracy of 0.8\n",
      "Iteration 12404: with minibatch training loss = 0.626 and accuracy of 0.78\n",
      "Iteration 12405: with minibatch training loss = 0.514 and accuracy of 0.84\n",
      "Iteration 12406: with minibatch training loss = 0.638 and accuracy of 0.8\n",
      "Iteration 12407: with minibatch training loss = 0.388 and accuracy of 0.89\n",
      "Iteration 12408: with minibatch training loss = 0.807 and accuracy of 0.75\n",
      "Iteration 12409: with minibatch training loss = 0.852 and accuracy of 0.75\n",
      "Iteration 12410: with minibatch training loss = 0.769 and accuracy of 0.81\n",
      "Iteration 12411: with minibatch training loss = 0.774 and accuracy of 0.81\n",
      "Iteration 12412: with minibatch training loss = 0.964 and accuracy of 0.72\n",
      "Iteration 12413: with minibatch training loss = 0.608 and accuracy of 0.83\n",
      "Iteration 12414: with minibatch training loss = 0.567 and accuracy of 0.84\n",
      "Iteration 12415: with minibatch training loss = 0.603 and accuracy of 0.86\n",
      "Iteration 12416: with minibatch training loss = 0.665 and accuracy of 0.78\n",
      "Iteration 12417: with minibatch training loss = 0.876 and accuracy of 0.73\n",
      "Iteration 12418: with minibatch training loss = 0.66 and accuracy of 0.78\n",
      "Iteration 12419: with minibatch training loss = 0.887 and accuracy of 0.73\n",
      "Iteration 12420: with minibatch training loss = 0.64 and accuracy of 0.83\n",
      "Iteration 12421: with minibatch training loss = 0.617 and accuracy of 0.83\n",
      "Iteration 12422: with minibatch training loss = 0.845 and accuracy of 0.75\n",
      "Iteration 12423: with minibatch training loss = 0.678 and accuracy of 0.83\n",
      "Iteration 12424: with minibatch training loss = 0.655 and accuracy of 0.83\n",
      "Iteration 12425: with minibatch training loss = 0.825 and accuracy of 0.75\n",
      "Iteration 12426: with minibatch training loss = 0.687 and accuracy of 0.81\n",
      "Iteration 12427: with minibatch training loss = 0.465 and accuracy of 0.88\n",
      "Iteration 12428: with minibatch training loss = 0.633 and accuracy of 0.8\n",
      "Iteration 12429: with minibatch training loss = 0.775 and accuracy of 0.78\n",
      "Iteration 12430: with minibatch training loss = 0.555 and accuracy of 0.84\n",
      "Iteration 12431: with minibatch training loss = 1.06 and accuracy of 0.7\n",
      "Iteration 12432: with minibatch training loss = 0.579 and accuracy of 0.81\n",
      "Iteration 12433: with minibatch training loss = 0.798 and accuracy of 0.78\n",
      "Iteration 12434: with minibatch training loss = 1.23 and accuracy of 0.66\n",
      "Iteration 12435: with minibatch training loss = 0.741 and accuracy of 0.77\n",
      "Iteration 12436: with minibatch training loss = 0.825 and accuracy of 0.73\n",
      "Iteration 12437: with minibatch training loss = 0.58 and accuracy of 0.84\n",
      "Iteration 12438: with minibatch training loss = 0.789 and accuracy of 0.83\n",
      "Iteration 12439: with minibatch training loss = 0.604 and accuracy of 0.8\n",
      "Iteration 12440: with minibatch training loss = 0.646 and accuracy of 0.81\n",
      "Iteration 12441: with minibatch training loss = 0.627 and accuracy of 0.81\n",
      "Iteration 12442: with minibatch training loss = 0.819 and accuracy of 0.75\n",
      "Iteration 12443: with minibatch training loss = 0.923 and accuracy of 0.7\n",
      "Iteration 12444: with minibatch training loss = 0.572 and accuracy of 0.8\n",
      "Iteration 12445: with minibatch training loss = 0.713 and accuracy of 0.75\n",
      "Iteration 12446: with minibatch training loss = 0.648 and accuracy of 0.81\n",
      "Iteration 12447: with minibatch training loss = 1.01 and accuracy of 0.69\n",
      "Iteration 12448: with minibatch training loss = 0.941 and accuracy of 0.7\n",
      "Iteration 12449: with minibatch training loss = 0.89 and accuracy of 0.73\n",
      "Iteration 12450: with minibatch training loss = 0.711 and accuracy of 0.78\n",
      "Iteration 12451: with minibatch training loss = 0.614 and accuracy of 0.8\n",
      "Iteration 12452: with minibatch training loss = 0.718 and accuracy of 0.8\n",
      "Iteration 12453: with minibatch training loss = 0.832 and accuracy of 0.75\n",
      "Iteration 12454: with minibatch training loss = 1.08 and accuracy of 0.72\n",
      "Iteration 12455: with minibatch training loss = 0.899 and accuracy of 0.72\n",
      "Iteration 12456: with minibatch training loss = 0.708 and accuracy of 0.78\n",
      "Iteration 12457: with minibatch training loss = 1.04 and accuracy of 0.66\n",
      "Iteration 12458: with minibatch training loss = 0.496 and accuracy of 0.84\n",
      "Iteration 12459: with minibatch training loss = 0.548 and accuracy of 0.83\n",
      "Iteration 12460: with minibatch training loss = 0.777 and accuracy of 0.77\n",
      "Iteration 12461: with minibatch training loss = 0.835 and accuracy of 0.73\n",
      "Iteration 12462: with minibatch training loss = 0.833 and accuracy of 0.77\n",
      "Iteration 12463: with minibatch training loss = 0.85 and accuracy of 0.73\n",
      "Iteration 12464: with minibatch training loss = 0.886 and accuracy of 0.69\n",
      "Iteration 12465: with minibatch training loss = 0.606 and accuracy of 0.8\n",
      "Iteration 12466: with minibatch training loss = 0.35 and accuracy of 0.89\n",
      "Iteration 12467: with minibatch training loss = 0.909 and accuracy of 0.7\n",
      "Iteration 12468: with minibatch training loss = 0.84 and accuracy of 0.75\n",
      "Iteration 12469: with minibatch training loss = 0.565 and accuracy of 0.88\n",
      "Iteration 12470: with minibatch training loss = 0.476 and accuracy of 0.88\n",
      "Iteration 12471: with minibatch training loss = 0.905 and accuracy of 0.72\n",
      "Iteration 12472: with minibatch training loss = 0.825 and accuracy of 0.73\n",
      "Iteration 12473: with minibatch training loss = 0.581 and accuracy of 0.83\n",
      "Iteration 12474: with minibatch training loss = 0.87 and accuracy of 0.73\n",
      "Iteration 12475: with minibatch training loss = 0.625 and accuracy of 0.81\n",
      "Iteration 12476: with minibatch training loss = 0.686 and accuracy of 0.8\n",
      "Iteration 12477: with minibatch training loss = 0.583 and accuracy of 0.81\n",
      "Iteration 12478: with minibatch training loss = 0.667 and accuracy of 0.78\n",
      "Iteration 12479: with minibatch training loss = 0.876 and accuracy of 0.72\n",
      "Iteration 12480: with minibatch training loss = 0.742 and accuracy of 0.77\n",
      "Iteration 12481: with minibatch training loss = 0.76 and accuracy of 0.77\n",
      "Iteration 12482: with minibatch training loss = 0.43 and accuracy of 0.88\n",
      "Iteration 12483: with minibatch training loss = 0.883 and accuracy of 0.73\n",
      "Iteration 12484: with minibatch training loss = 0.757 and accuracy of 0.78\n",
      "Iteration 12485: with minibatch training loss = 0.757 and accuracy of 0.73\n",
      "Iteration 12486: with minibatch training loss = 0.907 and accuracy of 0.73\n",
      "Iteration 12487: with minibatch training loss = 0.574 and accuracy of 0.8\n",
      "Iteration 12488: with minibatch training loss = 0.652 and accuracy of 0.81\n",
      "Iteration 12489: with minibatch training loss = 0.928 and accuracy of 0.72\n",
      "Iteration 12490: with minibatch training loss = 0.878 and accuracy of 0.73\n",
      "Validation loss: 0.26147935\n",
      "Epoch 9, Overall loss = 0.723 and accuracy of 0.786\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1210769e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12492: with minibatch training loss = 0.521 and accuracy of 0.84\n",
      "Iteration 12493: with minibatch training loss = 0.634 and accuracy of 0.84\n",
      "Iteration 12494: with minibatch training loss = 0.641 and accuracy of 0.8\n",
      "Iteration 12495: with minibatch training loss = 0.947 and accuracy of 0.75\n",
      "Iteration 12496: with minibatch training loss = 0.853 and accuracy of 0.77\n",
      "Iteration 12497: with minibatch training loss = 0.625 and accuracy of 0.83\n",
      "Iteration 12498: with minibatch training loss = 0.789 and accuracy of 0.77\n",
      "Iteration 12499: with minibatch training loss = 0.61 and accuracy of 0.81\n",
      "Iteration 12500: with minibatch training loss = 0.632 and accuracy of 0.81\n",
      "Iteration 12501: with minibatch training loss = 1.13 and accuracy of 0.64\n",
      "Iteration 12502: with minibatch training loss = 1.02 and accuracy of 0.67\n",
      "Iteration 12503: with minibatch training loss = 0.612 and accuracy of 0.84\n",
      "Iteration 12504: with minibatch training loss = 0.609 and accuracy of 0.83\n",
      "Iteration 12505: with minibatch training loss = 0.692 and accuracy of 0.81\n",
      "Iteration 12506: with minibatch training loss = 0.649 and accuracy of 0.81\n",
      "Iteration 12507: with minibatch training loss = 0.62 and accuracy of 0.8\n",
      "Iteration 12508: with minibatch training loss = 0.877 and accuracy of 0.73\n",
      "Iteration 12509: with minibatch training loss = 0.568 and accuracy of 0.83\n",
      "Iteration 12510: with minibatch training loss = 0.617 and accuracy of 0.81\n",
      "Iteration 12511: with minibatch training loss = 0.655 and accuracy of 0.83\n",
      "Iteration 12512: with minibatch training loss = 0.728 and accuracy of 0.8\n",
      "Iteration 12513: with minibatch training loss = 0.528 and accuracy of 0.81\n",
      "Iteration 12514: with minibatch training loss = 0.659 and accuracy of 0.78\n",
      "Iteration 12515: with minibatch training loss = 0.716 and accuracy of 0.77\n",
      "Iteration 12516: with minibatch training loss = 0.64 and accuracy of 0.83\n",
      "Iteration 12517: with minibatch training loss = 0.407 and accuracy of 0.88\n",
      "Iteration 12518: with minibatch training loss = 0.768 and accuracy of 0.78\n",
      "Iteration 12519: with minibatch training loss = 0.763 and accuracy of 0.78\n",
      "Iteration 12520: with minibatch training loss = 0.877 and accuracy of 0.72\n",
      "Iteration 12521: with minibatch training loss = 0.931 and accuracy of 0.72\n",
      "Iteration 12522: with minibatch training loss = 0.979 and accuracy of 0.73\n",
      "Iteration 12523: with minibatch training loss = 1.03 and accuracy of 0.67\n",
      "Iteration 12524: with minibatch training loss = 0.602 and accuracy of 0.83\n",
      "Iteration 12525: with minibatch training loss = 0.545 and accuracy of 0.83\n",
      "Iteration 12526: with minibatch training loss = 0.725 and accuracy of 0.78\n",
      "Iteration 12527: with minibatch training loss = 0.833 and accuracy of 0.75\n",
      "Iteration 12528: with minibatch training loss = 0.67 and accuracy of 0.81\n",
      "Iteration 12529: with minibatch training loss = 0.651 and accuracy of 0.84\n",
      "Iteration 12530: with minibatch training loss = 0.717 and accuracy of 0.78\n",
      "Iteration 12531: with minibatch training loss = 1.12 and accuracy of 0.67\n",
      "Iteration 12532: with minibatch training loss = 0.793 and accuracy of 0.77\n",
      "Iteration 12533: with minibatch training loss = 0.619 and accuracy of 0.83\n",
      "Iteration 12534: with minibatch training loss = 0.485 and accuracy of 0.88\n",
      "Iteration 12535: with minibatch training loss = 0.647 and accuracy of 0.78\n",
      "Iteration 12536: with minibatch training loss = 0.637 and accuracy of 0.83\n",
      "Iteration 12537: with minibatch training loss = 0.573 and accuracy of 0.83\n",
      "Iteration 12538: with minibatch training loss = 0.566 and accuracy of 0.81\n",
      "Iteration 12539: with minibatch training loss = 0.69 and accuracy of 0.77\n",
      "Iteration 12540: with minibatch training loss = 0.796 and accuracy of 0.78\n",
      "Iteration 12541: with minibatch training loss = 0.841 and accuracy of 0.72\n",
      "Iteration 12542: with minibatch training loss = 0.705 and accuracy of 0.8\n",
      "Iteration 12543: with minibatch training loss = 0.678 and accuracy of 0.78\n",
      "Iteration 12544: with minibatch training loss = 0.558 and accuracy of 0.83\n",
      "Iteration 12545: with minibatch training loss = 0.697 and accuracy of 0.78\n",
      "Iteration 12546: with minibatch training loss = 0.853 and accuracy of 0.77\n",
      "Iteration 12547: with minibatch training loss = 0.861 and accuracy of 0.73\n",
      "Iteration 12548: with minibatch training loss = 0.776 and accuracy of 0.78\n",
      "Iteration 12549: with minibatch training loss = 0.766 and accuracy of 0.78\n",
      "Iteration 12550: with minibatch training loss = 0.844 and accuracy of 0.75\n",
      "Iteration 12551: with minibatch training loss = 0.854 and accuracy of 0.77\n",
      "Iteration 12552: with minibatch training loss = 0.711 and accuracy of 0.81\n",
      "Iteration 12553: with minibatch training loss = 0.655 and accuracy of 0.8\n",
      "Iteration 12554: with minibatch training loss = 0.784 and accuracy of 0.77\n",
      "Iteration 12555: with minibatch training loss = 0.621 and accuracy of 0.81\n",
      "Iteration 12556: with minibatch training loss = 0.724 and accuracy of 0.8\n",
      "Iteration 12557: with minibatch training loss = 0.59 and accuracy of 0.81\n",
      "Iteration 12558: with minibatch training loss = 0.621 and accuracy of 0.81\n",
      "Iteration 12559: with minibatch training loss = 0.592 and accuracy of 0.81\n",
      "Iteration 12560: with minibatch training loss = 0.592 and accuracy of 0.83\n",
      "Iteration 12561: with minibatch training loss = 0.902 and accuracy of 0.73\n",
      "Iteration 12562: with minibatch training loss = 0.704 and accuracy of 0.81\n",
      "Iteration 12563: with minibatch training loss = 0.566 and accuracy of 0.83\n",
      "Iteration 12564: with minibatch training loss = 0.711 and accuracy of 0.81\n",
      "Iteration 12565: with minibatch training loss = 0.594 and accuracy of 0.81\n",
      "Iteration 12566: with minibatch training loss = 0.59 and accuracy of 0.81\n",
      "Iteration 12567: with minibatch training loss = 0.61 and accuracy of 0.83\n",
      "Iteration 12568: with minibatch training loss = 0.735 and accuracy of 0.78\n",
      "Iteration 12569: with minibatch training loss = 0.826 and accuracy of 0.75\n",
      "Iteration 12570: with minibatch training loss = 0.779 and accuracy of 0.8\n",
      "Iteration 12571: with minibatch training loss = 0.541 and accuracy of 0.81\n",
      "Iteration 12572: with minibatch training loss = 0.835 and accuracy of 0.75\n",
      "Iteration 12573: with minibatch training loss = 0.477 and accuracy of 0.86\n",
      "Iteration 12574: with minibatch training loss = 0.821 and accuracy of 0.73\n",
      "Iteration 12575: with minibatch training loss = 0.642 and accuracy of 0.81\n",
      "Iteration 12576: with minibatch training loss = 0.599 and accuracy of 0.8\n",
      "Iteration 12577: with minibatch training loss = 1.01 and accuracy of 0.7\n",
      "Iteration 12578: with minibatch training loss = 0.697 and accuracy of 0.78\n",
      "Iteration 12579: with minibatch training loss = 0.8 and accuracy of 0.78\n",
      "Iteration 12580: with minibatch training loss = 0.708 and accuracy of 0.78\n",
      "Iteration 12581: with minibatch training loss = 0.85 and accuracy of 0.75\n",
      "Iteration 12582: with minibatch training loss = 0.705 and accuracy of 0.78\n",
      "Iteration 12583: with minibatch training loss = 0.776 and accuracy of 0.75\n",
      "Iteration 12584: with minibatch training loss = 0.499 and accuracy of 0.86\n",
      "Iteration 12585: with minibatch training loss = 0.557 and accuracy of 0.83\n",
      "Iteration 12586: with minibatch training loss = 0.588 and accuracy of 0.81\n",
      "Iteration 12587: with minibatch training loss = 0.666 and accuracy of 0.78\n",
      "Iteration 12588: with minibatch training loss = 0.745 and accuracy of 0.75\n",
      "Iteration 12589: with minibatch training loss = 0.65 and accuracy of 0.78\n",
      "Iteration 12590: with minibatch training loss = 0.803 and accuracy of 0.78\n",
      "Iteration 12591: with minibatch training loss = 0.942 and accuracy of 0.72\n",
      "Iteration 12592: with minibatch training loss = 0.817 and accuracy of 0.77\n",
      "Iteration 12593: with minibatch training loss = 0.699 and accuracy of 0.81\n",
      "Iteration 12594: with minibatch training loss = 0.711 and accuracy of 0.77\n",
      "Iteration 12595: with minibatch training loss = 1.1 and accuracy of 0.69\n",
      "Iteration 12596: with minibatch training loss = 0.578 and accuracy of 0.8\n",
      "Iteration 12597: with minibatch training loss = 0.462 and accuracy of 0.89\n",
      "Iteration 12598: with minibatch training loss = 0.917 and accuracy of 0.7\n",
      "Iteration 12599: with minibatch training loss = 0.673 and accuracy of 0.78\n",
      "Iteration 12600: with minibatch training loss = 0.71 and accuracy of 0.77\n",
      "Iteration 12601: with minibatch training loss = 0.606 and accuracy of 0.83\n",
      "Iteration 12602: with minibatch training loss = 0.578 and accuracy of 0.83\n",
      "Iteration 12603: with minibatch training loss = 0.683 and accuracy of 0.81\n",
      "Iteration 12604: with minibatch training loss = 0.551 and accuracy of 0.88\n",
      "Iteration 12605: with minibatch training loss = 0.62 and accuracy of 0.86\n",
      "Iteration 12606: with minibatch training loss = 0.405 and accuracy of 0.88\n",
      "Iteration 12607: with minibatch training loss = 0.851 and accuracy of 0.72\n",
      "Iteration 12608: with minibatch training loss = 0.664 and accuracy of 0.8\n",
      "Iteration 12609: with minibatch training loss = 1.22 and accuracy of 0.64\n",
      "Iteration 12610: with minibatch training loss = 0.737 and accuracy of 0.77\n",
      "Iteration 12611: with minibatch training loss = 0.527 and accuracy of 0.84\n",
      "Iteration 12612: with minibatch training loss = 0.344 and accuracy of 0.92\n",
      "Iteration 12613: with minibatch training loss = 0.603 and accuracy of 0.84\n",
      "Iteration 12614: with minibatch training loss = 0.643 and accuracy of 0.81\n",
      "Iteration 12615: with minibatch training loss = 0.737 and accuracy of 0.78\n",
      "Iteration 12616: with minibatch training loss = 0.631 and accuracy of 0.84\n",
      "Iteration 12617: with minibatch training loss = 0.563 and accuracy of 0.81\n",
      "Iteration 12618: with minibatch training loss = 0.9 and accuracy of 0.75\n",
      "Iteration 12619: with minibatch training loss = 0.791 and accuracy of 0.75\n",
      "Iteration 12620: with minibatch training loss = 0.961 and accuracy of 0.7\n",
      "Iteration 12621: with minibatch training loss = 1.05 and accuracy of 0.67\n",
      "Iteration 12622: with minibatch training loss = 0.69 and accuracy of 0.8\n",
      "Iteration 12623: with minibatch training loss = 0.373 and accuracy of 0.89\n",
      "Iteration 12624: with minibatch training loss = 0.927 and accuracy of 0.7\n",
      "Iteration 12625: with minibatch training loss = 0.629 and accuracy of 0.81\n",
      "Iteration 12626: with minibatch training loss = 0.921 and accuracy of 0.72\n",
      "Iteration 12627: with minibatch training loss = 0.398 and accuracy of 0.89\n",
      "Iteration 12628: with minibatch training loss = 1.06 and accuracy of 0.69\n",
      "Iteration 12629: with minibatch training loss = 0.758 and accuracy of 0.77\n",
      "Iteration 12630: with minibatch training loss = 0.741 and accuracy of 0.77\n",
      "Iteration 12631: with minibatch training loss = 0.485 and accuracy of 0.86\n",
      "Iteration 12632: with minibatch training loss = 0.815 and accuracy of 0.75\n",
      "Iteration 12633: with minibatch training loss = 0.813 and accuracy of 0.77\n",
      "Iteration 12634: with minibatch training loss = 0.823 and accuracy of 0.73\n",
      "Iteration 12635: with minibatch training loss = 0.529 and accuracy of 0.83\n",
      "Iteration 12636: with minibatch training loss = 0.801 and accuracy of 0.73\n",
      "Iteration 12637: with minibatch training loss = 0.707 and accuracy of 0.81\n",
      "Iteration 12638: with minibatch training loss = 0.671 and accuracy of 0.83\n",
      "Iteration 12639: with minibatch training loss = 0.872 and accuracy of 0.77\n",
      "Iteration 12640: with minibatch training loss = 0.675 and accuracy of 0.78\n",
      "Iteration 12641: with minibatch training loss = 0.735 and accuracy of 0.77\n",
      "Iteration 12642: with minibatch training loss = 0.603 and accuracy of 0.81\n",
      "Iteration 12643: with minibatch training loss = 0.668 and accuracy of 0.8\n",
      "Iteration 12644: with minibatch training loss = 0.788 and accuracy of 0.78\n",
      "Iteration 12645: with minibatch training loss = 0.696 and accuracy of 0.8\n",
      "Iteration 12646: with minibatch training loss = 0.628 and accuracy of 0.8\n",
      "Iteration 12647: with minibatch training loss = 0.622 and accuracy of 0.81\n",
      "Iteration 12648: with minibatch training loss = 0.574 and accuracy of 0.84\n",
      "Iteration 12649: with minibatch training loss = 0.668 and accuracy of 0.8\n",
      "Iteration 12650: with minibatch training loss = 1.02 and accuracy of 0.67\n",
      "Iteration 12651: with minibatch training loss = 0.683 and accuracy of 0.78\n",
      "Iteration 12652: with minibatch training loss = 0.445 and accuracy of 0.88\n",
      "Iteration 12653: with minibatch training loss = 0.745 and accuracy of 0.8\n",
      "Iteration 12654: with minibatch training loss = 0.767 and accuracy of 0.8\n",
      "Iteration 12655: with minibatch training loss = 0.507 and accuracy of 0.86\n",
      "Iteration 12656: with minibatch training loss = 0.474 and accuracy of 0.83\n",
      "Iteration 12657: with minibatch training loss = 0.639 and accuracy of 0.81\n",
      "Iteration 12658: with minibatch training loss = 0.733 and accuracy of 0.8\n",
      "Iteration 12659: with minibatch training loss = 0.641 and accuracy of 0.84\n",
      "Iteration 12660: with minibatch training loss = 0.854 and accuracy of 0.73\n",
      "Iteration 12661: with minibatch training loss = 0.816 and accuracy of 0.75\n",
      "Iteration 12662: with minibatch training loss = 0.816 and accuracy of 0.77\n",
      "Iteration 12663: with minibatch training loss = 0.914 and accuracy of 0.69\n",
      "Iteration 12664: with minibatch training loss = 0.481 and accuracy of 0.88\n",
      "Iteration 12665: with minibatch training loss = 0.669 and accuracy of 0.83\n",
      "Iteration 12666: with minibatch training loss = 0.661 and accuracy of 0.8\n",
      "Iteration 12667: with minibatch training loss = 0.881 and accuracy of 0.72\n",
      "Iteration 12668: with minibatch training loss = 0.638 and accuracy of 0.77\n",
      "Iteration 12669: with minibatch training loss = 0.576 and accuracy of 0.83\n",
      "Iteration 12670: with minibatch training loss = 0.762 and accuracy of 0.8\n",
      "Iteration 12671: with minibatch training loss = 0.657 and accuracy of 0.83\n",
      "Iteration 12672: with minibatch training loss = 0.688 and accuracy of 0.81\n",
      "Iteration 12673: with minibatch training loss = 0.747 and accuracy of 0.78\n",
      "Iteration 12674: with minibatch training loss = 0.643 and accuracy of 0.8\n",
      "Iteration 12675: with minibatch training loss = 0.478 and accuracy of 0.84\n",
      "Iteration 12676: with minibatch training loss = 0.68 and accuracy of 0.81\n",
      "Iteration 12677: with minibatch training loss = 0.742 and accuracy of 0.78\n",
      "Iteration 12678: with minibatch training loss = 0.771 and accuracy of 0.75\n",
      "Iteration 12679: with minibatch training loss = 0.503 and accuracy of 0.86\n",
      "Iteration 12680: with minibatch training loss = 0.804 and accuracy of 0.8\n",
      "Iteration 12681: with minibatch training loss = 0.558 and accuracy of 0.83\n",
      "Iteration 12682: with minibatch training loss = 0.789 and accuracy of 0.77\n",
      "Iteration 12683: with minibatch training loss = 0.497 and accuracy of 0.88\n",
      "Iteration 12684: with minibatch training loss = 0.738 and accuracy of 0.78\n",
      "Iteration 12685: with minibatch training loss = 0.979 and accuracy of 0.7\n",
      "Iteration 12686: with minibatch training loss = 0.713 and accuracy of 0.78\n",
      "Iteration 12687: with minibatch training loss = 0.881 and accuracy of 0.73\n",
      "Iteration 12688: with minibatch training loss = 0.68 and accuracy of 0.81\n",
      "Iteration 12689: with minibatch training loss = 0.427 and accuracy of 0.88\n",
      "Iteration 12690: with minibatch training loss = 0.718 and accuracy of 0.8\n",
      "Iteration 12691: with minibatch training loss = 0.61 and accuracy of 0.81\n",
      "Iteration 12692: with minibatch training loss = 0.798 and accuracy of 0.8\n",
      "Iteration 12693: with minibatch training loss = 0.913 and accuracy of 0.7\n",
      "Iteration 12694: with minibatch training loss = 0.644 and accuracy of 0.81\n",
      "Iteration 12695: with minibatch training loss = 0.816 and accuracy of 0.78\n",
      "Iteration 12696: with minibatch training loss = 0.512 and accuracy of 0.84\n",
      "Iteration 12697: with minibatch training loss = 0.693 and accuracy of 0.78\n",
      "Iteration 12698: with minibatch training loss = 0.857 and accuracy of 0.75\n",
      "Iteration 12699: with minibatch training loss = 0.969 and accuracy of 0.72\n",
      "Iteration 12700: with minibatch training loss = 1.03 and accuracy of 0.69\n",
      "Iteration 12701: with minibatch training loss = 0.942 and accuracy of 0.72\n",
      "Iteration 12702: with minibatch training loss = 0.595 and accuracy of 0.81\n",
      "Iteration 12703: with minibatch training loss = 0.633 and accuracy of 0.8\n",
      "Iteration 12704: with minibatch training loss = 0.564 and accuracy of 0.83\n",
      "Iteration 12705: with minibatch training loss = 0.993 and accuracy of 0.69\n",
      "Iteration 12706: with minibatch training loss = 0.44 and accuracy of 0.88\n",
      "Iteration 12707: with minibatch training loss = 0.715 and accuracy of 0.81\n",
      "Iteration 12708: with minibatch training loss = 0.86 and accuracy of 0.75\n",
      "Iteration 12709: with minibatch training loss = 1.02 and accuracy of 0.7\n",
      "Iteration 12710: with minibatch training loss = 1.1 and accuracy of 0.66\n",
      "Iteration 12711: with minibatch training loss = 0.993 and accuracy of 0.7\n",
      "Iteration 12712: with minibatch training loss = 0.726 and accuracy of 0.78\n",
      "Iteration 12713: with minibatch training loss = 0.692 and accuracy of 0.78\n",
      "Iteration 12714: with minibatch training loss = 0.673 and accuracy of 0.8\n",
      "Iteration 12715: with minibatch training loss = 0.707 and accuracy of 0.81\n",
      "Iteration 12716: with minibatch training loss = 0.909 and accuracy of 0.73\n",
      "Iteration 12717: with minibatch training loss = 0.615 and accuracy of 0.83\n",
      "Iteration 12718: with minibatch training loss = 0.861 and accuracy of 0.73\n",
      "Iteration 12719: with minibatch training loss = 0.795 and accuracy of 0.75\n",
      "Iteration 12720: with minibatch training loss = 0.872 and accuracy of 0.73\n",
      "Iteration 12721: with minibatch training loss = 0.747 and accuracy of 0.77\n",
      "Iteration 12722: with minibatch training loss = 0.58 and accuracy of 0.83\n",
      "Iteration 12723: with minibatch training loss = 0.648 and accuracy of 0.78\n",
      "Iteration 12724: with minibatch training loss = 0.685 and accuracy of 0.77\n",
      "Iteration 12725: with minibatch training loss = 0.718 and accuracy of 0.77\n",
      "Iteration 12726: with minibatch training loss = 1.14 and accuracy of 0.66\n",
      "Iteration 12727: with minibatch training loss = 0.586 and accuracy of 0.86\n",
      "Iteration 12728: with minibatch training loss = 0.691 and accuracy of 0.77\n",
      "Iteration 12729: with minibatch training loss = 0.967 and accuracy of 0.7\n",
      "Iteration 12730: with minibatch training loss = 0.594 and accuracy of 0.81\n",
      "Iteration 12731: with minibatch training loss = 0.884 and accuracy of 0.73\n",
      "Iteration 12732: with minibatch training loss = 0.719 and accuracy of 0.77\n",
      "Iteration 12733: with minibatch training loss = 0.883 and accuracy of 0.72\n",
      "Iteration 12734: with minibatch training loss = 0.682 and accuracy of 0.78\n",
      "Iteration 12735: with minibatch training loss = 0.641 and accuracy of 0.78\n",
      "Iteration 12736: with minibatch training loss = 0.892 and accuracy of 0.72\n",
      "Iteration 12737: with minibatch training loss = 0.877 and accuracy of 0.75\n",
      "Iteration 12738: with minibatch training loss = 0.749 and accuracy of 0.77\n",
      "Iteration 12739: with minibatch training loss = 0.648 and accuracy of 0.81\n",
      "Iteration 12740: with minibatch training loss = 0.759 and accuracy of 0.73\n",
      "Iteration 12741: with minibatch training loss = 0.689 and accuracy of 0.78\n",
      "Iteration 12742: with minibatch training loss = 0.685 and accuracy of 0.77\n",
      "Iteration 12743: with minibatch training loss = 0.951 and accuracy of 0.67\n",
      "Iteration 12744: with minibatch training loss = 0.527 and accuracy of 0.84\n",
      "Iteration 12745: with minibatch training loss = 0.575 and accuracy of 0.86\n",
      "Iteration 12746: with minibatch training loss = 0.711 and accuracy of 0.78\n",
      "Iteration 12747: with minibatch training loss = 0.521 and accuracy of 0.83\n",
      "Iteration 12748: with minibatch training loss = 0.801 and accuracy of 0.73\n",
      "Iteration 12749: with minibatch training loss = 0.708 and accuracy of 0.8\n",
      "Iteration 12750: with minibatch training loss = 1.04 and accuracy of 0.67\n",
      "Iteration 12751: with minibatch training loss = 0.942 and accuracy of 0.73\n",
      "Iteration 12752: with minibatch training loss = 0.875 and accuracy of 0.72\n",
      "Iteration 12753: with minibatch training loss = 0.541 and accuracy of 0.86\n",
      "Iteration 12754: with minibatch training loss = 0.742 and accuracy of 0.77\n",
      "Iteration 12755: with minibatch training loss = 0.409 and accuracy of 0.86\n",
      "Iteration 12756: with minibatch training loss = 0.553 and accuracy of 0.86\n",
      "Iteration 12757: with minibatch training loss = 0.782 and accuracy of 0.75\n",
      "Iteration 12758: with minibatch training loss = 0.689 and accuracy of 0.8\n",
      "Iteration 12759: with minibatch training loss = 0.696 and accuracy of 0.78\n",
      "Iteration 12760: with minibatch training loss = 0.78 and accuracy of 0.75\n",
      "Iteration 12761: with minibatch training loss = 0.894 and accuracy of 0.73\n",
      "Iteration 12762: with minibatch training loss = 0.728 and accuracy of 0.78\n",
      "Iteration 12763: with minibatch training loss = 0.622 and accuracy of 0.83\n",
      "Iteration 12764: with minibatch training loss = 0.706 and accuracy of 0.78\n",
      "Iteration 12765: with minibatch training loss = 0.999 and accuracy of 0.73\n",
      "Iteration 12766: with minibatch training loss = 0.787 and accuracy of 0.77\n",
      "Iteration 12767: with minibatch training loss = 0.711 and accuracy of 0.81\n",
      "Iteration 12768: with minibatch training loss = 0.762 and accuracy of 0.78\n",
      "Iteration 12769: with minibatch training loss = 0.613 and accuracy of 0.81\n",
      "Iteration 12770: with minibatch training loss = 0.753 and accuracy of 0.77\n",
      "Iteration 12771: with minibatch training loss = 0.534 and accuracy of 0.86\n",
      "Iteration 12772: with minibatch training loss = 0.819 and accuracy of 0.73\n",
      "Iteration 12773: with minibatch training loss = 0.743 and accuracy of 0.8\n",
      "Iteration 12774: with minibatch training loss = 0.687 and accuracy of 0.83\n",
      "Iteration 12775: with minibatch training loss = 0.846 and accuracy of 0.72\n",
      "Iteration 12776: with minibatch training loss = 0.715 and accuracy of 0.77\n",
      "Iteration 12777: with minibatch training loss = 0.603 and accuracy of 0.8\n",
      "Iteration 12778: with minibatch training loss = 0.807 and accuracy of 0.75\n",
      "Iteration 12779: with minibatch training loss = 0.861 and accuracy of 0.75\n",
      "Iteration 12780: with minibatch training loss = 0.791 and accuracy of 0.77\n",
      "Iteration 12781: with minibatch training loss = 0.562 and accuracy of 0.84\n",
      "Iteration 12782: with minibatch training loss = 0.731 and accuracy of 0.8\n",
      "Iteration 12783: with minibatch training loss = 0.723 and accuracy of 0.78\n",
      "Iteration 12784: with minibatch training loss = 0.66 and accuracy of 0.84\n",
      "Iteration 12785: with minibatch training loss = 0.583 and accuracy of 0.86\n",
      "Iteration 12786: with minibatch training loss = 0.744 and accuracy of 0.77\n",
      "Iteration 12787: with minibatch training loss = 0.884 and accuracy of 0.77\n",
      "Iteration 12788: with minibatch training loss = 0.861 and accuracy of 0.72\n",
      "Iteration 12789: with minibatch training loss = 0.959 and accuracy of 0.73\n",
      "Iteration 12790: with minibatch training loss = 0.56 and accuracy of 0.83\n",
      "Iteration 12791: with minibatch training loss = 0.736 and accuracy of 0.8\n",
      "Iteration 12792: with minibatch training loss = 0.561 and accuracy of 0.83\n",
      "Iteration 12793: with minibatch training loss = 0.96 and accuracy of 0.73\n",
      "Iteration 12794: with minibatch training loss = 0.53 and accuracy of 0.81\n",
      "Iteration 12795: with minibatch training loss = 0.81 and accuracy of 0.75\n",
      "Iteration 12796: with minibatch training loss = 0.639 and accuracy of 0.8\n",
      "Iteration 12797: with minibatch training loss = 0.598 and accuracy of 0.81\n",
      "Iteration 12798: with minibatch training loss = 0.493 and accuracy of 0.83\n",
      "Iteration 12799: with minibatch training loss = 0.782 and accuracy of 0.75\n",
      "Iteration 12800: with minibatch training loss = 0.621 and accuracy of 0.78\n",
      "Iteration 12801: with minibatch training loss = 0.725 and accuracy of 0.75\n",
      "Iteration 12802: with minibatch training loss = 0.787 and accuracy of 0.78\n",
      "Iteration 12803: with minibatch training loss = 0.562 and accuracy of 0.83\n",
      "Iteration 12804: with minibatch training loss = 0.625 and accuracy of 0.83\n",
      "Iteration 12805: with minibatch training loss = 0.681 and accuracy of 0.81\n",
      "Iteration 12806: with minibatch training loss = 0.877 and accuracy of 0.72\n",
      "Iteration 12807: with minibatch training loss = 0.916 and accuracy of 0.73\n",
      "Iteration 12808: with minibatch training loss = 0.716 and accuracy of 0.78\n",
      "Iteration 12809: with minibatch training loss = 1 and accuracy of 0.69\n",
      "Iteration 12810: with minibatch training loss = 0.625 and accuracy of 0.81\n",
      "Iteration 12811: with minibatch training loss = 0.835 and accuracy of 0.75\n",
      "Iteration 12812: with minibatch training loss = 0.72 and accuracy of 0.81\n",
      "Iteration 12813: with minibatch training loss = 0.657 and accuracy of 0.83\n",
      "Iteration 12814: with minibatch training loss = 0.646 and accuracy of 0.78\n",
      "Iteration 12815: with minibatch training loss = 0.88 and accuracy of 0.7\n",
      "Iteration 12816: with minibatch training loss = 0.75 and accuracy of 0.77\n",
      "Iteration 12817: with minibatch training loss = 0.532 and accuracy of 0.83\n",
      "Iteration 12818: with minibatch training loss = 0.929 and accuracy of 0.73\n",
      "Iteration 12819: with minibatch training loss = 0.997 and accuracy of 0.7\n",
      "Iteration 12820: with minibatch training loss = 0.683 and accuracy of 0.8\n",
      "Iteration 12821: with minibatch training loss = 0.542 and accuracy of 0.84\n",
      "Iteration 12822: with minibatch training loss = 0.935 and accuracy of 0.73\n",
      "Iteration 12823: with minibatch training loss = 0.732 and accuracy of 0.77\n",
      "Iteration 12824: with minibatch training loss = 0.832 and accuracy of 0.75\n",
      "Iteration 12825: with minibatch training loss = 0.736 and accuracy of 0.78\n",
      "Iteration 12826: with minibatch training loss = 0.6 and accuracy of 0.81\n",
      "Iteration 12827: with minibatch training loss = 0.784 and accuracy of 0.77\n",
      "Iteration 12828: with minibatch training loss = 0.901 and accuracy of 0.73\n",
      "Iteration 12829: with minibatch training loss = 0.794 and accuracy of 0.78\n",
      "Iteration 12830: with minibatch training loss = 0.77 and accuracy of 0.8\n",
      "Iteration 12831: with minibatch training loss = 0.751 and accuracy of 0.78\n",
      "Iteration 12832: with minibatch training loss = 0.806 and accuracy of 0.73\n",
      "Iteration 12833: with minibatch training loss = 0.822 and accuracy of 0.77\n",
      "Iteration 12834: with minibatch training loss = 0.89 and accuracy of 0.73\n",
      "Iteration 12835: with minibatch training loss = 0.725 and accuracy of 0.77\n",
      "Iteration 12836: with minibatch training loss = 0.687 and accuracy of 0.83\n",
      "Iteration 12837: with minibatch training loss = 0.694 and accuracy of 0.8\n",
      "Iteration 12838: with minibatch training loss = 0.905 and accuracy of 0.72\n",
      "Iteration 12839: with minibatch training loss = 0.407 and accuracy of 0.86\n",
      "Iteration 12840: with minibatch training loss = 0.578 and accuracy of 0.84\n",
      "Iteration 12841: with minibatch training loss = 0.529 and accuracy of 0.84\n",
      "Iteration 12842: with minibatch training loss = 0.648 and accuracy of 0.81\n",
      "Iteration 12843: with minibatch training loss = 0.699 and accuracy of 0.78\n",
      "Iteration 12844: with minibatch training loss = 1.06 and accuracy of 0.7\n",
      "Iteration 12845: with minibatch training loss = 0.965 and accuracy of 0.66\n",
      "Iteration 12846: with minibatch training loss = 0.823 and accuracy of 0.73\n",
      "Iteration 12847: with minibatch training loss = 0.763 and accuracy of 0.78\n",
      "Iteration 12848: with minibatch training loss = 0.971 and accuracy of 0.69\n",
      "Iteration 12849: with minibatch training loss = 0.74 and accuracy of 0.78\n",
      "Iteration 12850: with minibatch training loss = 0.546 and accuracy of 0.86\n",
      "Iteration 12851: with minibatch training loss = 0.698 and accuracy of 0.8\n",
      "Iteration 12852: with minibatch training loss = 0.658 and accuracy of 0.8\n",
      "Iteration 12853: with minibatch training loss = 0.564 and accuracy of 0.86\n",
      "Iteration 12854: with minibatch training loss = 0.682 and accuracy of 0.8\n",
      "Iteration 12855: with minibatch training loss = 0.659 and accuracy of 0.78\n",
      "Iteration 12856: with minibatch training loss = 0.721 and accuracy of 0.81\n",
      "Iteration 12857: with minibatch training loss = 0.745 and accuracy of 0.81\n",
      "Iteration 12858: with minibatch training loss = 0.881 and accuracy of 0.73\n",
      "Iteration 12859: with minibatch training loss = 0.965 and accuracy of 0.72\n",
      "Iteration 12860: with minibatch training loss = 0.619 and accuracy of 0.81\n",
      "Iteration 12861: with minibatch training loss = 0.588 and accuracy of 0.81\n",
      "Iteration 12862: with minibatch training loss = 0.669 and accuracy of 0.8\n",
      "Iteration 12863: with minibatch training loss = 0.69 and accuracy of 0.77\n",
      "Iteration 12864: with minibatch training loss = 0.692 and accuracy of 0.78\n",
      "Iteration 12865: with minibatch training loss = 0.624 and accuracy of 0.8\n",
      "Iteration 12866: with minibatch training loss = 0.891 and accuracy of 0.73\n",
      "Iteration 12867: with minibatch training loss = 0.56 and accuracy of 0.83\n",
      "Iteration 12868: with minibatch training loss = 0.689 and accuracy of 0.78\n",
      "Iteration 12869: with minibatch training loss = 0.624 and accuracy of 0.81\n",
      "Iteration 12870: with minibatch training loss = 0.585 and accuracy of 0.86\n",
      "Iteration 12871: with minibatch training loss = 0.778 and accuracy of 0.77\n",
      "Iteration 12872: with minibatch training loss = 0.541 and accuracy of 0.86\n",
      "Iteration 12873: with minibatch training loss = 0.557 and accuracy of 0.81\n",
      "Iteration 12874: with minibatch training loss = 0.811 and accuracy of 0.77\n",
      "Iteration 12875: with minibatch training loss = 0.8 and accuracy of 0.75\n",
      "Iteration 12876: with minibatch training loss = 0.792 and accuracy of 0.72\n",
      "Iteration 12877: with minibatch training loss = 0.862 and accuracy of 0.73\n",
      "Iteration 12878: with minibatch training loss = 0.436 and accuracy of 0.89\n",
      "Iteration 12879: with minibatch training loss = 0.91 and accuracy of 0.7\n",
      "Iteration 12880: with minibatch training loss = 0.538 and accuracy of 0.86\n",
      "Iteration 12881: with minibatch training loss = 0.534 and accuracy of 0.83\n",
      "Iteration 12882: with minibatch training loss = 0.734 and accuracy of 0.78\n",
      "Iteration 12883: with minibatch training loss = 0.593 and accuracy of 0.81\n",
      "Iteration 12884: with minibatch training loss = 0.747 and accuracy of 0.77\n",
      "Iteration 12885: with minibatch training loss = 0.657 and accuracy of 0.86\n",
      "Iteration 12886: with minibatch training loss = 0.544 and accuracy of 0.83\n",
      "Iteration 12887: with minibatch training loss = 0.651 and accuracy of 0.8\n",
      "Iteration 12888: with minibatch training loss = 0.578 and accuracy of 0.83\n",
      "Iteration 12889: with minibatch training loss = 0.84 and accuracy of 0.73\n",
      "Iteration 12890: with minibatch training loss = 0.777 and accuracy of 0.78\n",
      "Iteration 12891: with minibatch training loss = 0.361 and accuracy of 0.92\n",
      "Iteration 12892: with minibatch training loss = 0.978 and accuracy of 0.69\n",
      "Iteration 12893: with minibatch training loss = 0.704 and accuracy of 0.78\n",
      "Iteration 12894: with minibatch training loss = 0.745 and accuracy of 0.77\n",
      "Iteration 12895: with minibatch training loss = 0.923 and accuracy of 0.69\n",
      "Iteration 12896: with minibatch training loss = 0.803 and accuracy of 0.73\n",
      "Iteration 12897: with minibatch training loss = 0.617 and accuracy of 0.8\n",
      "Iteration 12898: with minibatch training loss = 0.981 and accuracy of 0.7\n",
      "Iteration 12899: with minibatch training loss = 0.651 and accuracy of 0.83\n",
      "Iteration 12900: with minibatch training loss = 0.802 and accuracy of 0.75\n",
      "Iteration 12901: with minibatch training loss = 0.759 and accuracy of 0.81\n",
      "Iteration 12902: with minibatch training loss = 0.728 and accuracy of 0.77\n",
      "Iteration 12903: with minibatch training loss = 0.861 and accuracy of 0.7\n",
      "Iteration 12904: with minibatch training loss = 0.601 and accuracy of 0.81\n",
      "Iteration 12905: with minibatch training loss = 0.865 and accuracy of 0.72\n",
      "Iteration 12906: with minibatch training loss = 0.723 and accuracy of 0.8\n",
      "Iteration 12907: with minibatch training loss = 0.893 and accuracy of 0.72\n",
      "Iteration 12908: with minibatch training loss = 0.68 and accuracy of 0.83\n",
      "Iteration 12909: with minibatch training loss = 0.581 and accuracy of 0.83\n",
      "Iteration 12910: with minibatch training loss = 0.475 and accuracy of 0.88\n",
      "Iteration 12911: with minibatch training loss = 0.75 and accuracy of 0.77\n",
      "Iteration 12912: with minibatch training loss = 0.726 and accuracy of 0.77\n",
      "Iteration 12913: with minibatch training loss = 0.859 and accuracy of 0.77\n",
      "Iteration 12914: with minibatch training loss = 0.797 and accuracy of 0.75\n",
      "Iteration 12915: with minibatch training loss = 1.08 and accuracy of 0.66\n",
      "Iteration 12916: with minibatch training loss = 0.687 and accuracy of 0.78\n",
      "Iteration 12917: with minibatch training loss = 0.813 and accuracy of 0.77\n",
      "Iteration 12918: with minibatch training loss = 0.389 and accuracy of 0.86\n",
      "Iteration 12919: with minibatch training loss = 0.788 and accuracy of 0.73\n",
      "Iteration 12920: with minibatch training loss = 0.723 and accuracy of 0.81\n",
      "Iteration 12921: with minibatch training loss = 0.803 and accuracy of 0.77\n",
      "Iteration 12922: with minibatch training loss = 0.782 and accuracy of 0.72\n",
      "Iteration 12923: with minibatch training loss = 0.825 and accuracy of 0.75\n",
      "Iteration 12924: with minibatch training loss = 0.615 and accuracy of 0.86\n",
      "Iteration 12925: with minibatch training loss = 0.788 and accuracy of 0.78\n",
      "Iteration 12926: with minibatch training loss = 0.584 and accuracy of 0.83\n",
      "Iteration 12927: with minibatch training loss = 0.683 and accuracy of 0.83\n",
      "Iteration 12928: with minibatch training loss = 0.856 and accuracy of 0.73\n",
      "Iteration 12929: with minibatch training loss = 0.788 and accuracy of 0.8\n",
      "Iteration 12930: with minibatch training loss = 0.799 and accuracy of 0.75\n",
      "Iteration 12931: with minibatch training loss = 0.627 and accuracy of 0.88\n",
      "Iteration 12932: with minibatch training loss = 0.636 and accuracy of 0.8\n",
      "Iteration 12933: with minibatch training loss = 0.777 and accuracy of 0.77\n",
      "Iteration 12934: with minibatch training loss = 0.597 and accuracy of 0.81\n",
      "Iteration 12935: with minibatch training loss = 0.792 and accuracy of 0.75\n",
      "Iteration 12936: with minibatch training loss = 0.479 and accuracy of 0.86\n",
      "Iteration 12937: with minibatch training loss = 0.6 and accuracy of 0.86\n",
      "Iteration 12938: with minibatch training loss = 0.387 and accuracy of 0.88\n",
      "Iteration 12939: with minibatch training loss = 0.412 and accuracy of 0.89\n",
      "Iteration 12940: with minibatch training loss = 0.721 and accuracy of 0.83\n",
      "Iteration 12941: with minibatch training loss = 0.689 and accuracy of 0.78\n",
      "Iteration 12942: with minibatch training loss = 0.461 and accuracy of 0.84\n",
      "Iteration 12943: with minibatch training loss = 0.569 and accuracy of 0.84\n",
      "Iteration 12944: with minibatch training loss = 1.02 and accuracy of 0.7\n",
      "Iteration 12945: with minibatch training loss = 0.639 and accuracy of 0.8\n",
      "Iteration 12946: with minibatch training loss = 0.871 and accuracy of 0.78\n",
      "Iteration 12947: with minibatch training loss = 0.819 and accuracy of 0.77\n",
      "Iteration 12948: with minibatch training loss = 0.754 and accuracy of 0.77\n",
      "Iteration 12949: with minibatch training loss = 0.702 and accuracy of 0.75\n",
      "Iteration 12950: with minibatch training loss = 0.97 and accuracy of 0.7\n",
      "Iteration 12951: with minibatch training loss = 0.794 and accuracy of 0.72\n",
      "Iteration 12952: with minibatch training loss = 0.507 and accuracy of 0.81\n",
      "Iteration 12953: with minibatch training loss = 0.571 and accuracy of 0.83\n",
      "Iteration 12954: with minibatch training loss = 0.842 and accuracy of 0.72\n",
      "Iteration 12955: with minibatch training loss = 0.609 and accuracy of 0.83\n",
      "Iteration 12956: with minibatch training loss = 0.506 and accuracy of 0.83\n",
      "Iteration 12957: with minibatch training loss = 0.766 and accuracy of 0.73\n",
      "Iteration 12958: with minibatch training loss = 0.756 and accuracy of 0.73\n",
      "Iteration 12959: with minibatch training loss = 0.593 and accuracy of 0.81\n",
      "Iteration 12960: with minibatch training loss = 0.752 and accuracy of 0.77\n",
      "Iteration 12961: with minibatch training loss = 1.01 and accuracy of 0.67\n",
      "Iteration 12962: with minibatch training loss = 0.609 and accuracy of 0.78\n",
      "Iteration 12963: with minibatch training loss = 0.686 and accuracy of 0.78\n",
      "Iteration 12964: with minibatch training loss = 0.607 and accuracy of 0.83\n",
      "Iteration 12965: with minibatch training loss = 0.629 and accuracy of 0.83\n",
      "Iteration 12966: with minibatch training loss = 0.886 and accuracy of 0.73\n",
      "Iteration 12967: with minibatch training loss = 0.778 and accuracy of 0.72\n",
      "Iteration 12968: with minibatch training loss = 0.498 and accuracy of 0.86\n",
      "Iteration 12969: with minibatch training loss = 0.796 and accuracy of 0.78\n",
      "Iteration 12970: with minibatch training loss = 0.624 and accuracy of 0.83\n",
      "Iteration 12971: with minibatch training loss = 0.645 and accuracy of 0.84\n",
      "Iteration 12972: with minibatch training loss = 0.744 and accuracy of 0.77\n",
      "Iteration 12973: with minibatch training loss = 0.827 and accuracy of 0.75\n",
      "Iteration 12974: with minibatch training loss = 0.744 and accuracy of 0.78\n",
      "Iteration 12975: with minibatch training loss = 0.821 and accuracy of 0.77\n",
      "Iteration 12976: with minibatch training loss = 0.979 and accuracy of 0.73\n",
      "Iteration 12977: with minibatch training loss = 0.934 and accuracy of 0.73\n",
      "Iteration 12978: with minibatch training loss = 0.799 and accuracy of 0.75\n",
      "Iteration 12979: with minibatch training loss = 0.679 and accuracy of 0.8\n",
      "Iteration 12980: with minibatch training loss = 0.829 and accuracy of 0.75\n",
      "Iteration 12981: with minibatch training loss = 0.753 and accuracy of 0.77\n",
      "Iteration 12982: with minibatch training loss = 0.861 and accuracy of 0.73\n",
      "Iteration 12983: with minibatch training loss = 0.46 and accuracy of 0.86\n",
      "Iteration 12984: with minibatch training loss = 0.698 and accuracy of 0.8\n",
      "Iteration 12985: with minibatch training loss = 0.655 and accuracy of 0.77\n",
      "Iteration 12986: with minibatch training loss = 0.505 and accuracy of 0.84\n",
      "Iteration 12987: with minibatch training loss = 0.677 and accuracy of 0.8\n",
      "Iteration 12988: with minibatch training loss = 0.458 and accuracy of 0.86\n",
      "Iteration 12989: with minibatch training loss = 0.767 and accuracy of 0.77\n",
      "Iteration 12990: with minibatch training loss = 0.711 and accuracy of 0.77\n",
      "Iteration 12991: with minibatch training loss = 0.687 and accuracy of 0.78\n",
      "Iteration 12992: with minibatch training loss = 0.886 and accuracy of 0.72\n",
      "Iteration 12993: with minibatch training loss = 0.563 and accuracy of 0.81\n",
      "Iteration 12994: with minibatch training loss = 0.529 and accuracy of 0.86\n",
      "Iteration 12995: with minibatch training loss = 0.71 and accuracy of 0.81\n",
      "Iteration 12996: with minibatch training loss = 0.726 and accuracy of 0.78\n",
      "Iteration 12997: with minibatch training loss = 0.747 and accuracy of 0.78\n",
      "Iteration 12998: with minibatch training loss = 0.914 and accuracy of 0.73\n",
      "Iteration 12999: with minibatch training loss = 0.689 and accuracy of 0.77\n",
      "Iteration 13000: with minibatch training loss = 0.805 and accuracy of 0.75\n",
      "Iteration 13001: with minibatch training loss = 0.706 and accuracy of 0.81\n",
      "Iteration 13002: with minibatch training loss = 0.616 and accuracy of 0.81\n",
      "Iteration 13003: with minibatch training loss = 0.919 and accuracy of 0.73\n",
      "Iteration 13004: with minibatch training loss = 0.813 and accuracy of 0.81\n",
      "Iteration 13005: with minibatch training loss = 0.702 and accuracy of 0.81\n",
      "Iteration 13006: with minibatch training loss = 0.653 and accuracy of 0.77\n",
      "Iteration 13007: with minibatch training loss = 0.562 and accuracy of 0.81\n",
      "Iteration 13008: with minibatch training loss = 0.934 and accuracy of 0.7\n",
      "Iteration 13009: with minibatch training loss = 0.726 and accuracy of 0.8\n",
      "Iteration 13010: with minibatch training loss = 0.837 and accuracy of 0.77\n",
      "Iteration 13011: with minibatch training loss = 0.864 and accuracy of 0.73\n",
      "Iteration 13012: with minibatch training loss = 0.831 and accuracy of 0.77\n",
      "Iteration 13013: with minibatch training loss = 0.698 and accuracy of 0.8\n",
      "Iteration 13014: with minibatch training loss = 0.477 and accuracy of 0.91\n",
      "Iteration 13015: with minibatch training loss = 0.62 and accuracy of 0.83\n",
      "Iteration 13016: with minibatch training loss = 0.751 and accuracy of 0.77\n",
      "Iteration 13017: with minibatch training loss = 0.398 and accuracy of 0.89\n",
      "Iteration 13018: with minibatch training loss = 0.663 and accuracy of 0.81\n",
      "Iteration 13019: with minibatch training loss = 0.516 and accuracy of 0.86\n",
      "Iteration 13020: with minibatch training loss = 0.728 and accuracy of 0.81\n",
      "Iteration 13021: with minibatch training loss = 0.681 and accuracy of 0.81\n",
      "Iteration 13022: with minibatch training loss = 0.554 and accuracy of 0.84\n",
      "Iteration 13023: with minibatch training loss = 0.251 and accuracy of 0.94\n",
      "Iteration 13024: with minibatch training loss = 0.523 and accuracy of 0.84\n",
      "Iteration 13025: with minibatch training loss = 0.781 and accuracy of 0.75\n",
      "Iteration 13026: with minibatch training loss = 0.641 and accuracy of 0.83\n",
      "Iteration 13027: with minibatch training loss = 0.78 and accuracy of 0.78\n",
      "Iteration 13028: with minibatch training loss = 0.664 and accuracy of 0.8\n",
      "Iteration 13029: with minibatch training loss = 0.43 and accuracy of 0.91\n",
      "Iteration 13030: with minibatch training loss = 0.693 and accuracy of 0.81\n",
      "Iteration 13031: with minibatch training loss = 0.702 and accuracy of 0.8\n",
      "Iteration 13032: with minibatch training loss = 0.747 and accuracy of 0.83\n",
      "Iteration 13033: with minibatch training loss = 0.665 and accuracy of 0.8\n",
      "Iteration 13034: with minibatch training loss = 0.539 and accuracy of 0.84\n",
      "Iteration 13035: with minibatch training loss = 0.561 and accuracy of 0.84\n",
      "Iteration 13036: with minibatch training loss = 0.505 and accuracy of 0.84\n",
      "Iteration 13037: with minibatch training loss = 0.521 and accuracy of 0.86\n",
      "Iteration 13038: with minibatch training loss = 0.729 and accuracy of 0.78\n",
      "Iteration 13039: with minibatch training loss = 0.529 and accuracy of 0.83\n",
      "Iteration 13040: with minibatch training loss = 0.824 and accuracy of 0.77\n",
      "Iteration 13041: with minibatch training loss = 0.739 and accuracy of 0.77\n",
      "Iteration 13042: with minibatch training loss = 0.67 and accuracy of 0.77\n",
      "Iteration 13043: with minibatch training loss = 0.954 and accuracy of 0.72\n",
      "Iteration 13044: with minibatch training loss = 1.04 and accuracy of 0.69\n",
      "Iteration 13045: with minibatch training loss = 0.789 and accuracy of 0.75\n",
      "Iteration 13046: with minibatch training loss = 0.866 and accuracy of 0.78\n",
      "Iteration 13047: with minibatch training loss = 0.733 and accuracy of 0.78\n",
      "Iteration 13048: with minibatch training loss = 0.82 and accuracy of 0.73\n",
      "Iteration 13049: with minibatch training loss = 0.624 and accuracy of 0.81\n",
      "Iteration 13050: with minibatch training loss = 0.782 and accuracy of 0.75\n",
      "Iteration 13051: with minibatch training loss = 0.722 and accuracy of 0.78\n",
      "Iteration 13052: with minibatch training loss = 0.68 and accuracy of 0.78\n",
      "Iteration 13053: with minibatch training loss = 0.346 and accuracy of 0.94\n",
      "Iteration 13054: with minibatch training loss = 0.696 and accuracy of 0.77\n",
      "Iteration 13055: with minibatch training loss = 0.745 and accuracy of 0.75\n",
      "Iteration 13056: with minibatch training loss = 0.699 and accuracy of 0.78\n",
      "Iteration 13057: with minibatch training loss = 0.81 and accuracy of 0.75\n",
      "Iteration 13058: with minibatch training loss = 0.754 and accuracy of 0.75\n",
      "Iteration 13059: with minibatch training loss = 0.573 and accuracy of 0.83\n",
      "Iteration 13060: with minibatch training loss = 0.524 and accuracy of 0.83\n",
      "Iteration 13061: with minibatch training loss = 0.688 and accuracy of 0.81\n",
      "Iteration 13062: with minibatch training loss = 0.601 and accuracy of 0.81\n",
      "Iteration 13063: with minibatch training loss = 0.824 and accuracy of 0.72\n",
      "Iteration 13064: with minibatch training loss = 0.488 and accuracy of 0.86\n",
      "Iteration 13065: with minibatch training loss = 0.96 and accuracy of 0.73\n",
      "Iteration 13066: with minibatch training loss = 0.751 and accuracy of 0.8\n",
      "Iteration 13067: with minibatch training loss = 0.687 and accuracy of 0.83\n",
      "Iteration 13068: with minibatch training loss = 0.662 and accuracy of 0.83\n",
      "Iteration 13069: with minibatch training loss = 0.718 and accuracy of 0.8\n",
      "Iteration 13070: with minibatch training loss = 1.02 and accuracy of 0.64\n",
      "Iteration 13071: with minibatch training loss = 0.646 and accuracy of 0.83\n",
      "Iteration 13072: with minibatch training loss = 0.463 and accuracy of 0.86\n",
      "Iteration 13073: with minibatch training loss = 0.71 and accuracy of 0.8\n",
      "Iteration 13074: with minibatch training loss = 0.545 and accuracy of 0.86\n",
      "Iteration 13075: with minibatch training loss = 0.591 and accuracy of 0.83\n",
      "Iteration 13076: with minibatch training loss = 0.752 and accuracy of 0.77\n",
      "Iteration 13077: with minibatch training loss = 0.602 and accuracy of 0.84\n",
      "Iteration 13078: with minibatch training loss = 0.827 and accuracy of 0.72\n",
      "Iteration 13079: with minibatch training loss = 0.645 and accuracy of 0.88\n",
      "Iteration 13080: with minibatch training loss = 0.802 and accuracy of 0.77\n",
      "Iteration 13081: with minibatch training loss = 0.554 and accuracy of 0.83\n",
      "Iteration 13082: with minibatch training loss = 0.87 and accuracy of 0.73\n",
      "Iteration 13083: with minibatch training loss = 1.03 and accuracy of 0.67\n",
      "Iteration 13084: with minibatch training loss = 1.1 and accuracy of 0.69\n",
      "Iteration 13085: with minibatch training loss = 0.725 and accuracy of 0.8\n",
      "Iteration 13086: with minibatch training loss = 0.581 and accuracy of 0.81\n",
      "Iteration 13087: with minibatch training loss = 0.614 and accuracy of 0.81\n",
      "Iteration 13088: with minibatch training loss = 0.622 and accuracy of 0.88\n",
      "Iteration 13089: with minibatch training loss = 0.991 and accuracy of 0.7\n",
      "Iteration 13090: with minibatch training loss = 0.72 and accuracy of 0.8\n",
      "Iteration 13091: with minibatch training loss = 0.818 and accuracy of 0.75\n",
      "Iteration 13092: with minibatch training loss = 0.549 and accuracy of 0.84\n",
      "Iteration 13093: with minibatch training loss = 0.823 and accuracy of 0.73\n",
      "Iteration 13094: with minibatch training loss = 0.413 and accuracy of 0.89\n",
      "Iteration 13095: with minibatch training loss = 0.352 and accuracy of 0.92\n",
      "Iteration 13096: with minibatch training loss = 0.61 and accuracy of 0.81\n",
      "Iteration 13097: with minibatch training loss = 0.741 and accuracy of 0.78\n",
      "Iteration 13098: with minibatch training loss = 0.705 and accuracy of 0.8\n",
      "Iteration 13099: with minibatch training loss = 0.532 and accuracy of 0.86\n",
      "Iteration 13100: with minibatch training loss = 0.757 and accuracy of 0.75\n",
      "Iteration 13101: with minibatch training loss = 0.779 and accuracy of 0.78\n",
      "Iteration 13102: with minibatch training loss = 0.646 and accuracy of 0.8\n",
      "Iteration 13103: with minibatch training loss = 0.676 and accuracy of 0.78\n",
      "Iteration 13104: with minibatch training loss = 0.901 and accuracy of 0.73\n",
      "Iteration 13105: with minibatch training loss = 0.584 and accuracy of 0.81\n",
      "Iteration 13106: with minibatch training loss = 0.856 and accuracy of 0.73\n",
      "Iteration 13107: with minibatch training loss = 0.819 and accuracy of 0.75\n",
      "Iteration 13108: with minibatch training loss = 0.555 and accuracy of 0.83\n",
      "Iteration 13109: with minibatch training loss = 0.809 and accuracy of 0.77\n",
      "Iteration 13110: with minibatch training loss = 0.774 and accuracy of 0.77\n",
      "Iteration 13111: with minibatch training loss = 0.982 and accuracy of 0.67\n",
      "Iteration 13112: with minibatch training loss = 0.635 and accuracy of 0.81\n",
      "Iteration 13113: with minibatch training loss = 0.78 and accuracy of 0.77\n",
      "Iteration 13114: with minibatch training loss = 0.487 and accuracy of 0.86\n",
      "Iteration 13115: with minibatch training loss = 0.58 and accuracy of 0.81\n",
      "Iteration 13116: with minibatch training loss = 0.463 and accuracy of 0.91\n",
      "Iteration 13117: with minibatch training loss = 0.84 and accuracy of 0.73\n",
      "Iteration 13118: with minibatch training loss = 0.8 and accuracy of 0.73\n",
      "Iteration 13119: with minibatch training loss = 0.796 and accuracy of 0.77\n",
      "Iteration 13120: with minibatch training loss = 0.676 and accuracy of 0.83\n",
      "Iteration 13121: with minibatch training loss = 0.83 and accuracy of 0.77\n",
      "Iteration 13122: with minibatch training loss = 0.872 and accuracy of 0.73\n",
      "Iteration 13123: with minibatch training loss = 0.5 and accuracy of 0.89\n",
      "Iteration 13124: with minibatch training loss = 0.812 and accuracy of 0.75\n",
      "Iteration 13125: with minibatch training loss = 0.958 and accuracy of 0.69\n",
      "Iteration 13126: with minibatch training loss = 0.852 and accuracy of 0.72\n",
      "Iteration 13127: with minibatch training loss = 0.819 and accuracy of 0.75\n",
      "Iteration 13128: with minibatch training loss = 0.715 and accuracy of 0.78\n",
      "Iteration 13129: with minibatch training loss = 0.736 and accuracy of 0.77\n",
      "Iteration 13130: with minibatch training loss = 0.592 and accuracy of 0.83\n",
      "Iteration 13131: with minibatch training loss = 0.586 and accuracy of 0.84\n",
      "Iteration 13132: with minibatch training loss = 0.531 and accuracy of 0.84\n",
      "Iteration 13133: with minibatch training loss = 0.669 and accuracy of 0.81\n",
      "Iteration 13134: with minibatch training loss = 0.644 and accuracy of 0.81\n",
      "Iteration 13135: with minibatch training loss = 0.728 and accuracy of 0.8\n",
      "Iteration 13136: with minibatch training loss = 0.897 and accuracy of 0.7\n",
      "Iteration 13137: with minibatch training loss = 0.714 and accuracy of 0.8\n",
      "Iteration 13138: with minibatch training loss = 0.685 and accuracy of 0.77\n",
      "Iteration 13139: with minibatch training loss = 0.55 and accuracy of 0.83\n",
      "Iteration 13140: with minibatch training loss = 0.798 and accuracy of 0.75\n",
      "Iteration 13141: with minibatch training loss = 0.881 and accuracy of 0.73\n",
      "Iteration 13142: with minibatch training loss = 0.934 and accuracy of 0.72\n",
      "Iteration 13143: with minibatch training loss = 0.511 and accuracy of 0.86\n",
      "Iteration 13144: with minibatch training loss = 0.659 and accuracy of 0.81\n",
      "Iteration 13145: with minibatch training loss = 0.954 and accuracy of 0.77\n",
      "Iteration 13146: with minibatch training loss = 0.936 and accuracy of 0.72\n",
      "Iteration 13147: with minibatch training loss = 0.894 and accuracy of 0.67\n",
      "Iteration 13148: with minibatch training loss = 0.817 and accuracy of 0.75\n",
      "Iteration 13149: with minibatch training loss = 0.686 and accuracy of 0.78\n",
      "Iteration 13150: with minibatch training loss = 0.568 and accuracy of 0.83\n",
      "Iteration 13151: with minibatch training loss = 0.771 and accuracy of 0.75\n",
      "Iteration 13152: with minibatch training loss = 0.821 and accuracy of 0.73\n",
      "Iteration 13153: with minibatch training loss = 0.627 and accuracy of 0.8\n",
      "Iteration 13154: with minibatch training loss = 0.718 and accuracy of 0.78\n",
      "Iteration 13155: with minibatch training loss = 0.592 and accuracy of 0.81\n",
      "Iteration 13156: with minibatch training loss = 0.886 and accuracy of 0.7\n",
      "Iteration 13157: with minibatch training loss = 0.648 and accuracy of 0.81\n",
      "Iteration 13158: with minibatch training loss = 0.584 and accuracy of 0.8\n",
      "Iteration 13159: with minibatch training loss = 0.617 and accuracy of 0.83\n",
      "Iteration 13160: with minibatch training loss = 0.992 and accuracy of 0.73\n",
      "Iteration 13161: with minibatch training loss = 0.651 and accuracy of 0.83\n",
      "Iteration 13162: with minibatch training loss = 0.715 and accuracy of 0.78\n",
      "Iteration 13163: with minibatch training loss = 0.786 and accuracy of 0.77\n",
      "Iteration 13164: with minibatch training loss = 0.831 and accuracy of 0.72\n",
      "Iteration 13165: with minibatch training loss = 0.676 and accuracy of 0.77\n",
      "Iteration 13166: with minibatch training loss = 0.629 and accuracy of 0.81\n",
      "Iteration 13167: with minibatch training loss = 0.712 and accuracy of 0.81\n",
      "Iteration 13168: with minibatch training loss = 0.585 and accuracy of 0.83\n",
      "Iteration 13169: with minibatch training loss = 0.58 and accuracy of 0.81\n",
      "Iteration 13170: with minibatch training loss = 0.847 and accuracy of 0.72\n",
      "Iteration 13171: with minibatch training loss = 0.592 and accuracy of 0.83\n",
      "Iteration 13172: with minibatch training loss = 0.445 and accuracy of 0.86\n",
      "Iteration 13173: with minibatch training loss = 0.722 and accuracy of 0.78\n",
      "Iteration 13174: with minibatch training loss = 0.759 and accuracy of 0.77\n",
      "Iteration 13175: with minibatch training loss = 0.696 and accuracy of 0.83\n",
      "Iteration 13176: with minibatch training loss = 0.673 and accuracy of 0.81\n",
      "Iteration 13177: with minibatch training loss = 0.661 and accuracy of 0.8\n",
      "Iteration 13178: with minibatch training loss = 0.678 and accuracy of 0.81\n",
      "Iteration 13179: with minibatch training loss = 0.82 and accuracy of 0.77\n",
      "Iteration 13180: with minibatch training loss = 0.704 and accuracy of 0.8\n",
      "Iteration 13181: with minibatch training loss = 0.672 and accuracy of 0.81\n",
      "Iteration 13182: with minibatch training loss = 0.589 and accuracy of 0.84\n",
      "Iteration 13183: with minibatch training loss = 0.723 and accuracy of 0.75\n",
      "Iteration 13184: with minibatch training loss = 0.658 and accuracy of 0.78\n",
      "Iteration 13185: with minibatch training loss = 0.565 and accuracy of 0.84\n",
      "Iteration 13186: with minibatch training loss = 0.76 and accuracy of 0.8\n",
      "Iteration 13187: with minibatch training loss = 0.856 and accuracy of 0.75\n",
      "Iteration 13188: with minibatch training loss = 0.691 and accuracy of 0.78\n",
      "Iteration 13189: with minibatch training loss = 0.881 and accuracy of 0.73\n",
      "Iteration 13190: with minibatch training loss = 0.833 and accuracy of 0.77\n",
      "Iteration 13191: with minibatch training loss = 0.528 and accuracy of 0.84\n",
      "Iteration 13192: with minibatch training loss = 0.851 and accuracy of 0.73\n",
      "Iteration 13193: with minibatch training loss = 0.842 and accuracy of 0.7\n",
      "Iteration 13194: with minibatch training loss = 0.563 and accuracy of 0.8\n",
      "Iteration 13195: with minibatch training loss = 0.796 and accuracy of 0.75\n",
      "Iteration 13196: with minibatch training loss = 0.791 and accuracy of 0.77\n",
      "Iteration 13197: with minibatch training loss = 0.7 and accuracy of 0.81\n",
      "Iteration 13198: with minibatch training loss = 0.902 and accuracy of 0.72\n",
      "Iteration 13199: with minibatch training loss = 0.754 and accuracy of 0.73\n",
      "Iteration 13200: with minibatch training loss = 1.05 and accuracy of 0.69\n",
      "Iteration 13201: with minibatch training loss = 0.675 and accuracy of 0.8\n",
      "Iteration 13202: with minibatch training loss = 0.567 and accuracy of 0.84\n",
      "Iteration 13203: with minibatch training loss = 0.765 and accuracy of 0.75\n",
      "Iteration 13204: with minibatch training loss = 0.471 and accuracy of 0.84\n",
      "Iteration 13205: with minibatch training loss = 0.916 and accuracy of 0.73\n",
      "Iteration 13206: with minibatch training loss = 0.906 and accuracy of 0.75\n",
      "Iteration 13207: with minibatch training loss = 0.844 and accuracy of 0.73\n",
      "Iteration 13208: with minibatch training loss = 0.775 and accuracy of 0.8\n",
      "Iteration 13209: with minibatch training loss = 0.693 and accuracy of 0.78\n",
      "Iteration 13210: with minibatch training loss = 0.968 and accuracy of 0.69\n",
      "Iteration 13211: with minibatch training loss = 0.417 and accuracy of 0.89\n",
      "Iteration 13212: with minibatch training loss = 0.816 and accuracy of 0.75\n",
      "Iteration 13213: with minibatch training loss = 0.798 and accuracy of 0.75\n",
      "Iteration 13214: with minibatch training loss = 0.701 and accuracy of 0.86\n",
      "Iteration 13215: with minibatch training loss = 0.802 and accuracy of 0.73\n",
      "Iteration 13216: with minibatch training loss = 0.6 and accuracy of 0.83\n",
      "Iteration 13217: with minibatch training loss = 1.15 and accuracy of 0.66\n",
      "Iteration 13218: with minibatch training loss = 0.971 and accuracy of 0.73\n",
      "Iteration 13219: with minibatch training loss = 0.61 and accuracy of 0.8\n",
      "Iteration 13220: with minibatch training loss = 0.742 and accuracy of 0.77\n",
      "Iteration 13221: with minibatch training loss = 0.632 and accuracy of 0.8\n",
      "Iteration 13222: with minibatch training loss = 0.608 and accuracy of 0.83\n",
      "Iteration 13223: with minibatch training loss = 0.589 and accuracy of 0.78\n",
      "Iteration 13224: with minibatch training loss = 0.747 and accuracy of 0.77\n",
      "Iteration 13225: with minibatch training loss = 0.544 and accuracy of 0.83\n",
      "Iteration 13226: with minibatch training loss = 0.529 and accuracy of 0.83\n",
      "Iteration 13227: with minibatch training loss = 0.979 and accuracy of 0.7\n",
      "Iteration 13228: with minibatch training loss = 0.925 and accuracy of 0.69\n",
      "Iteration 13229: with minibatch training loss = 0.673 and accuracy of 0.78\n",
      "Iteration 13230: with minibatch training loss = 0.623 and accuracy of 0.83\n",
      "Iteration 13231: with minibatch training loss = 0.67 and accuracy of 0.81\n",
      "Iteration 13232: with minibatch training loss = 0.641 and accuracy of 0.81\n",
      "Iteration 13233: with minibatch training loss = 0.716 and accuracy of 0.77\n",
      "Iteration 13234: with minibatch training loss = 0.635 and accuracy of 0.8\n",
      "Iteration 13235: with minibatch training loss = 0.595 and accuracy of 0.84\n",
      "Iteration 13236: with minibatch training loss = 1.03 and accuracy of 0.69\n",
      "Iteration 13237: with minibatch training loss = 0.784 and accuracy of 0.78\n",
      "Iteration 13238: with minibatch training loss = 0.718 and accuracy of 0.8\n",
      "Iteration 13239: with minibatch training loss = 0.631 and accuracy of 0.81\n",
      "Iteration 13240: with minibatch training loss = 1.01 and accuracy of 0.72\n",
      "Iteration 13241: with minibatch training loss = 0.738 and accuracy of 0.8\n",
      "Iteration 13242: with minibatch training loss = 0.765 and accuracy of 0.77\n",
      "Iteration 13243: with minibatch training loss = 0.454 and accuracy of 0.84\n",
      "Iteration 13244: with minibatch training loss = 0.727 and accuracy of 0.81\n",
      "Iteration 13245: with minibatch training loss = 0.715 and accuracy of 0.8\n",
      "Iteration 13246: with minibatch training loss = 0.825 and accuracy of 0.73\n",
      "Iteration 13247: with minibatch training loss = 0.512 and accuracy of 0.84\n",
      "Iteration 13248: with minibatch training loss = 0.437 and accuracy of 0.89\n",
      "Iteration 13249: with minibatch training loss = 0.949 and accuracy of 0.72\n",
      "Iteration 13250: with minibatch training loss = 0.745 and accuracy of 0.78\n",
      "Iteration 13251: with minibatch training loss = 0.93 and accuracy of 0.73\n",
      "Iteration 13252: with minibatch training loss = 0.739 and accuracy of 0.78\n",
      "Iteration 13253: with minibatch training loss = 0.696 and accuracy of 0.78\n",
      "Iteration 13254: with minibatch training loss = 0.926 and accuracy of 0.73\n",
      "Iteration 13255: with minibatch training loss = 0.936 and accuracy of 0.72\n",
      "Iteration 13256: with minibatch training loss = 0.613 and accuracy of 0.81\n",
      "Iteration 13257: with minibatch training loss = 0.954 and accuracy of 0.73\n",
      "Iteration 13258: with minibatch training loss = 0.838 and accuracy of 0.75\n",
      "Iteration 13259: with minibatch training loss = 0.608 and accuracy of 0.83\n",
      "Iteration 13260: with minibatch training loss = 0.725 and accuracy of 0.8\n",
      "Iteration 13261: with minibatch training loss = 0.681 and accuracy of 0.81\n",
      "Iteration 13262: with minibatch training loss = 0.73 and accuracy of 0.78\n",
      "Iteration 13263: with minibatch training loss = 0.604 and accuracy of 0.84\n",
      "Iteration 13264: with minibatch training loss = 0.601 and accuracy of 0.83\n",
      "Iteration 13265: with minibatch training loss = 0.702 and accuracy of 0.81\n",
      "Iteration 13266: with minibatch training loss = 0.907 and accuracy of 0.72\n",
      "Iteration 13267: with minibatch training loss = 0.699 and accuracy of 0.78\n",
      "Iteration 13268: with minibatch training loss = 0.727 and accuracy of 0.78\n",
      "Iteration 13269: with minibatch training loss = 0.646 and accuracy of 0.81\n",
      "Iteration 13270: with minibatch training loss = 0.801 and accuracy of 0.78\n",
      "Iteration 13271: with minibatch training loss = 0.493 and accuracy of 0.84\n",
      "Iteration 13272: with minibatch training loss = 0.714 and accuracy of 0.78\n",
      "Iteration 13273: with minibatch training loss = 0.739 and accuracy of 0.78\n",
      "Iteration 13274: with minibatch training loss = 0.509 and accuracy of 0.86\n",
      "Iteration 13275: with minibatch training loss = 0.663 and accuracy of 0.8\n",
      "Iteration 13276: with minibatch training loss = 1.1 and accuracy of 0.64\n",
      "Iteration 13277: with minibatch training loss = 0.698 and accuracy of 0.8\n",
      "Iteration 13278: with minibatch training loss = 0.569 and accuracy of 0.84\n",
      "Iteration 13279: with minibatch training loss = 0.602 and accuracy of 0.83\n",
      "Iteration 13280: with minibatch training loss = 0.896 and accuracy of 0.77\n",
      "Iteration 13281: with minibatch training loss = 0.507 and accuracy of 0.86\n",
      "Iteration 13282: with minibatch training loss = 0.493 and accuracy of 0.86\n",
      "Iteration 13283: with minibatch training loss = 0.86 and accuracy of 0.72\n",
      "Iteration 13284: with minibatch training loss = 0.74 and accuracy of 0.77\n",
      "Iteration 13285: with minibatch training loss = 0.77 and accuracy of 0.75\n",
      "Iteration 13286: with minibatch training loss = 0.508 and accuracy of 0.81\n",
      "Iteration 13287: with minibatch training loss = 0.497 and accuracy of 0.84\n",
      "Iteration 13288: with minibatch training loss = 0.993 and accuracy of 0.72\n",
      "Iteration 13289: with minibatch training loss = 1.06 and accuracy of 0.66\n",
      "Iteration 13290: with minibatch training loss = 0.757 and accuracy of 0.77\n",
      "Iteration 13291: with minibatch training loss = 0.822 and accuracy of 0.77\n",
      "Iteration 13292: with minibatch training loss = 0.904 and accuracy of 0.7\n",
      "Iteration 13293: with minibatch training loss = 0.608 and accuracy of 0.84\n",
      "Iteration 13294: with minibatch training loss = 0.664 and accuracy of 0.8\n",
      "Iteration 13295: with minibatch training loss = 0.62 and accuracy of 0.83\n",
      "Iteration 13296: with minibatch training loss = 0.516 and accuracy of 0.84\n",
      "Iteration 13297: with minibatch training loss = 0.819 and accuracy of 0.75\n",
      "Iteration 13298: with minibatch training loss = 1.04 and accuracy of 0.7\n",
      "Iteration 13299: with minibatch training loss = 0.794 and accuracy of 0.73\n",
      "Iteration 13300: with minibatch training loss = 0.848 and accuracy of 0.77\n",
      "Iteration 13301: with minibatch training loss = 0.673 and accuracy of 0.8\n",
      "Iteration 13302: with minibatch training loss = 0.522 and accuracy of 0.86\n",
      "Iteration 13303: with minibatch training loss = 0.672 and accuracy of 0.78\n",
      "Iteration 13304: with minibatch training loss = 0.789 and accuracy of 0.77\n",
      "Iteration 13305: with minibatch training loss = 0.524 and accuracy of 0.88\n",
      "Iteration 13306: with minibatch training loss = 0.696 and accuracy of 0.8\n",
      "Iteration 13307: with minibatch training loss = 0.658 and accuracy of 0.81\n",
      "Iteration 13308: with minibatch training loss = 0.759 and accuracy of 0.77\n",
      "Iteration 13309: with minibatch training loss = 1.01 and accuracy of 0.67\n",
      "Iteration 13310: with minibatch training loss = 0.732 and accuracy of 0.81\n",
      "Iteration 13311: with minibatch training loss = 0.488 and accuracy of 0.84\n",
      "Iteration 13312: with minibatch training loss = 0.767 and accuracy of 0.77\n",
      "Iteration 13313: with minibatch training loss = 0.725 and accuracy of 0.8\n",
      "Iteration 13314: with minibatch training loss = 0.453 and accuracy of 0.89\n",
      "Iteration 13315: with minibatch training loss = 0.816 and accuracy of 0.77\n",
      "Iteration 13316: with minibatch training loss = 0.523 and accuracy of 0.89\n",
      "Iteration 13317: with minibatch training loss = 0.561 and accuracy of 0.81\n",
      "Iteration 13318: with minibatch training loss = 0.947 and accuracy of 0.69\n",
      "Iteration 13319: with minibatch training loss = 1.12 and accuracy of 0.64\n",
      "Iteration 13320: with minibatch training loss = 0.683 and accuracy of 0.78\n",
      "Iteration 13321: with minibatch training loss = 0.592 and accuracy of 0.86\n",
      "Iteration 13322: with minibatch training loss = 0.688 and accuracy of 0.78\n",
      "Iteration 13323: with minibatch training loss = 1.01 and accuracy of 0.67\n",
      "Iteration 13324: with minibatch training loss = 0.76 and accuracy of 0.78\n",
      "Iteration 13325: with minibatch training loss = 0.94 and accuracy of 0.73\n",
      "Iteration 13326: with minibatch training loss = 0.916 and accuracy of 0.73\n",
      "Iteration 13327: with minibatch training loss = 0.472 and accuracy of 0.88\n",
      "Iteration 13328: with minibatch training loss = 0.726 and accuracy of 0.77\n",
      "Iteration 13329: with minibatch training loss = 0.856 and accuracy of 0.75\n",
      "Iteration 13330: with minibatch training loss = 0.803 and accuracy of 0.77\n",
      "Iteration 13331: with minibatch training loss = 0.849 and accuracy of 0.8\n",
      "Iteration 13332: with minibatch training loss = 0.899 and accuracy of 0.7\n",
      "Iteration 13333: with minibatch training loss = 0.675 and accuracy of 0.78\n",
      "Iteration 13334: with minibatch training loss = 0.569 and accuracy of 0.83\n",
      "Iteration 13335: with minibatch training loss = 0.811 and accuracy of 0.75\n",
      "Iteration 13336: with minibatch training loss = 0.614 and accuracy of 0.78\n",
      "Iteration 13337: with minibatch training loss = 0.612 and accuracy of 0.81\n",
      "Iteration 13338: with minibatch training loss = 0.891 and accuracy of 0.72\n",
      "Iteration 13339: with minibatch training loss = 0.929 and accuracy of 0.7\n",
      "Iteration 13340: with minibatch training loss = 0.699 and accuracy of 0.78\n",
      "Iteration 13341: with minibatch training loss = 0.554 and accuracy of 0.83\n",
      "Iteration 13342: with minibatch training loss = 0.764 and accuracy of 0.75\n",
      "Iteration 13343: with minibatch training loss = 0.68 and accuracy of 0.78\n",
      "Iteration 13344: with minibatch training loss = 0.646 and accuracy of 0.81\n",
      "Iteration 13345: with minibatch training loss = 0.714 and accuracy of 0.77\n",
      "Iteration 13346: with minibatch training loss = 0.619 and accuracy of 0.8\n",
      "Iteration 13347: with minibatch training loss = 0.68 and accuracy of 0.78\n",
      "Iteration 13348: with minibatch training loss = 0.83 and accuracy of 0.77\n",
      "Iteration 13349: with minibatch training loss = 0.472 and accuracy of 0.88\n",
      "Iteration 13350: with minibatch training loss = 0.462 and accuracy of 0.84\n",
      "Iteration 13351: with minibatch training loss = 0.956 and accuracy of 0.72\n",
      "Iteration 13352: with minibatch training loss = 0.876 and accuracy of 0.73\n",
      "Iteration 13353: with minibatch training loss = 0.447 and accuracy of 0.88\n",
      "Iteration 13354: with minibatch training loss = 0.441 and accuracy of 0.88\n",
      "Iteration 13355: with minibatch training loss = 0.637 and accuracy of 0.81\n",
      "Iteration 13356: with minibatch training loss = 0.372 and accuracy of 0.92\n",
      "Iteration 13357: with minibatch training loss = 0.928 and accuracy of 0.7\n",
      "Iteration 13358: with minibatch training loss = 0.506 and accuracy of 0.84\n",
      "Iteration 13359: with minibatch training loss = 1.02 and accuracy of 0.75\n",
      "Iteration 13360: with minibatch training loss = 0.732 and accuracy of 0.78\n",
      "Iteration 13361: with minibatch training loss = 0.67 and accuracy of 0.83\n",
      "Iteration 13362: with minibatch training loss = 0.748 and accuracy of 0.78\n",
      "Iteration 13363: with minibatch training loss = 0.653 and accuracy of 0.8\n",
      "Iteration 13364: with minibatch training loss = 0.802 and accuracy of 0.73\n",
      "Iteration 13365: with minibatch training loss = 0.414 and accuracy of 0.89\n",
      "Iteration 13366: with minibatch training loss = 0.918 and accuracy of 0.72\n",
      "Iteration 13367: with minibatch training loss = 0.615 and accuracy of 0.81\n",
      "Iteration 13368: with minibatch training loss = 0.604 and accuracy of 0.81\n",
      "Iteration 13369: with minibatch training loss = 0.861 and accuracy of 0.73\n",
      "Iteration 13370: with minibatch training loss = 0.653 and accuracy of 0.81\n",
      "Iteration 13371: with minibatch training loss = 0.61 and accuracy of 0.8\n",
      "Iteration 13372: with minibatch training loss = 0.811 and accuracy of 0.77\n",
      "Iteration 13373: with minibatch training loss = 0.943 and accuracy of 0.7\n",
      "Iteration 13374: with minibatch training loss = 0.609 and accuracy of 0.84\n",
      "Iteration 13375: with minibatch training loss = 1.06 and accuracy of 0.66\n",
      "Iteration 13376: with minibatch training loss = 0.706 and accuracy of 0.8\n",
      "Iteration 13377: with minibatch training loss = 0.589 and accuracy of 0.8\n",
      "Iteration 13378: with minibatch training loss = 0.313 and accuracy of 0.92\n",
      "Iteration 13379: with minibatch training loss = 0.879 and accuracy of 0.72\n",
      "Iteration 13380: with minibatch training loss = 0.65 and accuracy of 0.81\n",
      "Iteration 13381: with minibatch training loss = 0.624 and accuracy of 0.78\n",
      "Iteration 13382: with minibatch training loss = 0.679 and accuracy of 0.81\n",
      "Iteration 13383: with minibatch training loss = 0.806 and accuracy of 0.75\n",
      "Iteration 13384: with minibatch training loss = 0.76 and accuracy of 0.73\n",
      "Iteration 13385: with minibatch training loss = 0.522 and accuracy of 0.84\n",
      "Iteration 13386: with minibatch training loss = 0.837 and accuracy of 0.72\n",
      "Iteration 13387: with minibatch training loss = 0.599 and accuracy of 0.83\n",
      "Iteration 13388: with minibatch training loss = 0.554 and accuracy of 0.8\n",
      "Iteration 13389: with minibatch training loss = 0.897 and accuracy of 0.75\n",
      "Iteration 13390: with minibatch training loss = 0.821 and accuracy of 0.75\n",
      "Iteration 13391: with minibatch training loss = 0.918 and accuracy of 0.72\n",
      "Iteration 13392: with minibatch training loss = 0.633 and accuracy of 0.86\n",
      "Iteration 13393: with minibatch training loss = 0.696 and accuracy of 0.8\n",
      "Iteration 13394: with minibatch training loss = 0.78 and accuracy of 0.78\n",
      "Iteration 13395: with minibatch training loss = 0.599 and accuracy of 0.81\n",
      "Iteration 13396: with minibatch training loss = 0.337 and accuracy of 0.92\n",
      "Iteration 13397: with minibatch training loss = 0.74 and accuracy of 0.78\n",
      "Iteration 13398: with minibatch training loss = 0.962 and accuracy of 0.73\n",
      "Iteration 13399: with minibatch training loss = 0.695 and accuracy of 0.83\n",
      "Iteration 13400: with minibatch training loss = 0.657 and accuracy of 0.83\n",
      "Iteration 13401: with minibatch training loss = 0.895 and accuracy of 0.75\n",
      "Iteration 13402: with minibatch training loss = 0.697 and accuracy of 0.75\n",
      "Iteration 13403: with minibatch training loss = 0.696 and accuracy of 0.81\n",
      "Iteration 13404: with minibatch training loss = 0.807 and accuracy of 0.73\n",
      "Iteration 13405: with minibatch training loss = 0.517 and accuracy of 0.84\n",
      "Iteration 13406: with minibatch training loss = 0.881 and accuracy of 0.72\n",
      "Iteration 13407: with minibatch training loss = 0.851 and accuracy of 0.72\n",
      "Iteration 13408: with minibatch training loss = 0.598 and accuracy of 0.84\n",
      "Iteration 13409: with minibatch training loss = 0.657 and accuracy of 0.78\n",
      "Iteration 13410: with minibatch training loss = 0.675 and accuracy of 0.8\n",
      "Iteration 13411: with minibatch training loss = 0.754 and accuracy of 0.78\n",
      "Iteration 13412: with minibatch training loss = 0.638 and accuracy of 0.81\n",
      "Iteration 13413: with minibatch training loss = 0.766 and accuracy of 0.75\n",
      "Iteration 13414: with minibatch training loss = 0.528 and accuracy of 0.84\n",
      "Iteration 13415: with minibatch training loss = 0.981 and accuracy of 0.7\n",
      "Iteration 13416: with minibatch training loss = 0.752 and accuracy of 0.77\n",
      "Iteration 13417: with minibatch training loss = 0.481 and accuracy of 0.83\n",
      "Iteration 13418: with minibatch training loss = 0.835 and accuracy of 0.73\n",
      "Iteration 13419: with minibatch training loss = 0.714 and accuracy of 0.78\n",
      "Iteration 13420: with minibatch training loss = 0.616 and accuracy of 0.8\n",
      "Iteration 13421: with minibatch training loss = 0.606 and accuracy of 0.83\n",
      "Iteration 13422: with minibatch training loss = 0.855 and accuracy of 0.72\n",
      "Iteration 13423: with minibatch training loss = 0.473 and accuracy of 0.88\n",
      "Iteration 13424: with minibatch training loss = 0.757 and accuracy of 0.77\n",
      "Iteration 13425: with minibatch training loss = 0.542 and accuracy of 0.84\n",
      "Iteration 13426: with minibatch training loss = 0.678 and accuracy of 0.78\n",
      "Iteration 13427: with minibatch training loss = 0.627 and accuracy of 0.83\n",
      "Iteration 13428: with minibatch training loss = 0.653 and accuracy of 0.8\n",
      "Iteration 13429: with minibatch training loss = 0.969 and accuracy of 0.67\n",
      "Iteration 13430: with minibatch training loss = 0.925 and accuracy of 0.72\n",
      "Iteration 13431: with minibatch training loss = 0.801 and accuracy of 0.75\n",
      "Iteration 13432: with minibatch training loss = 0.626 and accuracy of 0.81\n",
      "Iteration 13433: with minibatch training loss = 0.592 and accuracy of 0.81\n",
      "Iteration 13434: with minibatch training loss = 0.471 and accuracy of 0.86\n",
      "Iteration 13435: with minibatch training loss = 0.749 and accuracy of 0.77\n",
      "Iteration 13436: with minibatch training loss = 0.703 and accuracy of 0.83\n",
      "Iteration 13437: with minibatch training loss = 0.652 and accuracy of 0.84\n",
      "Iteration 13438: with minibatch training loss = 0.875 and accuracy of 0.75\n",
      "Iteration 13439: with minibatch training loss = 0.572 and accuracy of 0.86\n",
      "Iteration 13440: with minibatch training loss = 0.725 and accuracy of 0.77\n",
      "Iteration 13441: with minibatch training loss = 0.8 and accuracy of 0.73\n",
      "Iteration 13442: with minibatch training loss = 0.723 and accuracy of 0.81\n",
      "Iteration 13443: with minibatch training loss = 0.73 and accuracy of 0.81\n",
      "Iteration 13444: with minibatch training loss = 0.401 and accuracy of 0.88\n",
      "Iteration 13445: with minibatch training loss = 0.619 and accuracy of 0.81\n",
      "Iteration 13446: with minibatch training loss = 0.763 and accuracy of 0.77\n",
      "Iteration 13447: with minibatch training loss = 0.76 and accuracy of 0.77\n",
      "Iteration 13448: with minibatch training loss = 0.573 and accuracy of 0.83\n",
      "Iteration 13449: with minibatch training loss = 0.645 and accuracy of 0.8\n",
      "Iteration 13450: with minibatch training loss = 0.648 and accuracy of 0.83\n",
      "Iteration 13451: with minibatch training loss = 0.811 and accuracy of 0.8\n",
      "Iteration 13452: with minibatch training loss = 0.567 and accuracy of 0.84\n",
      "Iteration 13453: with minibatch training loss = 0.598 and accuracy of 0.84\n",
      "Iteration 13454: with minibatch training loss = 0.417 and accuracy of 0.89\n",
      "Iteration 13455: with minibatch training loss = 0.865 and accuracy of 0.75\n",
      "Iteration 13456: with minibatch training loss = 0.524 and accuracy of 0.86\n",
      "Iteration 13457: with minibatch training loss = 0.734 and accuracy of 0.77\n",
      "Iteration 13458: with minibatch training loss = 0.544 and accuracy of 0.83\n",
      "Iteration 13459: with minibatch training loss = 0.845 and accuracy of 0.75\n",
      "Iteration 13460: with minibatch training loss = 0.713 and accuracy of 0.81\n",
      "Iteration 13461: with minibatch training loss = 0.56 and accuracy of 0.83\n",
      "Iteration 13462: with minibatch training loss = 0.528 and accuracy of 0.86\n",
      "Iteration 13463: with minibatch training loss = 0.576 and accuracy of 0.81\n",
      "Iteration 13464: with minibatch training loss = 0.775 and accuracy of 0.75\n",
      "Iteration 13465: with minibatch training loss = 0.595 and accuracy of 0.81\n",
      "Iteration 13466: with minibatch training loss = 0.609 and accuracy of 0.81\n",
      "Iteration 13467: with minibatch training loss = 0.632 and accuracy of 0.78\n",
      "Iteration 13468: with minibatch training loss = 0.754 and accuracy of 0.83\n",
      "Iteration 13469: with minibatch training loss = 0.812 and accuracy of 0.73\n",
      "Iteration 13470: with minibatch training loss = 0.596 and accuracy of 0.81\n",
      "Iteration 13471: with minibatch training loss = 0.692 and accuracy of 0.8\n",
      "Iteration 13472: with minibatch training loss = 0.796 and accuracy of 0.78\n",
      "Iteration 13473: with minibatch training loss = 0.844 and accuracy of 0.78\n",
      "Iteration 13474: with minibatch training loss = 0.625 and accuracy of 0.83\n",
      "Iteration 13475: with minibatch training loss = 0.752 and accuracy of 0.78\n",
      "Iteration 13476: with minibatch training loss = 0.739 and accuracy of 0.77\n",
      "Iteration 13477: with minibatch training loss = 0.944 and accuracy of 0.7\n",
      "Iteration 13478: with minibatch training loss = 1.02 and accuracy of 0.7\n",
      "Iteration 13479: with minibatch training loss = 0.979 and accuracy of 0.73\n",
      "Iteration 13480: with minibatch training loss = 0.712 and accuracy of 0.84\n",
      "Iteration 13481: with minibatch training loss = 0.529 and accuracy of 0.86\n",
      "Iteration 13482: with minibatch training loss = 0.755 and accuracy of 0.75\n",
      "Iteration 13483: with minibatch training loss = 0.53 and accuracy of 0.83\n",
      "Iteration 13484: with minibatch training loss = 1.14 and accuracy of 0.66\n",
      "Iteration 13485: with minibatch training loss = 0.49 and accuracy of 0.86\n",
      "Iteration 13486: with minibatch training loss = 0.965 and accuracy of 0.7\n",
      "Iteration 13487: with minibatch training loss = 0.703 and accuracy of 0.78\n",
      "Iteration 13488: with minibatch training loss = 0.74 and accuracy of 0.77\n",
      "Iteration 13489: with minibatch training loss = 0.748 and accuracy of 0.78\n",
      "Iteration 13490: with minibatch training loss = 0.757 and accuracy of 0.8\n",
      "Iteration 13491: with minibatch training loss = 0.733 and accuracy of 0.8\n",
      "Iteration 13492: with minibatch training loss = 0.577 and accuracy of 0.86\n",
      "Iteration 13493: with minibatch training loss = 0.598 and accuracy of 0.84\n",
      "Iteration 13494: with minibatch training loss = 0.648 and accuracy of 0.8\n",
      "Iteration 13495: with minibatch training loss = 0.564 and accuracy of 0.86\n",
      "Iteration 13496: with minibatch training loss = 0.766 and accuracy of 0.75\n",
      "Iteration 13497: with minibatch training loss = 0.809 and accuracy of 0.75\n",
      "Iteration 13498: with minibatch training loss = 0.594 and accuracy of 0.81\n",
      "Iteration 13499: with minibatch training loss = 0.696 and accuracy of 0.77\n",
      "Iteration 13500: with minibatch training loss = 0.694 and accuracy of 0.8\n",
      "Iteration 13501: with minibatch training loss = 0.835 and accuracy of 0.73\n",
      "Iteration 13502: with minibatch training loss = 0.323 and accuracy of 0.89\n",
      "Iteration 13503: with minibatch training loss = 0.442 and accuracy of 0.88\n",
      "Iteration 13504: with minibatch training loss = 0.791 and accuracy of 0.78\n",
      "Iteration 13505: with minibatch training loss = 0.623 and accuracy of 0.83\n",
      "Iteration 13506: with minibatch training loss = 0.699 and accuracy of 0.8\n",
      "Iteration 13507: with minibatch training loss = 0.771 and accuracy of 0.78\n",
      "Iteration 13508: with minibatch training loss = 0.65 and accuracy of 0.77\n",
      "Iteration 13509: with minibatch training loss = 0.68 and accuracy of 0.78\n",
      "Iteration 13510: with minibatch training loss = 0.539 and accuracy of 0.83\n",
      "Iteration 13511: with minibatch training loss = 0.807 and accuracy of 0.73\n",
      "Iteration 13512: with minibatch training loss = 0.661 and accuracy of 0.84\n",
      "Iteration 13513: with minibatch training loss = 0.837 and accuracy of 0.75\n",
      "Iteration 13514: with minibatch training loss = 0.835 and accuracy of 0.78\n",
      "Iteration 13515: with minibatch training loss = 0.735 and accuracy of 0.78\n",
      "Iteration 13516: with minibatch training loss = 0.486 and accuracy of 0.89\n",
      "Iteration 13517: with minibatch training loss = 0.493 and accuracy of 0.88\n",
      "Iteration 13518: with minibatch training loss = 0.696 and accuracy of 0.8\n",
      "Iteration 13519: with minibatch training loss = 0.658 and accuracy of 0.78\n",
      "Iteration 13520: with minibatch training loss = 0.928 and accuracy of 0.72\n",
      "Iteration 13521: with minibatch training loss = 0.53 and accuracy of 0.81\n",
      "Iteration 13522: with minibatch training loss = 0.735 and accuracy of 0.78\n",
      "Iteration 13523: with minibatch training loss = 0.568 and accuracy of 0.86\n",
      "Iteration 13524: with minibatch training loss = 0.538 and accuracy of 0.84\n",
      "Iteration 13525: with minibatch training loss = 0.705 and accuracy of 0.83\n",
      "Iteration 13526: with minibatch training loss = 0.643 and accuracy of 0.8\n",
      "Iteration 13527: with minibatch training loss = 0.491 and accuracy of 0.86\n",
      "Iteration 13528: with minibatch training loss = 0.57 and accuracy of 0.86\n",
      "Iteration 13529: with minibatch training loss = 0.708 and accuracy of 0.78\n",
      "Iteration 13530: with minibatch training loss = 0.699 and accuracy of 0.77\n",
      "Iteration 13531: with minibatch training loss = 0.583 and accuracy of 0.83\n",
      "Iteration 13532: with minibatch training loss = 0.658 and accuracy of 0.78\n",
      "Iteration 13533: with minibatch training loss = 0.693 and accuracy of 0.81\n",
      "Iteration 13534: with minibatch training loss = 0.481 and accuracy of 0.88\n",
      "Iteration 13535: with minibatch training loss = 0.758 and accuracy of 0.77\n",
      "Iteration 13536: with minibatch training loss = 0.89 and accuracy of 0.72\n",
      "Iteration 13537: with minibatch training loss = 0.929 and accuracy of 0.73\n",
      "Iteration 13538: with minibatch training loss = 0.686 and accuracy of 0.8\n",
      "Iteration 13539: with minibatch training loss = 0.431 and accuracy of 0.86\n",
      "Iteration 13540: with minibatch training loss = 0.662 and accuracy of 0.77\n",
      "Iteration 13541: with minibatch training loss = 0.783 and accuracy of 0.78\n",
      "Iteration 13542: with minibatch training loss = 0.618 and accuracy of 0.8\n",
      "Iteration 13543: with minibatch training loss = 0.815 and accuracy of 0.77\n",
      "Iteration 13544: with minibatch training loss = 0.693 and accuracy of 0.77\n",
      "Iteration 13545: with minibatch training loss = 0.516 and accuracy of 0.83\n",
      "Iteration 13546: with minibatch training loss = 0.459 and accuracy of 0.86\n",
      "Iteration 13547: with minibatch training loss = 0.859 and accuracy of 0.73\n",
      "Iteration 13548: with minibatch training loss = 0.618 and accuracy of 0.83\n",
      "Iteration 13549: with minibatch training loss = 0.597 and accuracy of 0.81\n",
      "Iteration 13550: with minibatch training loss = 0.741 and accuracy of 0.8\n",
      "Iteration 13551: with minibatch training loss = 0.725 and accuracy of 0.75\n",
      "Iteration 13552: with minibatch training loss = 0.826 and accuracy of 0.73\n",
      "Iteration 13553: with minibatch training loss = 0.802 and accuracy of 0.75\n",
      "Iteration 13554: with minibatch training loss = 1.11 and accuracy of 0.66\n",
      "Iteration 13555: with minibatch training loss = 0.848 and accuracy of 0.75\n",
      "Iteration 13556: with minibatch training loss = 0.696 and accuracy of 0.8\n",
      "Iteration 13557: with minibatch training loss = 0.542 and accuracy of 0.84\n",
      "Iteration 13558: with minibatch training loss = 0.689 and accuracy of 0.77\n",
      "Iteration 13559: with minibatch training loss = 0.647 and accuracy of 0.8\n",
      "Iteration 13560: with minibatch training loss = 0.785 and accuracy of 0.77\n",
      "Iteration 13561: with minibatch training loss = 0.862 and accuracy of 0.77\n",
      "Iteration 13562: with minibatch training loss = 0.558 and accuracy of 0.8\n",
      "Iteration 13563: with minibatch training loss = 0.903 and accuracy of 0.7\n",
      "Iteration 13564: with minibatch training loss = 0.767 and accuracy of 0.75\n",
      "Iteration 13565: with minibatch training loss = 0.604 and accuracy of 0.8\n",
      "Iteration 13566: with minibatch training loss = 0.58 and accuracy of 0.83\n",
      "Iteration 13567: with minibatch training loss = 0.891 and accuracy of 0.75\n",
      "Iteration 13568: with minibatch training loss = 0.347 and accuracy of 0.88\n",
      "Iteration 13569: with minibatch training loss = 0.59 and accuracy of 0.8\n",
      "Iteration 13570: with minibatch training loss = 0.772 and accuracy of 0.77\n",
      "Iteration 13571: with minibatch training loss = 0.373 and accuracy of 0.89\n",
      "Iteration 13572: with minibatch training loss = 0.79 and accuracy of 0.73\n",
      "Iteration 13573: with minibatch training loss = 0.968 and accuracy of 0.72\n",
      "Iteration 13574: with minibatch training loss = 0.778 and accuracy of 0.8\n",
      "Iteration 13575: with minibatch training loss = 0.509 and accuracy of 0.84\n",
      "Iteration 13576: with minibatch training loss = 0.592 and accuracy of 0.81\n",
      "Iteration 13577: with minibatch training loss = 0.434 and accuracy of 0.89\n",
      "Iteration 13578: with minibatch training loss = 0.76 and accuracy of 0.77\n",
      "Iteration 13579: with minibatch training loss = 0.749 and accuracy of 0.75\n",
      "Iteration 13580: with minibatch training loss = 0.822 and accuracy of 0.73\n",
      "Iteration 13581: with minibatch training loss = 0.708 and accuracy of 0.8\n",
      "Iteration 13582: with minibatch training loss = 0.578 and accuracy of 0.81\n",
      "Iteration 13583: with minibatch training loss = 0.721 and accuracy of 0.78\n",
      "Iteration 13584: with minibatch training loss = 0.971 and accuracy of 0.67\n",
      "Iteration 13585: with minibatch training loss = 0.817 and accuracy of 0.73\n",
      "Iteration 13586: with minibatch training loss = 0.531 and accuracy of 0.88\n",
      "Iteration 13587: with minibatch training loss = 0.793 and accuracy of 0.77\n",
      "Iteration 13588: with minibatch training loss = 0.618 and accuracy of 0.83\n",
      "Iteration 13589: with minibatch training loss = 0.642 and accuracy of 0.78\n",
      "Iteration 13590: with minibatch training loss = 0.619 and accuracy of 0.83\n",
      "Iteration 13591: with minibatch training loss = 0.813 and accuracy of 0.72\n",
      "Iteration 13592: with minibatch training loss = 0.761 and accuracy of 0.78\n",
      "Iteration 13593: with minibatch training loss = 0.776 and accuracy of 0.78\n",
      "Iteration 13594: with minibatch training loss = 0.858 and accuracy of 0.77\n",
      "Iteration 13595: with minibatch training loss = 0.477 and accuracy of 0.84\n",
      "Iteration 13596: with minibatch training loss = 1.07 and accuracy of 0.73\n",
      "Iteration 13597: with minibatch training loss = 0.647 and accuracy of 0.8\n",
      "Iteration 13598: with minibatch training loss = 0.763 and accuracy of 0.72\n",
      "Iteration 13599: with minibatch training loss = 0.589 and accuracy of 0.81\n",
      "Iteration 13600: with minibatch training loss = 0.794 and accuracy of 0.75\n",
      "Iteration 13601: with minibatch training loss = 0.542 and accuracy of 0.84\n",
      "Iteration 13602: with minibatch training loss = 0.674 and accuracy of 0.8\n",
      "Iteration 13603: with minibatch training loss = 0.421 and accuracy of 0.88\n",
      "Iteration 13604: with minibatch training loss = 1.06 and accuracy of 0.66\n",
      "Iteration 13605: with minibatch training loss = 0.585 and accuracy of 0.83\n",
      "Iteration 13606: with minibatch training loss = 0.738 and accuracy of 0.78\n",
      "Iteration 13607: with minibatch training loss = 0.615 and accuracy of 0.83\n",
      "Iteration 13608: with minibatch training loss = 0.579 and accuracy of 0.83\n",
      "Iteration 13609: with minibatch training loss = 0.657 and accuracy of 0.8\n",
      "Iteration 13610: with minibatch training loss = 0.639 and accuracy of 0.8\n",
      "Iteration 13611: with minibatch training loss = 0.778 and accuracy of 0.73\n",
      "Iteration 13612: with minibatch training loss = 0.445 and accuracy of 0.86\n",
      "Iteration 13613: with minibatch training loss = 1.1 and accuracy of 0.66\n",
      "Iteration 13614: with minibatch training loss = 0.76 and accuracy of 0.8\n",
      "Iteration 13615: with minibatch training loss = 0.776 and accuracy of 0.78\n",
      "Iteration 13616: with minibatch training loss = 0.881 and accuracy of 0.73\n",
      "Iteration 13617: with minibatch training loss = 0.678 and accuracy of 0.78\n",
      "Iteration 13618: with minibatch training loss = 0.536 and accuracy of 0.84\n",
      "Iteration 13619: with minibatch training loss = 0.881 and accuracy of 0.69\n",
      "Iteration 13620: with minibatch training loss = 0.595 and accuracy of 0.8\n",
      "Iteration 13621: with minibatch training loss = 0.769 and accuracy of 0.77\n",
      "Iteration 13622: with minibatch training loss = 0.452 and accuracy of 0.86\n",
      "Iteration 13623: with minibatch training loss = 0.927 and accuracy of 0.73\n",
      "Iteration 13624: with minibatch training loss = 0.731 and accuracy of 0.75\n",
      "Iteration 13625: with minibatch training loss = 0.524 and accuracy of 0.84\n",
      "Iteration 13626: with minibatch training loss = 0.7 and accuracy of 0.77\n",
      "Iteration 13627: with minibatch training loss = 0.589 and accuracy of 0.81\n",
      "Iteration 13628: with minibatch training loss = 0.72 and accuracy of 0.78\n",
      "Iteration 13629: with minibatch training loss = 0.715 and accuracy of 0.78\n",
      "Iteration 13630: with minibatch training loss = 0.618 and accuracy of 0.81\n",
      "Iteration 13631: with minibatch training loss = 0.722 and accuracy of 0.83\n",
      "Iteration 13632: with minibatch training loss = 0.55 and accuracy of 0.84\n",
      "Iteration 13633: with minibatch training loss = 0.814 and accuracy of 0.73\n",
      "Iteration 13634: with minibatch training loss = 0.819 and accuracy of 0.75\n",
      "Iteration 13635: with minibatch training loss = 0.529 and accuracy of 0.84\n",
      "Iteration 13636: with minibatch training loss = 0.77 and accuracy of 0.77\n",
      "Iteration 13637: with minibatch training loss = 0.41 and accuracy of 0.86\n",
      "Iteration 13638: with minibatch training loss = 0.899 and accuracy of 0.7\n",
      "Iteration 13639: with minibatch training loss = 0.648 and accuracy of 0.8\n",
      "Iteration 13640: with minibatch training loss = 0.853 and accuracy of 0.7\n",
      "Iteration 13641: with minibatch training loss = 0.673 and accuracy of 0.77\n",
      "Iteration 13642: with minibatch training loss = 0.775 and accuracy of 0.78\n",
      "Iteration 13643: with minibatch training loss = 0.802 and accuracy of 0.77\n",
      "Iteration 13644: with minibatch training loss = 0.614 and accuracy of 0.8\n",
      "Iteration 13645: with minibatch training loss = 0.857 and accuracy of 0.73\n",
      "Iteration 13646: with minibatch training loss = 0.705 and accuracy of 0.8\n",
      "Iteration 13647: with minibatch training loss = 0.907 and accuracy of 0.72\n",
      "Iteration 13648: with minibatch training loss = 0.928 and accuracy of 0.7\n",
      "Iteration 13649: with minibatch training loss = 0.745 and accuracy of 0.8\n",
      "Iteration 13650: with minibatch training loss = 0.644 and accuracy of 0.78\n",
      "Iteration 13651: with minibatch training loss = 0.45 and accuracy of 0.86\n",
      "Iteration 13652: with minibatch training loss = 0.69 and accuracy of 0.77\n",
      "Iteration 13653: with minibatch training loss = 0.696 and accuracy of 0.78\n",
      "Iteration 13654: with minibatch training loss = 0.613 and accuracy of 0.86\n",
      "Iteration 13655: with minibatch training loss = 0.642 and accuracy of 0.8\n",
      "Iteration 13656: with minibatch training loss = 0.794 and accuracy of 0.73\n",
      "Iteration 13657: with minibatch training loss = 0.742 and accuracy of 0.78\n",
      "Iteration 13658: with minibatch training loss = 0.813 and accuracy of 0.77\n",
      "Iteration 13659: with minibatch training loss = 0.789 and accuracy of 0.77\n",
      "Iteration 13660: with minibatch training loss = 0.742 and accuracy of 0.78\n",
      "Iteration 13661: with minibatch training loss = 0.535 and accuracy of 0.84\n",
      "Iteration 13662: with minibatch training loss = 0.76 and accuracy of 0.73\n",
      "Iteration 13663: with minibatch training loss = 0.751 and accuracy of 0.73\n",
      "Iteration 13664: with minibatch training loss = 0.59 and accuracy of 0.81\n",
      "Iteration 13665: with minibatch training loss = 0.699 and accuracy of 0.78\n",
      "Iteration 13666: with minibatch training loss = 0.691 and accuracy of 0.78\n",
      "Iteration 13667: with minibatch training loss = 0.745 and accuracy of 0.78\n",
      "Iteration 13668: with minibatch training loss = 0.507 and accuracy of 0.84\n",
      "Iteration 13669: with minibatch training loss = 0.816 and accuracy of 0.75\n",
      "Iteration 13670: with minibatch training loss = 0.622 and accuracy of 0.8\n",
      "Iteration 13671: with minibatch training loss = 0.518 and accuracy of 0.84\n",
      "Iteration 13672: with minibatch training loss = 0.477 and accuracy of 0.88\n",
      "Iteration 13673: with minibatch training loss = 0.715 and accuracy of 0.77\n",
      "Iteration 13674: with minibatch training loss = 0.855 and accuracy of 0.73\n",
      "Iteration 13675: with minibatch training loss = 0.384 and accuracy of 0.88\n",
      "Iteration 13676: with minibatch training loss = 0.828 and accuracy of 0.77\n",
      "Iteration 13677: with minibatch training loss = 0.744 and accuracy of 0.78\n",
      "Iteration 13678: with minibatch training loss = 0.952 and accuracy of 0.7\n",
      "Iteration 13679: with minibatch training loss = 0.641 and accuracy of 0.83\n",
      "Iteration 13680: with minibatch training loss = 0.739 and accuracy of 0.81\n",
      "Iteration 13681: with minibatch training loss = 0.801 and accuracy of 0.77\n",
      "Iteration 13682: with minibatch training loss = 0.588 and accuracy of 0.86\n",
      "Iteration 13683: with minibatch training loss = 0.666 and accuracy of 0.81\n",
      "Iteration 13684: with minibatch training loss = 0.743 and accuracy of 0.75\n",
      "Iteration 13685: with minibatch training loss = 0.868 and accuracy of 0.72\n",
      "Iteration 13686: with minibatch training loss = 0.46 and accuracy of 0.86\n",
      "Iteration 13687: with minibatch training loss = 0.764 and accuracy of 0.75\n",
      "Iteration 13688: with minibatch training loss = 0.593 and accuracy of 0.81\n",
      "Iteration 13689: with minibatch training loss = 0.485 and accuracy of 0.86\n",
      "Iteration 13690: with minibatch training loss = 0.624 and accuracy of 0.81\n",
      "Iteration 13691: with minibatch training loss = 0.562 and accuracy of 0.83\n",
      "Iteration 13692: with minibatch training loss = 0.458 and accuracy of 0.88\n",
      "Iteration 13693: with minibatch training loss = 0.418 and accuracy of 0.88\n",
      "Iteration 13694: with minibatch training loss = 0.637 and accuracy of 0.8\n",
      "Iteration 13695: with minibatch training loss = 0.708 and accuracy of 0.75\n",
      "Iteration 13696: with minibatch training loss = 0.83 and accuracy of 0.75\n",
      "Iteration 13697: with minibatch training loss = 0.503 and accuracy of 0.84\n",
      "Iteration 13698: with minibatch training loss = 0.62 and accuracy of 0.86\n",
      "Iteration 13699: with minibatch training loss = 0.462 and accuracy of 0.84\n",
      "Iteration 13700: with minibatch training loss = 0.697 and accuracy of 0.75\n",
      "Iteration 13701: with minibatch training loss = 0.465 and accuracy of 0.86\n",
      "Iteration 13702: with minibatch training loss = 0.567 and accuracy of 0.83\n",
      "Iteration 13703: with minibatch training loss = 0.382 and accuracy of 0.89\n",
      "Iteration 13704: with minibatch training loss = 0.736 and accuracy of 0.8\n",
      "Iteration 13705: with minibatch training loss = 0.442 and accuracy of 0.89\n",
      "Iteration 13706: with minibatch training loss = 0.71 and accuracy of 0.75\n",
      "Iteration 13707: with minibatch training loss = 0.655 and accuracy of 0.8\n",
      "Iteration 13708: with minibatch training loss = 0.864 and accuracy of 0.78\n",
      "Iteration 13709: with minibatch training loss = 0.784 and accuracy of 0.78\n",
      "Iteration 13710: with minibatch training loss = 0.734 and accuracy of 0.78\n",
      "Iteration 13711: with minibatch training loss = 0.755 and accuracy of 0.78\n",
      "Iteration 13712: with minibatch training loss = 0.632 and accuracy of 0.81\n",
      "Iteration 13713: with minibatch training loss = 0.57 and accuracy of 0.84\n",
      "Iteration 13714: with minibatch training loss = 0.418 and accuracy of 0.88\n",
      "Iteration 13715: with minibatch training loss = 0.809 and accuracy of 0.81\n",
      "Iteration 13716: with minibatch training loss = 0.534 and accuracy of 0.86\n",
      "Iteration 13717: with minibatch training loss = 0.642 and accuracy of 0.8\n",
      "Iteration 13718: with minibatch training loss = 0.691 and accuracy of 0.81\n",
      "Iteration 13719: with minibatch training loss = 0.618 and accuracy of 0.81\n",
      "Iteration 13720: with minibatch training loss = 0.813 and accuracy of 0.75\n",
      "Iteration 13721: with minibatch training loss = 0.532 and accuracy of 0.84\n",
      "Iteration 13722: with minibatch training loss = 0.49 and accuracy of 0.84\n",
      "Iteration 13723: with minibatch training loss = 0.493 and accuracy of 0.88\n",
      "Iteration 13724: with minibatch training loss = 0.64 and accuracy of 0.81\n",
      "Iteration 13725: with minibatch training loss = 0.887 and accuracy of 0.7\n",
      "Iteration 13726: with minibatch training loss = 0.54 and accuracy of 0.84\n",
      "Iteration 13727: with minibatch training loss = 0.451 and accuracy of 0.86\n",
      "Iteration 13728: with minibatch training loss = 0.474 and accuracy of 0.88\n",
      "Iteration 13729: with minibatch training loss = 0.765 and accuracy of 0.77\n",
      "Iteration 13730: with minibatch training loss = 0.674 and accuracy of 0.8\n",
      "Iteration 13731: with minibatch training loss = 0.609 and accuracy of 0.81\n",
      "Iteration 13732: with minibatch training loss = 0.344 and accuracy of 0.91\n",
      "Iteration 13733: with minibatch training loss = 0.739 and accuracy of 0.75\n",
      "Iteration 13734: with minibatch training loss = 0.826 and accuracy of 0.77\n",
      "Iteration 13735: with minibatch training loss = 0.835 and accuracy of 0.72\n",
      "Iteration 13736: with minibatch training loss = 0.383 and accuracy of 0.88\n",
      "Iteration 13737: with minibatch training loss = 0.77 and accuracy of 0.81\n",
      "Iteration 13738: with minibatch training loss = 0.597 and accuracy of 0.81\n",
      "Iteration 13739: with minibatch training loss = 0.661 and accuracy of 0.8\n",
      "Iteration 13740: with minibatch training loss = 0.76 and accuracy of 0.77\n",
      "Iteration 13741: with minibatch training loss = 0.737 and accuracy of 0.77\n",
      "Iteration 13742: with minibatch training loss = 0.688 and accuracy of 0.8\n",
      "Iteration 13743: with minibatch training loss = 0.91 and accuracy of 0.77\n",
      "Iteration 13744: with minibatch training loss = 0.719 and accuracy of 0.77\n",
      "Iteration 13745: with minibatch training loss = 0.68 and accuracy of 0.8\n",
      "Iteration 13746: with minibatch training loss = 0.738 and accuracy of 0.73\n",
      "Iteration 13747: with minibatch training loss = 0.517 and accuracy of 0.84\n",
      "Iteration 13748: with minibatch training loss = 0.451 and accuracy of 0.89\n",
      "Iteration 13749: with minibatch training loss = 0.768 and accuracy of 0.78\n",
      "Iteration 13750: with minibatch training loss = 0.696 and accuracy of 0.8\n",
      "Iteration 13751: with minibatch training loss = 0.727 and accuracy of 0.78\n",
      "Iteration 13752: with minibatch training loss = 0.616 and accuracy of 0.78\n",
      "Iteration 13753: with minibatch training loss = 0.705 and accuracy of 0.77\n",
      "Iteration 13754: with minibatch training loss = 0.594 and accuracy of 0.81\n",
      "Iteration 13755: with minibatch training loss = 0.891 and accuracy of 0.75\n",
      "Iteration 13756: with minibatch training loss = 0.616 and accuracy of 0.8\n",
      "Iteration 13757: with minibatch training loss = 0.711 and accuracy of 0.75\n",
      "Iteration 13758: with minibatch training loss = 0.804 and accuracy of 0.75\n",
      "Iteration 13759: with minibatch training loss = 0.842 and accuracy of 0.73\n",
      "Iteration 13760: with minibatch training loss = 0.745 and accuracy of 0.77\n",
      "Iteration 13761: with minibatch training loss = 0.728 and accuracy of 0.81\n",
      "Iteration 13762: with minibatch training loss = 0.509 and accuracy of 0.81\n",
      "Iteration 13763: with minibatch training loss = 0.731 and accuracy of 0.77\n",
      "Iteration 13764: with minibatch training loss = 0.515 and accuracy of 0.88\n",
      "Iteration 13765: with minibatch training loss = 0.587 and accuracy of 0.86\n",
      "Iteration 13766: with minibatch training loss = 0.719 and accuracy of 0.8\n",
      "Iteration 13767: with minibatch training loss = 0.714 and accuracy of 0.75\n",
      "Iteration 13768: with minibatch training loss = 0.91 and accuracy of 0.7\n",
      "Iteration 13769: with minibatch training loss = 0.554 and accuracy of 0.84\n",
      "Iteration 13770: with minibatch training loss = 0.699 and accuracy of 0.81\n",
      "Iteration 13771: with minibatch training loss = 0.359 and accuracy of 0.91\n",
      "Iteration 13772: with minibatch training loss = 0.45 and accuracy of 0.86\n",
      "Iteration 13773: with minibatch training loss = 0.691 and accuracy of 0.8\n",
      "Iteration 13774: with minibatch training loss = 0.741 and accuracy of 0.77\n",
      "Iteration 13775: with minibatch training loss = 0.523 and accuracy of 0.83\n",
      "Iteration 13776: with minibatch training loss = 0.661 and accuracy of 0.8\n",
      "Iteration 13777: with minibatch training loss = 0.912 and accuracy of 0.7\n",
      "Iteration 13778: with minibatch training loss = 0.736 and accuracy of 0.77\n",
      "Iteration 13779: with minibatch training loss = 0.58 and accuracy of 0.83\n",
      "Iteration 13780: with minibatch training loss = 0.762 and accuracy of 0.8\n",
      "Iteration 13781: with minibatch training loss = 0.798 and accuracy of 0.77\n",
      "Iteration 13782: with minibatch training loss = 0.614 and accuracy of 0.8\n",
      "Iteration 13783: with minibatch training loss = 0.517 and accuracy of 0.89\n",
      "Iteration 13784: with minibatch training loss = 0.742 and accuracy of 0.78\n",
      "Iteration 13785: with minibatch training loss = 0.612 and accuracy of 0.81\n",
      "Iteration 13786: with minibatch training loss = 0.426 and accuracy of 0.89\n",
      "Iteration 13787: with minibatch training loss = 0.67 and accuracy of 0.8\n",
      "Iteration 13788: with minibatch training loss = 0.575 and accuracy of 0.88\n",
      "Iteration 13789: with minibatch training loss = 0.628 and accuracy of 0.83\n",
      "Iteration 13790: with minibatch training loss = 0.849 and accuracy of 0.75\n",
      "Iteration 13791: with minibatch training loss = 0.651 and accuracy of 0.8\n",
      "Iteration 13792: with minibatch training loss = 0.563 and accuracy of 0.86\n",
      "Iteration 13793: with minibatch training loss = 0.608 and accuracy of 0.8\n",
      "Iteration 13794: with minibatch training loss = 0.503 and accuracy of 0.86\n",
      "Iteration 13795: with minibatch training loss = 0.299 and accuracy of 0.92\n",
      "Iteration 13796: with minibatch training loss = 0.557 and accuracy of 0.84\n",
      "Iteration 13797: with minibatch training loss = 0.73 and accuracy of 0.73\n",
      "Iteration 13798: with minibatch training loss = 0.459 and accuracy of 0.88\n",
      "Iteration 13799: with minibatch training loss = 0.7 and accuracy of 0.83\n",
      "Iteration 13800: with minibatch training loss = 1.08 and accuracy of 0.7\n",
      "Iteration 13801: with minibatch training loss = 0.794 and accuracy of 0.73\n",
      "Iteration 13802: with minibatch training loss = 0.951 and accuracy of 0.7\n",
      "Iteration 13803: with minibatch training loss = 0.952 and accuracy of 0.7\n",
      "Iteration 13804: with minibatch training loss = 0.779 and accuracy of 0.77\n",
      "Iteration 13805: with minibatch training loss = 0.752 and accuracy of 0.78\n",
      "Iteration 13806: with minibatch training loss = 0.828 and accuracy of 0.77\n",
      "Iteration 13807: with minibatch training loss = 0.626 and accuracy of 0.8\n",
      "Iteration 13808: with minibatch training loss = 0.907 and accuracy of 0.72\n",
      "Iteration 13809: with minibatch training loss = 0.82 and accuracy of 0.75\n",
      "Iteration 13810: with minibatch training loss = 0.613 and accuracy of 0.83\n",
      "Iteration 13811: with minibatch training loss = 0.49 and accuracy of 0.84\n",
      "Iteration 13812: with minibatch training loss = 0.718 and accuracy of 0.75\n",
      "Iteration 13813: with minibatch training loss = 0.453 and accuracy of 0.86\n",
      "Iteration 13814: with minibatch training loss = 0.7 and accuracy of 0.8\n",
      "Iteration 13815: with minibatch training loss = 0.335 and accuracy of 0.89\n",
      "Iteration 13816: with minibatch training loss = 0.767 and accuracy of 0.77\n",
      "Iteration 13817: with minibatch training loss = 0.925 and accuracy of 0.7\n",
      "Iteration 13818: with minibatch training loss = 0.9 and accuracy of 0.73\n",
      "Iteration 13819: with minibatch training loss = 0.74 and accuracy of 0.77\n",
      "Iteration 13820: with minibatch training loss = 0.623 and accuracy of 0.8\n",
      "Iteration 13821: with minibatch training loss = 0.625 and accuracy of 0.83\n",
      "Iteration 13822: with minibatch training loss = 0.722 and accuracy of 0.8\n",
      "Iteration 13823: with minibatch training loss = 0.49 and accuracy of 0.86\n",
      "Iteration 13824: with minibatch training loss = 0.459 and accuracy of 0.88\n",
      "Iteration 13825: with minibatch training loss = 0.628 and accuracy of 0.88\n",
      "Iteration 13826: with minibatch training loss = 0.516 and accuracy of 0.84\n",
      "Iteration 13827: with minibatch training loss = 0.622 and accuracy of 0.8\n",
      "Iteration 13828: with minibatch training loss = 0.654 and accuracy of 0.78\n",
      "Iteration 13829: with minibatch training loss = 0.711 and accuracy of 0.8\n",
      "Iteration 13830: with minibatch training loss = 0.816 and accuracy of 0.73\n",
      "Iteration 13831: with minibatch training loss = 0.668 and accuracy of 0.81\n",
      "Iteration 13832: with minibatch training loss = 0.718 and accuracy of 0.75\n",
      "Iteration 13833: with minibatch training loss = 0.67 and accuracy of 0.81\n",
      "Iteration 13834: with minibatch training loss = 0.4 and accuracy of 0.89\n",
      "Iteration 13835: with minibatch training loss = 1.03 and accuracy of 0.7\n",
      "Iteration 13836: with minibatch training loss = 0.565 and accuracy of 0.83\n",
      "Iteration 13837: with minibatch training loss = 0.628 and accuracy of 0.8\n",
      "Iteration 13838: with minibatch training loss = 0.648 and accuracy of 0.8\n",
      "Iteration 13839: with minibatch training loss = 0.752 and accuracy of 0.8\n",
      "Iteration 13840: with minibatch training loss = 0.767 and accuracy of 0.75\n",
      "Iteration 13841: with minibatch training loss = 0.603 and accuracy of 0.83\n",
      "Iteration 13842: with minibatch training loss = 0.793 and accuracy of 0.77\n",
      "Iteration 13843: with minibatch training loss = 0.572 and accuracy of 0.83\n",
      "Iteration 13844: with minibatch training loss = 0.529 and accuracy of 0.83\n",
      "Iteration 13845: with minibatch training loss = 0.881 and accuracy of 0.72\n",
      "Iteration 13846: with minibatch training loss = 0.576 and accuracy of 0.83\n",
      "Iteration 13847: with minibatch training loss = 0.547 and accuracy of 0.84\n",
      "Iteration 13848: with minibatch training loss = 0.643 and accuracy of 0.81\n",
      "Iteration 13849: with minibatch training loss = 0.536 and accuracy of 0.84\n",
      "Iteration 13850: with minibatch training loss = 0.818 and accuracy of 0.8\n",
      "Iteration 13851: with minibatch training loss = 0.692 and accuracy of 0.8\n",
      "Iteration 13852: with minibatch training loss = 0.507 and accuracy of 0.81\n",
      "Iteration 13853: with minibatch training loss = 0.786 and accuracy of 0.8\n",
      "Iteration 13854: with minibatch training loss = 0.772 and accuracy of 0.75\n",
      "Iteration 13855: with minibatch training loss = 0.523 and accuracy of 0.83\n",
      "Iteration 13856: with minibatch training loss = 0.675 and accuracy of 0.83\n",
      "Iteration 13857: with minibatch training loss = 0.741 and accuracy of 0.78\n",
      "Iteration 13858: with minibatch training loss = 0.693 and accuracy of 0.83\n",
      "Iteration 13859: with minibatch training loss = 0.739 and accuracy of 0.78\n",
      "Iteration 13860: with minibatch training loss = 0.861 and accuracy of 0.75\n",
      "Iteration 13861: with minibatch training loss = 0.757 and accuracy of 0.73\n",
      "Iteration 13862: with minibatch training loss = 0.517 and accuracy of 0.86\n",
      "Iteration 13863: with minibatch training loss = 0.339 and accuracy of 0.91\n",
      "Iteration 13864: with minibatch training loss = 0.818 and accuracy of 0.75\n",
      "Iteration 13865: with minibatch training loss = 0.581 and accuracy of 0.81\n",
      "Iteration 13866: with minibatch training loss = 0.545 and accuracy of 0.86\n",
      "Iteration 13867: with minibatch training loss = 0.748 and accuracy of 0.8\n",
      "Iteration 13868: with minibatch training loss = 0.611 and accuracy of 0.81\n",
      "Iteration 13869: with minibatch training loss = 0.634 and accuracy of 0.81\n",
      "Iteration 13870: with minibatch training loss = 0.625 and accuracy of 0.81\n",
      "Iteration 13871: with minibatch training loss = 0.571 and accuracy of 0.86\n",
      "Iteration 13872: with minibatch training loss = 0.492 and accuracy of 0.86\n",
      "Iteration 13873: with minibatch training loss = 0.568 and accuracy of 0.83\n",
      "Iteration 13874: with minibatch training loss = 0.689 and accuracy of 0.81\n",
      "Iteration 13875: with minibatch training loss = 0.669 and accuracy of 0.77\n",
      "Iteration 13876: with minibatch training loss = 0.667 and accuracy of 0.77\n",
      "Iteration 13877: with minibatch training loss = 0.628 and accuracy of 0.8\n",
      "Iteration 13878: with minibatch training loss = 0.717 and accuracy of 0.77\n",
      "Validation loss: 0.22990756\n",
      "Epoch 10, Overall loss = 0.705 and accuracy of 0.788\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJNBnDBbcAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12068e6a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13880: with minibatch training loss = 0.744 and accuracy of 0.77\n",
      "Iteration 13881: with minibatch training loss = 0.949 and accuracy of 0.73\n",
      "Iteration 13882: with minibatch training loss = 0.537 and accuracy of 0.86\n",
      "Iteration 13883: with minibatch training loss = 0.576 and accuracy of 0.81\n",
      "Iteration 13884: with minibatch training loss = 0.808 and accuracy of 0.75\n",
      "Iteration 13885: with minibatch training loss = 1.01 and accuracy of 0.7\n",
      "Iteration 13886: with minibatch training loss = 0.54 and accuracy of 0.84\n",
      "Iteration 13887: with minibatch training loss = 0.8 and accuracy of 0.8\n",
      "Iteration 13888: with minibatch training loss = 0.531 and accuracy of 0.84\n",
      "Iteration 13889: with minibatch training loss = 0.69 and accuracy of 0.81\n",
      "Iteration 13890: with minibatch training loss = 1.11 and accuracy of 0.7\n",
      "Iteration 13891: with minibatch training loss = 0.816 and accuracy of 0.77\n",
      "Iteration 13892: with minibatch training loss = 0.634 and accuracy of 0.8\n",
      "Iteration 13893: with minibatch training loss = 0.913 and accuracy of 0.72\n",
      "Iteration 13894: with minibatch training loss = 0.702 and accuracy of 0.77\n",
      "Iteration 13895: with minibatch training loss = 0.963 and accuracy of 0.69\n",
      "Iteration 13896: with minibatch training loss = 0.595 and accuracy of 0.8\n",
      "Iteration 13897: with minibatch training loss = 0.56 and accuracy of 0.86\n",
      "Iteration 13898: with minibatch training loss = 0.74 and accuracy of 0.78\n",
      "Iteration 13899: with minibatch training loss = 0.667 and accuracy of 0.78\n",
      "Iteration 13900: with minibatch training loss = 0.543 and accuracy of 0.83\n",
      "Iteration 13901: with minibatch training loss = 0.416 and accuracy of 0.88\n",
      "Iteration 13902: with minibatch training loss = 0.62 and accuracy of 0.81\n",
      "Iteration 13903: with minibatch training loss = 0.79 and accuracy of 0.72\n",
      "Iteration 13904: with minibatch training loss = 0.754 and accuracy of 0.77\n",
      "Iteration 13905: with minibatch training loss = 0.475 and accuracy of 0.84\n",
      "Iteration 13906: with minibatch training loss = 0.839 and accuracy of 0.75\n",
      "Iteration 13907: with minibatch training loss = 0.62 and accuracy of 0.83\n",
      "Iteration 13908: with minibatch training loss = 0.52 and accuracy of 0.86\n",
      "Iteration 13909: with minibatch training loss = 0.803 and accuracy of 0.75\n",
      "Iteration 13910: with minibatch training loss = 0.914 and accuracy of 0.77\n",
      "Iteration 13911: with minibatch training loss = 0.844 and accuracy of 0.73\n",
      "Iteration 13912: with minibatch training loss = 0.532 and accuracy of 0.86\n",
      "Iteration 13913: with minibatch training loss = 0.536 and accuracy of 0.84\n",
      "Iteration 13914: with minibatch training loss = 0.876 and accuracy of 0.72\n",
      "Iteration 13915: with minibatch training loss = 0.601 and accuracy of 0.83\n",
      "Iteration 13916: with minibatch training loss = 0.548 and accuracy of 0.83\n",
      "Iteration 13917: with minibatch training loss = 0.513 and accuracy of 0.88\n",
      "Iteration 13918: with minibatch training loss = 0.665 and accuracy of 0.8\n",
      "Iteration 13919: with minibatch training loss = 0.687 and accuracy of 0.78\n",
      "Iteration 13920: with minibatch training loss = 0.753 and accuracy of 0.78\n",
      "Iteration 13921: with minibatch training loss = 0.859 and accuracy of 0.72\n",
      "Iteration 13922: with minibatch training loss = 0.687 and accuracy of 0.77\n",
      "Iteration 13923: with minibatch training loss = 0.806 and accuracy of 0.75\n",
      "Iteration 13924: with minibatch training loss = 0.636 and accuracy of 0.8\n",
      "Iteration 13925: with minibatch training loss = 0.633 and accuracy of 0.8\n",
      "Iteration 13926: with minibatch training loss = 0.523 and accuracy of 0.86\n",
      "Iteration 13927: with minibatch training loss = 0.697 and accuracy of 0.77\n",
      "Iteration 13928: with minibatch training loss = 0.802 and accuracy of 0.77\n",
      "Iteration 13929: with minibatch training loss = 0.57 and accuracy of 0.83\n",
      "Iteration 13930: with minibatch training loss = 0.694 and accuracy of 0.78\n",
      "Iteration 13931: with minibatch training loss = 0.782 and accuracy of 0.73\n",
      "Iteration 13932: with minibatch training loss = 0.521 and accuracy of 0.86\n",
      "Iteration 13933: with minibatch training loss = 0.483 and accuracy of 0.86\n",
      "Iteration 13934: with minibatch training loss = 0.685 and accuracy of 0.83\n",
      "Iteration 13935: with minibatch training loss = 0.666 and accuracy of 0.8\n",
      "Iteration 13936: with minibatch training loss = 0.716 and accuracy of 0.78\n",
      "Iteration 13937: with minibatch training loss = 0.622 and accuracy of 0.83\n",
      "Iteration 13938: with minibatch training loss = 0.627 and accuracy of 0.81\n",
      "Iteration 13939: with minibatch training loss = 0.741 and accuracy of 0.8\n",
      "Iteration 13940: with minibatch training loss = 0.695 and accuracy of 0.8\n",
      "Iteration 13941: with minibatch training loss = 0.578 and accuracy of 0.8\n",
      "Iteration 13942: with minibatch training loss = 0.928 and accuracy of 0.7\n",
      "Iteration 13943: with minibatch training loss = 0.833 and accuracy of 0.72\n",
      "Iteration 13944: with minibatch training loss = 0.862 and accuracy of 0.75\n",
      "Iteration 13945: with minibatch training loss = 0.659 and accuracy of 0.8\n",
      "Iteration 13946: with minibatch training loss = 0.653 and accuracy of 0.78\n",
      "Iteration 13947: with minibatch training loss = 0.784 and accuracy of 0.75\n",
      "Iteration 13948: with minibatch training loss = 0.81 and accuracy of 0.7\n",
      "Iteration 13949: with minibatch training loss = 0.716 and accuracy of 0.78\n",
      "Iteration 13950: with minibatch training loss = 0.821 and accuracy of 0.77\n",
      "Iteration 13951: with minibatch training loss = 0.884 and accuracy of 0.73\n",
      "Iteration 13952: with minibatch training loss = 0.977 and accuracy of 0.78\n",
      "Iteration 13953: with minibatch training loss = 0.743 and accuracy of 0.77\n",
      "Iteration 13954: with minibatch training loss = 0.763 and accuracy of 0.75\n",
      "Iteration 13955: with minibatch training loss = 0.637 and accuracy of 0.81\n",
      "Iteration 13956: with minibatch training loss = 0.75 and accuracy of 0.8\n",
      "Iteration 13957: with minibatch training loss = 0.891 and accuracy of 0.77\n",
      "Iteration 13958: with minibatch training loss = 0.438 and accuracy of 0.88\n",
      "Iteration 13959: with minibatch training loss = 0.703 and accuracy of 0.78\n",
      "Iteration 13960: with minibatch training loss = 0.7 and accuracy of 0.77\n",
      "Iteration 13961: with minibatch training loss = 0.689 and accuracy of 0.8\n",
      "Iteration 13962: with minibatch training loss = 0.604 and accuracy of 0.8\n",
      "Iteration 13963: with minibatch training loss = 0.506 and accuracy of 0.84\n",
      "Iteration 13964: with minibatch training loss = 0.598 and accuracy of 0.81\n",
      "Iteration 13965: with minibatch training loss = 0.631 and accuracy of 0.81\n",
      "Iteration 13966: with minibatch training loss = 0.822 and accuracy of 0.73\n",
      "Iteration 13967: with minibatch training loss = 0.51 and accuracy of 0.86\n",
      "Iteration 13968: with minibatch training loss = 0.496 and accuracy of 0.84\n",
      "Iteration 13969: with minibatch training loss = 0.692 and accuracy of 0.77\n",
      "Iteration 13970: with minibatch training loss = 0.793 and accuracy of 0.75\n",
      "Iteration 13971: with minibatch training loss = 0.969 and accuracy of 0.7\n",
      "Iteration 13972: with minibatch training loss = 0.808 and accuracy of 0.73\n",
      "Iteration 13973: with minibatch training loss = 0.519 and accuracy of 0.83\n",
      "Iteration 13974: with minibatch training loss = 0.407 and accuracy of 0.86\n",
      "Iteration 13975: with minibatch training loss = 0.558 and accuracy of 0.84\n",
      "Iteration 13976: with minibatch training loss = 0.581 and accuracy of 0.83\n",
      "Iteration 13977: with minibatch training loss = 0.615 and accuracy of 0.8\n",
      "Iteration 13978: with minibatch training loss = 0.511 and accuracy of 0.86\n",
      "Iteration 13979: with minibatch training loss = 0.599 and accuracy of 0.81\n",
      "Iteration 13980: with minibatch training loss = 0.599 and accuracy of 0.83\n",
      "Iteration 13981: with minibatch training loss = 0.921 and accuracy of 0.73\n",
      "Iteration 13982: with minibatch training loss = 0.819 and accuracy of 0.77\n",
      "Iteration 13983: with minibatch training loss = 0.906 and accuracy of 0.75\n",
      "Iteration 13984: with minibatch training loss = 0.715 and accuracy of 0.8\n",
      "Iteration 13985: with minibatch training loss = 0.485 and accuracy of 0.86\n",
      "Iteration 13986: with minibatch training loss = 0.685 and accuracy of 0.81\n",
      "Iteration 13987: with minibatch training loss = 0.572 and accuracy of 0.83\n",
      "Iteration 13988: with minibatch training loss = 0.684 and accuracy of 0.81\n",
      "Iteration 13989: with minibatch training loss = 0.6 and accuracy of 0.83\n",
      "Iteration 13990: with minibatch training loss = 0.613 and accuracy of 0.81\n",
      "Iteration 13991: with minibatch training loss = 0.821 and accuracy of 0.73\n",
      "Iteration 13992: with minibatch training loss = 0.793 and accuracy of 0.75\n",
      "Iteration 13993: with minibatch training loss = 0.387 and accuracy of 0.88\n",
      "Iteration 13994: with minibatch training loss = 0.524 and accuracy of 0.86\n",
      "Iteration 13995: with minibatch training loss = 0.58 and accuracy of 0.84\n",
      "Iteration 13996: with minibatch training loss = 0.561 and accuracy of 0.86\n",
      "Iteration 13997: with minibatch training loss = 0.62 and accuracy of 0.83\n",
      "Iteration 13998: with minibatch training loss = 0.37 and accuracy of 0.91\n",
      "Iteration 13999: with minibatch training loss = 0.644 and accuracy of 0.8\n",
      "Iteration 14000: with minibatch training loss = 0.799 and accuracy of 0.73\n",
      "Iteration 14001: with minibatch training loss = 0.49 and accuracy of 0.86\n",
      "Iteration 14002: with minibatch training loss = 0.798 and accuracy of 0.73\n",
      "Iteration 14003: with minibatch training loss = 0.613 and accuracy of 0.81\n",
      "Iteration 14004: with minibatch training loss = 0.619 and accuracy of 0.84\n",
      "Iteration 14005: with minibatch training loss = 0.824 and accuracy of 0.75\n",
      "Iteration 14006: with minibatch training loss = 0.605 and accuracy of 0.81\n",
      "Iteration 14007: with minibatch training loss = 0.905 and accuracy of 0.72\n",
      "Iteration 14008: with minibatch training loss = 0.471 and accuracy of 0.86\n",
      "Iteration 14009: with minibatch training loss = 0.721 and accuracy of 0.83\n",
      "Iteration 14010: with minibatch training loss = 0.535 and accuracy of 0.84\n",
      "Iteration 14011: with minibatch training loss = 0.8 and accuracy of 0.75\n",
      "Iteration 14012: with minibatch training loss = 0.53 and accuracy of 0.84\n",
      "Iteration 14013: with minibatch training loss = 0.776 and accuracy of 0.75\n",
      "Iteration 14014: with minibatch training loss = 0.749 and accuracy of 0.75\n",
      "Iteration 14015: with minibatch training loss = 0.786 and accuracy of 0.77\n",
      "Iteration 14016: with minibatch training loss = 0.898 and accuracy of 0.72\n",
      "Iteration 14017: with minibatch training loss = 0.681 and accuracy of 0.84\n",
      "Iteration 14018: with minibatch training loss = 0.459 and accuracy of 0.88\n",
      "Iteration 14019: with minibatch training loss = 0.623 and accuracy of 0.77\n",
      "Iteration 14020: with minibatch training loss = 0.553 and accuracy of 0.83\n",
      "Iteration 14021: with minibatch training loss = 0.663 and accuracy of 0.8\n",
      "Iteration 14022: with minibatch training loss = 0.66 and accuracy of 0.83\n",
      "Iteration 14023: with minibatch training loss = 0.822 and accuracy of 0.8\n",
      "Iteration 14024: with minibatch training loss = 0.837 and accuracy of 0.75\n",
      "Iteration 14025: with minibatch training loss = 0.87 and accuracy of 0.75\n",
      "Iteration 14026: with minibatch training loss = 0.934 and accuracy of 0.72\n",
      "Iteration 14027: with minibatch training loss = 0.713 and accuracy of 0.78\n",
      "Iteration 14028: with minibatch training loss = 0.721 and accuracy of 0.77\n",
      "Iteration 14029: with minibatch training loss = 0.534 and accuracy of 0.84\n",
      "Iteration 14030: with minibatch training loss = 0.773 and accuracy of 0.77\n",
      "Iteration 14031: with minibatch training loss = 0.687 and accuracy of 0.77\n",
      "Iteration 14032: with minibatch training loss = 0.786 and accuracy of 0.75\n",
      "Iteration 14033: with minibatch training loss = 0.565 and accuracy of 0.83\n",
      "Iteration 14034: with minibatch training loss = 0.772 and accuracy of 0.75\n",
      "Iteration 14035: with minibatch training loss = 0.573 and accuracy of 0.8\n",
      "Iteration 14036: with minibatch training loss = 0.511 and accuracy of 0.86\n",
      "Iteration 14037: with minibatch training loss = 0.472 and accuracy of 0.83\n",
      "Iteration 14038: with minibatch training loss = 0.759 and accuracy of 0.77\n",
      "Iteration 14039: with minibatch training loss = 0.589 and accuracy of 0.83\n",
      "Iteration 14040: with minibatch training loss = 0.572 and accuracy of 0.83\n",
      "Iteration 14041: with minibatch training loss = 0.896 and accuracy of 0.73\n",
      "Iteration 14042: with minibatch training loss = 0.621 and accuracy of 0.84\n",
      "Iteration 14043: with minibatch training loss = 0.768 and accuracy of 0.77\n",
      "Iteration 14044: with minibatch training loss = 0.569 and accuracy of 0.83\n",
      "Iteration 14045: with minibatch training loss = 0.813 and accuracy of 0.75\n",
      "Iteration 14046: with minibatch training loss = 0.368 and accuracy of 0.89\n",
      "Iteration 14047: with minibatch training loss = 0.534 and accuracy of 0.83\n",
      "Iteration 14048: with minibatch training loss = 0.628 and accuracy of 0.86\n",
      "Iteration 14049: with minibatch training loss = 0.699 and accuracy of 0.81\n",
      "Iteration 14050: with minibatch training loss = 0.642 and accuracy of 0.81\n",
      "Iteration 14051: with minibatch training loss = 0.881 and accuracy of 0.7\n",
      "Iteration 14052: with minibatch training loss = 0.568 and accuracy of 0.89\n",
      "Iteration 14053: with minibatch training loss = 0.483 and accuracy of 0.84\n",
      "Iteration 14054: with minibatch training loss = 0.736 and accuracy of 0.78\n",
      "Iteration 14055: with minibatch training loss = 0.673 and accuracy of 0.8\n",
      "Iteration 14056: with minibatch training loss = 0.744 and accuracy of 0.77\n",
      "Iteration 14057: with minibatch training loss = 0.618 and accuracy of 0.8\n",
      "Iteration 14058: with minibatch training loss = 0.726 and accuracy of 0.78\n",
      "Iteration 14059: with minibatch training loss = 0.627 and accuracy of 0.83\n",
      "Iteration 14060: with minibatch training loss = 0.902 and accuracy of 0.77\n",
      "Iteration 14061: with minibatch training loss = 0.728 and accuracy of 0.78\n",
      "Iteration 14062: with minibatch training loss = 0.523 and accuracy of 0.86\n",
      "Iteration 14063: with minibatch training loss = 0.826 and accuracy of 0.75\n",
      "Iteration 14064: with minibatch training loss = 0.57 and accuracy of 0.84\n",
      "Iteration 14065: with minibatch training loss = 0.616 and accuracy of 0.81\n",
      "Iteration 14066: with minibatch training loss = 0.557 and accuracy of 0.84\n",
      "Iteration 14067: with minibatch training loss = 0.581 and accuracy of 0.83\n",
      "Iteration 14068: with minibatch training loss = 0.862 and accuracy of 0.7\n",
      "Iteration 14069: with minibatch training loss = 0.639 and accuracy of 0.84\n",
      "Iteration 14070: with minibatch training loss = 0.469 and accuracy of 0.86\n",
      "Iteration 14071: with minibatch training loss = 0.5 and accuracy of 0.84\n",
      "Iteration 14072: with minibatch training loss = 0.754 and accuracy of 0.8\n",
      "Iteration 14073: with minibatch training loss = 0.93 and accuracy of 0.73\n",
      "Iteration 14074: with minibatch training loss = 0.492 and accuracy of 0.84\n",
      "Iteration 14075: with minibatch training loss = 0.806 and accuracy of 0.8\n",
      "Iteration 14076: with minibatch training loss = 0.672 and accuracy of 0.81\n",
      "Iteration 14077: with minibatch training loss = 0.511 and accuracy of 0.86\n",
      "Iteration 14078: with minibatch training loss = 1.03 and accuracy of 0.66\n",
      "Iteration 14079: with minibatch training loss = 0.531 and accuracy of 0.88\n",
      "Iteration 14080: with minibatch training loss = 0.734 and accuracy of 0.8\n",
      "Iteration 14081: with minibatch training loss = 0.441 and accuracy of 0.88\n",
      "Iteration 14082: with minibatch training loss = 0.447 and accuracy of 0.88\n",
      "Iteration 14083: with minibatch training loss = 0.731 and accuracy of 0.78\n",
      "Iteration 14084: with minibatch training loss = 0.361 and accuracy of 0.91\n",
      "Iteration 14085: with minibatch training loss = 0.694 and accuracy of 0.8\n",
      "Iteration 14086: with minibatch training loss = 0.797 and accuracy of 0.77\n",
      "Iteration 14087: with minibatch training loss = 0.672 and accuracy of 0.8\n",
      "Iteration 14088: with minibatch training loss = 0.923 and accuracy of 0.72\n",
      "Iteration 14089: with minibatch training loss = 0.61 and accuracy of 0.86\n",
      "Iteration 14090: with minibatch training loss = 0.936 and accuracy of 0.75\n",
      "Iteration 14091: with minibatch training loss = 0.772 and accuracy of 0.75\n",
      "Iteration 14092: with minibatch training loss = 0.813 and accuracy of 0.8\n",
      "Iteration 14093: with minibatch training loss = 0.697 and accuracy of 0.78\n",
      "Iteration 14094: with minibatch training loss = 0.668 and accuracy of 0.77\n",
      "Iteration 14095: with minibatch training loss = 0.616 and accuracy of 0.8\n",
      "Iteration 14096: with minibatch training loss = 0.792 and accuracy of 0.78\n",
      "Iteration 14097: with minibatch training loss = 0.821 and accuracy of 0.75\n",
      "Iteration 14098: with minibatch training loss = 0.792 and accuracy of 0.75\n",
      "Iteration 14099: with minibatch training loss = 0.659 and accuracy of 0.78\n",
      "Iteration 14100: with minibatch training loss = 0.71 and accuracy of 0.78\n",
      "Iteration 14101: with minibatch training loss = 0.81 and accuracy of 0.78\n",
      "Iteration 14102: with minibatch training loss = 0.62 and accuracy of 0.83\n",
      "Iteration 14103: with minibatch training loss = 0.349 and accuracy of 0.91\n",
      "Iteration 14104: with minibatch training loss = 0.647 and accuracy of 0.78\n",
      "Iteration 14105: with minibatch training loss = 0.674 and accuracy of 0.77\n",
      "Iteration 14106: with minibatch training loss = 0.451 and accuracy of 0.86\n",
      "Iteration 14107: with minibatch training loss = 0.862 and accuracy of 0.75\n",
      "Iteration 14108: with minibatch training loss = 0.582 and accuracy of 0.83\n",
      "Iteration 14109: with minibatch training loss = 0.516 and accuracy of 0.86\n",
      "Iteration 14110: with minibatch training loss = 0.973 and accuracy of 0.7\n",
      "Iteration 14111: with minibatch training loss = 0.959 and accuracy of 0.69\n",
      "Iteration 14112: with minibatch training loss = 0.836 and accuracy of 0.77\n",
      "Iteration 14113: with minibatch training loss = 0.651 and accuracy of 0.8\n",
      "Iteration 14114: with minibatch training loss = 0.689 and accuracy of 0.8\n",
      "Iteration 14115: with minibatch training loss = 0.865 and accuracy of 0.72\n",
      "Iteration 14116: with minibatch training loss = 0.845 and accuracy of 0.73\n",
      "Iteration 14117: with minibatch training loss = 0.654 and accuracy of 0.8\n",
      "Iteration 14118: with minibatch training loss = 0.427 and accuracy of 0.86\n",
      "Iteration 14119: with minibatch training loss = 0.669 and accuracy of 0.8\n",
      "Iteration 14120: with minibatch training loss = 0.77 and accuracy of 0.75\n",
      "Iteration 14121: with minibatch training loss = 0.693 and accuracy of 0.77\n",
      "Iteration 14122: with minibatch training loss = 0.743 and accuracy of 0.73\n",
      "Iteration 14123: with minibatch training loss = 0.879 and accuracy of 0.72\n",
      "Iteration 14124: with minibatch training loss = 0.637 and accuracy of 0.81\n",
      "Iteration 14125: with minibatch training loss = 0.784 and accuracy of 0.77\n",
      "Iteration 14126: with minibatch training loss = 0.593 and accuracy of 0.83\n",
      "Iteration 14127: with minibatch training loss = 0.555 and accuracy of 0.86\n",
      "Iteration 14128: with minibatch training loss = 0.609 and accuracy of 0.81\n",
      "Iteration 14129: with minibatch training loss = 0.727 and accuracy of 0.78\n",
      "Iteration 14130: with minibatch training loss = 0.81 and accuracy of 0.77\n",
      "Iteration 14131: with minibatch training loss = 0.792 and accuracy of 0.75\n",
      "Iteration 14132: with minibatch training loss = 0.627 and accuracy of 0.78\n",
      "Iteration 14133: with minibatch training loss = 0.707 and accuracy of 0.8\n",
      "Iteration 14134: with minibatch training loss = 0.68 and accuracy of 0.78\n",
      "Iteration 14135: with minibatch training loss = 0.526 and accuracy of 0.83\n",
      "Iteration 14136: with minibatch training loss = 0.553 and accuracy of 0.84\n",
      "Iteration 14137: with minibatch training loss = 0.58 and accuracy of 0.86\n",
      "Iteration 14138: with minibatch training loss = 0.654 and accuracy of 0.81\n",
      "Iteration 14139: with minibatch training loss = 0.464 and accuracy of 0.89\n",
      "Iteration 14140: with minibatch training loss = 0.582 and accuracy of 0.84\n",
      "Iteration 14141: with minibatch training loss = 0.988 and accuracy of 0.73\n",
      "Iteration 14142: with minibatch training loss = 0.609 and accuracy of 0.78\n",
      "Iteration 14143: with minibatch training loss = 0.642 and accuracy of 0.83\n",
      "Iteration 14144: with minibatch training loss = 0.769 and accuracy of 0.78\n",
      "Iteration 14145: with minibatch training loss = 0.708 and accuracy of 0.78\n",
      "Iteration 14146: with minibatch training loss = 0.723 and accuracy of 0.78\n",
      "Iteration 14147: with minibatch training loss = 0.7 and accuracy of 0.78\n",
      "Iteration 14148: with minibatch training loss = 0.634 and accuracy of 0.8\n",
      "Iteration 14149: with minibatch training loss = 0.704 and accuracy of 0.81\n",
      "Iteration 14150: with minibatch training loss = 0.685 and accuracy of 0.8\n",
      "Iteration 14151: with minibatch training loss = 0.669 and accuracy of 0.78\n",
      "Iteration 14152: with minibatch training loss = 0.348 and accuracy of 0.91\n",
      "Iteration 14153: with minibatch training loss = 0.748 and accuracy of 0.77\n",
      "Iteration 14154: with minibatch training loss = 0.794 and accuracy of 0.75\n",
      "Iteration 14155: with minibatch training loss = 0.736 and accuracy of 0.8\n",
      "Iteration 14156: with minibatch training loss = 1.02 and accuracy of 0.67\n",
      "Iteration 14157: with minibatch training loss = 0.466 and accuracy of 0.89\n",
      "Iteration 14158: with minibatch training loss = 0.541 and accuracy of 0.83\n",
      "Iteration 14159: with minibatch training loss = 0.62 and accuracy of 0.83\n",
      "Iteration 14160: with minibatch training loss = 0.649 and accuracy of 0.8\n",
      "Iteration 14161: with minibatch training loss = 0.905 and accuracy of 0.75\n",
      "Iteration 14162: with minibatch training loss = 0.692 and accuracy of 0.8\n",
      "Iteration 14163: with minibatch training loss = 0.501 and accuracy of 0.84\n",
      "Iteration 14164: with minibatch training loss = 0.66 and accuracy of 0.81\n",
      "Iteration 14165: with minibatch training loss = 0.723 and accuracy of 0.78\n",
      "Iteration 14166: with minibatch training loss = 0.372 and accuracy of 0.92\n",
      "Iteration 14167: with minibatch training loss = 0.64 and accuracy of 0.83\n",
      "Iteration 14168: with minibatch training loss = 0.733 and accuracy of 0.78\n",
      "Iteration 14169: with minibatch training loss = 0.776 and accuracy of 0.77\n",
      "Iteration 14170: with minibatch training loss = 0.766 and accuracy of 0.78\n",
      "Iteration 14171: with minibatch training loss = 0.822 and accuracy of 0.73\n",
      "Iteration 14172: with minibatch training loss = 0.838 and accuracy of 0.73\n",
      "Iteration 14173: with minibatch training loss = 0.655 and accuracy of 0.84\n",
      "Iteration 14174: with minibatch training loss = 0.903 and accuracy of 0.7\n",
      "Iteration 14175: with minibatch training loss = 0.703 and accuracy of 0.77\n",
      "Iteration 14176: with minibatch training loss = 0.659 and accuracy of 0.81\n",
      "Iteration 14177: with minibatch training loss = 0.854 and accuracy of 0.77\n",
      "Iteration 14178: with minibatch training loss = 0.787 and accuracy of 0.75\n",
      "Iteration 14179: with minibatch training loss = 0.564 and accuracy of 0.83\n",
      "Iteration 14180: with minibatch training loss = 0.688 and accuracy of 0.77\n",
      "Iteration 14181: with minibatch training loss = 0.768 and accuracy of 0.77\n",
      "Iteration 14182: with minibatch training loss = 0.865 and accuracy of 0.72\n",
      "Iteration 14183: with minibatch training loss = 1.08 and accuracy of 0.67\n",
      "Iteration 14184: with minibatch training loss = 0.967 and accuracy of 0.7\n",
      "Iteration 14185: with minibatch training loss = 0.654 and accuracy of 0.78\n",
      "Iteration 14186: with minibatch training loss = 0.688 and accuracy of 0.78\n",
      "Iteration 14187: with minibatch training loss = 0.588 and accuracy of 0.81\n",
      "Iteration 14188: with minibatch training loss = 0.76 and accuracy of 0.77\n",
      "Iteration 14189: with minibatch training loss = 0.549 and accuracy of 0.84\n",
      "Iteration 14190: with minibatch training loss = 0.695 and accuracy of 0.8\n",
      "Iteration 14191: with minibatch training loss = 0.687 and accuracy of 0.75\n",
      "Iteration 14192: with minibatch training loss = 0.58 and accuracy of 0.81\n",
      "Iteration 14193: with minibatch training loss = 0.669 and accuracy of 0.81\n",
      "Iteration 14194: with minibatch training loss = 0.754 and accuracy of 0.84\n",
      "Iteration 14195: with minibatch training loss = 0.559 and accuracy of 0.83\n",
      "Iteration 14196: with minibatch training loss = 0.572 and accuracy of 0.8\n",
      "Iteration 14197: with minibatch training loss = 0.739 and accuracy of 0.8\n",
      "Iteration 14198: with minibatch training loss = 1.06 and accuracy of 0.67\n",
      "Iteration 14199: with minibatch training loss = 0.591 and accuracy of 0.83\n",
      "Iteration 14200: with minibatch training loss = 0.496 and accuracy of 0.83\n",
      "Iteration 14201: with minibatch training loss = 0.516 and accuracy of 0.84\n",
      "Iteration 14202: with minibatch training loss = 0.622 and accuracy of 0.84\n",
      "Iteration 14203: with minibatch training loss = 0.603 and accuracy of 0.78\n",
      "Iteration 14204: with minibatch training loss = 0.552 and accuracy of 0.89\n",
      "Iteration 14205: with minibatch training loss = 0.607 and accuracy of 0.8\n",
      "Iteration 14206: with minibatch training loss = 0.879 and accuracy of 0.72\n",
      "Iteration 14207: with minibatch training loss = 0.759 and accuracy of 0.77\n",
      "Iteration 14208: with minibatch training loss = 0.701 and accuracy of 0.77\n",
      "Iteration 14209: with minibatch training loss = 0.545 and accuracy of 0.83\n",
      "Iteration 14210: with minibatch training loss = 0.673 and accuracy of 0.8\n",
      "Iteration 14211: with minibatch training loss = 0.58 and accuracy of 0.83\n",
      "Iteration 14212: with minibatch training loss = 0.406 and accuracy of 0.91\n",
      "Iteration 14213: with minibatch training loss = 0.504 and accuracy of 0.86\n",
      "Iteration 14214: with minibatch training loss = 0.842 and accuracy of 0.75\n",
      "Iteration 14215: with minibatch training loss = 0.801 and accuracy of 0.77\n",
      "Iteration 14216: with minibatch training loss = 0.747 and accuracy of 0.8\n",
      "Iteration 14217: with minibatch training loss = 0.935 and accuracy of 0.72\n",
      "Iteration 14218: with minibatch training loss = 0.749 and accuracy of 0.75\n",
      "Iteration 14219: with minibatch training loss = 0.818 and accuracy of 0.73\n",
      "Iteration 14220: with minibatch training loss = 0.74 and accuracy of 0.78\n",
      "Iteration 14221: with minibatch training loss = 0.46 and accuracy of 0.88\n",
      "Iteration 14222: with minibatch training loss = 0.678 and accuracy of 0.81\n",
      "Iteration 14223: with minibatch training loss = 0.599 and accuracy of 0.81\n",
      "Iteration 14224: with minibatch training loss = 0.84 and accuracy of 0.77\n",
      "Iteration 14225: with minibatch training loss = 0.568 and accuracy of 0.84\n",
      "Iteration 14226: with minibatch training loss = 0.693 and accuracy of 0.8\n",
      "Iteration 14227: with minibatch training loss = 0.391 and accuracy of 0.89\n",
      "Iteration 14228: with minibatch training loss = 0.659 and accuracy of 0.83\n",
      "Iteration 14229: with minibatch training loss = 0.65 and accuracy of 0.81\n",
      "Iteration 14230: with minibatch training loss = 0.616 and accuracy of 0.84\n",
      "Iteration 14231: with minibatch training loss = 0.959 and accuracy of 0.69\n",
      "Iteration 14232: with minibatch training loss = 0.708 and accuracy of 0.78\n",
      "Iteration 14233: with minibatch training loss = 0.651 and accuracy of 0.78\n",
      "Iteration 14234: with minibatch training loss = 0.786 and accuracy of 0.75\n",
      "Iteration 14235: with minibatch training loss = 0.805 and accuracy of 0.75\n",
      "Iteration 14236: with minibatch training loss = 0.744 and accuracy of 0.75\n",
      "Iteration 14237: with minibatch training loss = 0.553 and accuracy of 0.84\n",
      "Iteration 14238: with minibatch training loss = 0.656 and accuracy of 0.78\n",
      "Iteration 14239: with minibatch training loss = 0.942 and accuracy of 0.72\n",
      "Iteration 14240: with minibatch training loss = 0.495 and accuracy of 0.86\n",
      "Iteration 14241: with minibatch training loss = 0.82 and accuracy of 0.73\n",
      "Iteration 14242: with minibatch training loss = 0.516 and accuracy of 0.84\n",
      "Iteration 14243: with minibatch training loss = 0.863 and accuracy of 0.73\n",
      "Iteration 14244: with minibatch training loss = 0.658 and accuracy of 0.81\n",
      "Iteration 14245: with minibatch training loss = 0.66 and accuracy of 0.78\n",
      "Iteration 14246: with minibatch training loss = 0.782 and accuracy of 0.77\n",
      "Iteration 14247: with minibatch training loss = 0.753 and accuracy of 0.8\n",
      "Iteration 14248: with minibatch training loss = 0.433 and accuracy of 0.88\n",
      "Iteration 14249: with minibatch training loss = 0.634 and accuracy of 0.83\n",
      "Iteration 14250: with minibatch training loss = 0.584 and accuracy of 0.84\n",
      "Iteration 14251: with minibatch training loss = 0.648 and accuracy of 0.81\n",
      "Iteration 14252: with minibatch training loss = 0.726 and accuracy of 0.75\n",
      "Iteration 14253: with minibatch training loss = 0.565 and accuracy of 0.83\n",
      "Iteration 14254: with minibatch training loss = 0.633 and accuracy of 0.8\n",
      "Iteration 14255: with minibatch training loss = 0.521 and accuracy of 0.84\n",
      "Iteration 14256: with minibatch training loss = 0.519 and accuracy of 0.84\n",
      "Iteration 14257: with minibatch training loss = 0.808 and accuracy of 0.78\n",
      "Iteration 14258: with minibatch training loss = 0.52 and accuracy of 0.86\n",
      "Iteration 14259: with minibatch training loss = 0.459 and accuracy of 0.88\n",
      "Iteration 14260: with minibatch training loss = 0.893 and accuracy of 0.7\n",
      "Iteration 14261: with minibatch training loss = 0.839 and accuracy of 0.75\n",
      "Iteration 14262: with minibatch training loss = 0.685 and accuracy of 0.81\n",
      "Iteration 14263: with minibatch training loss = 0.901 and accuracy of 0.7\n",
      "Iteration 14264: with minibatch training loss = 0.682 and accuracy of 0.78\n",
      "Iteration 14265: with minibatch training loss = 0.616 and accuracy of 0.8\n",
      "Iteration 14266: with minibatch training loss = 0.559 and accuracy of 0.84\n",
      "Iteration 14267: with minibatch training loss = 0.752 and accuracy of 0.77\n",
      "Iteration 14268: with minibatch training loss = 0.602 and accuracy of 0.81\n",
      "Iteration 14269: with minibatch training loss = 0.755 and accuracy of 0.77\n",
      "Iteration 14270: with minibatch training loss = 0.767 and accuracy of 0.73\n",
      "Iteration 14271: with minibatch training loss = 0.471 and accuracy of 0.88\n",
      "Iteration 14272: with minibatch training loss = 0.582 and accuracy of 0.81\n",
      "Iteration 14273: with minibatch training loss = 0.555 and accuracy of 0.84\n",
      "Iteration 14274: with minibatch training loss = 0.75 and accuracy of 0.77\n",
      "Iteration 14275: with minibatch training loss = 0.737 and accuracy of 0.8\n",
      "Iteration 14276: with minibatch training loss = 0.529 and accuracy of 0.81\n",
      "Iteration 14277: with minibatch training loss = 0.638 and accuracy of 0.83\n",
      "Iteration 14278: with minibatch training loss = 0.878 and accuracy of 0.72\n",
      "Iteration 14279: with minibatch training loss = 0.696 and accuracy of 0.83\n",
      "Iteration 14280: with minibatch training loss = 0.744 and accuracy of 0.78\n",
      "Iteration 14281: with minibatch training loss = 0.644 and accuracy of 0.81\n",
      "Iteration 14282: with minibatch training loss = 0.761 and accuracy of 0.78\n",
      "Iteration 14283: with minibatch training loss = 0.62 and accuracy of 0.81\n",
      "Iteration 14284: with minibatch training loss = 0.592 and accuracy of 0.81\n",
      "Iteration 14285: with minibatch training loss = 0.807 and accuracy of 0.73\n",
      "Iteration 14286: with minibatch training loss = 0.607 and accuracy of 0.81\n",
      "Iteration 14287: with minibatch training loss = 0.816 and accuracy of 0.8\n",
      "Iteration 14288: with minibatch training loss = 0.506 and accuracy of 0.86\n",
      "Iteration 14289: with minibatch training loss = 0.489 and accuracy of 0.86\n",
      "Iteration 14290: with minibatch training loss = 0.723 and accuracy of 0.81\n",
      "Iteration 14291: with minibatch training loss = 0.802 and accuracy of 0.77\n",
      "Iteration 14292: with minibatch training loss = 0.721 and accuracy of 0.78\n",
      "Iteration 14293: with minibatch training loss = 0.56 and accuracy of 0.84\n",
      "Iteration 14294: with minibatch training loss = 0.643 and accuracy of 0.81\n",
      "Iteration 14295: with minibatch training loss = 0.645 and accuracy of 0.77\n",
      "Iteration 14296: with minibatch training loss = 0.685 and accuracy of 0.8\n",
      "Iteration 14297: with minibatch training loss = 1.22 and accuracy of 0.62\n",
      "Iteration 14298: with minibatch training loss = 0.414 and accuracy of 0.88\n",
      "Iteration 14299: with minibatch training loss = 0.683 and accuracy of 0.77\n",
      "Iteration 14300: with minibatch training loss = 0.476 and accuracy of 0.84\n",
      "Iteration 14301: with minibatch training loss = 0.714 and accuracy of 0.73\n",
      "Iteration 14302: with minibatch training loss = 0.637 and accuracy of 0.8\n",
      "Iteration 14303: with minibatch training loss = 0.912 and accuracy of 0.7\n",
      "Iteration 14304: with minibatch training loss = 0.783 and accuracy of 0.75\n",
      "Iteration 14305: with minibatch training loss = 0.518 and accuracy of 0.84\n",
      "Iteration 14306: with minibatch training loss = 0.541 and accuracy of 0.86\n",
      "Iteration 14307: with minibatch training loss = 0.429 and accuracy of 0.88\n",
      "Iteration 14308: with minibatch training loss = 0.673 and accuracy of 0.81\n",
      "Iteration 14309: with minibatch training loss = 0.621 and accuracy of 0.8\n",
      "Iteration 14310: with minibatch training loss = 0.739 and accuracy of 0.78\n",
      "Iteration 14311: with minibatch training loss = 0.652 and accuracy of 0.8\n",
      "Iteration 14312: with minibatch training loss = 0.787 and accuracy of 0.77\n",
      "Iteration 14313: with minibatch training loss = 0.634 and accuracy of 0.81\n",
      "Iteration 14314: with minibatch training loss = 0.699 and accuracy of 0.77\n",
      "Iteration 14315: with minibatch training loss = 0.541 and accuracy of 0.83\n",
      "Iteration 14316: with minibatch training loss = 1.01 and accuracy of 0.64\n",
      "Iteration 14317: with minibatch training loss = 0.79 and accuracy of 0.77\n",
      "Iteration 14318: with minibatch training loss = 0.578 and accuracy of 0.81\n",
      "Iteration 14319: with minibatch training loss = 0.671 and accuracy of 0.77\n",
      "Iteration 14320: with minibatch training loss = 0.688 and accuracy of 0.78\n",
      "Iteration 14321: with minibatch training loss = 0.826 and accuracy of 0.72\n",
      "Iteration 14322: with minibatch training loss = 1.06 and accuracy of 0.67\n",
      "Iteration 14323: with minibatch training loss = 0.574 and accuracy of 0.81\n",
      "Iteration 14324: with minibatch training loss = 0.881 and accuracy of 0.75\n",
      "Iteration 14325: with minibatch training loss = 0.69 and accuracy of 0.8\n",
      "Iteration 14326: with minibatch training loss = 0.429 and accuracy of 0.84\n",
      "Iteration 14327: with minibatch training loss = 0.535 and accuracy of 0.83\n",
      "Iteration 14328: with minibatch training loss = 0.824 and accuracy of 0.73\n",
      "Iteration 14329: with minibatch training loss = 0.355 and accuracy of 0.89\n",
      "Iteration 14330: with minibatch training loss = 0.625 and accuracy of 0.81\n",
      "Iteration 14331: with minibatch training loss = 1.12 and accuracy of 0.66\n",
      "Iteration 14332: with minibatch training loss = 0.888 and accuracy of 0.73\n",
      "Iteration 14333: with minibatch training loss = 0.729 and accuracy of 0.75\n",
      "Iteration 14334: with minibatch training loss = 0.654 and accuracy of 0.83\n",
      "Iteration 14335: with minibatch training loss = 0.639 and accuracy of 0.81\n",
      "Iteration 14336: with minibatch training loss = 0.552 and accuracy of 0.81\n",
      "Iteration 14337: with minibatch training loss = 0.478 and accuracy of 0.84\n",
      "Iteration 14338: with minibatch training loss = 0.72 and accuracy of 0.8\n",
      "Iteration 14339: with minibatch training loss = 0.647 and accuracy of 0.84\n",
      "Iteration 14340: with minibatch training loss = 0.563 and accuracy of 0.81\n",
      "Iteration 14341: with minibatch training loss = 0.576 and accuracy of 0.83\n",
      "Iteration 14342: with minibatch training loss = 0.562 and accuracy of 0.83\n",
      "Iteration 14343: with minibatch training loss = 0.733 and accuracy of 0.81\n",
      "Iteration 14344: with minibatch training loss = 0.537 and accuracy of 0.86\n",
      "Iteration 14345: with minibatch training loss = 0.544 and accuracy of 0.84\n",
      "Iteration 14346: with minibatch training loss = 0.98 and accuracy of 0.7\n",
      "Iteration 14347: with minibatch training loss = 0.997 and accuracy of 0.72\n",
      "Iteration 14348: with minibatch training loss = 0.721 and accuracy of 0.78\n",
      "Iteration 14349: with minibatch training loss = 0.647 and accuracy of 0.78\n",
      "Iteration 14350: with minibatch training loss = 0.729 and accuracy of 0.75\n",
      "Iteration 14351: with minibatch training loss = 0.882 and accuracy of 0.72\n",
      "Iteration 14352: with minibatch training loss = 0.608 and accuracy of 0.8\n",
      "Iteration 14353: with minibatch training loss = 0.704 and accuracy of 0.8\n",
      "Iteration 14354: with minibatch training loss = 0.771 and accuracy of 0.8\n",
      "Iteration 14355: with minibatch training loss = 0.749 and accuracy of 0.78\n",
      "Iteration 14356: with minibatch training loss = 0.574 and accuracy of 0.81\n",
      "Iteration 14357: with minibatch training loss = 0.644 and accuracy of 0.81\n",
      "Iteration 14358: with minibatch training loss = 0.617 and accuracy of 0.81\n",
      "Iteration 14359: with minibatch training loss = 0.813 and accuracy of 0.75\n",
      "Iteration 14360: with minibatch training loss = 0.601 and accuracy of 0.86\n",
      "Iteration 14361: with minibatch training loss = 0.677 and accuracy of 0.78\n",
      "Iteration 14362: with minibatch training loss = 0.733 and accuracy of 0.78\n",
      "Iteration 14363: with minibatch training loss = 0.653 and accuracy of 0.8\n",
      "Iteration 14364: with minibatch training loss = 0.883 and accuracy of 0.78\n",
      "Iteration 14365: with minibatch training loss = 0.954 and accuracy of 0.69\n",
      "Iteration 14366: with minibatch training loss = 0.446 and accuracy of 0.86\n",
      "Iteration 14367: with minibatch training loss = 0.613 and accuracy of 0.84\n",
      "Iteration 14368: with minibatch training loss = 0.723 and accuracy of 0.81\n",
      "Iteration 14369: with minibatch training loss = 0.981 and accuracy of 0.72\n",
      "Iteration 14370: with minibatch training loss = 0.775 and accuracy of 0.73\n",
      "Iteration 14371: with minibatch training loss = 0.554 and accuracy of 0.83\n",
      "Iteration 14372: with minibatch training loss = 0.619 and accuracy of 0.81\n",
      "Iteration 14373: with minibatch training loss = 0.434 and accuracy of 0.89\n",
      "Iteration 14374: with minibatch training loss = 0.439 and accuracy of 0.86\n",
      "Iteration 14375: with minibatch training loss = 0.65 and accuracy of 0.8\n",
      "Iteration 14376: with minibatch training loss = 0.628 and accuracy of 0.78\n",
      "Iteration 14377: with minibatch training loss = 0.568 and accuracy of 0.84\n",
      "Iteration 14378: with minibatch training loss = 0.482 and accuracy of 0.88\n",
      "Iteration 14379: with minibatch training loss = 0.712 and accuracy of 0.81\n",
      "Iteration 14380: with minibatch training loss = 0.685 and accuracy of 0.78\n",
      "Iteration 14381: with minibatch training loss = 0.876 and accuracy of 0.7\n",
      "Iteration 14382: with minibatch training loss = 0.635 and accuracy of 0.81\n",
      "Iteration 14383: with minibatch training loss = 0.655 and accuracy of 0.8\n",
      "Iteration 14384: with minibatch training loss = 0.734 and accuracy of 0.78\n",
      "Iteration 14385: with minibatch training loss = 0.494 and accuracy of 0.89\n",
      "Iteration 14386: with minibatch training loss = 0.679 and accuracy of 0.78\n",
      "Iteration 14387: with minibatch training loss = 0.937 and accuracy of 0.72\n",
      "Iteration 14388: with minibatch training loss = 0.682 and accuracy of 0.77\n",
      "Iteration 14389: with minibatch training loss = 0.528 and accuracy of 0.83\n",
      "Iteration 14390: with minibatch training loss = 0.505 and accuracy of 0.84\n",
      "Iteration 14391: with minibatch training loss = 0.676 and accuracy of 0.8\n",
      "Iteration 14392: with minibatch training loss = 0.851 and accuracy of 0.72\n",
      "Iteration 14393: with minibatch training loss = 0.605 and accuracy of 0.83\n",
      "Iteration 14394: with minibatch training loss = 0.59 and accuracy of 0.83\n",
      "Iteration 14395: with minibatch training loss = 0.49 and accuracy of 0.84\n",
      "Iteration 14396: with minibatch training loss = 0.901 and accuracy of 0.75\n",
      "Iteration 14397: with minibatch training loss = 0.836 and accuracy of 0.75\n",
      "Iteration 14398: with minibatch training loss = 0.47 and accuracy of 0.89\n",
      "Iteration 14399: with minibatch training loss = 0.621 and accuracy of 0.81\n",
      "Iteration 14400: with minibatch training loss = 0.764 and accuracy of 0.78\n",
      "Iteration 14401: with minibatch training loss = 0.713 and accuracy of 0.77\n",
      "Iteration 14402: with minibatch training loss = 0.628 and accuracy of 0.83\n",
      "Iteration 14403: with minibatch training loss = 0.357 and accuracy of 0.92\n",
      "Iteration 14404: with minibatch training loss = 0.613 and accuracy of 0.81\n",
      "Iteration 14405: with minibatch training loss = 0.662 and accuracy of 0.81\n",
      "Iteration 14406: with minibatch training loss = 0.82 and accuracy of 0.73\n",
      "Iteration 14407: with minibatch training loss = 0.542 and accuracy of 0.84\n",
      "Iteration 14408: with minibatch training loss = 0.617 and accuracy of 0.81\n",
      "Iteration 14409: with minibatch training loss = 0.514 and accuracy of 0.83\n",
      "Iteration 14410: with minibatch training loss = 0.533 and accuracy of 0.83\n",
      "Iteration 14411: with minibatch training loss = 0.4 and accuracy of 0.88\n",
      "Iteration 14412: with minibatch training loss = 0.247 and accuracy of 0.94\n",
      "Iteration 14413: with minibatch training loss = 0.547 and accuracy of 0.81\n",
      "Iteration 14414: with minibatch training loss = 0.75 and accuracy of 0.8\n",
      "Iteration 14415: with minibatch training loss = 0.548 and accuracy of 0.84\n",
      "Iteration 14416: with minibatch training loss = 0.528 and accuracy of 0.84\n",
      "Iteration 14417: with minibatch training loss = 0.652 and accuracy of 0.83\n",
      "Iteration 14418: with minibatch training loss = 0.483 and accuracy of 0.83\n",
      "Iteration 14419: with minibatch training loss = 0.78 and accuracy of 0.75\n",
      "Iteration 14420: with minibatch training loss = 0.473 and accuracy of 0.88\n",
      "Iteration 14421: with minibatch training loss = 0.691 and accuracy of 0.78\n",
      "Iteration 14422: with minibatch training loss = 0.506 and accuracy of 0.84\n",
      "Iteration 14423: with minibatch training loss = 0.821 and accuracy of 0.73\n",
      "Iteration 14424: with minibatch training loss = 0.545 and accuracy of 0.81\n",
      "Iteration 14425: with minibatch training loss = 0.646 and accuracy of 0.83\n",
      "Iteration 14426: with minibatch training loss = 0.741 and accuracy of 0.75\n",
      "Iteration 14427: with minibatch training loss = 0.679 and accuracy of 0.81\n",
      "Iteration 14428: with minibatch training loss = 0.753 and accuracy of 0.75\n",
      "Iteration 14429: with minibatch training loss = 0.661 and accuracy of 0.78\n",
      "Iteration 14430: with minibatch training loss = 0.426 and accuracy of 0.88\n",
      "Iteration 14431: with minibatch training loss = 1.04 and accuracy of 0.7\n",
      "Iteration 14432: with minibatch training loss = 0.522 and accuracy of 0.88\n",
      "Iteration 14433: with minibatch training loss = 0.791 and accuracy of 0.77\n",
      "Iteration 14434: with minibatch training loss = 0.691 and accuracy of 0.8\n",
      "Iteration 14435: with minibatch training loss = 0.906 and accuracy of 0.73\n",
      "Iteration 14436: with minibatch training loss = 0.492 and accuracy of 0.84\n",
      "Iteration 14437: with minibatch training loss = 0.609 and accuracy of 0.81\n",
      "Iteration 14438: with minibatch training loss = 0.564 and accuracy of 0.81\n",
      "Iteration 14439: with minibatch training loss = 0.6 and accuracy of 0.84\n",
      "Iteration 14440: with minibatch training loss = 0.575 and accuracy of 0.81\n",
      "Iteration 14441: with minibatch training loss = 0.862 and accuracy of 0.72\n",
      "Iteration 14442: with minibatch training loss = 0.709 and accuracy of 0.78\n",
      "Iteration 14443: with minibatch training loss = 0.333 and accuracy of 0.92\n",
      "Iteration 14444: with minibatch training loss = 0.917 and accuracy of 0.72\n",
      "Iteration 14445: with minibatch training loss = 0.69 and accuracy of 0.78\n",
      "Iteration 14446: with minibatch training loss = 0.69 and accuracy of 0.81\n",
      "Iteration 14447: with minibatch training loss = 0.714 and accuracy of 0.75\n",
      "Iteration 14448: with minibatch training loss = 0.745 and accuracy of 0.78\n",
      "Iteration 14449: with minibatch training loss = 0.701 and accuracy of 0.78\n",
      "Iteration 14450: with minibatch training loss = 1.01 and accuracy of 0.67\n",
      "Iteration 14451: with minibatch training loss = 0.57 and accuracy of 0.81\n",
      "Iteration 14452: with minibatch training loss = 1.03 and accuracy of 0.7\n",
      "Iteration 14453: with minibatch training loss = 0.776 and accuracy of 0.8\n",
      "Iteration 14454: with minibatch training loss = 0.634 and accuracy of 0.83\n",
      "Iteration 14455: with minibatch training loss = 0.403 and accuracy of 0.91\n",
      "Iteration 14456: with minibatch training loss = 0.676 and accuracy of 0.78\n",
      "Iteration 14457: with minibatch training loss = 0.685 and accuracy of 0.77\n",
      "Iteration 14458: with minibatch training loss = 0.777 and accuracy of 0.77\n",
      "Iteration 14459: with minibatch training loss = 0.58 and accuracy of 0.81\n",
      "Iteration 14460: with minibatch training loss = 0.842 and accuracy of 0.77\n",
      "Iteration 14461: with minibatch training loss = 0.648 and accuracy of 0.81\n",
      "Iteration 14462: with minibatch training loss = 0.544 and accuracy of 0.83\n",
      "Iteration 14463: with minibatch training loss = 0.584 and accuracy of 0.81\n",
      "Iteration 14464: with minibatch training loss = 0.612 and accuracy of 0.84\n",
      "Iteration 14465: with minibatch training loss = 0.624 and accuracy of 0.81\n",
      "Iteration 14466: with minibatch training loss = 0.671 and accuracy of 0.8\n",
      "Iteration 14467: with minibatch training loss = 1.03 and accuracy of 0.7\n",
      "Iteration 14468: with minibatch training loss = 0.488 and accuracy of 0.83\n",
      "Iteration 14469: with minibatch training loss = 0.841 and accuracy of 0.73\n",
      "Iteration 14470: with minibatch training loss = 0.516 and accuracy of 0.83\n",
      "Iteration 14471: with minibatch training loss = 0.824 and accuracy of 0.73\n",
      "Iteration 14472: with minibatch training loss = 0.505 and accuracy of 0.88\n",
      "Iteration 14473: with minibatch training loss = 0.867 and accuracy of 0.73\n",
      "Iteration 14474: with minibatch training loss = 0.67 and accuracy of 0.78\n",
      "Iteration 14475: with minibatch training loss = 1.05 and accuracy of 0.64\n",
      "Iteration 14476: with minibatch training loss = 0.599 and accuracy of 0.8\n",
      "Iteration 14477: with minibatch training loss = 0.613 and accuracy of 0.81\n",
      "Iteration 14478: with minibatch training loss = 0.644 and accuracy of 0.78\n",
      "Iteration 14479: with minibatch training loss = 0.884 and accuracy of 0.72\n",
      "Iteration 14480: with minibatch training loss = 0.723 and accuracy of 0.78\n",
      "Iteration 14481: with minibatch training loss = 0.686 and accuracy of 0.8\n",
      "Iteration 14482: with minibatch training loss = 0.578 and accuracy of 0.81\n",
      "Iteration 14483: with minibatch training loss = 0.659 and accuracy of 0.8\n",
      "Iteration 14484: with minibatch training loss = 0.799 and accuracy of 0.77\n",
      "Iteration 14485: with minibatch training loss = 0.318 and accuracy of 0.92\n",
      "Iteration 14486: with minibatch training loss = 0.633 and accuracy of 0.8\n",
      "Iteration 14487: with minibatch training loss = 0.757 and accuracy of 0.77\n",
      "Iteration 14488: with minibatch training loss = 0.704 and accuracy of 0.8\n",
      "Iteration 14489: with minibatch training loss = 0.637 and accuracy of 0.81\n",
      "Iteration 14490: with minibatch training loss = 0.633 and accuracy of 0.83\n",
      "Iteration 14491: with minibatch training loss = 0.775 and accuracy of 0.77\n",
      "Iteration 14492: with minibatch training loss = 0.761 and accuracy of 0.75\n",
      "Iteration 14493: with minibatch training loss = 0.946 and accuracy of 0.7\n",
      "Iteration 14494: with minibatch training loss = 0.747 and accuracy of 0.77\n",
      "Iteration 14495: with minibatch training loss = 0.697 and accuracy of 0.78\n",
      "Iteration 14496: with minibatch training loss = 0.65 and accuracy of 0.78\n",
      "Iteration 14497: with minibatch training loss = 0.758 and accuracy of 0.8\n",
      "Iteration 14498: with minibatch training loss = 0.572 and accuracy of 0.84\n",
      "Iteration 14499: with minibatch training loss = 0.65 and accuracy of 0.81\n",
      "Iteration 14500: with minibatch training loss = 0.784 and accuracy of 0.75\n",
      "Iteration 14501: with minibatch training loss = 0.439 and accuracy of 0.91\n",
      "Iteration 14502: with minibatch training loss = 0.595 and accuracy of 0.81\n",
      "Iteration 14503: with minibatch training loss = 0.819 and accuracy of 0.77\n",
      "Iteration 14504: with minibatch training loss = 0.827 and accuracy of 0.75\n",
      "Iteration 14505: with minibatch training loss = 0.742 and accuracy of 0.78\n",
      "Iteration 14506: with minibatch training loss = 0.862 and accuracy of 0.77\n",
      "Iteration 14507: with minibatch training loss = 0.688 and accuracy of 0.8\n",
      "Iteration 14508: with minibatch training loss = 0.469 and accuracy of 0.86\n",
      "Iteration 14509: with minibatch training loss = 0.904 and accuracy of 0.69\n",
      "Iteration 14510: with minibatch training loss = 0.452 and accuracy of 0.84\n",
      "Iteration 14511: with minibatch training loss = 0.961 and accuracy of 0.69\n",
      "Iteration 14512: with minibatch training loss = 0.824 and accuracy of 0.72\n",
      "Iteration 14513: with minibatch training loss = 0.791 and accuracy of 0.75\n",
      "Iteration 14514: with minibatch training loss = 0.51 and accuracy of 0.84\n",
      "Iteration 14515: with minibatch training loss = 0.648 and accuracy of 0.78\n",
      "Iteration 14516: with minibatch training loss = 0.669 and accuracy of 0.8\n",
      "Iteration 14517: with minibatch training loss = 0.864 and accuracy of 0.73\n",
      "Iteration 14518: with minibatch training loss = 0.493 and accuracy of 0.84\n",
      "Iteration 14519: with minibatch training loss = 0.643 and accuracy of 0.8\n",
      "Iteration 14520: with minibatch training loss = 0.601 and accuracy of 0.81\n",
      "Iteration 14521: with minibatch training loss = 0.747 and accuracy of 0.75\n",
      "Iteration 14522: with minibatch training loss = 0.703 and accuracy of 0.77\n",
      "Iteration 14523: with minibatch training loss = 1.06 and accuracy of 0.67\n",
      "Iteration 14524: with minibatch training loss = 0.919 and accuracy of 0.72\n",
      "Iteration 14525: with minibatch training loss = 0.652 and accuracy of 0.77\n",
      "Iteration 14526: with minibatch training loss = 0.709 and accuracy of 0.8\n",
      "Iteration 14527: with minibatch training loss = 0.938 and accuracy of 0.69\n",
      "Iteration 14528: with minibatch training loss = 0.586 and accuracy of 0.8\n",
      "Iteration 14529: with minibatch training loss = 0.75 and accuracy of 0.78\n",
      "Iteration 14530: with minibatch training loss = 0.682 and accuracy of 0.78\n",
      "Iteration 14531: with minibatch training loss = 0.612 and accuracy of 0.84\n",
      "Iteration 14532: with minibatch training loss = 0.697 and accuracy of 0.78\n",
      "Iteration 14533: with minibatch training loss = 0.758 and accuracy of 0.77\n",
      "Iteration 14534: with minibatch training loss = 0.611 and accuracy of 0.83\n",
      "Iteration 14535: with minibatch training loss = 0.522 and accuracy of 0.81\n",
      "Iteration 14536: with minibatch training loss = 0.583 and accuracy of 0.8\n",
      "Iteration 14537: with minibatch training loss = 1.03 and accuracy of 0.66\n",
      "Iteration 14538: with minibatch training loss = 0.586 and accuracy of 0.83\n",
      "Iteration 14539: with minibatch training loss = 0.41 and accuracy of 0.89\n",
      "Iteration 14540: with minibatch training loss = 0.935 and accuracy of 0.69\n",
      "Iteration 14541: with minibatch training loss = 0.669 and accuracy of 0.78\n",
      "Iteration 14542: with minibatch training loss = 0.838 and accuracy of 0.72\n",
      "Iteration 14543: with minibatch training loss = 0.674 and accuracy of 0.8\n",
      "Iteration 14544: with minibatch training loss = 0.511 and accuracy of 0.86\n",
      "Iteration 14545: with minibatch training loss = 0.756 and accuracy of 0.75\n",
      "Iteration 14546: with minibatch training loss = 0.694 and accuracy of 0.78\n",
      "Iteration 14547: with minibatch training loss = 0.704 and accuracy of 0.75\n",
      "Iteration 14548: with minibatch training loss = 0.709 and accuracy of 0.78\n",
      "Iteration 14549: with minibatch training loss = 0.44 and accuracy of 0.88\n",
      "Iteration 14550: with minibatch training loss = 0.761 and accuracy of 0.75\n",
      "Iteration 14551: with minibatch training loss = 0.549 and accuracy of 0.81\n",
      "Iteration 14552: with minibatch training loss = 0.472 and accuracy of 0.83\n",
      "Iteration 14553: with minibatch training loss = 0.688 and accuracy of 0.77\n",
      "Iteration 14554: with minibatch training loss = 0.59 and accuracy of 0.81\n",
      "Iteration 14555: with minibatch training loss = 0.445 and accuracy of 0.89\n",
      "Iteration 14556: with minibatch training loss = 0.78 and accuracy of 0.75\n",
      "Iteration 14557: with minibatch training loss = 0.713 and accuracy of 0.78\n",
      "Iteration 14558: with minibatch training loss = 0.635 and accuracy of 0.8\n",
      "Iteration 14559: with minibatch training loss = 0.703 and accuracy of 0.81\n",
      "Iteration 14560: with minibatch training loss = 0.497 and accuracy of 0.86\n",
      "Iteration 14561: with minibatch training loss = 0.751 and accuracy of 0.81\n",
      "Iteration 14562: with minibatch training loss = 0.495 and accuracy of 0.86\n",
      "Iteration 14563: with minibatch training loss = 0.851 and accuracy of 0.77\n",
      "Iteration 14564: with minibatch training loss = 0.802 and accuracy of 0.8\n",
      "Iteration 14565: with minibatch training loss = 0.57 and accuracy of 0.83\n",
      "Iteration 14566: with minibatch training loss = 0.775 and accuracy of 0.73\n",
      "Iteration 14567: with minibatch training loss = 0.662 and accuracy of 0.77\n",
      "Iteration 14568: with minibatch training loss = 0.285 and accuracy of 0.92\n",
      "Iteration 14569: with minibatch training loss = 0.756 and accuracy of 0.78\n",
      "Iteration 14570: with minibatch training loss = 0.781 and accuracy of 0.8\n",
      "Iteration 14571: with minibatch training loss = 0.665 and accuracy of 0.84\n",
      "Iteration 14572: with minibatch training loss = 0.531 and accuracy of 0.84\n",
      "Iteration 14573: with minibatch training loss = 0.855 and accuracy of 0.73\n",
      "Iteration 14574: with minibatch training loss = 0.936 and accuracy of 0.72\n",
      "Iteration 14575: with minibatch training loss = 0.734 and accuracy of 0.8\n",
      "Iteration 14576: with minibatch training loss = 0.762 and accuracy of 0.72\n",
      "Iteration 14577: with minibatch training loss = 0.491 and accuracy of 0.86\n",
      "Iteration 14578: with minibatch training loss = 0.679 and accuracy of 0.81\n",
      "Iteration 14579: with minibatch training loss = 0.494 and accuracy of 0.89\n",
      "Iteration 14580: with minibatch training loss = 0.493 and accuracy of 0.84\n",
      "Iteration 14581: with minibatch training loss = 0.569 and accuracy of 0.84\n",
      "Iteration 14582: with minibatch training loss = 0.681 and accuracy of 0.78\n",
      "Iteration 14583: with minibatch training loss = 0.621 and accuracy of 0.81\n",
      "Iteration 14584: with minibatch training loss = 0.54 and accuracy of 0.83\n",
      "Iteration 14585: with minibatch training loss = 0.892 and accuracy of 0.72\n",
      "Iteration 14586: with minibatch training loss = 0.855 and accuracy of 0.72\n",
      "Iteration 14587: with minibatch training loss = 0.67 and accuracy of 0.8\n",
      "Iteration 14588: with minibatch training loss = 0.61 and accuracy of 0.84\n",
      "Iteration 14589: with minibatch training loss = 0.589 and accuracy of 0.81\n",
      "Iteration 14590: with minibatch training loss = 0.854 and accuracy of 0.72\n",
      "Iteration 14591: with minibatch training loss = 0.786 and accuracy of 0.77\n",
      "Iteration 14592: with minibatch training loss = 0.492 and accuracy of 0.86\n",
      "Iteration 14593: with minibatch training loss = 0.754 and accuracy of 0.75\n",
      "Iteration 14594: with minibatch training loss = 0.614 and accuracy of 0.81\n",
      "Iteration 14595: with minibatch training loss = 0.586 and accuracy of 0.83\n",
      "Iteration 14596: with minibatch training loss = 0.906 and accuracy of 0.75\n",
      "Iteration 14597: with minibatch training loss = 0.695 and accuracy of 0.8\n",
      "Iteration 14598: with minibatch training loss = 0.802 and accuracy of 0.75\n",
      "Iteration 14599: with minibatch training loss = 0.521 and accuracy of 0.86\n",
      "Iteration 14600: with minibatch training loss = 0.701 and accuracy of 0.8\n",
      "Iteration 14601: with minibatch training loss = 0.78 and accuracy of 0.75\n",
      "Iteration 14602: with minibatch training loss = 0.672 and accuracy of 0.78\n",
      "Iteration 14603: with minibatch training loss = 0.52 and accuracy of 0.84\n",
      "Iteration 14604: with minibatch training loss = 0.707 and accuracy of 0.8\n",
      "Iteration 14605: with minibatch training loss = 0.886 and accuracy of 0.77\n",
      "Iteration 14606: with minibatch training loss = 0.56 and accuracy of 0.81\n",
      "Iteration 14607: with minibatch training loss = 0.41 and accuracy of 0.86\n",
      "Iteration 14608: with minibatch training loss = 0.481 and accuracy of 0.86\n",
      "Iteration 14609: with minibatch training loss = 0.646 and accuracy of 0.78\n",
      "Iteration 14610: with minibatch training loss = 0.51 and accuracy of 0.86\n",
      "Iteration 14611: with minibatch training loss = 0.898 and accuracy of 0.7\n",
      "Iteration 14612: with minibatch training loss = 0.602 and accuracy of 0.83\n",
      "Iteration 14613: with minibatch training loss = 0.561 and accuracy of 0.83\n",
      "Iteration 14614: with minibatch training loss = 0.972 and accuracy of 0.67\n",
      "Iteration 14615: with minibatch training loss = 0.943 and accuracy of 0.7\n",
      "Iteration 14616: with minibatch training loss = 0.669 and accuracy of 0.78\n",
      "Iteration 14617: with minibatch training loss = 0.878 and accuracy of 0.73\n",
      "Iteration 14618: with minibatch training loss = 0.811 and accuracy of 0.72\n",
      "Iteration 14619: with minibatch training loss = 0.454 and accuracy of 0.86\n",
      "Iteration 14620: with minibatch training loss = 0.656 and accuracy of 0.8\n",
      "Iteration 14621: with minibatch training loss = 0.699 and accuracy of 0.77\n",
      "Iteration 14622: with minibatch training loss = 0.847 and accuracy of 0.75\n",
      "Iteration 14623: with minibatch training loss = 0.91 and accuracy of 0.73\n",
      "Iteration 14624: with minibatch training loss = 0.737 and accuracy of 0.78\n",
      "Iteration 14625: with minibatch training loss = 0.793 and accuracy of 0.77\n",
      "Iteration 14626: with minibatch training loss = 0.491 and accuracy of 0.86\n",
      "Iteration 14627: with minibatch training loss = 0.856 and accuracy of 0.77\n",
      "Iteration 14628: with minibatch training loss = 0.606 and accuracy of 0.81\n",
      "Iteration 14629: with minibatch training loss = 0.634 and accuracy of 0.83\n",
      "Iteration 14630: with minibatch training loss = 0.505 and accuracy of 0.86\n",
      "Iteration 14631: with minibatch training loss = 0.461 and accuracy of 0.86\n",
      "Iteration 14632: with minibatch training loss = 0.692 and accuracy of 0.84\n",
      "Iteration 14633: with minibatch training loss = 0.598 and accuracy of 0.84\n",
      "Iteration 14634: with minibatch training loss = 0.517 and accuracy of 0.84\n",
      "Iteration 14635: with minibatch training loss = 0.856 and accuracy of 0.72\n",
      "Iteration 14636: with minibatch training loss = 0.488 and accuracy of 0.83\n",
      "Iteration 14637: with minibatch training loss = 0.708 and accuracy of 0.78\n",
      "Iteration 14638: with minibatch training loss = 0.548 and accuracy of 0.84\n",
      "Iteration 14639: with minibatch training loss = 0.7 and accuracy of 0.8\n",
      "Iteration 14640: with minibatch training loss = 0.856 and accuracy of 0.73\n",
      "Iteration 14641: with minibatch training loss = 0.637 and accuracy of 0.81\n",
      "Iteration 14642: with minibatch training loss = 0.673 and accuracy of 0.81\n",
      "Iteration 14643: with minibatch training loss = 0.768 and accuracy of 0.78\n",
      "Iteration 14644: with minibatch training loss = 0.748 and accuracy of 0.8\n",
      "Iteration 14645: with minibatch training loss = 0.618 and accuracy of 0.81\n",
      "Iteration 14646: with minibatch training loss = 0.604 and accuracy of 0.83\n",
      "Iteration 14647: with minibatch training loss = 0.753 and accuracy of 0.77\n",
      "Iteration 14648: with minibatch training loss = 0.514 and accuracy of 0.86\n",
      "Iteration 14649: with minibatch training loss = 0.754 and accuracy of 0.77\n",
      "Iteration 14650: with minibatch training loss = 0.652 and accuracy of 0.83\n",
      "Iteration 14651: with minibatch training loss = 0.593 and accuracy of 0.86\n",
      "Iteration 14652: with minibatch training loss = 0.632 and accuracy of 0.78\n",
      "Iteration 14653: with minibatch training loss = 0.614 and accuracy of 0.83\n",
      "Iteration 14654: with minibatch training loss = 0.706 and accuracy of 0.78\n",
      "Iteration 14655: with minibatch training loss = 0.615 and accuracy of 0.8\n",
      "Iteration 14656: with minibatch training loss = 0.506 and accuracy of 0.88\n",
      "Iteration 14657: with minibatch training loss = 0.653 and accuracy of 0.8\n",
      "Iteration 14658: with minibatch training loss = 0.6 and accuracy of 0.81\n",
      "Iteration 14659: with minibatch training loss = 0.54 and accuracy of 0.83\n",
      "Iteration 14660: with minibatch training loss = 0.774 and accuracy of 0.77\n",
      "Iteration 14661: with minibatch training loss = 0.978 and accuracy of 0.7\n",
      "Iteration 14662: with minibatch training loss = 0.533 and accuracy of 0.84\n",
      "Iteration 14663: with minibatch training loss = 0.659 and accuracy of 0.78\n",
      "Iteration 14664: with minibatch training loss = 0.981 and accuracy of 0.64\n",
      "Iteration 14665: with minibatch training loss = 0.905 and accuracy of 0.73\n",
      "Iteration 14666: with minibatch training loss = 0.719 and accuracy of 0.78\n",
      "Iteration 14667: with minibatch training loss = 0.733 and accuracy of 0.73\n",
      "Iteration 14668: with minibatch training loss = 0.837 and accuracy of 0.77\n",
      "Iteration 14669: with minibatch training loss = 0.758 and accuracy of 0.73\n",
      "Iteration 14670: with minibatch training loss = 0.514 and accuracy of 0.84\n",
      "Iteration 14671: with minibatch training loss = 0.739 and accuracy of 0.75\n",
      "Iteration 14672: with minibatch training loss = 0.632 and accuracy of 0.8\n",
      "Iteration 14673: with minibatch training loss = 0.739 and accuracy of 0.75\n",
      "Iteration 14674: with minibatch training loss = 0.605 and accuracy of 0.81\n",
      "Iteration 14675: with minibatch training loss = 0.553 and accuracy of 0.83\n",
      "Iteration 14676: with minibatch training loss = 0.75 and accuracy of 0.78\n",
      "Iteration 14677: with minibatch training loss = 0.867 and accuracy of 0.73\n",
      "Iteration 14678: with minibatch training loss = 0.791 and accuracy of 0.81\n",
      "Iteration 14679: with minibatch training loss = 0.591 and accuracy of 0.84\n",
      "Iteration 14680: with minibatch training loss = 0.561 and accuracy of 0.89\n",
      "Iteration 14681: with minibatch training loss = 0.543 and accuracy of 0.84\n",
      "Iteration 14682: with minibatch training loss = 0.557 and accuracy of 0.83\n",
      "Iteration 14683: with minibatch training loss = 0.655 and accuracy of 0.81\n",
      "Iteration 14684: with minibatch training loss = 0.577 and accuracy of 0.81\n",
      "Iteration 14685: with minibatch training loss = 0.739 and accuracy of 0.78\n",
      "Iteration 14686: with minibatch training loss = 0.377 and accuracy of 0.89\n",
      "Iteration 14687: with minibatch training loss = 0.851 and accuracy of 0.7\n",
      "Iteration 14688: with minibatch training loss = 0.579 and accuracy of 0.83\n",
      "Iteration 14689: with minibatch training loss = 0.935 and accuracy of 0.7\n",
      "Iteration 14690: with minibatch training loss = 0.907 and accuracy of 0.69\n",
      "Iteration 14691: with minibatch training loss = 0.581 and accuracy of 0.83\n",
      "Iteration 14692: with minibatch training loss = 0.632 and accuracy of 0.83\n",
      "Iteration 14693: with minibatch training loss = 0.768 and accuracy of 0.78\n",
      "Iteration 14694: with minibatch training loss = 0.62 and accuracy of 0.8\n",
      "Iteration 14695: with minibatch training loss = 0.635 and accuracy of 0.8\n",
      "Iteration 14696: with minibatch training loss = 1.02 and accuracy of 0.66\n",
      "Iteration 14697: with minibatch training loss = 0.697 and accuracy of 0.8\n",
      "Iteration 14698: with minibatch training loss = 0.816 and accuracy of 0.75\n",
      "Iteration 14699: with minibatch training loss = 0.727 and accuracy of 0.77\n",
      "Iteration 14700: with minibatch training loss = 0.672 and accuracy of 0.78\n",
      "Iteration 14701: with minibatch training loss = 0.701 and accuracy of 0.8\n",
      "Iteration 14702: with minibatch training loss = 0.744 and accuracy of 0.75\n",
      "Iteration 14703: with minibatch training loss = 0.823 and accuracy of 0.78\n",
      "Iteration 14704: with minibatch training loss = 0.801 and accuracy of 0.77\n",
      "Iteration 14705: with minibatch training loss = 0.529 and accuracy of 0.83\n",
      "Iteration 14706: with minibatch training loss = 0.684 and accuracy of 0.8\n",
      "Iteration 14707: with minibatch training loss = 0.955 and accuracy of 0.67\n",
      "Iteration 14708: with minibatch training loss = 0.484 and accuracy of 0.84\n",
      "Iteration 14709: with minibatch training loss = 0.562 and accuracy of 0.8\n",
      "Iteration 14710: with minibatch training loss = 0.446 and accuracy of 0.86\n",
      "Iteration 14711: with minibatch training loss = 0.547 and accuracy of 0.84\n",
      "Iteration 14712: with minibatch training loss = 0.722 and accuracy of 0.8\n",
      "Iteration 14713: with minibatch training loss = 0.609 and accuracy of 0.81\n",
      "Iteration 14714: with minibatch training loss = 0.798 and accuracy of 0.77\n",
      "Iteration 14715: with minibatch training loss = 0.664 and accuracy of 0.78\n",
      "Iteration 14716: with minibatch training loss = 0.598 and accuracy of 0.8\n",
      "Iteration 14717: with minibatch training loss = 0.726 and accuracy of 0.78\n",
      "Iteration 14718: with minibatch training loss = 0.672 and accuracy of 0.78\n",
      "Iteration 14719: with minibatch training loss = 0.781 and accuracy of 0.77\n",
      "Iteration 14720: with minibatch training loss = 0.539 and accuracy of 0.83\n",
      "Iteration 14721: with minibatch training loss = 0.665 and accuracy of 0.81\n",
      "Iteration 14722: with minibatch training loss = 0.682 and accuracy of 0.8\n",
      "Iteration 14723: with minibatch training loss = 0.523 and accuracy of 0.84\n",
      "Iteration 14724: with minibatch training loss = 0.512 and accuracy of 0.81\n",
      "Iteration 14725: with minibatch training loss = 0.945 and accuracy of 0.72\n",
      "Iteration 14726: with minibatch training loss = 0.559 and accuracy of 0.83\n",
      "Iteration 14727: with minibatch training loss = 0.684 and accuracy of 0.8\n",
      "Iteration 14728: with minibatch training loss = 0.771 and accuracy of 0.75\n",
      "Iteration 14729: with minibatch training loss = 0.606 and accuracy of 0.8\n",
      "Iteration 14730: with minibatch training loss = 0.728 and accuracy of 0.75\n",
      "Iteration 14731: with minibatch training loss = 0.748 and accuracy of 0.78\n",
      "Iteration 14732: with minibatch training loss = 0.659 and accuracy of 0.78\n",
      "Iteration 14733: with minibatch training loss = 0.534 and accuracy of 0.84\n",
      "Iteration 14734: with minibatch training loss = 0.464 and accuracy of 0.86\n",
      "Iteration 14735: with minibatch training loss = 0.586 and accuracy of 0.83\n",
      "Iteration 14736: with minibatch training loss = 0.616 and accuracy of 0.81\n",
      "Iteration 14737: with minibatch training loss = 0.66 and accuracy of 0.83\n",
      "Iteration 14738: with minibatch training loss = 0.468 and accuracy of 0.88\n",
      "Iteration 14739: with minibatch training loss = 0.871 and accuracy of 0.73\n",
      "Iteration 14740: with minibatch training loss = 0.696 and accuracy of 0.78\n",
      "Iteration 14741: with minibatch training loss = 0.609 and accuracy of 0.83\n",
      "Iteration 14742: with minibatch training loss = 0.579 and accuracy of 0.83\n",
      "Iteration 14743: with minibatch training loss = 0.696 and accuracy of 0.75\n",
      "Iteration 14744: with minibatch training loss = 0.648 and accuracy of 0.81\n",
      "Iteration 14745: with minibatch training loss = 0.763 and accuracy of 0.77\n",
      "Iteration 14746: with minibatch training loss = 0.439 and accuracy of 0.86\n",
      "Iteration 14747: with minibatch training loss = 0.892 and accuracy of 0.77\n",
      "Iteration 14748: with minibatch training loss = 0.626 and accuracy of 0.81\n",
      "Iteration 14749: with minibatch training loss = 0.726 and accuracy of 0.8\n",
      "Iteration 14750: with minibatch training loss = 0.796 and accuracy of 0.72\n",
      "Iteration 14751: with minibatch training loss = 0.927 and accuracy of 0.72\n",
      "Iteration 14752: with minibatch training loss = 0.746 and accuracy of 0.75\n",
      "Iteration 14753: with minibatch training loss = 0.887 and accuracy of 0.73\n",
      "Iteration 14754: with minibatch training loss = 0.669 and accuracy of 0.78\n",
      "Iteration 14755: with minibatch training loss = 0.804 and accuracy of 0.73\n",
      "Iteration 14756: with minibatch training loss = 0.687 and accuracy of 0.78\n",
      "Iteration 14757: with minibatch training loss = 0.889 and accuracy of 0.72\n",
      "Iteration 14758: with minibatch training loss = 0.704 and accuracy of 0.8\n",
      "Iteration 14759: with minibatch training loss = 0.503 and accuracy of 0.83\n",
      "Iteration 14760: with minibatch training loss = 0.711 and accuracy of 0.77\n",
      "Iteration 14761: with minibatch training loss = 0.872 and accuracy of 0.77\n",
      "Iteration 14762: with minibatch training loss = 0.596 and accuracy of 0.81\n",
      "Iteration 14763: with minibatch training loss = 0.66 and accuracy of 0.81\n",
      "Iteration 14764: with minibatch training loss = 0.791 and accuracy of 0.8\n",
      "Iteration 14765: with minibatch training loss = 0.401 and accuracy of 0.89\n",
      "Iteration 14766: with minibatch training loss = 0.663 and accuracy of 0.78\n",
      "Iteration 14767: with minibatch training loss = 0.631 and accuracy of 0.83\n",
      "Iteration 14768: with minibatch training loss = 0.609 and accuracy of 0.8\n",
      "Iteration 14769: with minibatch training loss = 0.696 and accuracy of 0.8\n",
      "Iteration 14770: with minibatch training loss = 0.978 and accuracy of 0.67\n",
      "Iteration 14771: with minibatch training loss = 0.951 and accuracy of 0.67\n",
      "Iteration 14772: with minibatch training loss = 0.598 and accuracy of 0.83\n",
      "Iteration 14773: with minibatch training loss = 0.508 and accuracy of 0.89\n",
      "Iteration 14774: with minibatch training loss = 0.721 and accuracy of 0.77\n",
      "Iteration 14775: with minibatch training loss = 0.684 and accuracy of 0.81\n",
      "Iteration 14776: with minibatch training loss = 0.512 and accuracy of 0.83\n",
      "Iteration 14777: with minibatch training loss = 0.994 and accuracy of 0.7\n",
      "Iteration 14778: with minibatch training loss = 0.757 and accuracy of 0.78\n",
      "Iteration 14779: with minibatch training loss = 0.848 and accuracy of 0.75\n",
      "Iteration 14780: with minibatch training loss = 0.751 and accuracy of 0.73\n",
      "Iteration 14781: with minibatch training loss = 0.87 and accuracy of 0.75\n",
      "Iteration 14782: with minibatch training loss = 0.786 and accuracy of 0.75\n",
      "Iteration 14783: with minibatch training loss = 0.594 and accuracy of 0.83\n",
      "Iteration 14784: with minibatch training loss = 0.557 and accuracy of 0.84\n",
      "Iteration 14785: with minibatch training loss = 0.442 and accuracy of 0.88\n",
      "Iteration 14786: with minibatch training loss = 1.11 and accuracy of 0.66\n",
      "Iteration 14787: with minibatch training loss = 0.533 and accuracy of 0.84\n",
      "Iteration 14788: with minibatch training loss = 0.665 and accuracy of 0.84\n",
      "Iteration 14789: with minibatch training loss = 0.649 and accuracy of 0.78\n",
      "Iteration 14790: with minibatch training loss = 0.783 and accuracy of 0.75\n",
      "Iteration 14791: with minibatch training loss = 0.625 and accuracy of 0.81\n",
      "Iteration 14792: with minibatch training loss = 0.546 and accuracy of 0.81\n",
      "Iteration 14793: with minibatch training loss = 0.71 and accuracy of 0.78\n",
      "Iteration 14794: with minibatch training loss = 0.661 and accuracy of 0.8\n",
      "Iteration 14795: with minibatch training loss = 0.531 and accuracy of 0.86\n",
      "Iteration 14796: with minibatch training loss = 0.601 and accuracy of 0.8\n",
      "Iteration 14797: with minibatch training loss = 0.647 and accuracy of 0.78\n",
      "Iteration 14798: with minibatch training loss = 0.473 and accuracy of 0.88\n",
      "Iteration 14799: with minibatch training loss = 0.51 and accuracy of 0.84\n",
      "Iteration 14800: with minibatch training loss = 0.914 and accuracy of 0.7\n",
      "Iteration 14801: with minibatch training loss = 0.744 and accuracy of 0.81\n",
      "Iteration 14802: with minibatch training loss = 0.75 and accuracy of 0.8\n",
      "Iteration 14803: with minibatch training loss = 0.639 and accuracy of 0.81\n",
      "Iteration 14804: with minibatch training loss = 0.966 and accuracy of 0.67\n",
      "Iteration 14805: with minibatch training loss = 0.716 and accuracy of 0.8\n",
      "Iteration 14806: with minibatch training loss = 0.861 and accuracy of 0.72\n",
      "Iteration 14807: with minibatch training loss = 0.587 and accuracy of 0.81\n",
      "Iteration 14808: with minibatch training loss = 0.525 and accuracy of 0.86\n",
      "Iteration 14809: with minibatch training loss = 0.755 and accuracy of 0.77\n",
      "Iteration 14810: with minibatch training loss = 0.567 and accuracy of 0.81\n",
      "Iteration 14811: with minibatch training loss = 0.48 and accuracy of 0.86\n",
      "Iteration 14812: with minibatch training loss = 0.605 and accuracy of 0.81\n",
      "Iteration 14813: with minibatch training loss = 0.591 and accuracy of 0.81\n",
      "Iteration 14814: with minibatch training loss = 0.925 and accuracy of 0.73\n",
      "Iteration 14815: with minibatch training loss = 0.624 and accuracy of 0.83\n",
      "Iteration 14816: with minibatch training loss = 0.413 and accuracy of 0.89\n",
      "Iteration 14817: with minibatch training loss = 0.778 and accuracy of 0.72\n",
      "Iteration 14818: with minibatch training loss = 0.768 and accuracy of 0.78\n",
      "Iteration 14819: with minibatch training loss = 0.52 and accuracy of 0.84\n",
      "Iteration 14820: with minibatch training loss = 0.621 and accuracy of 0.83\n",
      "Iteration 14821: with minibatch training loss = 0.778 and accuracy of 0.77\n",
      "Iteration 14822: with minibatch training loss = 0.665 and accuracy of 0.81\n",
      "Iteration 14823: with minibatch training loss = 0.636 and accuracy of 0.83\n",
      "Iteration 14824: with minibatch training loss = 0.706 and accuracy of 0.81\n",
      "Iteration 14825: with minibatch training loss = 0.637 and accuracy of 0.84\n",
      "Iteration 14826: with minibatch training loss = 0.922 and accuracy of 0.77\n",
      "Iteration 14827: with minibatch training loss = 0.498 and accuracy of 0.88\n",
      "Iteration 14828: with minibatch training loss = 0.776 and accuracy of 0.75\n",
      "Iteration 14829: with minibatch training loss = 0.632 and accuracy of 0.8\n",
      "Iteration 14830: with minibatch training loss = 0.642 and accuracy of 0.81\n",
      "Iteration 14831: with minibatch training loss = 0.432 and accuracy of 0.86\n",
      "Iteration 14832: with minibatch training loss = 0.623 and accuracy of 0.8\n",
      "Iteration 14833: with minibatch training loss = 0.392 and accuracy of 0.89\n",
      "Iteration 14834: with minibatch training loss = 0.74 and accuracy of 0.78\n",
      "Iteration 14835: with minibatch training loss = 0.811 and accuracy of 0.73\n",
      "Iteration 14836: with minibatch training loss = 0.745 and accuracy of 0.77\n",
      "Iteration 14837: with minibatch training loss = 0.824 and accuracy of 0.75\n",
      "Iteration 14838: with minibatch training loss = 0.947 and accuracy of 0.7\n",
      "Iteration 14839: with minibatch training loss = 0.629 and accuracy of 0.86\n",
      "Iteration 14840: with minibatch training loss = 0.765 and accuracy of 0.77\n",
      "Iteration 14841: with minibatch training loss = 0.656 and accuracy of 0.8\n",
      "Iteration 14842: with minibatch training loss = 0.701 and accuracy of 0.78\n",
      "Iteration 14843: with minibatch training loss = 0.58 and accuracy of 0.83\n",
      "Iteration 14844: with minibatch training loss = 0.79 and accuracy of 0.75\n",
      "Iteration 14845: with minibatch training loss = 0.749 and accuracy of 0.8\n",
      "Iteration 14846: with minibatch training loss = 0.601 and accuracy of 0.83\n",
      "Iteration 14847: with minibatch training loss = 0.748 and accuracy of 0.77\n",
      "Iteration 14848: with minibatch training loss = 0.478 and accuracy of 0.84\n",
      "Iteration 14849: with minibatch training loss = 0.519 and accuracy of 0.86\n",
      "Iteration 14850: with minibatch training loss = 0.704 and accuracy of 0.83\n",
      "Iteration 14851: with minibatch training loss = 0.467 and accuracy of 0.86\n",
      "Iteration 14852: with minibatch training loss = 0.575 and accuracy of 0.83\n",
      "Iteration 14853: with minibatch training loss = 0.398 and accuracy of 0.88\n",
      "Iteration 14854: with minibatch training loss = 0.67 and accuracy of 0.78\n",
      "Iteration 14855: with minibatch training loss = 0.587 and accuracy of 0.78\n",
      "Iteration 14856: with minibatch training loss = 0.685 and accuracy of 0.83\n",
      "Iteration 14857: with minibatch training loss = 0.605 and accuracy of 0.83\n",
      "Iteration 14858: with minibatch training loss = 0.67 and accuracy of 0.83\n",
      "Iteration 14859: with minibatch training loss = 0.775 and accuracy of 0.75\n",
      "Iteration 14860: with minibatch training loss = 0.774 and accuracy of 0.75\n",
      "Iteration 14861: with minibatch training loss = 0.789 and accuracy of 0.75\n",
      "Iteration 14862: with minibatch training loss = 0.659 and accuracy of 0.8\n",
      "Iteration 14863: with minibatch training loss = 0.551 and accuracy of 0.86\n",
      "Iteration 14864: with minibatch training loss = 0.459 and accuracy of 0.88\n",
      "Iteration 14865: with minibatch training loss = 0.741 and accuracy of 0.78\n",
      "Iteration 14866: with minibatch training loss = 0.641 and accuracy of 0.8\n",
      "Iteration 14867: with minibatch training loss = 1.1 and accuracy of 0.64\n",
      "Iteration 14868: with minibatch training loss = 0.634 and accuracy of 0.8\n",
      "Iteration 14869: with minibatch training loss = 0.733 and accuracy of 0.8\n",
      "Iteration 14870: with minibatch training loss = 0.44 and accuracy of 0.86\n",
      "Iteration 14871: with minibatch training loss = 0.756 and accuracy of 0.77\n",
      "Iteration 14872: with minibatch training loss = 0.974 and accuracy of 0.72\n",
      "Iteration 14873: with minibatch training loss = 0.735 and accuracy of 0.75\n",
      "Iteration 14874: with minibatch training loss = 0.566 and accuracy of 0.83\n",
      "Iteration 14875: with minibatch training loss = 0.682 and accuracy of 0.81\n",
      "Iteration 14876: with minibatch training loss = 0.596 and accuracy of 0.81\n",
      "Iteration 14877: with minibatch training loss = 0.734 and accuracy of 0.77\n",
      "Iteration 14878: with minibatch training loss = 1.05 and accuracy of 0.67\n",
      "Iteration 14879: with minibatch training loss = 0.693 and accuracy of 0.8\n",
      "Iteration 14880: with minibatch training loss = 0.617 and accuracy of 0.83\n",
      "Iteration 14881: with minibatch training loss = 0.76 and accuracy of 0.8\n",
      "Iteration 14882: with minibatch training loss = 0.686 and accuracy of 0.8\n",
      "Iteration 14883: with minibatch training loss = 0.513 and accuracy of 0.88\n",
      "Iteration 14884: with minibatch training loss = 0.762 and accuracy of 0.8\n",
      "Iteration 14885: with minibatch training loss = 0.584 and accuracy of 0.81\n",
      "Iteration 14886: with minibatch training loss = 0.724 and accuracy of 0.8\n",
      "Iteration 14887: with minibatch training loss = 0.679 and accuracy of 0.77\n",
      "Iteration 14888: with minibatch training loss = 0.603 and accuracy of 0.83\n",
      "Iteration 14889: with minibatch training loss = 0.539 and accuracy of 0.86\n",
      "Iteration 14890: with minibatch training loss = 0.543 and accuracy of 0.83\n",
      "Iteration 14891: with minibatch training loss = 0.754 and accuracy of 0.75\n",
      "Iteration 14892: with minibatch training loss = 0.658 and accuracy of 0.78\n",
      "Iteration 14893: with minibatch training loss = 0.807 and accuracy of 0.75\n",
      "Iteration 14894: with minibatch training loss = 0.56 and accuracy of 0.83\n",
      "Iteration 14895: with minibatch training loss = 0.477 and accuracy of 0.84\n",
      "Iteration 14896: with minibatch training loss = 0.441 and accuracy of 0.88\n",
      "Iteration 14897: with minibatch training loss = 0.704 and accuracy of 0.8\n",
      "Iteration 14898: with minibatch training loss = 0.548 and accuracy of 0.84\n",
      "Iteration 14899: with minibatch training loss = 0.585 and accuracy of 0.83\n",
      "Iteration 14900: with minibatch training loss = 0.6 and accuracy of 0.86\n",
      "Iteration 14901: with minibatch training loss = 0.699 and accuracy of 0.78\n",
      "Iteration 14902: with minibatch training loss = 0.587 and accuracy of 0.81\n",
      "Iteration 14903: with minibatch training loss = 0.709 and accuracy of 0.81\n",
      "Iteration 14904: with minibatch training loss = 0.666 and accuracy of 0.78\n",
      "Iteration 14905: with minibatch training loss = 0.838 and accuracy of 0.72\n",
      "Iteration 14906: with minibatch training loss = 0.558 and accuracy of 0.83\n",
      "Iteration 14907: with minibatch training loss = 0.599 and accuracy of 0.78\n",
      "Iteration 14908: with minibatch training loss = 0.915 and accuracy of 0.73\n",
      "Iteration 14909: with minibatch training loss = 0.692 and accuracy of 0.75\n",
      "Iteration 14910: with minibatch training loss = 0.764 and accuracy of 0.78\n",
      "Iteration 14911: with minibatch training loss = 0.672 and accuracy of 0.78\n",
      "Iteration 14912: with minibatch training loss = 0.673 and accuracy of 0.8\n",
      "Iteration 14913: with minibatch training loss = 0.492 and accuracy of 0.86\n",
      "Iteration 14914: with minibatch training loss = 0.792 and accuracy of 0.75\n",
      "Iteration 14915: with minibatch training loss = 0.878 and accuracy of 0.69\n",
      "Iteration 14916: with minibatch training loss = 0.726 and accuracy of 0.78\n",
      "Iteration 14917: with minibatch training loss = 0.537 and accuracy of 0.83\n",
      "Iteration 14918: with minibatch training loss = 0.588 and accuracy of 0.78\n",
      "Iteration 14919: with minibatch training loss = 0.72 and accuracy of 0.77\n",
      "Iteration 14920: with minibatch training loss = 0.506 and accuracy of 0.88\n",
      "Iteration 14921: with minibatch training loss = 0.662 and accuracy of 0.8\n",
      "Iteration 14922: with minibatch training loss = 0.765 and accuracy of 0.77\n",
      "Iteration 14923: with minibatch training loss = 1.01 and accuracy of 0.7\n",
      "Iteration 14924: with minibatch training loss = 1.04 and accuracy of 0.66\n",
      "Iteration 14925: with minibatch training loss = 0.69 and accuracy of 0.8\n",
      "Iteration 14926: with minibatch training loss = 0.959 and accuracy of 0.77\n",
      "Iteration 14927: with minibatch training loss = 0.517 and accuracy of 0.84\n",
      "Iteration 14928: with minibatch training loss = 0.534 and accuracy of 0.86\n",
      "Iteration 14929: with minibatch training loss = 0.68 and accuracy of 0.81\n",
      "Iteration 14930: with minibatch training loss = 0.904 and accuracy of 0.73\n",
      "Iteration 14931: with minibatch training loss = 0.559 and accuracy of 0.83\n",
      "Iteration 14932: with minibatch training loss = 0.466 and accuracy of 0.88\n",
      "Iteration 14933: with minibatch training loss = 0.74 and accuracy of 0.8\n",
      "Iteration 14934: with minibatch training loss = 0.589 and accuracy of 0.84\n",
      "Iteration 14935: with minibatch training loss = 0.568 and accuracy of 0.83\n",
      "Iteration 14936: with minibatch training loss = 0.512 and accuracy of 0.81\n",
      "Iteration 14937: with minibatch training loss = 0.722 and accuracy of 0.78\n",
      "Iteration 14938: with minibatch training loss = 0.409 and accuracy of 0.86\n",
      "Iteration 14939: with minibatch training loss = 0.693 and accuracy of 0.8\n",
      "Iteration 14940: with minibatch training loss = 0.897 and accuracy of 0.73\n",
      "Iteration 14941: with minibatch training loss = 0.394 and accuracy of 0.89\n",
      "Iteration 14942: with minibatch training loss = 1.07 and accuracy of 0.66\n",
      "Iteration 14943: with minibatch training loss = 0.595 and accuracy of 0.83\n",
      "Iteration 14944: with minibatch training loss = 0.832 and accuracy of 0.73\n",
      "Iteration 14945: with minibatch training loss = 0.814 and accuracy of 0.75\n",
      "Iteration 14946: with minibatch training loss = 0.57 and accuracy of 0.81\n",
      "Iteration 14947: with minibatch training loss = 0.599 and accuracy of 0.81\n",
      "Iteration 14948: with minibatch training loss = 0.693 and accuracy of 0.78\n",
      "Iteration 14949: with minibatch training loss = 0.656 and accuracy of 0.83\n",
      "Iteration 14950: with minibatch training loss = 0.679 and accuracy of 0.75\n",
      "Iteration 14951: with minibatch training loss = 0.574 and accuracy of 0.83\n",
      "Iteration 14952: with minibatch training loss = 0.516 and accuracy of 0.88\n",
      "Iteration 14953: with minibatch training loss = 0.83 and accuracy of 0.72\n",
      "Iteration 14954: with minibatch training loss = 0.905 and accuracy of 0.67\n",
      "Iteration 14955: with minibatch training loss = 0.862 and accuracy of 0.73\n",
      "Iteration 14956: with minibatch training loss = 0.831 and accuracy of 0.7\n",
      "Iteration 14957: with minibatch training loss = 0.594 and accuracy of 0.84\n",
      "Iteration 14958: with minibatch training loss = 0.833 and accuracy of 0.73\n",
      "Iteration 14959: with minibatch training loss = 0.523 and accuracy of 0.89\n",
      "Iteration 14960: with minibatch training loss = 0.707 and accuracy of 0.73\n",
      "Iteration 14961: with minibatch training loss = 0.674 and accuracy of 0.83\n",
      "Iteration 14962: with minibatch training loss = 0.695 and accuracy of 0.81\n",
      "Iteration 14963: with minibatch training loss = 0.818 and accuracy of 0.8\n",
      "Iteration 14964: with minibatch training loss = 0.76 and accuracy of 0.77\n",
      "Iteration 14965: with minibatch training loss = 0.57 and accuracy of 0.81\n",
      "Iteration 14966: with minibatch training loss = 0.76 and accuracy of 0.75\n",
      "Iteration 14967: with minibatch training loss = 0.558 and accuracy of 0.8\n",
      "Iteration 14968: with minibatch training loss = 0.698 and accuracy of 0.78\n",
      "Iteration 14969: with minibatch training loss = 0.691 and accuracy of 0.78\n",
      "Iteration 14970: with minibatch training loss = 0.774 and accuracy of 0.73\n",
      "Iteration 14971: with minibatch training loss = 0.588 and accuracy of 0.81\n",
      "Iteration 14972: with minibatch training loss = 0.83 and accuracy of 0.77\n",
      "Iteration 14973: with minibatch training loss = 0.672 and accuracy of 0.77\n",
      "Iteration 14974: with minibatch training loss = 0.618 and accuracy of 0.84\n",
      "Iteration 14975: with minibatch training loss = 0.824 and accuracy of 0.75\n",
      "Iteration 14976: with minibatch training loss = 0.45 and accuracy of 0.88\n",
      "Iteration 14977: with minibatch training loss = 0.565 and accuracy of 0.81\n",
      "Iteration 14978: with minibatch training loss = 0.635 and accuracy of 0.8\n",
      "Iteration 14979: with minibatch training loss = 0.731 and accuracy of 0.77\n",
      "Iteration 14980: with minibatch training loss = 0.666 and accuracy of 0.81\n",
      "Iteration 14981: with minibatch training loss = 0.66 and accuracy of 0.84\n",
      "Iteration 14982: with minibatch training loss = 0.464 and accuracy of 0.88\n",
      "Iteration 14983: with minibatch training loss = 0.789 and accuracy of 0.75\n",
      "Iteration 14984: with minibatch training loss = 0.877 and accuracy of 0.75\n",
      "Iteration 14985: with minibatch training loss = 0.956 and accuracy of 0.69\n",
      "Iteration 14986: with minibatch training loss = 0.793 and accuracy of 0.75\n",
      "Iteration 14987: with minibatch training loss = 0.673 and accuracy of 0.8\n",
      "Iteration 14988: with minibatch training loss = 0.447 and accuracy of 0.88\n",
      "Iteration 14989: with minibatch training loss = 0.679 and accuracy of 0.8\n",
      "Iteration 14990: with minibatch training loss = 0.547 and accuracy of 0.83\n",
      "Iteration 14991: with minibatch training loss = 0.809 and accuracy of 0.77\n",
      "Iteration 14992: with minibatch training loss = 0.776 and accuracy of 0.75\n",
      "Iteration 14993: with minibatch training loss = 0.657 and accuracy of 0.81\n",
      "Iteration 14994: with minibatch training loss = 0.746 and accuracy of 0.75\n",
      "Iteration 14995: with minibatch training loss = 0.723 and accuracy of 0.78\n",
      "Iteration 14996: with minibatch training loss = 0.843 and accuracy of 0.73\n",
      "Iteration 14997: with minibatch training loss = 0.936 and accuracy of 0.7\n",
      "Iteration 14998: with minibatch training loss = 0.505 and accuracy of 0.86\n",
      "Iteration 14999: with minibatch training loss = 0.911 and accuracy of 0.7\n",
      "Iteration 15000: with minibatch training loss = 0.784 and accuracy of 0.75\n",
      "Iteration 15001: with minibatch training loss = 0.723 and accuracy of 0.83\n",
      "Iteration 15002: with minibatch training loss = 0.766 and accuracy of 0.78\n",
      "Iteration 15003: with minibatch training loss = 0.624 and accuracy of 0.78\n",
      "Iteration 15004: with minibatch training loss = 0.654 and accuracy of 0.8\n",
      "Iteration 15005: with minibatch training loss = 0.82 and accuracy of 0.77\n",
      "Iteration 15006: with minibatch training loss = 0.928 and accuracy of 0.72\n",
      "Iteration 15007: with minibatch training loss = 0.555 and accuracy of 0.81\n",
      "Iteration 15008: with minibatch training loss = 0.806 and accuracy of 0.73\n",
      "Iteration 15009: with minibatch training loss = 0.627 and accuracy of 0.78\n",
      "Iteration 15010: with minibatch training loss = 0.736 and accuracy of 0.8\n",
      "Iteration 15011: with minibatch training loss = 0.862 and accuracy of 0.75\n",
      "Iteration 15012: with minibatch training loss = 0.603 and accuracy of 0.83\n",
      "Iteration 15013: with minibatch training loss = 0.809 and accuracy of 0.73\n",
      "Iteration 15014: with minibatch training loss = 0.904 and accuracy of 0.69\n",
      "Iteration 15015: with minibatch training loss = 0.701 and accuracy of 0.8\n",
      "Iteration 15016: with minibatch training loss = 0.601 and accuracy of 0.81\n",
      "Iteration 15017: with minibatch training loss = 0.53 and accuracy of 0.84\n",
      "Iteration 15018: with minibatch training loss = 0.563 and accuracy of 0.84\n",
      "Iteration 15019: with minibatch training loss = 0.503 and accuracy of 0.83\n",
      "Iteration 15020: with minibatch training loss = 0.553 and accuracy of 0.83\n",
      "Iteration 15021: with minibatch training loss = 0.41 and accuracy of 0.88\n",
      "Iteration 15022: with minibatch training loss = 0.975 and accuracy of 0.72\n",
      "Iteration 15023: with minibatch training loss = 0.825 and accuracy of 0.73\n",
      "Iteration 15024: with minibatch training loss = 0.627 and accuracy of 0.84\n",
      "Iteration 15025: with minibatch training loss = 0.52 and accuracy of 0.84\n",
      "Iteration 15026: with minibatch training loss = 0.422 and accuracy of 0.91\n",
      "Iteration 15027: with minibatch training loss = 0.725 and accuracy of 0.77\n",
      "Iteration 15028: with minibatch training loss = 0.665 and accuracy of 0.81\n",
      "Iteration 15029: with minibatch training loss = 0.565 and accuracy of 0.83\n",
      "Iteration 15030: with minibatch training loss = 0.62 and accuracy of 0.84\n",
      "Iteration 15031: with minibatch training loss = 0.671 and accuracy of 0.81\n",
      "Iteration 15032: with minibatch training loss = 0.426 and accuracy of 0.86\n",
      "Iteration 15033: with minibatch training loss = 0.55 and accuracy of 0.84\n",
      "Iteration 15034: with minibatch training loss = 0.463 and accuracy of 0.83\n",
      "Iteration 15035: with minibatch training loss = 0.376 and accuracy of 0.91\n",
      "Iteration 15036: with minibatch training loss = 0.773 and accuracy of 0.8\n",
      "Iteration 15037: with minibatch training loss = 0.533 and accuracy of 0.81\n",
      "Iteration 15038: with minibatch training loss = 0.845 and accuracy of 0.73\n",
      "Iteration 15039: with minibatch training loss = 0.452 and accuracy of 0.91\n",
      "Iteration 15040: with minibatch training loss = 0.691 and accuracy of 0.78\n",
      "Iteration 15041: with minibatch training loss = 0.631 and accuracy of 0.77\n",
      "Iteration 15042: with minibatch training loss = 0.714 and accuracy of 0.8\n",
      "Iteration 15043: with minibatch training loss = 0.66 and accuracy of 0.8\n",
      "Iteration 15044: with minibatch training loss = 0.776 and accuracy of 0.73\n",
      "Iteration 15045: with minibatch training loss = 0.517 and accuracy of 0.86\n",
      "Iteration 15046: with minibatch training loss = 0.514 and accuracy of 0.83\n",
      "Iteration 15047: with minibatch training loss = 0.69 and accuracy of 0.83\n",
      "Iteration 15048: with minibatch training loss = 0.408 and accuracy of 0.89\n",
      "Iteration 15049: with minibatch training loss = 0.602 and accuracy of 0.8\n",
      "Iteration 15050: with minibatch training loss = 0.522 and accuracy of 0.86\n",
      "Iteration 15051: with minibatch training loss = 0.589 and accuracy of 0.84\n",
      "Iteration 15052: with minibatch training loss = 0.496 and accuracy of 0.86\n",
      "Iteration 15053: with minibatch training loss = 0.501 and accuracy of 0.86\n",
      "Iteration 15054: with minibatch training loss = 0.527 and accuracy of 0.86\n",
      "Iteration 15055: with minibatch training loss = 0.57 and accuracy of 0.86\n",
      "Iteration 15056: with minibatch training loss = 0.826 and accuracy of 0.77\n",
      "Iteration 15057: with minibatch training loss = 0.67 and accuracy of 0.84\n",
      "Iteration 15058: with minibatch training loss = 0.695 and accuracy of 0.78\n",
      "Iteration 15059: with minibatch training loss = 0.889 and accuracy of 0.73\n",
      "Iteration 15060: with minibatch training loss = 0.551 and accuracy of 0.83\n",
      "Iteration 15061: with minibatch training loss = 0.704 and accuracy of 0.81\n",
      "Iteration 15062: with minibatch training loss = 0.636 and accuracy of 0.83\n",
      "Iteration 15063: with minibatch training loss = 0.565 and accuracy of 0.83\n",
      "Iteration 15064: with minibatch training loss = 0.719 and accuracy of 0.81\n",
      "Iteration 15065: with minibatch training loss = 1.02 and accuracy of 0.66\n",
      "Iteration 15066: with minibatch training loss = 0.62 and accuracy of 0.83\n",
      "Iteration 15067: with minibatch training loss = 0.615 and accuracy of 0.84\n",
      "Iteration 15068: with minibatch training loss = 0.601 and accuracy of 0.81\n",
      "Iteration 15069: with minibatch training loss = 0.68 and accuracy of 0.78\n",
      "Iteration 15070: with minibatch training loss = 0.802 and accuracy of 0.77\n",
      "Iteration 15071: with minibatch training loss = 0.752 and accuracy of 0.75\n",
      "Iteration 15072: with minibatch training loss = 0.582 and accuracy of 0.83\n",
      "Iteration 15073: with minibatch training loss = 0.592 and accuracy of 0.8\n",
      "Iteration 15074: with minibatch training loss = 0.793 and accuracy of 0.75\n",
      "Iteration 15075: with minibatch training loss = 0.868 and accuracy of 0.73\n",
      "Iteration 15076: with minibatch training loss = 0.505 and accuracy of 0.83\n",
      "Iteration 15077: with minibatch training loss = 0.814 and accuracy of 0.75\n",
      "Iteration 15078: with minibatch training loss = 0.567 and accuracy of 0.83\n",
      "Iteration 15079: with minibatch training loss = 0.63 and accuracy of 0.81\n",
      "Iteration 15080: with minibatch training loss = 0.887 and accuracy of 0.75\n",
      "Iteration 15081: with minibatch training loss = 0.73 and accuracy of 0.77\n",
      "Iteration 15082: with minibatch training loss = 0.905 and accuracy of 0.72\n",
      "Iteration 15083: with minibatch training loss = 0.626 and accuracy of 0.83\n",
      "Iteration 15084: with minibatch training loss = 0.881 and accuracy of 0.73\n",
      "Iteration 15085: with minibatch training loss = 0.503 and accuracy of 0.86\n",
      "Iteration 15086: with minibatch training loss = 0.798 and accuracy of 0.77\n",
      "Iteration 15087: with minibatch training loss = 0.765 and accuracy of 0.75\n",
      "Iteration 15088: with minibatch training loss = 0.593 and accuracy of 0.83\n",
      "Iteration 15089: with minibatch training loss = 0.514 and accuracy of 0.83\n",
      "Iteration 15090: with minibatch training loss = 0.64 and accuracy of 0.78\n",
      "Iteration 15091: with minibatch training loss = 0.694 and accuracy of 0.8\n",
      "Iteration 15092: with minibatch training loss = 0.705 and accuracy of 0.78\n",
      "Iteration 15093: with minibatch training loss = 0.725 and accuracy of 0.8\n",
      "Iteration 15094: with minibatch training loss = 0.604 and accuracy of 0.83\n",
      "Iteration 15095: with minibatch training loss = 0.569 and accuracy of 0.83\n",
      "Iteration 15096: with minibatch training loss = 0.807 and accuracy of 0.8\n",
      "Iteration 15097: with minibatch training loss = 0.707 and accuracy of 0.75\n",
      "Iteration 15098: with minibatch training loss = 0.621 and accuracy of 0.83\n",
      "Iteration 15099: with minibatch training loss = 0.428 and accuracy of 0.88\n",
      "Iteration 15100: with minibatch training loss = 0.691 and accuracy of 0.77\n",
      "Iteration 15101: with minibatch training loss = 0.581 and accuracy of 0.84\n",
      "Iteration 15102: with minibatch training loss = 0.514 and accuracy of 0.83\n",
      "Iteration 15103: with minibatch training loss = 1.02 and accuracy of 0.75\n",
      "Iteration 15104: with minibatch training loss = 0.45 and accuracy of 0.86\n",
      "Iteration 15105: with minibatch training loss = 1.03 and accuracy of 0.64\n",
      "Iteration 15106: with minibatch training loss = 0.696 and accuracy of 0.8\n",
      "Iteration 15107: with minibatch training loss = 0.496 and accuracy of 0.86\n",
      "Iteration 15108: with minibatch training loss = 0.89 and accuracy of 0.69\n",
      "Iteration 15109: with minibatch training loss = 0.704 and accuracy of 0.75\n",
      "Iteration 15110: with minibatch training loss = 0.721 and accuracy of 0.8\n",
      "Iteration 15111: with minibatch training loss = 0.692 and accuracy of 0.77\n",
      "Iteration 15112: with minibatch training loss = 0.647 and accuracy of 0.81\n",
      "Iteration 15113: with minibatch training loss = 0.688 and accuracy of 0.77\n",
      "Iteration 15114: with minibatch training loss = 0.39 and accuracy of 0.89\n",
      "Iteration 15115: with minibatch training loss = 0.547 and accuracy of 0.86\n",
      "Iteration 15116: with minibatch training loss = 0.734 and accuracy of 0.84\n",
      "Iteration 15117: with minibatch training loss = 0.425 and accuracy of 0.89\n",
      "Iteration 15118: with minibatch training loss = 0.574 and accuracy of 0.81\n",
      "Iteration 15119: with minibatch training loss = 0.555 and accuracy of 0.84\n",
      "Iteration 15120: with minibatch training loss = 0.697 and accuracy of 0.78\n",
      "Iteration 15121: with minibatch training loss = 0.545 and accuracy of 0.81\n",
      "Iteration 15122: with minibatch training loss = 0.881 and accuracy of 0.72\n",
      "Iteration 15123: with minibatch training loss = 0.711 and accuracy of 0.77\n",
      "Iteration 15124: with minibatch training loss = 0.86 and accuracy of 0.7\n",
      "Iteration 15125: with minibatch training loss = 0.61 and accuracy of 0.8\n",
      "Iteration 15126: with minibatch training loss = 0.873 and accuracy of 0.73\n",
      "Iteration 15127: with minibatch training loss = 0.699 and accuracy of 0.78\n",
      "Iteration 15128: with minibatch training loss = 0.783 and accuracy of 0.8\n",
      "Iteration 15129: with minibatch training loss = 0.785 and accuracy of 0.75\n",
      "Iteration 15130: with minibatch training loss = 0.769 and accuracy of 0.75\n",
      "Iteration 15131: with minibatch training loss = 0.658 and accuracy of 0.81\n",
      "Iteration 15132: with minibatch training loss = 0.574 and accuracy of 0.8\n",
      "Iteration 15133: with minibatch training loss = 0.662 and accuracy of 0.8\n",
      "Iteration 15134: with minibatch training loss = 0.52 and accuracy of 0.83\n",
      "Iteration 15135: with minibatch training loss = 0.859 and accuracy of 0.7\n",
      "Iteration 15136: with minibatch training loss = 0.723 and accuracy of 0.77\n",
      "Iteration 15137: with minibatch training loss = 0.455 and accuracy of 0.86\n",
      "Iteration 15138: with minibatch training loss = 0.761 and accuracy of 0.78\n",
      "Iteration 15139: with minibatch training loss = 0.627 and accuracy of 0.81\n",
      "Iteration 15140: with minibatch training loss = 0.542 and accuracy of 0.83\n",
      "Iteration 15141: with minibatch training loss = 0.465 and accuracy of 0.86\n",
      "Iteration 15142: with minibatch training loss = 0.977 and accuracy of 0.7\n",
      "Iteration 15143: with minibatch training loss = 0.719 and accuracy of 0.8\n",
      "Iteration 15144: with minibatch training loss = 0.414 and accuracy of 0.89\n",
      "Iteration 15145: with minibatch training loss = 0.479 and accuracy of 0.86\n",
      "Iteration 15146: with minibatch training loss = 0.863 and accuracy of 0.72\n",
      "Iteration 15147: with minibatch training loss = 0.724 and accuracy of 0.78\n",
      "Iteration 15148: with minibatch training loss = 0.493 and accuracy of 0.86\n",
      "Iteration 15149: with minibatch training loss = 0.658 and accuracy of 0.78\n",
      "Iteration 15150: with minibatch training loss = 0.642 and accuracy of 0.81\n",
      "Iteration 15151: with minibatch training loss = 0.581 and accuracy of 0.81\n",
      "Iteration 15152: with minibatch training loss = 0.546 and accuracy of 0.83\n",
      "Iteration 15153: with minibatch training loss = 0.73 and accuracy of 0.81\n",
      "Iteration 15154: with minibatch training loss = 0.861 and accuracy of 0.72\n",
      "Iteration 15155: with minibatch training loss = 0.503 and accuracy of 0.86\n",
      "Iteration 15156: with minibatch training loss = 0.882 and accuracy of 0.72\n",
      "Iteration 15157: with minibatch training loss = 0.704 and accuracy of 0.78\n",
      "Iteration 15158: with minibatch training loss = 0.43 and accuracy of 0.88\n",
      "Iteration 15159: with minibatch training loss = 0.519 and accuracy of 0.84\n",
      "Iteration 15160: with minibatch training loss = 0.421 and accuracy of 0.88\n",
      "Iteration 15161: with minibatch training loss = 0.495 and accuracy of 0.81\n",
      "Iteration 15162: with minibatch training loss = 0.541 and accuracy of 0.83\n",
      "Iteration 15163: with minibatch training loss = 0.598 and accuracy of 0.83\n",
      "Iteration 15164: with minibatch training loss = 0.695 and accuracy of 0.8\n",
      "Iteration 15165: with minibatch training loss = 0.854 and accuracy of 0.75\n",
      "Iteration 15166: with minibatch training loss = 0.931 and accuracy of 0.73\n",
      "Iteration 15167: with minibatch training loss = 0.537 and accuracy of 0.84\n",
      "Iteration 15168: with minibatch training loss = 0.597 and accuracy of 0.84\n",
      "Iteration 15169: with minibatch training loss = 0.689 and accuracy of 0.83\n",
      "Iteration 15170: with minibatch training loss = 0.567 and accuracy of 0.88\n",
      "Iteration 15171: with minibatch training loss = 0.769 and accuracy of 0.8\n",
      "Iteration 15172: with minibatch training loss = 0.749 and accuracy of 0.81\n",
      "Iteration 15173: with minibatch training loss = 0.753 and accuracy of 0.8\n",
      "Iteration 15174: with minibatch training loss = 0.938 and accuracy of 0.72\n",
      "Iteration 15175: with minibatch training loss = 0.841 and accuracy of 0.72\n",
      "Iteration 15176: with minibatch training loss = 0.703 and accuracy of 0.8\n",
      "Iteration 15177: with minibatch training loss = 0.636 and accuracy of 0.81\n",
      "Iteration 15178: with minibatch training loss = 0.399 and accuracy of 0.91\n",
      "Iteration 15179: with minibatch training loss = 0.751 and accuracy of 0.78\n",
      "Iteration 15180: with minibatch training loss = 0.712 and accuracy of 0.77\n",
      "Iteration 15181: with minibatch training loss = 0.579 and accuracy of 0.84\n",
      "Iteration 15182: with minibatch training loss = 0.675 and accuracy of 0.8\n",
      "Iteration 15183: with minibatch training loss = 0.636 and accuracy of 0.8\n",
      "Iteration 15184: with minibatch training loss = 0.577 and accuracy of 0.8\n",
      "Iteration 15185: with minibatch training loss = 0.793 and accuracy of 0.75\n",
      "Iteration 15186: with minibatch training loss = 0.765 and accuracy of 0.75\n",
      "Iteration 15187: with minibatch training loss = 0.503 and accuracy of 0.88\n",
      "Iteration 15188: with minibatch training loss = 0.561 and accuracy of 0.83\n",
      "Iteration 15189: with minibatch training loss = 0.681 and accuracy of 0.77\n",
      "Iteration 15190: with minibatch training loss = 0.871 and accuracy of 0.72\n",
      "Iteration 15191: with minibatch training loss = 0.614 and accuracy of 0.81\n",
      "Iteration 15192: with minibatch training loss = 0.601 and accuracy of 0.8\n",
      "Iteration 15193: with minibatch training loss = 0.81 and accuracy of 0.77\n",
      "Iteration 15194: with minibatch training loss = 0.627 and accuracy of 0.81\n",
      "Iteration 15195: with minibatch training loss = 0.821 and accuracy of 0.72\n",
      "Iteration 15196: with minibatch training loss = 0.619 and accuracy of 0.81\n",
      "Iteration 15197: with minibatch training loss = 0.685 and accuracy of 0.77\n",
      "Iteration 15198: with minibatch training loss = 0.686 and accuracy of 0.78\n",
      "Iteration 15199: with minibatch training loss = 0.714 and accuracy of 0.78\n",
      "Iteration 15200: with minibatch training loss = 0.579 and accuracy of 0.81\n",
      "Iteration 15201: with minibatch training loss = 0.662 and accuracy of 0.77\n",
      "Iteration 15202: with minibatch training loss = 0.578 and accuracy of 0.86\n",
      "Iteration 15203: with minibatch training loss = 0.867 and accuracy of 0.75\n",
      "Iteration 15204: with minibatch training loss = 0.557 and accuracy of 0.83\n",
      "Iteration 15205: with minibatch training loss = 0.812 and accuracy of 0.77\n",
      "Iteration 15206: with minibatch training loss = 0.836 and accuracy of 0.72\n",
      "Iteration 15207: with minibatch training loss = 0.85 and accuracy of 0.77\n",
      "Iteration 15208: with minibatch training loss = 0.542 and accuracy of 0.83\n",
      "Iteration 15209: with minibatch training loss = 0.428 and accuracy of 0.88\n",
      "Iteration 15210: with minibatch training loss = 1.27 and accuracy of 0.59\n",
      "Iteration 15211: with minibatch training loss = 0.778 and accuracy of 0.78\n",
      "Iteration 15212: with minibatch training loss = 0.624 and accuracy of 0.81\n",
      "Iteration 15213: with minibatch training loss = 0.358 and accuracy of 0.94\n",
      "Iteration 15214: with minibatch training loss = 0.826 and accuracy of 0.77\n",
      "Iteration 15215: with minibatch training loss = 0.957 and accuracy of 0.7\n",
      "Iteration 15216: with minibatch training loss = 0.645 and accuracy of 0.8\n",
      "Iteration 15217: with minibatch training loss = 0.41 and accuracy of 0.89\n",
      "Iteration 15218: with minibatch training loss = 0.624 and accuracy of 0.81\n",
      "Iteration 15219: with minibatch training loss = 0.846 and accuracy of 0.77\n",
      "Iteration 15220: with minibatch training loss = 0.765 and accuracy of 0.75\n",
      "Iteration 15221: with minibatch training loss = 0.824 and accuracy of 0.72\n",
      "Iteration 15222: with minibatch training loss = 0.656 and accuracy of 0.78\n",
      "Iteration 15223: with minibatch training loss = 0.528 and accuracy of 0.84\n",
      "Iteration 15224: with minibatch training loss = 0.391 and accuracy of 0.89\n",
      "Iteration 15225: with minibatch training loss = 0.465 and accuracy of 0.84\n",
      "Iteration 15226: with minibatch training loss = 0.75 and accuracy of 0.78\n",
      "Iteration 15227: with minibatch training loss = 0.745 and accuracy of 0.77\n",
      "Iteration 15228: with minibatch training loss = 0.628 and accuracy of 0.78\n",
      "Iteration 15229: with minibatch training loss = 0.692 and accuracy of 0.81\n",
      "Iteration 15230: with minibatch training loss = 0.63 and accuracy of 0.86\n",
      "Iteration 15231: with minibatch training loss = 0.849 and accuracy of 0.73\n",
      "Iteration 15232: with minibatch training loss = 0.479 and accuracy of 0.84\n",
      "Iteration 15233: with minibatch training loss = 0.909 and accuracy of 0.72\n",
      "Iteration 15234: with minibatch training loss = 0.43 and accuracy of 0.89\n",
      "Iteration 15235: with minibatch training loss = 0.836 and accuracy of 0.75\n",
      "Iteration 15236: with minibatch training loss = 0.475 and accuracy of 0.88\n",
      "Iteration 15237: with minibatch training loss = 1.02 and accuracy of 0.69\n",
      "Iteration 15238: with minibatch training loss = 0.626 and accuracy of 0.81\n",
      "Iteration 15239: with minibatch training loss = 0.39 and accuracy of 0.91\n",
      "Iteration 15240: with minibatch training loss = 0.734 and accuracy of 0.77\n",
      "Iteration 15241: with minibatch training loss = 0.937 and accuracy of 0.7\n",
      "Iteration 15242: with minibatch training loss = 0.768 and accuracy of 0.73\n",
      "Iteration 15243: with minibatch training loss = 1.09 and accuracy of 0.66\n",
      "Iteration 15244: with minibatch training loss = 0.497 and accuracy of 0.84\n",
      "Iteration 15245: with minibatch training loss = 0.613 and accuracy of 0.81\n",
      "Iteration 15246: with minibatch training loss = 0.887 and accuracy of 0.72\n",
      "Iteration 15247: with minibatch training loss = 0.818 and accuracy of 0.75\n",
      "Iteration 15248: with minibatch training loss = 0.791 and accuracy of 0.75\n",
      "Iteration 15249: with minibatch training loss = 0.565 and accuracy of 0.83\n",
      "Iteration 15250: with minibatch training loss = 0.593 and accuracy of 0.81\n",
      "Iteration 15251: with minibatch training loss = 0.46 and accuracy of 0.88\n",
      "Iteration 15252: with minibatch training loss = 0.661 and accuracy of 0.81\n",
      "Iteration 15253: with minibatch training loss = 0.755 and accuracy of 0.78\n",
      "Iteration 15254: with minibatch training loss = 0.478 and accuracy of 0.86\n",
      "Iteration 15255: with minibatch training loss = 0.694 and accuracy of 0.8\n",
      "Iteration 15256: with minibatch training loss = 0.798 and accuracy of 0.73\n",
      "Iteration 15257: with minibatch training loss = 0.747 and accuracy of 0.77\n",
      "Iteration 15258: with minibatch training loss = 0.459 and accuracy of 0.84\n",
      "Iteration 15259: with minibatch training loss = 0.408 and accuracy of 0.91\n",
      "Iteration 15260: with minibatch training loss = 0.575 and accuracy of 0.81\n",
      "Iteration 15261: with minibatch training loss = 0.512 and accuracy of 0.84\n",
      "Iteration 15262: with minibatch training loss = 0.795 and accuracy of 0.78\n",
      "Iteration 15263: with minibatch training loss = 0.779 and accuracy of 0.77\n",
      "Iteration 15264: with minibatch training loss = 0.642 and accuracy of 0.8\n",
      "Iteration 15265: with minibatch training loss = 0.461 and accuracy of 0.88\n",
      "Iteration 15266: with minibatch training loss = 0.661 and accuracy of 0.78\n",
      "Validation loss: 0.27177006\n",
      "Epoch 11, Overall loss = 0.678 and accuracy of 0.794\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13c717e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15268: with minibatch training loss = 0.591 and accuracy of 0.83\n",
      "Iteration 15269: with minibatch training loss = 0.577 and accuracy of 0.84\n",
      "Iteration 15270: with minibatch training loss = 0.855 and accuracy of 0.73\n",
      "Iteration 15271: with minibatch training loss = 0.496 and accuracy of 0.88\n",
      "Iteration 15272: with minibatch training loss = 0.692 and accuracy of 0.8\n",
      "Iteration 15273: with minibatch training loss = 0.629 and accuracy of 0.81\n",
      "Iteration 15274: with minibatch training loss = 0.536 and accuracy of 0.83\n",
      "Iteration 15275: with minibatch training loss = 0.572 and accuracy of 0.84\n",
      "Iteration 15276: with minibatch training loss = 0.837 and accuracy of 0.73\n",
      "Iteration 15277: with minibatch training loss = 0.501 and accuracy of 0.86\n",
      "Iteration 15278: with minibatch training loss = 0.754 and accuracy of 0.77\n",
      "Iteration 15279: with minibatch training loss = 0.875 and accuracy of 0.72\n",
      "Iteration 15280: with minibatch training loss = 0.735 and accuracy of 0.77\n",
      "Iteration 15281: with minibatch training loss = 0.778 and accuracy of 0.78\n",
      "Iteration 15282: with minibatch training loss = 0.537 and accuracy of 0.84\n",
      "Iteration 15283: with minibatch training loss = 0.502 and accuracy of 0.86\n",
      "Iteration 15284: with minibatch training loss = 0.838 and accuracy of 0.7\n",
      "Iteration 15285: with minibatch training loss = 0.624 and accuracy of 0.8\n",
      "Iteration 15286: with minibatch training loss = 0.816 and accuracy of 0.75\n",
      "Iteration 15287: with minibatch training loss = 0.769 and accuracy of 0.73\n",
      "Iteration 15288: with minibatch training loss = 0.411 and accuracy of 0.89\n",
      "Iteration 15289: with minibatch training loss = 0.689 and accuracy of 0.8\n",
      "Iteration 15290: with minibatch training loss = 0.714 and accuracy of 0.78\n",
      "Iteration 15291: with minibatch training loss = 0.524 and accuracy of 0.83\n",
      "Iteration 15292: with minibatch training loss = 0.8 and accuracy of 0.72\n",
      "Iteration 15293: with minibatch training loss = 0.579 and accuracy of 0.86\n",
      "Iteration 15294: with minibatch training loss = 0.387 and accuracy of 0.89\n",
      "Iteration 15295: with minibatch training loss = 0.702 and accuracy of 0.78\n",
      "Iteration 15296: with minibatch training loss = 0.497 and accuracy of 0.86\n",
      "Iteration 15297: with minibatch training loss = 0.844 and accuracy of 0.73\n",
      "Iteration 15298: with minibatch training loss = 0.951 and accuracy of 0.75\n",
      "Iteration 15299: with minibatch training loss = 0.716 and accuracy of 0.81\n",
      "Iteration 15300: with minibatch training loss = 0.58 and accuracy of 0.86\n",
      "Iteration 15301: with minibatch training loss = 0.625 and accuracy of 0.8\n",
      "Iteration 15302: with minibatch training loss = 0.751 and accuracy of 0.77\n",
      "Iteration 15303: with minibatch training loss = 0.452 and accuracy of 0.86\n",
      "Iteration 15304: with minibatch training loss = 0.759 and accuracy of 0.73\n",
      "Iteration 15305: with minibatch training loss = 0.931 and accuracy of 0.69\n",
      "Iteration 15306: with minibatch training loss = 0.655 and accuracy of 0.8\n",
      "Iteration 15307: with minibatch training loss = 0.745 and accuracy of 0.75\n",
      "Iteration 15308: with minibatch training loss = 0.724 and accuracy of 0.8\n",
      "Iteration 15309: with minibatch training loss = 0.604 and accuracy of 0.83\n",
      "Iteration 15310: with minibatch training loss = 0.759 and accuracy of 0.78\n",
      "Iteration 15311: with minibatch training loss = 0.489 and accuracy of 0.88\n",
      "Iteration 15312: with minibatch training loss = 0.59 and accuracy of 0.83\n",
      "Iteration 15313: with minibatch training loss = 0.34 and accuracy of 0.91\n",
      "Iteration 15314: with minibatch training loss = 0.611 and accuracy of 0.83\n",
      "Iteration 15315: with minibatch training loss = 0.693 and accuracy of 0.8\n",
      "Iteration 15316: with minibatch training loss = 0.937 and accuracy of 0.7\n",
      "Iteration 15317: with minibatch training loss = 0.662 and accuracy of 0.81\n",
      "Iteration 15318: with minibatch training loss = 0.617 and accuracy of 0.81\n",
      "Iteration 15319: with minibatch training loss = 0.61 and accuracy of 0.8\n",
      "Iteration 15320: with minibatch training loss = 0.418 and accuracy of 0.86\n",
      "Iteration 15321: with minibatch training loss = 0.406 and accuracy of 0.89\n",
      "Iteration 15322: with minibatch training loss = 0.532 and accuracy of 0.84\n",
      "Iteration 15323: with minibatch training loss = 0.777 and accuracy of 0.77\n",
      "Iteration 15324: with minibatch training loss = 0.532 and accuracy of 0.84\n",
      "Iteration 15325: with minibatch training loss = 0.723 and accuracy of 0.8\n",
      "Iteration 15326: with minibatch training loss = 0.578 and accuracy of 0.81\n",
      "Iteration 15327: with minibatch training loss = 0.803 and accuracy of 0.78\n",
      "Iteration 15328: with minibatch training loss = 0.707 and accuracy of 0.8\n",
      "Iteration 15329: with minibatch training loss = 0.757 and accuracy of 0.77\n",
      "Iteration 15330: with minibatch training loss = 1.11 and accuracy of 0.67\n",
      "Iteration 15331: with minibatch training loss = 0.482 and accuracy of 0.88\n",
      "Iteration 15332: with minibatch training loss = 0.611 and accuracy of 0.81\n",
      "Iteration 15333: with minibatch training loss = 0.823 and accuracy of 0.77\n",
      "Iteration 15334: with minibatch training loss = 0.773 and accuracy of 0.77\n",
      "Iteration 15335: with minibatch training loss = 0.712 and accuracy of 0.75\n",
      "Iteration 15336: with minibatch training loss = 0.639 and accuracy of 0.83\n",
      "Iteration 15337: with minibatch training loss = 0.763 and accuracy of 0.73\n",
      "Iteration 15338: with minibatch training loss = 0.642 and accuracy of 0.8\n",
      "Iteration 15339: with minibatch training loss = 0.566 and accuracy of 0.84\n",
      "Iteration 15340: with minibatch training loss = 0.928 and accuracy of 0.73\n",
      "Iteration 15341: with minibatch training loss = 0.567 and accuracy of 0.83\n",
      "Iteration 15342: with minibatch training loss = 0.551 and accuracy of 0.83\n",
      "Iteration 15343: with minibatch training loss = 0.558 and accuracy of 0.88\n",
      "Iteration 15344: with minibatch training loss = 0.952 and accuracy of 0.7\n",
      "Iteration 15345: with minibatch training loss = 0.697 and accuracy of 0.77\n",
      "Iteration 15346: with minibatch training loss = 0.696 and accuracy of 0.78\n",
      "Iteration 15347: with minibatch training loss = 0.621 and accuracy of 0.83\n",
      "Iteration 15348: with minibatch training loss = 0.6 and accuracy of 0.8\n",
      "Iteration 15349: with minibatch training loss = 0.539 and accuracy of 0.86\n",
      "Iteration 15350: with minibatch training loss = 0.847 and accuracy of 0.77\n",
      "Iteration 15351: with minibatch training loss = 0.45 and accuracy of 0.88\n",
      "Iteration 15352: with minibatch training loss = 0.743 and accuracy of 0.78\n",
      "Iteration 15353: with minibatch training loss = 0.524 and accuracy of 0.84\n",
      "Iteration 15354: with minibatch training loss = 0.631 and accuracy of 0.81\n",
      "Iteration 15355: with minibatch training loss = 0.498 and accuracy of 0.84\n",
      "Iteration 15356: with minibatch training loss = 0.385 and accuracy of 0.88\n",
      "Iteration 15357: with minibatch training loss = 0.533 and accuracy of 0.84\n",
      "Iteration 15358: with minibatch training loss = 0.876 and accuracy of 0.73\n",
      "Iteration 15359: with minibatch training loss = 0.665 and accuracy of 0.78\n",
      "Iteration 15360: with minibatch training loss = 0.583 and accuracy of 0.81\n",
      "Iteration 15361: with minibatch training loss = 0.456 and accuracy of 0.86\n",
      "Iteration 15362: with minibatch training loss = 0.69 and accuracy of 0.77\n",
      "Iteration 15363: with minibatch training loss = 0.834 and accuracy of 0.75\n",
      "Iteration 15364: with minibatch training loss = 0.449 and accuracy of 0.88\n",
      "Iteration 15365: with minibatch training loss = 0.522 and accuracy of 0.88\n",
      "Iteration 15366: with minibatch training loss = 0.671 and accuracy of 0.81\n",
      "Iteration 15367: with minibatch training loss = 0.406 and accuracy of 0.88\n",
      "Iteration 15368: with minibatch training loss = 1.05 and accuracy of 0.7\n",
      "Iteration 15369: with minibatch training loss = 0.519 and accuracy of 0.86\n",
      "Iteration 15370: with minibatch training loss = 0.487 and accuracy of 0.88\n",
      "Iteration 15371: with minibatch training loss = 0.745 and accuracy of 0.73\n",
      "Iteration 15372: with minibatch training loss = 0.589 and accuracy of 0.81\n",
      "Iteration 15373: with minibatch training loss = 0.376 and accuracy of 0.88\n",
      "Iteration 15374: with minibatch training loss = 0.688 and accuracy of 0.8\n",
      "Iteration 15375: with minibatch training loss = 0.617 and accuracy of 0.83\n",
      "Iteration 15376: with minibatch training loss = 0.51 and accuracy of 0.83\n",
      "Iteration 15377: with minibatch training loss = 0.655 and accuracy of 0.8\n",
      "Iteration 15378: with minibatch training loss = 0.622 and accuracy of 0.78\n",
      "Iteration 15379: with minibatch training loss = 0.933 and accuracy of 0.75\n",
      "Iteration 15380: with minibatch training loss = 0.815 and accuracy of 0.78\n",
      "Iteration 15381: with minibatch training loss = 0.467 and accuracy of 0.84\n",
      "Iteration 15382: with minibatch training loss = 0.514 and accuracy of 0.88\n",
      "Iteration 15383: with minibatch training loss = 0.804 and accuracy of 0.75\n",
      "Iteration 15384: with minibatch training loss = 0.847 and accuracy of 0.73\n",
      "Iteration 15385: with minibatch training loss = 0.577 and accuracy of 0.81\n",
      "Iteration 15386: with minibatch training loss = 0.699 and accuracy of 0.77\n",
      "Iteration 15387: with minibatch training loss = 0.723 and accuracy of 0.78\n",
      "Iteration 15388: with minibatch training loss = 0.574 and accuracy of 0.83\n",
      "Iteration 15389: with minibatch training loss = 0.795 and accuracy of 0.75\n",
      "Iteration 15390: with minibatch training loss = 0.566 and accuracy of 0.83\n",
      "Iteration 15391: with minibatch training loss = 0.509 and accuracy of 0.84\n",
      "Iteration 15392: with minibatch training loss = 0.615 and accuracy of 0.81\n",
      "Iteration 15393: with minibatch training loss = 0.422 and accuracy of 0.91\n",
      "Iteration 15394: with minibatch training loss = 0.604 and accuracy of 0.84\n",
      "Iteration 15395: with minibatch training loss = 0.82 and accuracy of 0.73\n",
      "Iteration 15396: with minibatch training loss = 0.491 and accuracy of 0.86\n",
      "Iteration 15397: with minibatch training loss = 0.721 and accuracy of 0.8\n",
      "Iteration 15398: with minibatch training loss = 0.551 and accuracy of 0.83\n",
      "Iteration 15399: with minibatch training loss = 0.52 and accuracy of 0.86\n",
      "Iteration 15400: with minibatch training loss = 0.568 and accuracy of 0.81\n",
      "Iteration 15401: with minibatch training loss = 0.805 and accuracy of 0.8\n",
      "Iteration 15402: with minibatch training loss = 0.561 and accuracy of 0.84\n",
      "Iteration 15403: with minibatch training loss = 0.637 and accuracy of 0.77\n",
      "Iteration 15404: with minibatch training loss = 0.608 and accuracy of 0.81\n",
      "Iteration 15405: with minibatch training loss = 0.512 and accuracy of 0.88\n",
      "Iteration 15406: with minibatch training loss = 0.8 and accuracy of 0.77\n",
      "Iteration 15407: with minibatch training loss = 0.697 and accuracy of 0.78\n",
      "Iteration 15408: with minibatch training loss = 0.435 and accuracy of 0.86\n",
      "Iteration 15409: with minibatch training loss = 0.533 and accuracy of 0.86\n",
      "Iteration 15410: with minibatch training loss = 0.668 and accuracy of 0.81\n",
      "Iteration 15411: with minibatch training loss = 0.802 and accuracy of 0.73\n",
      "Iteration 15412: with minibatch training loss = 0.529 and accuracy of 0.84\n",
      "Iteration 15413: with minibatch training loss = 0.495 and accuracy of 0.84\n",
      "Iteration 15414: with minibatch training loss = 0.574 and accuracy of 0.8\n",
      "Iteration 15415: with minibatch training loss = 0.636 and accuracy of 0.8\n",
      "Iteration 15416: with minibatch training loss = 0.611 and accuracy of 0.8\n",
      "Iteration 15417: with minibatch training loss = 0.385 and accuracy of 0.91\n",
      "Iteration 15418: with minibatch training loss = 0.871 and accuracy of 0.7\n",
      "Iteration 15419: with minibatch training loss = 0.565 and accuracy of 0.84\n",
      "Iteration 15420: with minibatch training loss = 0.834 and accuracy of 0.75\n",
      "Iteration 15421: with minibatch training loss = 0.591 and accuracy of 0.84\n",
      "Iteration 15422: with minibatch training loss = 0.734 and accuracy of 0.75\n",
      "Iteration 15423: with minibatch training loss = 0.549 and accuracy of 0.84\n",
      "Iteration 15424: with minibatch training loss = 0.664 and accuracy of 0.78\n",
      "Iteration 15425: with minibatch training loss = 0.719 and accuracy of 0.77\n",
      "Iteration 15426: with minibatch training loss = 0.647 and accuracy of 0.81\n",
      "Iteration 15427: with minibatch training loss = 0.305 and accuracy of 0.91\n",
      "Iteration 15428: with minibatch training loss = 0.506 and accuracy of 0.84\n",
      "Iteration 15429: with minibatch training loss = 0.817 and accuracy of 0.75\n",
      "Iteration 15430: with minibatch training loss = 0.702 and accuracy of 0.8\n",
      "Iteration 15431: with minibatch training loss = 0.629 and accuracy of 0.78\n",
      "Iteration 15432: with minibatch training loss = 0.543 and accuracy of 0.83\n",
      "Iteration 15433: with minibatch training loss = 0.912 and accuracy of 0.7\n",
      "Iteration 15434: with minibatch training loss = 0.454 and accuracy of 0.86\n",
      "Iteration 15435: with minibatch training loss = 0.716 and accuracy of 0.78\n",
      "Iteration 15436: with minibatch training loss = 0.733 and accuracy of 0.77\n",
      "Iteration 15437: with minibatch training loss = 0.74 and accuracy of 0.77\n",
      "Iteration 15438: with minibatch training loss = 0.762 and accuracy of 0.8\n",
      "Iteration 15439: with minibatch training loss = 0.56 and accuracy of 0.83\n",
      "Iteration 15440: with minibatch training loss = 0.454 and accuracy of 0.88\n",
      "Iteration 15441: with minibatch training loss = 0.616 and accuracy of 0.81\n",
      "Iteration 15442: with minibatch training loss = 0.681 and accuracy of 0.8\n",
      "Iteration 15443: with minibatch training loss = 0.612 and accuracy of 0.81\n",
      "Iteration 15444: with minibatch training loss = 0.574 and accuracy of 0.84\n",
      "Iteration 15445: with minibatch training loss = 0.646 and accuracy of 0.83\n",
      "Iteration 15446: with minibatch training loss = 0.739 and accuracy of 0.8\n",
      "Iteration 15447: with minibatch training loss = 0.706 and accuracy of 0.83\n",
      "Iteration 15448: with minibatch training loss = 0.66 and accuracy of 0.78\n",
      "Iteration 15449: with minibatch training loss = 0.808 and accuracy of 0.78\n",
      "Iteration 15450: with minibatch training loss = 0.733 and accuracy of 0.78\n",
      "Iteration 15451: with minibatch training loss = 0.637 and accuracy of 0.81\n",
      "Iteration 15452: with minibatch training loss = 0.495 and accuracy of 0.86\n",
      "Iteration 15453: with minibatch training loss = 0.875 and accuracy of 0.75\n",
      "Iteration 15454: with minibatch training loss = 0.771 and accuracy of 0.77\n",
      "Iteration 15455: with minibatch training loss = 0.6 and accuracy of 0.81\n",
      "Iteration 15456: with minibatch training loss = 0.636 and accuracy of 0.81\n",
      "Iteration 15457: with minibatch training loss = 0.644 and accuracy of 0.8\n",
      "Iteration 15458: with minibatch training loss = 0.455 and accuracy of 0.89\n",
      "Iteration 15459: with minibatch training loss = 0.465 and accuracy of 0.86\n",
      "Iteration 15460: with minibatch training loss = 0.696 and accuracy of 0.77\n",
      "Iteration 15461: with minibatch training loss = 0.66 and accuracy of 0.8\n",
      "Iteration 15462: with minibatch training loss = 0.521 and accuracy of 0.83\n",
      "Iteration 15463: with minibatch training loss = 0.83 and accuracy of 0.73\n",
      "Iteration 15464: with minibatch training loss = 0.741 and accuracy of 0.77\n",
      "Iteration 15465: with minibatch training loss = 0.252 and accuracy of 0.92\n",
      "Iteration 15466: with minibatch training loss = 0.665 and accuracy of 0.81\n",
      "Iteration 15467: with minibatch training loss = 0.665 and accuracy of 0.8\n",
      "Iteration 15468: with minibatch training loss = 0.906 and accuracy of 0.72\n",
      "Iteration 15469: with minibatch training loss = 0.657 and accuracy of 0.77\n",
      "Iteration 15470: with minibatch training loss = 0.72 and accuracy of 0.78\n",
      "Iteration 15471: with minibatch training loss = 0.796 and accuracy of 0.75\n",
      "Iteration 15472: with minibatch training loss = 0.61 and accuracy of 0.81\n",
      "Iteration 15473: with minibatch training loss = 0.871 and accuracy of 0.72\n",
      "Iteration 15474: with minibatch training loss = 0.762 and accuracy of 0.8\n",
      "Iteration 15475: with minibatch training loss = 0.642 and accuracy of 0.81\n",
      "Iteration 15476: with minibatch training loss = 0.423 and accuracy of 0.89\n",
      "Iteration 15477: with minibatch training loss = 0.754 and accuracy of 0.8\n",
      "Iteration 15478: with minibatch training loss = 0.661 and accuracy of 0.8\n",
      "Iteration 15479: with minibatch training loss = 0.624 and accuracy of 0.8\n",
      "Iteration 15480: with minibatch training loss = 0.661 and accuracy of 0.81\n",
      "Iteration 15481: with minibatch training loss = 0.817 and accuracy of 0.72\n",
      "Iteration 15482: with minibatch training loss = 0.517 and accuracy of 0.84\n",
      "Iteration 15483: with minibatch training loss = 0.557 and accuracy of 0.83\n",
      "Iteration 15484: with minibatch training loss = 0.482 and accuracy of 0.86\n",
      "Iteration 15485: with minibatch training loss = 0.631 and accuracy of 0.83\n",
      "Iteration 15486: with minibatch training loss = 0.872 and accuracy of 0.73\n",
      "Iteration 15487: with minibatch training loss = 0.841 and accuracy of 0.8\n",
      "Iteration 15488: with minibatch training loss = 0.785 and accuracy of 0.78\n",
      "Iteration 15489: with minibatch training loss = 0.777 and accuracy of 0.77\n",
      "Iteration 15490: with minibatch training loss = 0.765 and accuracy of 0.77\n",
      "Iteration 15491: with minibatch training loss = 0.853 and accuracy of 0.73\n",
      "Iteration 15492: with minibatch training loss = 0.705 and accuracy of 0.77\n",
      "Iteration 15493: with minibatch training loss = 0.627 and accuracy of 0.84\n",
      "Iteration 15494: with minibatch training loss = 0.623 and accuracy of 0.81\n",
      "Iteration 15495: with minibatch training loss = 0.591 and accuracy of 0.86\n",
      "Iteration 15496: with minibatch training loss = 0.848 and accuracy of 0.73\n",
      "Iteration 15497: with minibatch training loss = 0.763 and accuracy of 0.8\n",
      "Iteration 15498: with minibatch training loss = 0.68 and accuracy of 0.78\n",
      "Iteration 15499: with minibatch training loss = 0.913 and accuracy of 0.73\n",
      "Iteration 15500: with minibatch training loss = 1.07 and accuracy of 0.64\n",
      "Iteration 15501: with minibatch training loss = 0.933 and accuracy of 0.7\n",
      "Iteration 15502: with minibatch training loss = 0.734 and accuracy of 0.8\n",
      "Iteration 15503: with minibatch training loss = 0.752 and accuracy of 0.75\n",
      "Iteration 15504: with minibatch training loss = 0.814 and accuracy of 0.69\n",
      "Iteration 15505: with minibatch training loss = 0.803 and accuracy of 0.73\n",
      "Iteration 15506: with minibatch training loss = 0.499 and accuracy of 0.83\n",
      "Iteration 15507: with minibatch training loss = 0.679 and accuracy of 0.78\n",
      "Iteration 15508: with minibatch training loss = 0.388 and accuracy of 0.91\n",
      "Iteration 15509: with minibatch training loss = 0.627 and accuracy of 0.8\n",
      "Iteration 15510: with minibatch training loss = 0.831 and accuracy of 0.75\n",
      "Iteration 15511: with minibatch training loss = 0.487 and accuracy of 0.86\n",
      "Iteration 15512: with minibatch training loss = 0.714 and accuracy of 0.78\n",
      "Iteration 15513: with minibatch training loss = 0.793 and accuracy of 0.75\n",
      "Iteration 15514: with minibatch training loss = 0.655 and accuracy of 0.78\n",
      "Iteration 15515: with minibatch training loss = 0.631 and accuracy of 0.84\n",
      "Iteration 15516: with minibatch training loss = 0.603 and accuracy of 0.81\n",
      "Iteration 15517: with minibatch training loss = 0.665 and accuracy of 0.83\n",
      "Iteration 15518: with minibatch training loss = 0.535 and accuracy of 0.83\n",
      "Iteration 15519: with minibatch training loss = 0.548 and accuracy of 0.81\n",
      "Iteration 15520: with minibatch training loss = 0.48 and accuracy of 0.84\n",
      "Iteration 15521: with minibatch training loss = 0.48 and accuracy of 0.88\n",
      "Iteration 15522: with minibatch training loss = 0.494 and accuracy of 0.83\n",
      "Iteration 15523: with minibatch training loss = 0.459 and accuracy of 0.84\n",
      "Iteration 15524: with minibatch training loss = 0.579 and accuracy of 0.81\n",
      "Iteration 15525: with minibatch training loss = 0.521 and accuracy of 0.83\n",
      "Iteration 15526: with minibatch training loss = 0.69 and accuracy of 0.78\n",
      "Iteration 15527: with minibatch training loss = 0.517 and accuracy of 0.86\n",
      "Iteration 15528: with minibatch training loss = 0.599 and accuracy of 0.81\n",
      "Iteration 15529: with minibatch training loss = 0.738 and accuracy of 0.8\n",
      "Iteration 15530: with minibatch training loss = 0.506 and accuracy of 0.81\n",
      "Iteration 15531: with minibatch training loss = 0.584 and accuracy of 0.81\n",
      "Iteration 15532: with minibatch training loss = 0.742 and accuracy of 0.78\n",
      "Iteration 15533: with minibatch training loss = 0.774 and accuracy of 0.73\n",
      "Iteration 15534: with minibatch training loss = 0.238 and accuracy of 0.94\n",
      "Iteration 15535: with minibatch training loss = 0.519 and accuracy of 0.81\n",
      "Iteration 15536: with minibatch training loss = 0.641 and accuracy of 0.8\n",
      "Iteration 15537: with minibatch training loss = 0.542 and accuracy of 0.86\n",
      "Iteration 15538: with minibatch training loss = 0.637 and accuracy of 0.84\n",
      "Iteration 15539: with minibatch training loss = 0.488 and accuracy of 0.83\n",
      "Iteration 15540: with minibatch training loss = 0.793 and accuracy of 0.73\n",
      "Iteration 15541: with minibatch training loss = 0.586 and accuracy of 0.84\n",
      "Iteration 15542: with minibatch training loss = 0.724 and accuracy of 0.75\n",
      "Iteration 15543: with minibatch training loss = 0.475 and accuracy of 0.88\n",
      "Iteration 15544: with minibatch training loss = 0.73 and accuracy of 0.78\n",
      "Iteration 15545: with minibatch training loss = 0.853 and accuracy of 0.72\n",
      "Iteration 15546: with minibatch training loss = 0.667 and accuracy of 0.78\n",
      "Iteration 15547: with minibatch training loss = 0.92 and accuracy of 0.73\n",
      "Iteration 15548: with minibatch training loss = 0.678 and accuracy of 0.81\n",
      "Iteration 15549: with minibatch training loss = 0.698 and accuracy of 0.8\n",
      "Iteration 15550: with minibatch training loss = 0.845 and accuracy of 0.72\n",
      "Iteration 15551: with minibatch training loss = 0.603 and accuracy of 0.81\n",
      "Iteration 15552: with minibatch training loss = 0.482 and accuracy of 0.83\n",
      "Iteration 15553: with minibatch training loss = 0.597 and accuracy of 0.84\n",
      "Iteration 15554: with minibatch training loss = 0.623 and accuracy of 0.81\n",
      "Iteration 15555: with minibatch training loss = 0.605 and accuracy of 0.86\n",
      "Iteration 15556: with minibatch training loss = 0.749 and accuracy of 0.77\n",
      "Iteration 15557: with minibatch training loss = 0.643 and accuracy of 0.81\n",
      "Iteration 15558: with minibatch training loss = 0.6 and accuracy of 0.8\n",
      "Iteration 15559: with minibatch training loss = 0.808 and accuracy of 0.77\n",
      "Iteration 15560: with minibatch training loss = 1.07 and accuracy of 0.66\n",
      "Iteration 15561: with minibatch training loss = 0.777 and accuracy of 0.78\n",
      "Iteration 15562: with minibatch training loss = 0.35 and accuracy of 0.89\n",
      "Iteration 15563: with minibatch training loss = 0.711 and accuracy of 0.8\n",
      "Iteration 15564: with minibatch training loss = 0.633 and accuracy of 0.83\n",
      "Iteration 15565: with minibatch training loss = 0.484 and accuracy of 0.88\n",
      "Iteration 15566: with minibatch training loss = 0.83 and accuracy of 0.73\n",
      "Iteration 15567: with minibatch training loss = 0.546 and accuracy of 0.84\n",
      "Iteration 15568: with minibatch training loss = 0.666 and accuracy of 0.8\n",
      "Iteration 15569: with minibatch training loss = 0.94 and accuracy of 0.69\n",
      "Iteration 15570: with minibatch training loss = 0.607 and accuracy of 0.78\n",
      "Iteration 15571: with minibatch training loss = 0.658 and accuracy of 0.8\n",
      "Iteration 15572: with minibatch training loss = 0.773 and accuracy of 0.77\n",
      "Iteration 15573: with minibatch training loss = 0.397 and accuracy of 0.88\n",
      "Iteration 15574: with minibatch training loss = 0.685 and accuracy of 0.78\n",
      "Iteration 15575: with minibatch training loss = 0.505 and accuracy of 0.84\n",
      "Iteration 15576: with minibatch training loss = 0.724 and accuracy of 0.8\n",
      "Iteration 15577: with minibatch training loss = 0.481 and accuracy of 0.84\n",
      "Iteration 15578: with minibatch training loss = 0.77 and accuracy of 0.77\n",
      "Iteration 15579: with minibatch training loss = 0.485 and accuracy of 0.88\n",
      "Iteration 15580: with minibatch training loss = 0.457 and accuracy of 0.86\n",
      "Iteration 15581: with minibatch training loss = 0.78 and accuracy of 0.78\n",
      "Iteration 15582: with minibatch training loss = 0.732 and accuracy of 0.78\n",
      "Iteration 15583: with minibatch training loss = 0.548 and accuracy of 0.84\n",
      "Iteration 15584: with minibatch training loss = 0.821 and accuracy of 0.73\n",
      "Iteration 15585: with minibatch training loss = 0.83 and accuracy of 0.77\n",
      "Iteration 15586: with minibatch training loss = 0.836 and accuracy of 0.75\n",
      "Iteration 15587: with minibatch training loss = 0.733 and accuracy of 0.77\n",
      "Iteration 15588: with minibatch training loss = 0.596 and accuracy of 0.83\n",
      "Iteration 15589: with minibatch training loss = 0.72 and accuracy of 0.8\n",
      "Iteration 15590: with minibatch training loss = 0.761 and accuracy of 0.77\n",
      "Iteration 15591: with minibatch training loss = 0.64 and accuracy of 0.81\n",
      "Iteration 15592: with minibatch training loss = 0.555 and accuracy of 0.84\n",
      "Iteration 15593: with minibatch training loss = 0.349 and accuracy of 0.92\n",
      "Iteration 15594: with minibatch training loss = 0.532 and accuracy of 0.84\n",
      "Iteration 15595: with minibatch training loss = 0.654 and accuracy of 0.8\n",
      "Iteration 15596: with minibatch training loss = 0.61 and accuracy of 0.83\n",
      "Iteration 15597: with minibatch training loss = 0.465 and accuracy of 0.84\n",
      "Iteration 15598: with minibatch training loss = 0.723 and accuracy of 0.77\n",
      "Iteration 15599: with minibatch training loss = 0.449 and accuracy of 0.86\n",
      "Iteration 15600: with minibatch training loss = 0.585 and accuracy of 0.81\n",
      "Iteration 15601: with minibatch training loss = 0.464 and accuracy of 0.89\n",
      "Iteration 15602: with minibatch training loss = 0.873 and accuracy of 0.73\n",
      "Iteration 15603: with minibatch training loss = 0.598 and accuracy of 0.81\n",
      "Iteration 15604: with minibatch training loss = 0.703 and accuracy of 0.8\n",
      "Iteration 15605: with minibatch training loss = 0.518 and accuracy of 0.88\n",
      "Iteration 15606: with minibatch training loss = 0.771 and accuracy of 0.72\n",
      "Iteration 15607: with minibatch training loss = 0.923 and accuracy of 0.72\n",
      "Iteration 15608: with minibatch training loss = 1.05 and accuracy of 0.67\n",
      "Iteration 15609: with minibatch training loss = 0.648 and accuracy of 0.8\n",
      "Iteration 15610: with minibatch training loss = 0.726 and accuracy of 0.78\n",
      "Iteration 15611: with minibatch training loss = 0.5 and accuracy of 0.84\n",
      "Iteration 15612: with minibatch training loss = 0.679 and accuracy of 0.81\n",
      "Iteration 15613: with minibatch training loss = 0.584 and accuracy of 0.81\n",
      "Iteration 15614: with minibatch training loss = 0.859 and accuracy of 0.75\n",
      "Iteration 15615: with minibatch training loss = 0.89 and accuracy of 0.7\n",
      "Iteration 15616: with minibatch training loss = 0.733 and accuracy of 0.77\n",
      "Iteration 15617: with minibatch training loss = 0.414 and accuracy of 0.89\n",
      "Iteration 15618: with minibatch training loss = 0.387 and accuracy of 0.89\n",
      "Iteration 15619: with minibatch training loss = 0.738 and accuracy of 0.77\n",
      "Iteration 15620: with minibatch training loss = 0.726 and accuracy of 0.78\n",
      "Iteration 15621: with minibatch training loss = 0.678 and accuracy of 0.81\n",
      "Iteration 15622: with minibatch training loss = 0.825 and accuracy of 0.73\n",
      "Iteration 15623: with minibatch training loss = 0.58 and accuracy of 0.83\n",
      "Iteration 15624: with minibatch training loss = 0.754 and accuracy of 0.77\n",
      "Iteration 15625: with minibatch training loss = 0.489 and accuracy of 0.84\n",
      "Iteration 15626: with minibatch training loss = 0.794 and accuracy of 0.73\n",
      "Iteration 15627: with minibatch training loss = 0.378 and accuracy of 0.91\n",
      "Iteration 15628: with minibatch training loss = 0.69 and accuracy of 0.78\n",
      "Iteration 15629: with minibatch training loss = 0.631 and accuracy of 0.77\n",
      "Iteration 15630: with minibatch training loss = 0.854 and accuracy of 0.73\n",
      "Iteration 15631: with minibatch training loss = 0.379 and accuracy of 0.88\n",
      "Iteration 15632: with minibatch training loss = 0.494 and accuracy of 0.88\n",
      "Iteration 15633: with minibatch training loss = 0.636 and accuracy of 0.77\n",
      "Iteration 15634: with minibatch training loss = 0.919 and accuracy of 0.7\n",
      "Iteration 15635: with minibatch training loss = 0.631 and accuracy of 0.86\n",
      "Iteration 15636: with minibatch training loss = 0.626 and accuracy of 0.8\n",
      "Iteration 15637: with minibatch training loss = 0.73 and accuracy of 0.78\n",
      "Iteration 15638: with minibatch training loss = 0.719 and accuracy of 0.75\n",
      "Iteration 15639: with minibatch training loss = 0.829 and accuracy of 0.73\n",
      "Iteration 15640: with minibatch training loss = 0.69 and accuracy of 0.83\n",
      "Iteration 15641: with minibatch training loss = 0.455 and accuracy of 0.84\n",
      "Iteration 15642: with minibatch training loss = 0.611 and accuracy of 0.83\n",
      "Iteration 15643: with minibatch training loss = 0.913 and accuracy of 0.72\n",
      "Iteration 15644: with minibatch training loss = 0.357 and accuracy of 0.91\n",
      "Iteration 15645: with minibatch training loss = 0.859 and accuracy of 0.72\n",
      "Iteration 15646: with minibatch training loss = 0.745 and accuracy of 0.77\n",
      "Iteration 15647: with minibatch training loss = 0.689 and accuracy of 0.8\n",
      "Iteration 15648: with minibatch training loss = 0.611 and accuracy of 0.81\n",
      "Iteration 15649: with minibatch training loss = 0.803 and accuracy of 0.75\n",
      "Iteration 15650: with minibatch training loss = 0.852 and accuracy of 0.75\n",
      "Iteration 15651: with minibatch training loss = 0.541 and accuracy of 0.8\n",
      "Iteration 15652: with minibatch training loss = 0.626 and accuracy of 0.83\n",
      "Iteration 15653: with minibatch training loss = 0.577 and accuracy of 0.81\n",
      "Iteration 15654: with minibatch training loss = 0.76 and accuracy of 0.77\n",
      "Iteration 15655: with minibatch training loss = 0.615 and accuracy of 0.81\n",
      "Iteration 15656: with minibatch training loss = 0.7 and accuracy of 0.83\n",
      "Iteration 15657: with minibatch training loss = 0.513 and accuracy of 0.84\n",
      "Iteration 15658: with minibatch training loss = 0.817 and accuracy of 0.75\n",
      "Iteration 15659: with minibatch training loss = 0.545 and accuracy of 0.83\n",
      "Iteration 15660: with minibatch training loss = 0.45 and accuracy of 0.88\n",
      "Iteration 15661: with minibatch training loss = 0.526 and accuracy of 0.84\n",
      "Iteration 15662: with minibatch training loss = 0.623 and accuracy of 0.8\n",
      "Iteration 15663: with minibatch training loss = 0.542 and accuracy of 0.81\n",
      "Iteration 15664: with minibatch training loss = 0.524 and accuracy of 0.83\n",
      "Iteration 15665: with minibatch training loss = 0.907 and accuracy of 0.73\n",
      "Iteration 15666: with minibatch training loss = 0.447 and accuracy of 0.84\n",
      "Iteration 15667: with minibatch training loss = 0.613 and accuracy of 0.83\n",
      "Iteration 15668: with minibatch training loss = 0.891 and accuracy of 0.75\n",
      "Iteration 15669: with minibatch training loss = 0.841 and accuracy of 0.73\n",
      "Iteration 15670: with minibatch training loss = 0.636 and accuracy of 0.84\n",
      "Iteration 15671: with minibatch training loss = 0.734 and accuracy of 0.75\n",
      "Iteration 15672: with minibatch training loss = 0.565 and accuracy of 0.84\n",
      "Iteration 15673: with minibatch training loss = 0.596 and accuracy of 0.84\n",
      "Iteration 15674: with minibatch training loss = 0.457 and accuracy of 0.86\n",
      "Iteration 15675: with minibatch training loss = 0.494 and accuracy of 0.88\n",
      "Iteration 15676: with minibatch training loss = 0.588 and accuracy of 0.84\n",
      "Iteration 15677: with minibatch training loss = 0.697 and accuracy of 0.77\n",
      "Iteration 15678: with minibatch training loss = 0.512 and accuracy of 0.88\n",
      "Iteration 15679: with minibatch training loss = 0.789 and accuracy of 0.77\n",
      "Iteration 15680: with minibatch training loss = 0.62 and accuracy of 0.81\n",
      "Iteration 15681: with minibatch training loss = 0.545 and accuracy of 0.84\n",
      "Iteration 15682: with minibatch training loss = 0.52 and accuracy of 0.84\n",
      "Iteration 15683: with minibatch training loss = 0.644 and accuracy of 0.81\n",
      "Iteration 15684: with minibatch training loss = 0.571 and accuracy of 0.84\n",
      "Iteration 15685: with minibatch training loss = 0.789 and accuracy of 0.77\n",
      "Iteration 15686: with minibatch training loss = 0.654 and accuracy of 0.81\n",
      "Iteration 15687: with minibatch training loss = 0.447 and accuracy of 0.88\n",
      "Iteration 15688: with minibatch training loss = 0.884 and accuracy of 0.75\n",
      "Iteration 15689: with minibatch training loss = 0.681 and accuracy of 0.81\n",
      "Iteration 15690: with minibatch training loss = 0.875 and accuracy of 0.73\n",
      "Iteration 15691: with minibatch training loss = 0.848 and accuracy of 0.69\n",
      "Iteration 15692: with minibatch training loss = 0.586 and accuracy of 0.8\n",
      "Iteration 15693: with minibatch training loss = 0.976 and accuracy of 0.7\n",
      "Iteration 15694: with minibatch training loss = 0.745 and accuracy of 0.75\n",
      "Iteration 15695: with minibatch training loss = 0.715 and accuracy of 0.8\n",
      "Iteration 15696: with minibatch training loss = 0.475 and accuracy of 0.86\n",
      "Iteration 15697: with minibatch training loss = 0.425 and accuracy of 0.91\n",
      "Iteration 15698: with minibatch training loss = 0.753 and accuracy of 0.73\n",
      "Iteration 15699: with minibatch training loss = 0.693 and accuracy of 0.77\n",
      "Iteration 15700: with minibatch training loss = 0.813 and accuracy of 0.73\n",
      "Iteration 15701: with minibatch training loss = 0.478 and accuracy of 0.86\n",
      "Iteration 15702: with minibatch training loss = 0.794 and accuracy of 0.75\n",
      "Iteration 15703: with minibatch training loss = 0.63 and accuracy of 0.81\n",
      "Iteration 15704: with minibatch training loss = 0.975 and accuracy of 0.69\n",
      "Iteration 15705: with minibatch training loss = 1.04 and accuracy of 0.69\n",
      "Iteration 15706: with minibatch training loss = 1.01 and accuracy of 0.64\n",
      "Iteration 15707: with minibatch training loss = 0.702 and accuracy of 0.75\n",
      "Iteration 15708: with minibatch training loss = 0.509 and accuracy of 0.86\n",
      "Iteration 15709: with minibatch training loss = 0.431 and accuracy of 0.88\n",
      "Iteration 15710: with minibatch training loss = 0.381 and accuracy of 0.91\n",
      "Iteration 15711: with minibatch training loss = 0.627 and accuracy of 0.78\n",
      "Iteration 15712: with minibatch training loss = 0.757 and accuracy of 0.8\n",
      "Iteration 15713: with minibatch training loss = 0.708 and accuracy of 0.8\n",
      "Iteration 15714: with minibatch training loss = 0.584 and accuracy of 0.8\n",
      "Iteration 15715: with minibatch training loss = 0.485 and accuracy of 0.84\n",
      "Iteration 15716: with minibatch training loss = 0.676 and accuracy of 0.83\n",
      "Iteration 15717: with minibatch training loss = 0.521 and accuracy of 0.86\n",
      "Iteration 15718: with minibatch training loss = 0.405 and accuracy of 0.88\n",
      "Iteration 15719: with minibatch training loss = 0.665 and accuracy of 0.81\n",
      "Iteration 15720: with minibatch training loss = 0.813 and accuracy of 0.7\n",
      "Iteration 15721: with minibatch training loss = 0.609 and accuracy of 0.81\n",
      "Iteration 15722: with minibatch training loss = 0.594 and accuracy of 0.84\n",
      "Iteration 15723: with minibatch training loss = 0.55 and accuracy of 0.83\n",
      "Iteration 15724: with minibatch training loss = 0.508 and accuracy of 0.84\n",
      "Iteration 15725: with minibatch training loss = 0.425 and accuracy of 0.86\n",
      "Iteration 15726: with minibatch training loss = 0.747 and accuracy of 0.75\n",
      "Iteration 15727: with minibatch training loss = 0.681 and accuracy of 0.77\n",
      "Iteration 15728: with minibatch training loss = 0.528 and accuracy of 0.83\n",
      "Iteration 15729: with minibatch training loss = 0.834 and accuracy of 0.73\n",
      "Iteration 15730: with minibatch training loss = 0.647 and accuracy of 0.84\n",
      "Iteration 15731: with minibatch training loss = 0.624 and accuracy of 0.8\n",
      "Iteration 15732: with minibatch training loss = 0.528 and accuracy of 0.8\n",
      "Iteration 15733: with minibatch training loss = 0.545 and accuracy of 0.84\n",
      "Iteration 15734: with minibatch training loss = 0.773 and accuracy of 0.77\n",
      "Iteration 15735: with minibatch training loss = 0.619 and accuracy of 0.8\n",
      "Iteration 15736: with minibatch training loss = 0.714 and accuracy of 0.75\n",
      "Iteration 15737: with minibatch training loss = 0.77 and accuracy of 0.75\n",
      "Iteration 15738: with minibatch training loss = 0.688 and accuracy of 0.8\n",
      "Iteration 15739: with minibatch training loss = 0.591 and accuracy of 0.84\n",
      "Iteration 15740: with minibatch training loss = 0.84 and accuracy of 0.75\n",
      "Iteration 15741: with minibatch training loss = 0.898 and accuracy of 0.7\n",
      "Iteration 15742: with minibatch training loss = 0.927 and accuracy of 0.72\n",
      "Iteration 15743: with minibatch training loss = 0.685 and accuracy of 0.83\n",
      "Iteration 15744: with minibatch training loss = 0.606 and accuracy of 0.83\n",
      "Iteration 15745: with minibatch training loss = 0.416 and accuracy of 0.89\n",
      "Iteration 15746: with minibatch training loss = 0.392 and accuracy of 0.91\n",
      "Iteration 15747: with minibatch training loss = 0.572 and accuracy of 0.81\n",
      "Iteration 15748: with minibatch training loss = 0.565 and accuracy of 0.81\n",
      "Iteration 15749: with minibatch training loss = 0.7 and accuracy of 0.8\n",
      "Iteration 15750: with minibatch training loss = 1.12 and accuracy of 0.66\n",
      "Iteration 15751: with minibatch training loss = 0.649 and accuracy of 0.81\n",
      "Iteration 15752: with minibatch training loss = 0.87 and accuracy of 0.73\n",
      "Iteration 15753: with minibatch training loss = 0.833 and accuracy of 0.75\n",
      "Iteration 15754: with minibatch training loss = 0.6 and accuracy of 0.83\n",
      "Iteration 15755: with minibatch training loss = 0.777 and accuracy of 0.78\n",
      "Iteration 15756: with minibatch training loss = 0.677 and accuracy of 0.78\n",
      "Iteration 15757: with minibatch training loss = 0.814 and accuracy of 0.75\n",
      "Iteration 15758: with minibatch training loss = 0.755 and accuracy of 0.75\n",
      "Iteration 15759: with minibatch training loss = 0.471 and accuracy of 0.84\n",
      "Iteration 15760: with minibatch training loss = 0.537 and accuracy of 0.89\n",
      "Iteration 15761: with minibatch training loss = 0.695 and accuracy of 0.78\n",
      "Iteration 15762: with minibatch training loss = 0.689 and accuracy of 0.81\n",
      "Iteration 15763: with minibatch training loss = 0.869 and accuracy of 0.73\n",
      "Iteration 15764: with minibatch training loss = 0.532 and accuracy of 0.8\n",
      "Iteration 15765: with minibatch training loss = 0.716 and accuracy of 0.81\n",
      "Iteration 15766: with minibatch training loss = 0.506 and accuracy of 0.83\n",
      "Iteration 15767: with minibatch training loss = 0.433 and accuracy of 0.84\n",
      "Iteration 15768: with minibatch training loss = 0.575 and accuracy of 0.81\n",
      "Iteration 15769: with minibatch training loss = 0.508 and accuracy of 0.83\n",
      "Iteration 15770: with minibatch training loss = 0.832 and accuracy of 0.7\n",
      "Iteration 15771: with minibatch training loss = 0.704 and accuracy of 0.77\n",
      "Iteration 15772: with minibatch training loss = 0.575 and accuracy of 0.84\n",
      "Iteration 15773: with minibatch training loss = 0.751 and accuracy of 0.77\n",
      "Iteration 15774: with minibatch training loss = 1.01 and accuracy of 0.69\n",
      "Iteration 15775: with minibatch training loss = 0.731 and accuracy of 0.75\n",
      "Iteration 15776: with minibatch training loss = 0.607 and accuracy of 0.81\n",
      "Iteration 15777: with minibatch training loss = 0.694 and accuracy of 0.8\n",
      "Iteration 15778: with minibatch training loss = 0.683 and accuracy of 0.78\n",
      "Iteration 15779: with minibatch training loss = 0.448 and accuracy of 0.86\n",
      "Iteration 15780: with minibatch training loss = 0.677 and accuracy of 0.78\n",
      "Iteration 15781: with minibatch training loss = 0.699 and accuracy of 0.78\n",
      "Iteration 15782: with minibatch training loss = 0.41 and accuracy of 0.89\n",
      "Iteration 15783: with minibatch training loss = 0.705 and accuracy of 0.8\n",
      "Iteration 15784: with minibatch training loss = 0.762 and accuracy of 0.77\n",
      "Iteration 15785: with minibatch training loss = 0.965 and accuracy of 0.72\n",
      "Iteration 15786: with minibatch training loss = 0.655 and accuracy of 0.83\n",
      "Iteration 15787: with minibatch training loss = 0.687 and accuracy of 0.81\n",
      "Iteration 15788: with minibatch training loss = 0.565 and accuracy of 0.84\n",
      "Iteration 15789: with minibatch training loss = 0.656 and accuracy of 0.8\n",
      "Iteration 15790: with minibatch training loss = 0.611 and accuracy of 0.88\n",
      "Iteration 15791: with minibatch training loss = 0.415 and accuracy of 0.91\n",
      "Iteration 15792: with minibatch training loss = 0.925 and accuracy of 0.69\n",
      "Iteration 15793: with minibatch training loss = 0.728 and accuracy of 0.8\n",
      "Iteration 15794: with minibatch training loss = 0.856 and accuracy of 0.72\n",
      "Iteration 15795: with minibatch training loss = 0.544 and accuracy of 0.89\n",
      "Iteration 15796: with minibatch training loss = 0.618 and accuracy of 0.84\n",
      "Iteration 15797: with minibatch training loss = 0.791 and accuracy of 0.73\n",
      "Iteration 15798: with minibatch training loss = 0.47 and accuracy of 0.88\n",
      "Iteration 15799: with minibatch training loss = 0.506 and accuracy of 0.88\n",
      "Iteration 15800: with minibatch training loss = 0.959 and accuracy of 0.72\n",
      "Iteration 15801: with minibatch training loss = 0.687 and accuracy of 0.78\n",
      "Iteration 15802: with minibatch training loss = 0.522 and accuracy of 0.84\n",
      "Iteration 15803: with minibatch training loss = 0.497 and accuracy of 0.86\n",
      "Iteration 15804: with minibatch training loss = 0.677 and accuracy of 0.8\n",
      "Iteration 15805: with minibatch training loss = 0.583 and accuracy of 0.81\n",
      "Iteration 15806: with minibatch training loss = 0.872 and accuracy of 0.72\n",
      "Iteration 15807: with minibatch training loss = 0.665 and accuracy of 0.8\n",
      "Iteration 15808: with minibatch training loss = 0.432 and accuracy of 0.86\n",
      "Iteration 15809: with minibatch training loss = 0.622 and accuracy of 0.86\n",
      "Iteration 15810: with minibatch training loss = 0.584 and accuracy of 0.84\n",
      "Iteration 15811: with minibatch training loss = 0.74 and accuracy of 0.78\n",
      "Iteration 15812: with minibatch training loss = 0.757 and accuracy of 0.75\n",
      "Iteration 15813: with minibatch training loss = 0.565 and accuracy of 0.83\n",
      "Iteration 15814: with minibatch training loss = 0.868 and accuracy of 0.73\n",
      "Iteration 15815: with minibatch training loss = 0.862 and accuracy of 0.72\n",
      "Iteration 15816: with minibatch training loss = 0.537 and accuracy of 0.86\n",
      "Iteration 15817: with minibatch training loss = 0.697 and accuracy of 0.78\n",
      "Iteration 15818: with minibatch training loss = 0.774 and accuracy of 0.78\n",
      "Iteration 15819: with minibatch training loss = 0.894 and accuracy of 0.73\n",
      "Iteration 15820: with minibatch training loss = 0.637 and accuracy of 0.83\n",
      "Iteration 15821: with minibatch training loss = 0.598 and accuracy of 0.83\n",
      "Iteration 15822: with minibatch training loss = 0.972 and accuracy of 0.75\n",
      "Iteration 15823: with minibatch training loss = 0.701 and accuracy of 0.81\n",
      "Iteration 15824: with minibatch training loss = 0.585 and accuracy of 0.78\n",
      "Iteration 15825: with minibatch training loss = 0.61 and accuracy of 0.81\n",
      "Iteration 15826: with minibatch training loss = 0.622 and accuracy of 0.8\n",
      "Iteration 15827: with minibatch training loss = 0.499 and accuracy of 0.86\n",
      "Iteration 15828: with minibatch training loss = 0.869 and accuracy of 0.7\n",
      "Iteration 15829: with minibatch training loss = 0.671 and accuracy of 0.78\n",
      "Iteration 15830: with minibatch training loss = 0.484 and accuracy of 0.84\n",
      "Iteration 15831: with minibatch training loss = 0.71 and accuracy of 0.78\n",
      "Iteration 15832: with minibatch training loss = 0.535 and accuracy of 0.84\n",
      "Iteration 15833: with minibatch training loss = 0.609 and accuracy of 0.83\n",
      "Iteration 15834: with minibatch training loss = 0.54 and accuracy of 0.83\n",
      "Iteration 15835: with minibatch training loss = 0.444 and accuracy of 0.88\n",
      "Iteration 15836: with minibatch training loss = 0.534 and accuracy of 0.81\n",
      "Iteration 15837: with minibatch training loss = 0.789 and accuracy of 0.77\n",
      "Iteration 15838: with minibatch training loss = 0.552 and accuracy of 0.86\n",
      "Iteration 15839: with minibatch training loss = 0.434 and accuracy of 0.84\n",
      "Iteration 15840: with minibatch training loss = 0.875 and accuracy of 0.75\n",
      "Iteration 15841: with minibatch training loss = 0.51 and accuracy of 0.84\n",
      "Iteration 15842: with minibatch training loss = 0.415 and accuracy of 0.89\n",
      "Iteration 15843: with minibatch training loss = 0.652 and accuracy of 0.81\n",
      "Iteration 15844: with minibatch training loss = 0.618 and accuracy of 0.84\n",
      "Iteration 15845: with minibatch training loss = 0.844 and accuracy of 0.78\n",
      "Iteration 15846: with minibatch training loss = 0.369 and accuracy of 0.89\n",
      "Iteration 15847: with minibatch training loss = 0.564 and accuracy of 0.83\n",
      "Iteration 15848: with minibatch training loss = 0.343 and accuracy of 0.91\n",
      "Iteration 15849: with minibatch training loss = 0.967 and accuracy of 0.72\n",
      "Iteration 15850: with minibatch training loss = 0.672 and accuracy of 0.78\n",
      "Iteration 15851: with minibatch training loss = 0.727 and accuracy of 0.75\n",
      "Iteration 15852: with minibatch training loss = 0.532 and accuracy of 0.83\n",
      "Iteration 15853: with minibatch training loss = 0.676 and accuracy of 0.81\n",
      "Iteration 15854: with minibatch training loss = 0.357 and accuracy of 0.92\n",
      "Iteration 15855: with minibatch training loss = 0.629 and accuracy of 0.84\n",
      "Iteration 15856: with minibatch training loss = 0.567 and accuracy of 0.83\n",
      "Iteration 15857: with minibatch training loss = 0.617 and accuracy of 0.81\n",
      "Iteration 15858: with minibatch training loss = 0.767 and accuracy of 0.78\n",
      "Iteration 15859: with minibatch training loss = 0.866 and accuracy of 0.75\n",
      "Iteration 15860: with minibatch training loss = 0.479 and accuracy of 0.86\n",
      "Iteration 15861: with minibatch training loss = 0.662 and accuracy of 0.78\n",
      "Iteration 15862: with minibatch training loss = 0.434 and accuracy of 0.88\n",
      "Iteration 15863: with minibatch training loss = 0.822 and accuracy of 0.75\n",
      "Iteration 15864: with minibatch training loss = 0.907 and accuracy of 0.73\n",
      "Iteration 15865: with minibatch training loss = 0.607 and accuracy of 0.83\n",
      "Iteration 15866: with minibatch training loss = 0.558 and accuracy of 0.83\n",
      "Iteration 15867: with minibatch training loss = 0.593 and accuracy of 0.83\n",
      "Iteration 15868: with minibatch training loss = 0.614 and accuracy of 0.81\n",
      "Iteration 15869: with minibatch training loss = 1.02 and accuracy of 0.69\n",
      "Iteration 15870: with minibatch training loss = 0.545 and accuracy of 0.83\n",
      "Iteration 15871: with minibatch training loss = 0.813 and accuracy of 0.73\n",
      "Iteration 15872: with minibatch training loss = 0.449 and accuracy of 0.86\n",
      "Iteration 15873: with minibatch training loss = 0.582 and accuracy of 0.81\n",
      "Iteration 15874: with minibatch training loss = 0.399 and accuracy of 0.91\n",
      "Iteration 15875: with minibatch training loss = 0.845 and accuracy of 0.77\n",
      "Iteration 15876: with minibatch training loss = 0.614 and accuracy of 0.83\n",
      "Iteration 15877: with minibatch training loss = 0.623 and accuracy of 0.8\n",
      "Iteration 15878: with minibatch training loss = 0.584 and accuracy of 0.83\n",
      "Iteration 15879: with minibatch training loss = 0.855 and accuracy of 0.7\n",
      "Iteration 15880: with minibatch training loss = 0.843 and accuracy of 0.72\n",
      "Iteration 15881: with minibatch training loss = 0.528 and accuracy of 0.83\n",
      "Iteration 15882: with minibatch training loss = 0.556 and accuracy of 0.84\n",
      "Iteration 15883: with minibatch training loss = 0.534 and accuracy of 0.84\n",
      "Iteration 15884: with minibatch training loss = 0.638 and accuracy of 0.81\n",
      "Iteration 15885: with minibatch training loss = 0.552 and accuracy of 0.84\n",
      "Iteration 15886: with minibatch training loss = 0.533 and accuracy of 0.83\n",
      "Iteration 15887: with minibatch training loss = 0.806 and accuracy of 0.77\n",
      "Iteration 15888: with minibatch training loss = 0.552 and accuracy of 0.83\n",
      "Iteration 15889: with minibatch training loss = 0.774 and accuracy of 0.77\n",
      "Iteration 15890: with minibatch training loss = 0.797 and accuracy of 0.75\n",
      "Iteration 15891: with minibatch training loss = 0.563 and accuracy of 0.84\n",
      "Iteration 15892: with minibatch training loss = 0.567 and accuracy of 0.81\n",
      "Iteration 15893: with minibatch training loss = 0.769 and accuracy of 0.8\n",
      "Iteration 15894: with minibatch training loss = 0.647 and accuracy of 0.83\n",
      "Iteration 15895: with minibatch training loss = 0.796 and accuracy of 0.73\n",
      "Iteration 15896: with minibatch training loss = 0.735 and accuracy of 0.78\n",
      "Iteration 15897: with minibatch training loss = 0.691 and accuracy of 0.75\n",
      "Iteration 15898: with minibatch training loss = 0.911 and accuracy of 0.72\n",
      "Iteration 15899: with minibatch training loss = 0.66 and accuracy of 0.81\n",
      "Iteration 15900: with minibatch training loss = 0.438 and accuracy of 0.88\n",
      "Iteration 15901: with minibatch training loss = 0.663 and accuracy of 0.81\n",
      "Iteration 15902: with minibatch training loss = 0.495 and accuracy of 0.86\n",
      "Iteration 15903: with minibatch training loss = 0.626 and accuracy of 0.8\n",
      "Iteration 15904: with minibatch training loss = 0.667 and accuracy of 0.78\n",
      "Iteration 15905: with minibatch training loss = 0.701 and accuracy of 0.78\n",
      "Iteration 15906: with minibatch training loss = 0.531 and accuracy of 0.83\n",
      "Iteration 15907: with minibatch training loss = 0.389 and accuracy of 0.89\n",
      "Iteration 15908: with minibatch training loss = 0.631 and accuracy of 0.78\n",
      "Iteration 15909: with minibatch training loss = 0.627 and accuracy of 0.77\n",
      "Iteration 15910: with minibatch training loss = 0.852 and accuracy of 0.73\n",
      "Iteration 15911: with minibatch training loss = 0.598 and accuracy of 0.83\n",
      "Iteration 15912: with minibatch training loss = 0.728 and accuracy of 0.77\n",
      "Iteration 15913: with minibatch training loss = 0.561 and accuracy of 0.8\n",
      "Iteration 15914: with minibatch training loss = 0.748 and accuracy of 0.75\n",
      "Iteration 15915: with minibatch training loss = 0.566 and accuracy of 0.83\n",
      "Iteration 15916: with minibatch training loss = 0.846 and accuracy of 0.72\n",
      "Iteration 15917: with minibatch training loss = 0.838 and accuracy of 0.72\n",
      "Iteration 15918: with minibatch training loss = 0.491 and accuracy of 0.84\n",
      "Iteration 15919: with minibatch training loss = 0.595 and accuracy of 0.83\n",
      "Iteration 15920: with minibatch training loss = 0.678 and accuracy of 0.77\n",
      "Iteration 15921: with minibatch training loss = 0.764 and accuracy of 0.8\n",
      "Iteration 15922: with minibatch training loss = 0.826 and accuracy of 0.75\n",
      "Iteration 15923: with minibatch training loss = 0.663 and accuracy of 0.78\n",
      "Iteration 15924: with minibatch training loss = 0.743 and accuracy of 0.73\n",
      "Iteration 15925: with minibatch training loss = 0.471 and accuracy of 0.83\n",
      "Iteration 15926: with minibatch training loss = 0.686 and accuracy of 0.83\n",
      "Iteration 15927: with minibatch training loss = 0.622 and accuracy of 0.8\n",
      "Iteration 15928: with minibatch training loss = 0.485 and accuracy of 0.84\n",
      "Iteration 15929: with minibatch training loss = 0.568 and accuracy of 0.78\n",
      "Iteration 15930: with minibatch training loss = 0.702 and accuracy of 0.8\n",
      "Iteration 15931: with minibatch training loss = 0.44 and accuracy of 0.91\n",
      "Iteration 15932: with minibatch training loss = 0.951 and accuracy of 0.72\n",
      "Iteration 15933: with minibatch training loss = 0.66 and accuracy of 0.78\n",
      "Iteration 15934: with minibatch training loss = 0.635 and accuracy of 0.8\n",
      "Iteration 15935: with minibatch training loss = 0.666 and accuracy of 0.8\n",
      "Iteration 15936: with minibatch training loss = 0.866 and accuracy of 0.75\n",
      "Iteration 15937: with minibatch training loss = 0.578 and accuracy of 0.84\n",
      "Iteration 15938: with minibatch training loss = 0.845 and accuracy of 0.73\n",
      "Iteration 15939: with minibatch training loss = 0.72 and accuracy of 0.81\n",
      "Iteration 15940: with minibatch training loss = 0.621 and accuracy of 0.83\n",
      "Iteration 15941: with minibatch training loss = 0.992 and accuracy of 0.69\n",
      "Iteration 15942: with minibatch training loss = 0.643 and accuracy of 0.8\n",
      "Iteration 15943: with minibatch training loss = 0.597 and accuracy of 0.83\n",
      "Iteration 15944: with minibatch training loss = 0.584 and accuracy of 0.8\n",
      "Iteration 15945: with minibatch training loss = 0.703 and accuracy of 0.78\n",
      "Iteration 15946: with minibatch training loss = 0.555 and accuracy of 0.83\n",
      "Iteration 15947: with minibatch training loss = 0.721 and accuracy of 0.8\n",
      "Iteration 15948: with minibatch training loss = 0.535 and accuracy of 0.84\n",
      "Iteration 15949: with minibatch training loss = 0.785 and accuracy of 0.8\n",
      "Iteration 15950: with minibatch training loss = 0.978 and accuracy of 0.67\n",
      "Iteration 15951: with minibatch training loss = 1.02 and accuracy of 0.67\n",
      "Iteration 15952: with minibatch training loss = 0.883 and accuracy of 0.73\n",
      "Iteration 15953: with minibatch training loss = 0.681 and accuracy of 0.77\n",
      "Iteration 15954: with minibatch training loss = 1.07 and accuracy of 0.66\n",
      "Iteration 15955: with minibatch training loss = 0.472 and accuracy of 0.84\n",
      "Iteration 15956: with minibatch training loss = 0.802 and accuracy of 0.75\n",
      "Iteration 15957: with minibatch training loss = 0.741 and accuracy of 0.72\n",
      "Iteration 15958: with minibatch training loss = 0.592 and accuracy of 0.84\n",
      "Iteration 15959: with minibatch training loss = 0.739 and accuracy of 0.77\n",
      "Iteration 15960: with minibatch training loss = 0.585 and accuracy of 0.83\n",
      "Iteration 15961: with minibatch training loss = 0.783 and accuracy of 0.75\n",
      "Iteration 15962: with minibatch training loss = 0.643 and accuracy of 0.8\n",
      "Iteration 15963: with minibatch training loss = 0.954 and accuracy of 0.8\n",
      "Iteration 15964: with minibatch training loss = 0.624 and accuracy of 0.81\n",
      "Iteration 15965: with minibatch training loss = 0.614 and accuracy of 0.8\n",
      "Iteration 15966: with minibatch training loss = 0.854 and accuracy of 0.75\n",
      "Iteration 15967: with minibatch training loss = 0.756 and accuracy of 0.78\n",
      "Iteration 15968: with minibatch training loss = 0.467 and accuracy of 0.86\n",
      "Iteration 15969: with minibatch training loss = 0.663 and accuracy of 0.8\n",
      "Iteration 15970: with minibatch training loss = 0.53 and accuracy of 0.83\n",
      "Iteration 15971: with minibatch training loss = 0.697 and accuracy of 0.77\n",
      "Iteration 15972: with minibatch training loss = 0.628 and accuracy of 0.78\n",
      "Iteration 15973: with minibatch training loss = 0.756 and accuracy of 0.77\n",
      "Iteration 15974: with minibatch training loss = 0.519 and accuracy of 0.83\n",
      "Iteration 15975: with minibatch training loss = 0.608 and accuracy of 0.77\n",
      "Iteration 15976: with minibatch training loss = 0.539 and accuracy of 0.86\n",
      "Iteration 15977: with minibatch training loss = 0.672 and accuracy of 0.81\n",
      "Iteration 15978: with minibatch training loss = 0.73 and accuracy of 0.77\n",
      "Iteration 15979: with minibatch training loss = 0.706 and accuracy of 0.75\n",
      "Iteration 15980: with minibatch training loss = 0.517 and accuracy of 0.83\n",
      "Iteration 15981: with minibatch training loss = 0.657 and accuracy of 0.77\n",
      "Iteration 15982: with minibatch training loss = 0.567 and accuracy of 0.83\n",
      "Iteration 15983: with minibatch training loss = 0.916 and accuracy of 0.7\n",
      "Iteration 15984: with minibatch training loss = 0.776 and accuracy of 0.77\n",
      "Iteration 15985: with minibatch training loss = 0.555 and accuracy of 0.84\n",
      "Iteration 15986: with minibatch training loss = 0.405 and accuracy of 0.88\n",
      "Iteration 15987: with minibatch training loss = 0.748 and accuracy of 0.8\n",
      "Iteration 15988: with minibatch training loss = 0.613 and accuracy of 0.84\n",
      "Iteration 15989: with minibatch training loss = 0.429 and accuracy of 0.84\n",
      "Iteration 15990: with minibatch training loss = 0.546 and accuracy of 0.83\n",
      "Iteration 15991: with minibatch training loss = 0.602 and accuracy of 0.81\n",
      "Iteration 15992: with minibatch training loss = 0.715 and accuracy of 0.78\n",
      "Iteration 15993: with minibatch training loss = 0.707 and accuracy of 0.78\n",
      "Iteration 15994: with minibatch training loss = 0.982 and accuracy of 0.7\n",
      "Iteration 15995: with minibatch training loss = 0.578 and accuracy of 0.83\n",
      "Iteration 15996: with minibatch training loss = 0.704 and accuracy of 0.8\n",
      "Iteration 15997: with minibatch training loss = 0.52 and accuracy of 0.84\n",
      "Iteration 15998: with minibatch training loss = 0.66 and accuracy of 0.77\n",
      "Iteration 15999: with minibatch training loss = 0.61 and accuracy of 0.83\n",
      "Iteration 16000: with minibatch training loss = 0.768 and accuracy of 0.8\n",
      "Iteration 16001: with minibatch training loss = 0.6 and accuracy of 0.78\n",
      "Iteration 16002: with minibatch training loss = 0.764 and accuracy of 0.75\n",
      "Iteration 16003: with minibatch training loss = 0.526 and accuracy of 0.86\n",
      "Iteration 16004: with minibatch training loss = 0.631 and accuracy of 0.8\n",
      "Iteration 16005: with minibatch training loss = 0.522 and accuracy of 0.83\n",
      "Iteration 16006: with minibatch training loss = 0.644 and accuracy of 0.78\n",
      "Iteration 16007: with minibatch training loss = 0.74 and accuracy of 0.8\n",
      "Iteration 16008: with minibatch training loss = 0.726 and accuracy of 0.78\n",
      "Iteration 16009: with minibatch training loss = 0.728 and accuracy of 0.77\n",
      "Iteration 16010: with minibatch training loss = 0.661 and accuracy of 0.8\n",
      "Iteration 16011: with minibatch training loss = 0.718 and accuracy of 0.77\n",
      "Iteration 16012: with minibatch training loss = 0.655 and accuracy of 0.78\n",
      "Iteration 16013: with minibatch training loss = 0.484 and accuracy of 0.86\n",
      "Iteration 16014: with minibatch training loss = 0.544 and accuracy of 0.83\n",
      "Iteration 16015: with minibatch training loss = 0.717 and accuracy of 0.81\n",
      "Iteration 16016: with minibatch training loss = 0.618 and accuracy of 0.8\n",
      "Iteration 16017: with minibatch training loss = 0.742 and accuracy of 0.77\n",
      "Iteration 16018: with minibatch training loss = 0.401 and accuracy of 0.89\n",
      "Iteration 16019: with minibatch training loss = 0.455 and accuracy of 0.86\n",
      "Iteration 16020: with minibatch training loss = 0.754 and accuracy of 0.83\n",
      "Iteration 16021: with minibatch training loss = 0.442 and accuracy of 0.88\n",
      "Iteration 16022: with minibatch training loss = 0.934 and accuracy of 0.72\n",
      "Iteration 16023: with minibatch training loss = 0.461 and accuracy of 0.88\n",
      "Iteration 16024: with minibatch training loss = 0.547 and accuracy of 0.81\n",
      "Iteration 16025: with minibatch training loss = 0.461 and accuracy of 0.89\n",
      "Iteration 16026: with minibatch training loss = 0.728 and accuracy of 0.8\n",
      "Iteration 16027: with minibatch training loss = 0.635 and accuracy of 0.8\n",
      "Iteration 16028: with minibatch training loss = 0.338 and accuracy of 0.91\n",
      "Iteration 16029: with minibatch training loss = 0.777 and accuracy of 0.75\n",
      "Iteration 16030: with minibatch training loss = 0.744 and accuracy of 0.78\n",
      "Iteration 16031: with minibatch training loss = 0.654 and accuracy of 0.78\n",
      "Iteration 16032: with minibatch training loss = 0.722 and accuracy of 0.73\n",
      "Iteration 16033: with minibatch training loss = 0.742 and accuracy of 0.78\n",
      "Iteration 16034: with minibatch training loss = 0.492 and accuracy of 0.83\n",
      "Iteration 16035: with minibatch training loss = 0.483 and accuracy of 0.84\n",
      "Iteration 16036: with minibatch training loss = 0.279 and accuracy of 0.94\n",
      "Iteration 16037: with minibatch training loss = 0.54 and accuracy of 0.83\n",
      "Iteration 16038: with minibatch training loss = 0.569 and accuracy of 0.83\n",
      "Iteration 16039: with minibatch training loss = 0.708 and accuracy of 0.77\n",
      "Iteration 16040: with minibatch training loss = 0.928 and accuracy of 0.72\n",
      "Iteration 16041: with minibatch training loss = 0.573 and accuracy of 0.84\n",
      "Iteration 16042: with minibatch training loss = 0.601 and accuracy of 0.83\n",
      "Iteration 16043: with minibatch training loss = 0.437 and accuracy of 0.88\n",
      "Iteration 16044: with minibatch training loss = 0.7 and accuracy of 0.78\n",
      "Iteration 16045: with minibatch training loss = 0.573 and accuracy of 0.81\n",
      "Iteration 16046: with minibatch training loss = 0.699 and accuracy of 0.78\n",
      "Iteration 16047: with minibatch training loss = 0.536 and accuracy of 0.83\n",
      "Iteration 16048: with minibatch training loss = 0.669 and accuracy of 0.83\n",
      "Iteration 16049: with minibatch training loss = 0.826 and accuracy of 0.75\n",
      "Iteration 16050: with minibatch training loss = 0.576 and accuracy of 0.81\n",
      "Iteration 16051: with minibatch training loss = 0.544 and accuracy of 0.83\n",
      "Iteration 16052: with minibatch training loss = 0.624 and accuracy of 0.8\n",
      "Iteration 16053: with minibatch training loss = 0.707 and accuracy of 0.81\n",
      "Iteration 16054: with minibatch training loss = 0.715 and accuracy of 0.78\n",
      "Iteration 16055: with minibatch training loss = 0.731 and accuracy of 0.78\n",
      "Iteration 16056: with minibatch training loss = 0.7 and accuracy of 0.8\n",
      "Iteration 16057: with minibatch training loss = 0.511 and accuracy of 0.84\n",
      "Iteration 16058: with minibatch training loss = 0.876 and accuracy of 0.72\n",
      "Iteration 16059: with minibatch training loss = 0.727 and accuracy of 0.77\n",
      "Iteration 16060: with minibatch training loss = 0.651 and accuracy of 0.78\n",
      "Iteration 16061: with minibatch training loss = 0.578 and accuracy of 0.83\n",
      "Iteration 16062: with minibatch training loss = 0.664 and accuracy of 0.8\n",
      "Iteration 16063: with minibatch training loss = 0.628 and accuracy of 0.8\n",
      "Iteration 16064: with minibatch training loss = 0.885 and accuracy of 0.7\n",
      "Iteration 16065: with minibatch training loss = 0.694 and accuracy of 0.78\n",
      "Iteration 16066: with minibatch training loss = 0.676 and accuracy of 0.81\n",
      "Iteration 16067: with minibatch training loss = 0.679 and accuracy of 0.8\n",
      "Iteration 16068: with minibatch training loss = 0.749 and accuracy of 0.73\n",
      "Iteration 16069: with minibatch training loss = 0.657 and accuracy of 0.81\n",
      "Iteration 16070: with minibatch training loss = 0.733 and accuracy of 0.78\n",
      "Iteration 16071: with minibatch training loss = 0.488 and accuracy of 0.84\n",
      "Iteration 16072: with minibatch training loss = 0.719 and accuracy of 0.75\n",
      "Iteration 16073: with minibatch training loss = 0.428 and accuracy of 0.88\n",
      "Iteration 16074: with minibatch training loss = 0.678 and accuracy of 0.78\n",
      "Iteration 16075: with minibatch training loss = 0.56 and accuracy of 0.86\n",
      "Iteration 16076: with minibatch training loss = 0.802 and accuracy of 0.75\n",
      "Iteration 16077: with minibatch training loss = 0.681 and accuracy of 0.8\n",
      "Iteration 16078: with minibatch training loss = 0.501 and accuracy of 0.84\n",
      "Iteration 16079: with minibatch training loss = 0.757 and accuracy of 0.75\n",
      "Iteration 16080: with minibatch training loss = 0.581 and accuracy of 0.81\n",
      "Iteration 16081: with minibatch training loss = 0.52 and accuracy of 0.84\n",
      "Iteration 16082: with minibatch training loss = 0.64 and accuracy of 0.78\n",
      "Iteration 16083: with minibatch training loss = 0.917 and accuracy of 0.73\n",
      "Iteration 16084: with minibatch training loss = 0.774 and accuracy of 0.75\n",
      "Iteration 16085: with minibatch training loss = 0.775 and accuracy of 0.78\n",
      "Iteration 16086: with minibatch training loss = 0.443 and accuracy of 0.89\n",
      "Iteration 16087: with minibatch training loss = 0.807 and accuracy of 0.77\n",
      "Iteration 16088: with minibatch training loss = 0.878 and accuracy of 0.72\n",
      "Iteration 16089: with minibatch training loss = 0.488 and accuracy of 0.83\n",
      "Iteration 16090: with minibatch training loss = 0.473 and accuracy of 0.86\n",
      "Iteration 16091: with minibatch training loss = 0.761 and accuracy of 0.75\n",
      "Iteration 16092: with minibatch training loss = 0.793 and accuracy of 0.73\n",
      "Iteration 16093: with minibatch training loss = 0.533 and accuracy of 0.83\n",
      "Iteration 16094: with minibatch training loss = 0.687 and accuracy of 0.78\n",
      "Iteration 16095: with minibatch training loss = 0.617 and accuracy of 0.83\n",
      "Iteration 16096: with minibatch training loss = 0.317 and accuracy of 0.91\n",
      "Iteration 16097: with minibatch training loss = 0.858 and accuracy of 0.77\n",
      "Iteration 16098: with minibatch training loss = 0.754 and accuracy of 0.77\n",
      "Iteration 16099: with minibatch training loss = 0.627 and accuracy of 0.83\n",
      "Iteration 16100: with minibatch training loss = 0.364 and accuracy of 0.89\n",
      "Iteration 16101: with minibatch training loss = 0.958 and accuracy of 0.72\n",
      "Iteration 16102: with minibatch training loss = 0.604 and accuracy of 0.84\n",
      "Iteration 16103: with minibatch training loss = 0.819 and accuracy of 0.73\n",
      "Iteration 16104: with minibatch training loss = 0.657 and accuracy of 0.75\n",
      "Iteration 16105: with minibatch training loss = 0.82 and accuracy of 0.77\n",
      "Iteration 16106: with minibatch training loss = 0.716 and accuracy of 0.8\n",
      "Iteration 16107: with minibatch training loss = 0.842 and accuracy of 0.75\n",
      "Iteration 16108: with minibatch training loss = 1.04 and accuracy of 0.67\n",
      "Iteration 16109: with minibatch training loss = 0.563 and accuracy of 0.83\n",
      "Iteration 16110: with minibatch training loss = 0.57 and accuracy of 0.86\n",
      "Iteration 16111: with minibatch training loss = 0.457 and accuracy of 0.84\n",
      "Iteration 16112: with minibatch training loss = 0.411 and accuracy of 0.86\n",
      "Iteration 16113: with minibatch training loss = 0.654 and accuracy of 0.81\n",
      "Iteration 16114: with minibatch training loss = 0.61 and accuracy of 0.8\n",
      "Iteration 16115: with minibatch training loss = 0.758 and accuracy of 0.75\n",
      "Iteration 16116: with minibatch training loss = 0.716 and accuracy of 0.81\n",
      "Iteration 16117: with minibatch training loss = 0.397 and accuracy of 0.89\n",
      "Iteration 16118: with minibatch training loss = 0.566 and accuracy of 0.81\n",
      "Iteration 16119: with minibatch training loss = 0.673 and accuracy of 0.78\n",
      "Iteration 16120: with minibatch training loss = 0.501 and accuracy of 0.83\n",
      "Iteration 16121: with minibatch training loss = 0.494 and accuracy of 0.83\n",
      "Iteration 16122: with minibatch training loss = 0.655 and accuracy of 0.8\n",
      "Iteration 16123: with minibatch training loss = 0.727 and accuracy of 0.75\n",
      "Iteration 16124: with minibatch training loss = 0.901 and accuracy of 0.77\n",
      "Iteration 16125: with minibatch training loss = 0.586 and accuracy of 0.81\n",
      "Iteration 16126: with minibatch training loss = 0.6 and accuracy of 0.81\n",
      "Iteration 16127: with minibatch training loss = 0.887 and accuracy of 0.73\n",
      "Iteration 16128: with minibatch training loss = 0.552 and accuracy of 0.88\n",
      "Iteration 16129: with minibatch training loss = 0.553 and accuracy of 0.84\n",
      "Iteration 16130: with minibatch training loss = 0.518 and accuracy of 0.86\n",
      "Iteration 16131: with minibatch training loss = 0.555 and accuracy of 0.83\n",
      "Iteration 16132: with minibatch training loss = 0.48 and accuracy of 0.84\n",
      "Iteration 16133: with minibatch training loss = 0.434 and accuracy of 0.86\n",
      "Iteration 16134: with minibatch training loss = 0.393 and accuracy of 0.89\n",
      "Iteration 16135: with minibatch training loss = 0.81 and accuracy of 0.77\n",
      "Iteration 16136: with minibatch training loss = 0.651 and accuracy of 0.78\n",
      "Iteration 16137: with minibatch training loss = 0.935 and accuracy of 0.69\n",
      "Iteration 16138: with minibatch training loss = 0.983 and accuracy of 0.67\n",
      "Iteration 16139: with minibatch training loss = 0.507 and accuracy of 0.83\n",
      "Iteration 16140: with minibatch training loss = 0.448 and accuracy of 0.84\n",
      "Iteration 16141: with minibatch training loss = 0.601 and accuracy of 0.83\n",
      "Iteration 16142: with minibatch training loss = 0.587 and accuracy of 0.81\n",
      "Iteration 16143: with minibatch training loss = 0.561 and accuracy of 0.8\n",
      "Iteration 16144: with minibatch training loss = 0.35 and accuracy of 0.91\n",
      "Iteration 16145: with minibatch training loss = 0.431 and accuracy of 0.88\n",
      "Iteration 16146: with minibatch training loss = 0.459 and accuracy of 0.86\n",
      "Iteration 16147: with minibatch training loss = 0.601 and accuracy of 0.81\n",
      "Iteration 16148: with minibatch training loss = 0.627 and accuracy of 0.78\n",
      "Iteration 16149: with minibatch training loss = 1.01 and accuracy of 0.7\n",
      "Iteration 16150: with minibatch training loss = 0.526 and accuracy of 0.84\n",
      "Iteration 16151: with minibatch training loss = 0.615 and accuracy of 0.81\n",
      "Iteration 16152: with minibatch training loss = 0.611 and accuracy of 0.8\n",
      "Iteration 16153: with minibatch training loss = 0.721 and accuracy of 0.73\n",
      "Iteration 16154: with minibatch training loss = 0.754 and accuracy of 0.78\n",
      "Iteration 16155: with minibatch training loss = 0.722 and accuracy of 0.78\n",
      "Iteration 16156: with minibatch training loss = 0.546 and accuracy of 0.81\n",
      "Iteration 16157: with minibatch training loss = 0.769 and accuracy of 0.75\n",
      "Iteration 16158: with minibatch training loss = 0.671 and accuracy of 0.81\n",
      "Iteration 16159: with minibatch training loss = 0.57 and accuracy of 0.81\n",
      "Iteration 16160: with minibatch training loss = 0.488 and accuracy of 0.86\n",
      "Iteration 16161: with minibatch training loss = 0.669 and accuracy of 0.83\n",
      "Iteration 16162: with minibatch training loss = 0.508 and accuracy of 0.84\n",
      "Iteration 16163: with minibatch training loss = 0.752 and accuracy of 0.77\n",
      "Iteration 16164: with minibatch training loss = 0.513 and accuracy of 0.84\n",
      "Iteration 16165: with minibatch training loss = 0.811 and accuracy of 0.77\n",
      "Iteration 16166: with minibatch training loss = 0.484 and accuracy of 0.89\n",
      "Iteration 16167: with minibatch training loss = 0.32 and accuracy of 0.92\n",
      "Iteration 16168: with minibatch training loss = 0.675 and accuracy of 0.81\n",
      "Iteration 16169: with minibatch training loss = 0.697 and accuracy of 0.8\n",
      "Iteration 16170: with minibatch training loss = 0.673 and accuracy of 0.77\n",
      "Iteration 16171: with minibatch training loss = 0.854 and accuracy of 0.72\n",
      "Iteration 16172: with minibatch training loss = 0.591 and accuracy of 0.81\n",
      "Iteration 16173: with minibatch training loss = 0.514 and accuracy of 0.84\n",
      "Iteration 16174: with minibatch training loss = 0.759 and accuracy of 0.77\n",
      "Iteration 16175: with minibatch training loss = 0.571 and accuracy of 0.81\n",
      "Iteration 16176: with minibatch training loss = 0.741 and accuracy of 0.78\n",
      "Iteration 16177: with minibatch training loss = 0.873 and accuracy of 0.73\n",
      "Iteration 16178: with minibatch training loss = 0.399 and accuracy of 0.86\n",
      "Iteration 16179: with minibatch training loss = 0.862 and accuracy of 0.72\n",
      "Iteration 16180: with minibatch training loss = 0.839 and accuracy of 0.75\n",
      "Iteration 16181: with minibatch training loss = 0.771 and accuracy of 0.78\n",
      "Iteration 16182: with minibatch training loss = 0.395 and accuracy of 0.92\n",
      "Iteration 16183: with minibatch training loss = 0.322 and accuracy of 0.89\n",
      "Iteration 16184: with minibatch training loss = 0.494 and accuracy of 0.83\n",
      "Iteration 16185: with minibatch training loss = 0.753 and accuracy of 0.75\n",
      "Iteration 16186: with minibatch training loss = 0.735 and accuracy of 0.78\n",
      "Iteration 16187: with minibatch training loss = 0.748 and accuracy of 0.75\n",
      "Iteration 16188: with minibatch training loss = 0.522 and accuracy of 0.81\n",
      "Iteration 16189: with minibatch training loss = 0.666 and accuracy of 0.81\n",
      "Iteration 16190: with minibatch training loss = 0.744 and accuracy of 0.8\n",
      "Iteration 16191: with minibatch training loss = 0.628 and accuracy of 0.83\n",
      "Iteration 16192: with minibatch training loss = 0.884 and accuracy of 0.7\n",
      "Iteration 16193: with minibatch training loss = 0.586 and accuracy of 0.81\n",
      "Iteration 16194: with minibatch training loss = 0.812 and accuracy of 0.72\n",
      "Iteration 16195: with minibatch training loss = 0.604 and accuracy of 0.81\n",
      "Iteration 16196: with minibatch training loss = 0.755 and accuracy of 0.77\n",
      "Iteration 16197: with minibatch training loss = 0.559 and accuracy of 0.81\n",
      "Iteration 16198: with minibatch training loss = 0.56 and accuracy of 0.8\n",
      "Iteration 16199: with minibatch training loss = 0.455 and accuracy of 0.86\n",
      "Iteration 16200: with minibatch training loss = 0.747 and accuracy of 0.75\n",
      "Iteration 16201: with minibatch training loss = 0.529 and accuracy of 0.83\n",
      "Iteration 16202: with minibatch training loss = 0.688 and accuracy of 0.78\n",
      "Iteration 16203: with minibatch training loss = 0.579 and accuracy of 0.81\n",
      "Iteration 16204: with minibatch training loss = 0.767 and accuracy of 0.75\n",
      "Iteration 16205: with minibatch training loss = 0.591 and accuracy of 0.81\n",
      "Iteration 16206: with minibatch training loss = 0.587 and accuracy of 0.83\n",
      "Iteration 16207: with minibatch training loss = 0.662 and accuracy of 0.78\n",
      "Iteration 16208: with minibatch training loss = 0.625 and accuracy of 0.8\n",
      "Iteration 16209: with minibatch training loss = 0.567 and accuracy of 0.81\n",
      "Iteration 16210: with minibatch training loss = 0.782 and accuracy of 0.72\n",
      "Iteration 16211: with minibatch training loss = 0.424 and accuracy of 0.88\n",
      "Iteration 16212: with minibatch training loss = 0.903 and accuracy of 0.73\n",
      "Iteration 16213: with minibatch training loss = 0.909 and accuracy of 0.73\n",
      "Iteration 16214: with minibatch training loss = 0.724 and accuracy of 0.8\n",
      "Iteration 16215: with minibatch training loss = 0.967 and accuracy of 0.67\n",
      "Iteration 16216: with minibatch training loss = 0.62 and accuracy of 0.81\n",
      "Iteration 16217: with minibatch training loss = 0.604 and accuracy of 0.8\n",
      "Iteration 16218: with minibatch training loss = 0.667 and accuracy of 0.8\n",
      "Iteration 16219: with minibatch training loss = 0.723 and accuracy of 0.77\n",
      "Iteration 16220: with minibatch training loss = 0.563 and accuracy of 0.8\n",
      "Iteration 16221: with minibatch training loss = 0.63 and accuracy of 0.83\n",
      "Iteration 16222: with minibatch training loss = 0.56 and accuracy of 0.81\n",
      "Iteration 16223: with minibatch training loss = 0.508 and accuracy of 0.86\n",
      "Iteration 16224: with minibatch training loss = 0.75 and accuracy of 0.72\n",
      "Iteration 16225: with minibatch training loss = 0.53 and accuracy of 0.84\n",
      "Iteration 16226: with minibatch training loss = 0.531 and accuracy of 0.83\n",
      "Iteration 16227: with minibatch training loss = 0.737 and accuracy of 0.8\n",
      "Iteration 16228: with minibatch training loss = 0.937 and accuracy of 0.72\n",
      "Iteration 16229: with minibatch training loss = 0.508 and accuracy of 0.84\n",
      "Iteration 16230: with minibatch training loss = 0.564 and accuracy of 0.83\n",
      "Iteration 16231: with minibatch training loss = 0.979 and accuracy of 0.69\n",
      "Iteration 16232: with minibatch training loss = 0.549 and accuracy of 0.83\n",
      "Iteration 16233: with minibatch training loss = 0.673 and accuracy of 0.81\n",
      "Iteration 16234: with minibatch training loss = 0.53 and accuracy of 0.83\n",
      "Iteration 16235: with minibatch training loss = 0.516 and accuracy of 0.83\n",
      "Iteration 16236: with minibatch training loss = 0.679 and accuracy of 0.78\n",
      "Iteration 16237: with minibatch training loss = 0.518 and accuracy of 0.84\n",
      "Iteration 16238: with minibatch training loss = 0.65 and accuracy of 0.8\n",
      "Iteration 16239: with minibatch training loss = 0.505 and accuracy of 0.83\n",
      "Iteration 16240: with minibatch training loss = 0.733 and accuracy of 0.78\n",
      "Iteration 16241: with minibatch training loss = 0.589 and accuracy of 0.84\n",
      "Iteration 16242: with minibatch training loss = 0.702 and accuracy of 0.78\n",
      "Iteration 16243: with minibatch training loss = 0.509 and accuracy of 0.83\n",
      "Iteration 16244: with minibatch training loss = 0.849 and accuracy of 0.77\n",
      "Iteration 16245: with minibatch training loss = 0.663 and accuracy of 0.81\n",
      "Iteration 16246: with minibatch training loss = 0.618 and accuracy of 0.81\n",
      "Iteration 16247: with minibatch training loss = 0.542 and accuracy of 0.83\n",
      "Iteration 16248: with minibatch training loss = 1.01 and accuracy of 0.67\n",
      "Iteration 16249: with minibatch training loss = 0.929 and accuracy of 0.73\n",
      "Iteration 16250: with minibatch training loss = 0.613 and accuracy of 0.83\n",
      "Iteration 16251: with minibatch training loss = 0.902 and accuracy of 0.72\n",
      "Iteration 16252: with minibatch training loss = 0.688 and accuracy of 0.78\n",
      "Iteration 16253: with minibatch training loss = 0.842 and accuracy of 0.72\n",
      "Iteration 16254: with minibatch training loss = 0.398 and accuracy of 0.92\n",
      "Iteration 16255: with minibatch training loss = 1.02 and accuracy of 0.69\n",
      "Iteration 16256: with minibatch training loss = 0.577 and accuracy of 0.83\n",
      "Iteration 16257: with minibatch training loss = 0.659 and accuracy of 0.83\n",
      "Iteration 16258: with minibatch training loss = 0.448 and accuracy of 0.88\n",
      "Iteration 16259: with minibatch training loss = 0.797 and accuracy of 0.75\n",
      "Iteration 16260: with minibatch training loss = 0.542 and accuracy of 0.84\n",
      "Iteration 16261: with minibatch training loss = 0.691 and accuracy of 0.77\n",
      "Iteration 16262: with minibatch training loss = 0.554 and accuracy of 0.81\n",
      "Iteration 16263: with minibatch training loss = 0.48 and accuracy of 0.88\n",
      "Iteration 16264: with minibatch training loss = 0.561 and accuracy of 0.81\n",
      "Iteration 16265: with minibatch training loss = 0.711 and accuracy of 0.73\n",
      "Iteration 16266: with minibatch training loss = 0.784 and accuracy of 0.73\n",
      "Iteration 16267: with minibatch training loss = 0.915 and accuracy of 0.69\n",
      "Iteration 16268: with minibatch training loss = 0.649 and accuracy of 0.8\n",
      "Iteration 16269: with minibatch training loss = 0.727 and accuracy of 0.77\n",
      "Iteration 16270: with minibatch training loss = 0.717 and accuracy of 0.78\n",
      "Iteration 16271: with minibatch training loss = 0.837 and accuracy of 0.72\n",
      "Iteration 16272: with minibatch training loss = 0.689 and accuracy of 0.8\n",
      "Iteration 16273: with minibatch training loss = 0.69 and accuracy of 0.77\n",
      "Iteration 16274: with minibatch training loss = 0.697 and accuracy of 0.83\n",
      "Iteration 16275: with minibatch training loss = 0.599 and accuracy of 0.81\n",
      "Iteration 16276: with minibatch training loss = 0.8 and accuracy of 0.75\n",
      "Iteration 16277: with minibatch training loss = 0.909 and accuracy of 0.69\n",
      "Iteration 16278: with minibatch training loss = 0.659 and accuracy of 0.78\n",
      "Iteration 16279: with minibatch training loss = 0.671 and accuracy of 0.8\n",
      "Iteration 16280: with minibatch training loss = 0.775 and accuracy of 0.78\n",
      "Iteration 16281: with minibatch training loss = 0.491 and accuracy of 0.88\n",
      "Iteration 16282: with minibatch training loss = 0.893 and accuracy of 0.7\n",
      "Iteration 16283: with minibatch training loss = 0.734 and accuracy of 0.77\n",
      "Iteration 16284: with minibatch training loss = 0.82 and accuracy of 0.75\n",
      "Iteration 16285: with minibatch training loss = 0.68 and accuracy of 0.75\n",
      "Iteration 16286: with minibatch training loss = 0.592 and accuracy of 0.8\n",
      "Iteration 16287: with minibatch training loss = 0.755 and accuracy of 0.75\n",
      "Iteration 16288: with minibatch training loss = 0.88 and accuracy of 0.73\n",
      "Iteration 16289: with minibatch training loss = 0.526 and accuracy of 0.84\n",
      "Iteration 16290: with minibatch training loss = 0.755 and accuracy of 0.77\n",
      "Iteration 16291: with minibatch training loss = 0.718 and accuracy of 0.78\n",
      "Iteration 16292: with minibatch training loss = 0.812 and accuracy of 0.73\n",
      "Iteration 16293: with minibatch training loss = 0.466 and accuracy of 0.88\n",
      "Iteration 16294: with minibatch training loss = 0.617 and accuracy of 0.83\n",
      "Iteration 16295: with minibatch training loss = 0.682 and accuracy of 0.78\n",
      "Iteration 16296: with minibatch training loss = 0.523 and accuracy of 0.84\n",
      "Iteration 16297: with minibatch training loss = 0.727 and accuracy of 0.78\n",
      "Iteration 16298: with minibatch training loss = 0.604 and accuracy of 0.81\n",
      "Iteration 16299: with minibatch training loss = 0.443 and accuracy of 0.88\n",
      "Iteration 16300: with minibatch training loss = 0.775 and accuracy of 0.75\n",
      "Iteration 16301: with minibatch training loss = 0.673 and accuracy of 0.78\n",
      "Iteration 16302: with minibatch training loss = 0.527 and accuracy of 0.81\n",
      "Iteration 16303: with minibatch training loss = 0.416 and accuracy of 0.89\n",
      "Iteration 16304: with minibatch training loss = 0.458 and accuracy of 0.86\n",
      "Iteration 16305: with minibatch training loss = 0.875 and accuracy of 0.7\n",
      "Iteration 16306: with minibatch training loss = 0.618 and accuracy of 0.8\n",
      "Iteration 16307: with minibatch training loss = 0.69 and accuracy of 0.8\n",
      "Iteration 16308: with minibatch training loss = 0.389 and accuracy of 0.89\n",
      "Iteration 16309: with minibatch training loss = 0.866 and accuracy of 0.72\n",
      "Iteration 16310: with minibatch training loss = 0.564 and accuracy of 0.81\n",
      "Iteration 16311: with minibatch training loss = 0.773 and accuracy of 0.77\n",
      "Iteration 16312: with minibatch training loss = 0.797 and accuracy of 0.69\n",
      "Iteration 16313: with minibatch training loss = 0.877 and accuracy of 0.72\n",
      "Iteration 16314: with minibatch training loss = 0.611 and accuracy of 0.8\n",
      "Iteration 16315: with minibatch training loss = 0.587 and accuracy of 0.83\n",
      "Iteration 16316: with minibatch training loss = 0.623 and accuracy of 0.81\n",
      "Iteration 16317: with minibatch training loss = 0.593 and accuracy of 0.83\n",
      "Iteration 16318: with minibatch training loss = 0.554 and accuracy of 0.86\n",
      "Iteration 16319: with minibatch training loss = 0.616 and accuracy of 0.81\n",
      "Iteration 16320: with minibatch training loss = 0.641 and accuracy of 0.8\n",
      "Iteration 16321: with minibatch training loss = 0.415 and accuracy of 0.88\n",
      "Iteration 16322: with minibatch training loss = 0.677 and accuracy of 0.83\n",
      "Iteration 16323: with minibatch training loss = 0.698 and accuracy of 0.83\n",
      "Iteration 16324: with minibatch training loss = 0.504 and accuracy of 0.84\n",
      "Iteration 16325: with minibatch training loss = 0.592 and accuracy of 0.81\n",
      "Iteration 16326: with minibatch training loss = 0.511 and accuracy of 0.84\n",
      "Iteration 16327: with minibatch training loss = 0.807 and accuracy of 0.75\n",
      "Iteration 16328: with minibatch training loss = 0.564 and accuracy of 0.81\n",
      "Iteration 16329: with minibatch training loss = 0.607 and accuracy of 0.8\n",
      "Iteration 16330: with minibatch training loss = 0.635 and accuracy of 0.81\n",
      "Iteration 16331: with minibatch training loss = 0.797 and accuracy of 0.73\n",
      "Iteration 16332: with minibatch training loss = 0.73 and accuracy of 0.77\n",
      "Iteration 16333: with minibatch training loss = 0.616 and accuracy of 0.81\n",
      "Iteration 16334: with minibatch training loss = 0.497 and accuracy of 0.84\n",
      "Iteration 16335: with minibatch training loss = 0.597 and accuracy of 0.8\n",
      "Iteration 16336: with minibatch training loss = 0.749 and accuracy of 0.77\n",
      "Iteration 16337: with minibatch training loss = 0.709 and accuracy of 0.78\n",
      "Iteration 16338: with minibatch training loss = 0.581 and accuracy of 0.8\n",
      "Iteration 16339: with minibatch training loss = 0.695 and accuracy of 0.77\n",
      "Iteration 16340: with minibatch training loss = 0.758 and accuracy of 0.73\n",
      "Iteration 16341: with minibatch training loss = 0.568 and accuracy of 0.81\n",
      "Iteration 16342: with minibatch training loss = 0.813 and accuracy of 0.72\n",
      "Iteration 16343: with minibatch training loss = 0.58 and accuracy of 0.84\n",
      "Iteration 16344: with minibatch training loss = 0.861 and accuracy of 0.75\n",
      "Iteration 16345: with minibatch training loss = 0.357 and accuracy of 0.88\n",
      "Iteration 16346: with minibatch training loss = 0.702 and accuracy of 0.77\n",
      "Iteration 16347: with minibatch training loss = 0.471 and accuracy of 0.86\n",
      "Iteration 16348: with minibatch training loss = 0.78 and accuracy of 0.73\n",
      "Iteration 16349: with minibatch training loss = 1.11 and accuracy of 0.64\n",
      "Iteration 16350: with minibatch training loss = 0.618 and accuracy of 0.84\n",
      "Iteration 16351: with minibatch training loss = 0.523 and accuracy of 0.81\n",
      "Iteration 16352: with minibatch training loss = 0.792 and accuracy of 0.75\n",
      "Iteration 16353: with minibatch training loss = 0.673 and accuracy of 0.78\n",
      "Iteration 16354: with minibatch training loss = 0.699 and accuracy of 0.77\n",
      "Iteration 16355: with minibatch training loss = 0.629 and accuracy of 0.83\n",
      "Iteration 16356: with minibatch training loss = 0.492 and accuracy of 0.83\n",
      "Iteration 16357: with minibatch training loss = 0.853 and accuracy of 0.77\n",
      "Iteration 16358: with minibatch training loss = 0.644 and accuracy of 0.78\n",
      "Iteration 16359: with minibatch training loss = 0.872 and accuracy of 0.73\n",
      "Iteration 16360: with minibatch training loss = 0.552 and accuracy of 0.84\n",
      "Iteration 16361: with minibatch training loss = 0.717 and accuracy of 0.77\n",
      "Iteration 16362: with minibatch training loss = 0.659 and accuracy of 0.81\n",
      "Iteration 16363: with minibatch training loss = 0.697 and accuracy of 0.78\n",
      "Iteration 16364: with minibatch training loss = 0.935 and accuracy of 0.72\n",
      "Iteration 16365: with minibatch training loss = 0.898 and accuracy of 0.73\n",
      "Iteration 16366: with minibatch training loss = 0.534 and accuracy of 0.88\n",
      "Iteration 16367: with minibatch training loss = 0.677 and accuracy of 0.78\n",
      "Iteration 16368: with minibatch training loss = 0.582 and accuracy of 0.81\n",
      "Iteration 16369: with minibatch training loss = 0.567 and accuracy of 0.84\n",
      "Iteration 16370: with minibatch training loss = 1.07 and accuracy of 0.67\n",
      "Iteration 16371: with minibatch training loss = 0.532 and accuracy of 0.83\n",
      "Iteration 16372: with minibatch training loss = 0.91 and accuracy of 0.78\n",
      "Iteration 16373: with minibatch training loss = 0.613 and accuracy of 0.83\n",
      "Iteration 16374: with minibatch training loss = 0.673 and accuracy of 0.78\n",
      "Iteration 16375: with minibatch training loss = 0.729 and accuracy of 0.73\n",
      "Iteration 16376: with minibatch training loss = 0.528 and accuracy of 0.86\n",
      "Iteration 16377: with minibatch training loss = 0.557 and accuracy of 0.83\n",
      "Iteration 16378: with minibatch training loss = 0.608 and accuracy of 0.81\n",
      "Iteration 16379: with minibatch training loss = 0.599 and accuracy of 0.81\n",
      "Iteration 16380: with minibatch training loss = 0.955 and accuracy of 0.67\n",
      "Iteration 16381: with minibatch training loss = 0.542 and accuracy of 0.84\n",
      "Iteration 16382: with minibatch training loss = 0.541 and accuracy of 0.81\n",
      "Iteration 16383: with minibatch training loss = 0.45 and accuracy of 0.86\n",
      "Iteration 16384: with minibatch training loss = 0.626 and accuracy of 0.81\n",
      "Iteration 16385: with minibatch training loss = 0.853 and accuracy of 0.72\n",
      "Iteration 16386: with minibatch training loss = 0.609 and accuracy of 0.86\n",
      "Iteration 16387: with minibatch training loss = 0.865 and accuracy of 0.72\n",
      "Iteration 16388: with minibatch training loss = 0.784 and accuracy of 0.77\n",
      "Iteration 16389: with minibatch training loss = 0.864 and accuracy of 0.73\n",
      "Iteration 16390: with minibatch training loss = 0.765 and accuracy of 0.77\n",
      "Iteration 16391: with minibatch training loss = 0.549 and accuracy of 0.83\n",
      "Iteration 16392: with minibatch training loss = 0.688 and accuracy of 0.78\n",
      "Iteration 16393: with minibatch training loss = 0.914 and accuracy of 0.73\n",
      "Iteration 16394: with minibatch training loss = 0.563 and accuracy of 0.83\n",
      "Iteration 16395: with minibatch training loss = 0.659 and accuracy of 0.77\n",
      "Iteration 16396: with minibatch training loss = 0.527 and accuracy of 0.8\n",
      "Iteration 16397: with minibatch training loss = 0.697 and accuracy of 0.8\n",
      "Iteration 16398: with minibatch training loss = 0.542 and accuracy of 0.81\n",
      "Iteration 16399: with minibatch training loss = 0.607 and accuracy of 0.77\n",
      "Iteration 16400: with minibatch training loss = 0.518 and accuracy of 0.84\n",
      "Iteration 16401: with minibatch training loss = 0.639 and accuracy of 0.78\n",
      "Iteration 16402: with minibatch training loss = 0.61 and accuracy of 0.83\n",
      "Iteration 16403: with minibatch training loss = 0.885 and accuracy of 0.72\n",
      "Iteration 16404: with minibatch training loss = 0.47 and accuracy of 0.86\n",
      "Iteration 16405: with minibatch training loss = 0.445 and accuracy of 0.86\n",
      "Iteration 16406: with minibatch training loss = 0.513 and accuracy of 0.88\n",
      "Iteration 16407: with minibatch training loss = 0.648 and accuracy of 0.8\n",
      "Iteration 16408: with minibatch training loss = 0.918 and accuracy of 0.67\n",
      "Iteration 16409: with minibatch training loss = 0.675 and accuracy of 0.78\n",
      "Iteration 16410: with minibatch training loss = 0.602 and accuracy of 0.81\n",
      "Iteration 16411: with minibatch training loss = 0.876 and accuracy of 0.72\n",
      "Iteration 16412: with minibatch training loss = 0.694 and accuracy of 0.78\n",
      "Iteration 16413: with minibatch training loss = 0.725 and accuracy of 0.77\n",
      "Iteration 16414: with minibatch training loss = 0.415 and accuracy of 0.88\n",
      "Iteration 16415: with minibatch training loss = 0.471 and accuracy of 0.84\n",
      "Iteration 16416: with minibatch training loss = 0.769 and accuracy of 0.77\n",
      "Iteration 16417: with minibatch training loss = 0.611 and accuracy of 0.83\n",
      "Iteration 16418: with minibatch training loss = 0.694 and accuracy of 0.8\n",
      "Iteration 16419: with minibatch training loss = 0.628 and accuracy of 0.81\n",
      "Iteration 16420: with minibatch training loss = 0.632 and accuracy of 0.81\n",
      "Iteration 16421: with minibatch training loss = 0.616 and accuracy of 0.8\n",
      "Iteration 16422: with minibatch training loss = 0.623 and accuracy of 0.83\n",
      "Iteration 16423: with minibatch training loss = 0.612 and accuracy of 0.8\n",
      "Iteration 16424: with minibatch training loss = 0.773 and accuracy of 0.78\n",
      "Iteration 16425: with minibatch training loss = 0.64 and accuracy of 0.8\n",
      "Iteration 16426: with minibatch training loss = 0.531 and accuracy of 0.84\n",
      "Iteration 16427: with minibatch training loss = 0.57 and accuracy of 0.81\n",
      "Iteration 16428: with minibatch training loss = 0.308 and accuracy of 0.91\n",
      "Iteration 16429: with minibatch training loss = 0.884 and accuracy of 0.69\n",
      "Iteration 16430: with minibatch training loss = 0.69 and accuracy of 0.83\n",
      "Iteration 16431: with minibatch training loss = 0.436 and accuracy of 0.89\n",
      "Iteration 16432: with minibatch training loss = 0.795 and accuracy of 0.75\n",
      "Iteration 16433: with minibatch training loss = 0.533 and accuracy of 0.84\n",
      "Iteration 16434: with minibatch training loss = 0.494 and accuracy of 0.84\n",
      "Iteration 16435: with minibatch training loss = 0.532 and accuracy of 0.86\n",
      "Iteration 16436: with minibatch training loss = 0.54 and accuracy of 0.81\n",
      "Iteration 16437: with minibatch training loss = 0.561 and accuracy of 0.84\n",
      "Iteration 16438: with minibatch training loss = 0.792 and accuracy of 0.78\n",
      "Iteration 16439: with minibatch training loss = 0.636 and accuracy of 0.77\n",
      "Iteration 16440: with minibatch training loss = 0.693 and accuracy of 0.78\n",
      "Iteration 16441: with minibatch training loss = 0.672 and accuracy of 0.78\n",
      "Iteration 16442: with minibatch training loss = 0.38 and accuracy of 0.88\n",
      "Iteration 16443: with minibatch training loss = 0.516 and accuracy of 0.83\n",
      "Iteration 16444: with minibatch training loss = 0.479 and accuracy of 0.84\n",
      "Iteration 16445: with minibatch training loss = 0.657 and accuracy of 0.8\n",
      "Iteration 16446: with minibatch training loss = 0.748 and accuracy of 0.75\n",
      "Iteration 16447: with minibatch training loss = 0.41 and accuracy of 0.86\n",
      "Iteration 16448: with minibatch training loss = 0.584 and accuracy of 0.83\n",
      "Iteration 16449: with minibatch training loss = 0.67 and accuracy of 0.75\n",
      "Iteration 16450: with minibatch training loss = 0.732 and accuracy of 0.78\n",
      "Iteration 16451: with minibatch training loss = 0.677 and accuracy of 0.8\n",
      "Iteration 16452: with minibatch training loss = 0.469 and accuracy of 0.83\n",
      "Iteration 16453: with minibatch training loss = 0.575 and accuracy of 0.84\n",
      "Iteration 16454: with minibatch training loss = 0.355 and accuracy of 0.91\n",
      "Iteration 16455: with minibatch training loss = 0.642 and accuracy of 0.81\n",
      "Iteration 16456: with minibatch training loss = 0.629 and accuracy of 0.81\n",
      "Iteration 16457: with minibatch training loss = 0.429 and accuracy of 0.84\n",
      "Iteration 16458: with minibatch training loss = 0.876 and accuracy of 0.75\n",
      "Iteration 16459: with minibatch training loss = 0.487 and accuracy of 0.83\n",
      "Iteration 16460: with minibatch training loss = 0.622 and accuracy of 0.78\n",
      "Iteration 16461: with minibatch training loss = 0.577 and accuracy of 0.8\n",
      "Iteration 16462: with minibatch training loss = 0.554 and accuracy of 0.88\n",
      "Iteration 16463: with minibatch training loss = 0.623 and accuracy of 0.78\n",
      "Iteration 16464: with minibatch training loss = 0.721 and accuracy of 0.78\n",
      "Iteration 16465: with minibatch training loss = 0.885 and accuracy of 0.7\n",
      "Iteration 16466: with minibatch training loss = 0.714 and accuracy of 0.77\n",
      "Iteration 16467: with minibatch training loss = 0.548 and accuracy of 0.83\n",
      "Iteration 16468: with minibatch training loss = 0.797 and accuracy of 0.77\n",
      "Iteration 16469: with minibatch training loss = 0.556 and accuracy of 0.8\n",
      "Iteration 16470: with minibatch training loss = 0.71 and accuracy of 0.81\n",
      "Iteration 16471: with minibatch training loss = 0.689 and accuracy of 0.78\n",
      "Iteration 16472: with minibatch training loss = 0.428 and accuracy of 0.86\n",
      "Iteration 16473: with minibatch training loss = 0.644 and accuracy of 0.8\n",
      "Iteration 16474: with minibatch training loss = 0.758 and accuracy of 0.78\n",
      "Iteration 16475: with minibatch training loss = 0.761 and accuracy of 0.75\n",
      "Iteration 16476: with minibatch training loss = 0.644 and accuracy of 0.8\n",
      "Iteration 16477: with minibatch training loss = 0.839 and accuracy of 0.77\n",
      "Iteration 16478: with minibatch training loss = 0.995 and accuracy of 0.7\n",
      "Iteration 16479: with minibatch training loss = 0.796 and accuracy of 0.72\n",
      "Iteration 16480: with minibatch training loss = 0.537 and accuracy of 0.84\n",
      "Iteration 16481: with minibatch training loss = 0.654 and accuracy of 0.81\n",
      "Iteration 16482: with minibatch training loss = 0.707 and accuracy of 0.77\n",
      "Iteration 16483: with minibatch training loss = 0.664 and accuracy of 0.77\n",
      "Iteration 16484: with minibatch training loss = 0.639 and accuracy of 0.8\n",
      "Iteration 16485: with minibatch training loss = 0.696 and accuracy of 0.77\n",
      "Iteration 16486: with minibatch training loss = 0.608 and accuracy of 0.8\n",
      "Iteration 16487: with minibatch training loss = 0.713 and accuracy of 0.78\n",
      "Iteration 16488: with minibatch training loss = 0.8 and accuracy of 0.75\n",
      "Iteration 16489: with minibatch training loss = 0.699 and accuracy of 0.78\n",
      "Iteration 16490: with minibatch training loss = 0.659 and accuracy of 0.78\n",
      "Iteration 16491: with minibatch training loss = 0.762 and accuracy of 0.77\n",
      "Iteration 16492: with minibatch training loss = 0.755 and accuracy of 0.75\n",
      "Iteration 16493: with minibatch training loss = 0.573 and accuracy of 0.84\n",
      "Iteration 16494: with minibatch training loss = 0.742 and accuracy of 0.78\n",
      "Iteration 16495: with minibatch training loss = 0.439 and accuracy of 0.88\n",
      "Iteration 16496: with minibatch training loss = 0.662 and accuracy of 0.77\n",
      "Iteration 16497: with minibatch training loss = 0.531 and accuracy of 0.84\n",
      "Iteration 16498: with minibatch training loss = 0.561 and accuracy of 0.81\n",
      "Iteration 16499: with minibatch training loss = 0.842 and accuracy of 0.77\n",
      "Iteration 16500: with minibatch training loss = 0.757 and accuracy of 0.73\n",
      "Iteration 16501: with minibatch training loss = 0.971 and accuracy of 0.66\n",
      "Iteration 16502: with minibatch training loss = 0.664 and accuracy of 0.8\n",
      "Iteration 16503: with minibatch training loss = 0.593 and accuracy of 0.8\n",
      "Iteration 16504: with minibatch training loss = 0.656 and accuracy of 0.81\n",
      "Iteration 16505: with minibatch training loss = 0.528 and accuracy of 0.86\n",
      "Iteration 16506: with minibatch training loss = 0.435 and accuracy of 0.84\n",
      "Iteration 16507: with minibatch training loss = 0.394 and accuracy of 0.91\n",
      "Iteration 16508: with minibatch training loss = 0.647 and accuracy of 0.81\n",
      "Iteration 16509: with minibatch training loss = 0.808 and accuracy of 0.75\n",
      "Iteration 16510: with minibatch training loss = 0.767 and accuracy of 0.73\n",
      "Iteration 16511: with minibatch training loss = 0.803 and accuracy of 0.72\n",
      "Iteration 16512: with minibatch training loss = 0.695 and accuracy of 0.75\n",
      "Iteration 16513: with minibatch training loss = 0.716 and accuracy of 0.77\n",
      "Iteration 16514: with minibatch training loss = 0.638 and accuracy of 0.8\n",
      "Iteration 16515: with minibatch training loss = 0.486 and accuracy of 0.84\n",
      "Iteration 16516: with minibatch training loss = 0.673 and accuracy of 0.81\n",
      "Iteration 16517: with minibatch training loss = 0.609 and accuracy of 0.8\n",
      "Iteration 16518: with minibatch training loss = 0.999 and accuracy of 0.66\n",
      "Iteration 16519: with minibatch training loss = 0.962 and accuracy of 0.7\n",
      "Iteration 16520: with minibatch training loss = 0.672 and accuracy of 0.75\n",
      "Iteration 16521: with minibatch training loss = 0.672 and accuracy of 0.83\n",
      "Iteration 16522: with minibatch training loss = 0.735 and accuracy of 0.75\n",
      "Iteration 16523: with minibatch training loss = 0.748 and accuracy of 0.77\n",
      "Iteration 16524: with minibatch training loss = 0.619 and accuracy of 0.81\n",
      "Iteration 16525: with minibatch training loss = 0.644 and accuracy of 0.8\n",
      "Iteration 16526: with minibatch training loss = 0.716 and accuracy of 0.78\n",
      "Iteration 16527: with minibatch training loss = 0.759 and accuracy of 0.78\n",
      "Iteration 16528: with minibatch training loss = 0.602 and accuracy of 0.84\n",
      "Iteration 16529: with minibatch training loss = 0.376 and accuracy of 0.88\n",
      "Iteration 16530: with minibatch training loss = 0.798 and accuracy of 0.73\n",
      "Iteration 16531: with minibatch training loss = 0.587 and accuracy of 0.81\n",
      "Iteration 16532: with minibatch training loss = 0.443 and accuracy of 0.89\n",
      "Iteration 16533: with minibatch training loss = 0.583 and accuracy of 0.8\n",
      "Iteration 16534: with minibatch training loss = 0.639 and accuracy of 0.8\n",
      "Iteration 16535: with minibatch training loss = 0.36 and accuracy of 0.88\n",
      "Iteration 16536: with minibatch training loss = 0.916 and accuracy of 0.75\n",
      "Iteration 16537: with minibatch training loss = 0.781 and accuracy of 0.77\n",
      "Iteration 16538: with minibatch training loss = 0.83 and accuracy of 0.75\n",
      "Iteration 16539: with minibatch training loss = 0.858 and accuracy of 0.72\n",
      "Iteration 16540: with minibatch training loss = 0.547 and accuracy of 0.83\n",
      "Iteration 16541: with minibatch training loss = 0.613 and accuracy of 0.81\n",
      "Iteration 16542: with minibatch training loss = 0.864 and accuracy of 0.75\n",
      "Iteration 16543: with minibatch training loss = 0.593 and accuracy of 0.8\n",
      "Iteration 16544: with minibatch training loss = 0.46 and accuracy of 0.84\n",
      "Iteration 16545: with minibatch training loss = 0.571 and accuracy of 0.81\n",
      "Iteration 16546: with minibatch training loss = 0.582 and accuracy of 0.83\n",
      "Iteration 16547: with minibatch training loss = 0.925 and accuracy of 0.72\n",
      "Iteration 16548: with minibatch training loss = 0.489 and accuracy of 0.86\n",
      "Iteration 16549: with minibatch training loss = 0.818 and accuracy of 0.7\n",
      "Iteration 16550: with minibatch training loss = 0.747 and accuracy of 0.75\n",
      "Iteration 16551: with minibatch training loss = 0.686 and accuracy of 0.8\n",
      "Iteration 16552: with minibatch training loss = 0.987 and accuracy of 0.67\n",
      "Iteration 16553: with minibatch training loss = 0.928 and accuracy of 0.69\n",
      "Iteration 16554: with minibatch training loss = 0.622 and accuracy of 0.81\n",
      "Iteration 16555: with minibatch training loss = 0.626 and accuracy of 0.81\n",
      "Iteration 16556: with minibatch training loss = 0.878 and accuracy of 0.72\n",
      "Iteration 16557: with minibatch training loss = 0.425 and accuracy of 0.86\n",
      "Iteration 16558: with minibatch training loss = 0.693 and accuracy of 0.78\n",
      "Iteration 16559: with minibatch training loss = 0.583 and accuracy of 0.84\n",
      "Iteration 16560: with minibatch training loss = 0.95 and accuracy of 0.69\n",
      "Iteration 16561: with minibatch training loss = 0.62 and accuracy of 0.81\n",
      "Iteration 16562: with minibatch training loss = 0.71 and accuracy of 0.75\n",
      "Iteration 16563: with minibatch training loss = 0.666 and accuracy of 0.78\n",
      "Iteration 16564: with minibatch training loss = 0.599 and accuracy of 0.84\n",
      "Iteration 16565: with minibatch training loss = 0.486 and accuracy of 0.84\n",
      "Iteration 16566: with minibatch training loss = 0.814 and accuracy of 0.72\n",
      "Iteration 16567: with minibatch training loss = 0.596 and accuracy of 0.83\n",
      "Iteration 16568: with minibatch training loss = 0.835 and accuracy of 0.77\n",
      "Iteration 16569: with minibatch training loss = 0.761 and accuracy of 0.78\n",
      "Iteration 16570: with minibatch training loss = 0.465 and accuracy of 0.86\n",
      "Iteration 16571: with minibatch training loss = 1.07 and accuracy of 0.66\n",
      "Iteration 16572: with minibatch training loss = 0.674 and accuracy of 0.77\n",
      "Iteration 16573: with minibatch training loss = 0.665 and accuracy of 0.75\n",
      "Iteration 16574: with minibatch training loss = 0.396 and accuracy of 0.88\n",
      "Iteration 16575: with minibatch training loss = 0.498 and accuracy of 0.88\n",
      "Iteration 16576: with minibatch training loss = 0.851 and accuracy of 0.75\n",
      "Iteration 16577: with minibatch training loss = 0.592 and accuracy of 0.83\n",
      "Iteration 16578: with minibatch training loss = 0.554 and accuracy of 0.83\n",
      "Iteration 16579: with minibatch training loss = 0.775 and accuracy of 0.77\n",
      "Iteration 16580: with minibatch training loss = 0.865 and accuracy of 0.7\n",
      "Iteration 16581: with minibatch training loss = 0.831 and accuracy of 0.75\n",
      "Iteration 16582: with minibatch training loss = 0.653 and accuracy of 0.8\n",
      "Iteration 16583: with minibatch training loss = 1.13 and accuracy of 0.61\n",
      "Iteration 16584: with minibatch training loss = 0.57 and accuracy of 0.81\n",
      "Iteration 16585: with minibatch training loss = 0.701 and accuracy of 0.78\n",
      "Iteration 16586: with minibatch training loss = 0.689 and accuracy of 0.81\n",
      "Iteration 16587: with minibatch training loss = 0.571 and accuracy of 0.86\n",
      "Iteration 16588: with minibatch training loss = 0.684 and accuracy of 0.78\n",
      "Iteration 16589: with minibatch training loss = 0.714 and accuracy of 0.81\n",
      "Iteration 16590: with minibatch training loss = 0.771 and accuracy of 0.77\n",
      "Iteration 16591: with minibatch training loss = 0.759 and accuracy of 0.72\n",
      "Iteration 16592: with minibatch training loss = 0.543 and accuracy of 0.81\n",
      "Iteration 16593: with minibatch training loss = 0.718 and accuracy of 0.8\n",
      "Iteration 16594: with minibatch training loss = 0.632 and accuracy of 0.8\n",
      "Iteration 16595: with minibatch training loss = 0.858 and accuracy of 0.75\n",
      "Iteration 16596: with minibatch training loss = 0.616 and accuracy of 0.83\n",
      "Iteration 16597: with minibatch training loss = 0.777 and accuracy of 0.75\n",
      "Iteration 16598: with minibatch training loss = 0.665 and accuracy of 0.8\n",
      "Iteration 16599: with minibatch training loss = 0.747 and accuracy of 0.78\n",
      "Iteration 16600: with minibatch training loss = 0.411 and accuracy of 0.88\n",
      "Iteration 16601: with minibatch training loss = 0.411 and accuracy of 0.89\n",
      "Iteration 16602: with minibatch training loss = 0.789 and accuracy of 0.81\n",
      "Iteration 16603: with minibatch training loss = 0.789 and accuracy of 0.75\n",
      "Iteration 16604: with minibatch training loss = 0.652 and accuracy of 0.78\n",
      "Iteration 16605: with minibatch training loss = 0.843 and accuracy of 0.73\n",
      "Iteration 16606: with minibatch training loss = 0.41 and accuracy of 0.88\n",
      "Iteration 16607: with minibatch training loss = 0.674 and accuracy of 0.81\n",
      "Iteration 16608: with minibatch training loss = 0.651 and accuracy of 0.77\n",
      "Iteration 16609: with minibatch training loss = 0.694 and accuracy of 0.8\n",
      "Iteration 16610: with minibatch training loss = 0.649 and accuracy of 0.81\n",
      "Iteration 16611: with minibatch training loss = 0.557 and accuracy of 0.83\n",
      "Iteration 16612: with minibatch training loss = 0.698 and accuracy of 0.83\n",
      "Iteration 16613: with minibatch training loss = 0.864 and accuracy of 0.73\n",
      "Iteration 16614: with minibatch training loss = 0.636 and accuracy of 0.84\n",
      "Iteration 16615: with minibatch training loss = 0.604 and accuracy of 0.81\n",
      "Iteration 16616: with minibatch training loss = 0.566 and accuracy of 0.8\n",
      "Iteration 16617: with minibatch training loss = 0.623 and accuracy of 0.78\n",
      "Iteration 16618: with minibatch training loss = 0.839 and accuracy of 0.72\n",
      "Iteration 16619: with minibatch training loss = 0.522 and accuracy of 0.88\n",
      "Iteration 16620: with minibatch training loss = 0.709 and accuracy of 0.78\n",
      "Iteration 16621: with minibatch training loss = 0.719 and accuracy of 0.8\n",
      "Iteration 16622: with minibatch training loss = 0.745 and accuracy of 0.75\n",
      "Iteration 16623: with minibatch training loss = 0.782 and accuracy of 0.77\n",
      "Iteration 16624: with minibatch training loss = 0.634 and accuracy of 0.83\n",
      "Iteration 16625: with minibatch training loss = 0.652 and accuracy of 0.81\n",
      "Iteration 16626: with minibatch training loss = 0.797 and accuracy of 0.73\n",
      "Iteration 16627: with minibatch training loss = 0.717 and accuracy of 0.78\n",
      "Iteration 16628: with minibatch training loss = 0.338 and accuracy of 0.88\n",
      "Iteration 16629: with minibatch training loss = 0.606 and accuracy of 0.81\n",
      "Iteration 16630: with minibatch training loss = 0.476 and accuracy of 0.86\n",
      "Iteration 16631: with minibatch training loss = 0.694 and accuracy of 0.78\n",
      "Iteration 16632: with minibatch training loss = 0.593 and accuracy of 0.78\n",
      "Iteration 16633: with minibatch training loss = 0.534 and accuracy of 0.88\n",
      "Iteration 16634: with minibatch training loss = 0.693 and accuracy of 0.8\n",
      "Iteration 16635: with minibatch training loss = 0.591 and accuracy of 0.81\n",
      "Iteration 16636: with minibatch training loss = 0.592 and accuracy of 0.8\n",
      "Iteration 16637: with minibatch training loss = 0.693 and accuracy of 0.72\n",
      "Iteration 16638: with minibatch training loss = 0.459 and accuracy of 0.88\n",
      "Iteration 16639: with minibatch training loss = 0.779 and accuracy of 0.77\n",
      "Iteration 16640: with minibatch training loss = 0.527 and accuracy of 0.86\n",
      "Iteration 16641: with minibatch training loss = 0.805 and accuracy of 0.73\n",
      "Iteration 16642: with minibatch training loss = 0.485 and accuracy of 0.83\n",
      "Iteration 16643: with minibatch training loss = 0.843 and accuracy of 0.73\n",
      "Iteration 16644: with minibatch training loss = 0.776 and accuracy of 0.72\n",
      "Iteration 16645: with minibatch training loss = 0.855 and accuracy of 0.75\n",
      "Iteration 16646: with minibatch training loss = 0.562 and accuracy of 0.81\n",
      "Iteration 16647: with minibatch training loss = 0.605 and accuracy of 0.81\n",
      "Iteration 16648: with minibatch training loss = 0.769 and accuracy of 0.77\n",
      "Iteration 16649: with minibatch training loss = 0.608 and accuracy of 0.86\n",
      "Iteration 16650: with minibatch training loss = 0.607 and accuracy of 0.86\n",
      "Iteration 16651: with minibatch training loss = 0.666 and accuracy of 0.8\n",
      "Iteration 16652: with minibatch training loss = 0.82 and accuracy of 0.73\n",
      "Iteration 16653: with minibatch training loss = 0.487 and accuracy of 0.84\n",
      "Iteration 16654: with minibatch training loss = 0.738 and accuracy of 0.78\n",
      "Validation loss: 0.2847138\n",
      "Epoch 12, Overall loss = 0.657 and accuracy of 0.798\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4xLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvAOZPmwAAIABJREFUeJzsnXmYFMX5x7/v3tywHCvIsSgIAnLI\niggeo6jx1hhvE4lnojlM9JeIR7xiEmMSjRoTQ9TEKBrvW0FERuS+b5ZjYReWm4XdZdmDPer3R3fP\n9PT0UX3N9MzU53n22enu6qq3q6vrreOtt4gxBoFAIBBkLlnJFkAgEAgEyUUoAoFAIMhwhCIQCASC\nDEcoAoFAIMhwhCIQCASCDEcoAoFAIMhwhCIQCGSIiBHRoGTLIRAkGqEIBIGEiMqJqIGI6lR/f0u2\nXApENIKIZhDRASJimmv5RPQyEVUQ0WEiWklEF5rE9UMimuu/1AKBPkIRCILMpYyxjqq/nyZbIBXN\nAN4GcKvOtRwAOwCcBaALgIcAvE1ExYkSTiCwg1AEgpRDbkHPI6K/EVENEZUS0STV9T5E9DERHSSi\nLUR0u+paNhE9QERlcmt9GRH1U0V/LhFtJqJqInqBiEhPBsbYRsbYywDW6Vw7whh7lDFWzhhrY4x9\nCmAbgLEOntXsWcYR0VIiqiWivUT0tHy+gIheJ6Iq+TmWEFGR3bQFmUNOsgUQCBxyKoB3AfQAcCWA\n94loIGPsIID/AVgLoA+AoQBmElEZY+xrAPcAuB7ARQA2ARgJoF4V7yUATgHQGcAyAJ8AmO5GULkS\nPgE6SoMDs2d5FsCzjLHXiKgjgBHyPZMh9UT6AWgCMBpAg5tnEKQ3okcgCDIfyi1a5e921bV9AP7K\nGGtmjL0FYCOAi+XW/UQA9zHGGhljKwG8BOAm+b7bADwkt+gZY2wVY6xKFe+TjLFqxth2ALMhVaKO\nIaJcANMAvMoYK7V5r9WzNAMYREQ9GGN1jLGFqvPdAQxijLUyxpYxxmrdPIcgvRGKQBBkrmCMdVX9\n/Ut1bSeL9ZhYAanV3AfAQcbYYc21Y+Xf/QCUmaS5R/W7HkBHp8ITURaA1wAcBeBkfsPqWW6F1NMo\nlYd/LpHPvwZgBoD/EdEuInpKVkgCgS5CEQhSlWM14/f9AeyS/wqJqJPm2k759w4Ax/stnCzbywCK\nAHyPMdbsIBrTZ2GMbWaMXQ+gF4A/AniXiDrIvaTHGGPDAEyANNx1EwQCA4QiEKQqvQD8nIhyiehq\nACcC+JwxtgPAfAB/kCdNR0JqOb8u3/cSgN8S0WCSGElE3e0mLt9bACBPPi4gonxVkH/IMl3KGOMZ\nnyc5jsif1bMQ0feJqCdjrA1AtRxPGxGdTUQnEVE2gFpIQ0Vtdp9RkDmIyWJBkPmEiFpVxzMZY9+V\nfy8CMBjAAQB7AVylGuu/HsCLkFrUhwA8whj7Sr72NIB8AF9CmmguBaDEaYcBkCyBFBogDdsUE9EA\nAD+CNFG7R9Vx+RFjbJpBfBOgmdCVh3PMnuUCAE8TUXs57esYYw1EdIx8T18AdQDegjRcJBDoQmJj\nGkGqQUQ/BHAbY+z0ZMsiEKQDYmhIIBAIMhyhCAQCgSDDEUNDAoFAkOGIHoFAIBBkOClhNdSjRw9W\nXFzs6N4jR46gQ4cO3grkI0JefxHy+ouQ1z+cyLps2bIDjLGelgEZY4H/Gzt2LHPK7NmzHd+bDIS8\n/iLk9Rchr384kRXAUsZRx4qhIYFAIMhwhCIQCASCDEcoAoFAIMhwhCIQCASCDEcoAoFAIMhwhCIQ\nCASCDEcoAoFAIMhwhCIQCAQCn/hm037sOFhvHTDJpMTKYoFAIEhFJr+yGLnZhM2/uyjZopgiegQC\ngUDgI82twXfsKRSBIOl8s2k/NuyuTbYYAkHGIhSBDp+s2oWaBid7jQucMPmVxbjw2W+TLYZAkLEI\nRaBh6/46/OzNFfjlWyuTLYpAIBAkBKEINDQ0S3ul76pusAgpEAi8ZuWOahz/wOfYV9uYbFEyCt8U\nARENIaKVqr9aIvoFERUS0Uwi2iz/7+aXDAKBILX497xtaG1jmF9WlWxRMgrfFAFjbCNjbDRjbDSA\nsQDqAXwAYAqAWYyxwQBmyccCgUAgSBKJGhqaBKCMMVYB4HIAr8rnXwVwRYJk4ELZwpmIkiuIQJCB\niC3Uk0OiFpRdB+BN+XcRY2y3/HsPgCK9G4joDgB3AEBRURHC4bCjhOvq6mzdW1ErzREcsXmfV9iV\nN9l4KW8injuT8zcRuJV33z5pbmDDhg3oWrPZI6mMSVT+epGGr7LybGPm5g9AHoADkBQAAFRrrh+y\niiORW1WuqaxmA+77lF341zmO03RDKm2dx5g38g6471M24L5P3QvDQSbmbyJxK+9P31jOBtz3Kftw\nRaU3Alngd/56WbZTfavKCwEsZ4ztlY/3ElFvAJD/70uADAKBQCAwIBGK4HpEh4UA4GMAk+XfkwF8\nlAAZMpLaxuaUcHglEKQTZfvrMGPdnmSLYQtfFQERdQBwHoD3VaefBHAeEW0GcK58HBiik8XJlUML\niw6lcXPp83NxxlOzfZJIIBDoMekv3+BHry1Lthi28FURMMaOMMa6M8ZqVOeqGGOTGGODGWPnMsYO\n+ilDulDyxFeY+OTXtu6pqBK9AT+oP9qC4imf4d/ztiVblLRDr7GzrOIQ5m85kARpMgexsjhFqDpy\nFLtqxGrLIFBVdxQA8PJcoQj8Qm2+/b1/zMcNLy1KojTpj1AEBpgNDTUcbUXD0dbECSMIFMLWXZBu\nCEXggOGPTMeJD09Pthie0tTSansOItMJ2jySQOAUoQg0MFhXhm1pVl82HG3FkIem46kZG5MtiiDD\nSbNPK2UQisAAQuY09w43SXsvvLO0MsmSCAQSmfP1BQOhCDRk4uiIXyazr8zdhqlzyryNNADw9BoT\nQUtrG/49bxuaW9uSLYp3BCNrMw6xeb0gqgg8jvfxT9cDAO4483iPYxYAwH8XVODxT9ejubVN5LHA\nFaJHIIi0cLPE7GdKUdsoDenVNbYkWRJBqiMUgQalZ5pJdWJbQFdTe83RljY0ezDTH+1BBSTD0vDF\npeEjBRoxNCSImI2m+7dX8sRMHG5swbZz3MVz+QvzvBHIJek4nxWU+ZdMQ/QIBBmzGU9tY4sn1UxN\nQ7MHsbgn0ntNogzFUz7D0zM3eR5vYHpbGYJQBAZkUjEMoqO9ppbWyBh4UAlKfpnJwRjDgrIqzxcL\nlu2vw6vzywEAz83ybgOZdOzlpAJCEWjIxNW1QZwsvn7qQox89MtkixFsOMrqRyt34fp/LcQ7y7xd\nI3LFC/PwyMfrPI1TkDyEIjAiQJWi3wRxsnj59upkixB4okNDxi9O8UDr9b4UdU3CUimdEIpAkDGT\nxelGMof0/O49BqlRUnmoHsVTPsOayhrrwCmKUAQaMm9gKPrMQRoaCiLbDhxBTX103iIouZUMObLT\nqKwMfvBzvPTtVsPr32zaDwB4Y3FFokRKOEIRCKLzIunzbfvC2X8O4+Lnv022GBGUuR2eOtnrV+uX\nHkjGFF1zK8MTn20wvJ6XLVWTR1v0hWtsbkVjc2q7pRfrCAS+uZhIRyoPNXCH3XGwHv0K2/smi5nZ\n7/aqel+trrKz0qO08BiH5MqKwMin05jHZ+JoaxvKfn+R7biDgugRaMjESlEMDXnPRyt34oynZmPu\nZv+2WDSrZs7802xc8vxcz9Jqbm3Di9+UoalFavmmy9AQT1WdG+kR6CuChuZWtKa4b3qhCAxIk3LO\nRRvjH2KwYsPuWnz/pUUxXeVUahl5xcodktVT6Z5a39JIZLZOW1iBJ78oxb/mSGPpWT73CBL1+fHk\nYV6OeY/ATdxBQSgCQaTAetEjePijtZi75QBW7Yiaf/r5QTQ2t6J4ymd4fWEwJ/L8XK1tZ47ALUfk\nrVmV/37pgUS7mDBK7eW52yLKPDdbelgv/FQFFaEIBJEegRfo2bS7jb10T61hr6JatuLRrm5tbm3D\n/72zChVVR1ymHnz8csfw9tIdeHvJDt1rfs8RJKpHblQ2f/vpelwh+5SKThbbmxBOJbUhFEEcqfT6\nvMEPX0PqXHQzNLSs4hAu+Ou3eHnuNt3rRiIvqziEd5dV4lfvrnacthVm+ZWQYQGOdQRuWti/fnc1\nfv2efv6li18qnveUm2M+R5AOCEWgISMniyNDQx5EphOHmzpRWRG7Zqe0mGfYw9Pxy7dW8keQZL3u\nVzl6ZuYm/FMer8+UdQRNLa3Yf7jJ0zjtFA+7I0OpNDeWsYpg/a5atHBM/nyyahf21DQmQKLk4eVk\nsR5uvgdti7b+aCs+WLFTJ1wsXj5K6Z5a3QqotqEZxVM+izhf08OvquBZDx29OcG3OQKTDPvJtBU4\n5XdfeZygt9GlKr4qAiLqSkTvElEpEW0gotOIqJCIZhLRZvl/Nz9l0KNsfx0ueu5b/HF6qWm4ppZW\n/OzNFbhu6oIESZYcIorAw+pT/UG7GZ6w6qFZSezF5OMFf/0WoT/NjjtfdeQoAOCNRdtdp+EGLgXu\nsZb322pI781+tWGv56nwDPbwNmSKp3yGv4e3RO9zJlJS8LtH8CyA6YyxoQBGAdgAYAqAWYyxwQBm\nyccJ5YDculu1w9h3CBFFCsDutO8RSP+9+Lb1ovCih2w0Jm0Utddj2Iq1jF0SMYCSDN/9fq05CWLl\naWeI56npG1X3+SGNP/imCIioC4AzAbwMAIyxo4yxagCXA3hVDvYqgCv8ksGIyPtRlWWzzUZS6H3q\n8ucZG3HRs8auEaIuJrycLA5GrqXSxxhkKg/Fei9Nk4XFtsb907ko+eliYiCA/QD+TUSjACwDcDeA\nIsbYbjnMHgBFejcT0R0A7gCAoqIihMNhR0LU1dXF3buhSmrd1VRXIxwOo/JwGx6a14BbR+ShqIOk\nG2tqajBnzhwAQFtbm276X8+ejcrDbejfOduRbLzyqnGSD3+bvcX03o0Hpfw4XFtrO36tvDU1kguG\nlStXRc59M2cO8rOtaw69tDdskHzA7Nm7J+a68ru6UercHz16NOb6pkPSM1XX1MTF67QsGXHkSPw7\nq6yUep1lW7Yg3BK7xmHZ3haM6JHNlSdW5QEAysrKEG7TH54qLy8HAFSUlyMc3mWZnh7hcBhvLpbM\ncLdXbEc4vAeNjQ1xYXjlNePAAan3vW7dWuTv34B/ro6fm/Hy/R05cgRKi1Av3nA4jFKH38c3c76J\nicctbvPWDD8VQQ6AkwH8jDG2iIiehWYYiDHGiEhX0TLGpgKYCgAlJSUsFAo5EiIcDkN7b96WA8CS\nRdh4qA1PLCfcPWkoMG8F9mR1xwVjioFFC9C1SxdMOH0cMHMGsrKyYuOY/hkAYCP1xx/nl+KDuyZg\nTH9vpjr05FWnaSsf5HsUjO4t2FoFLF6Irl06IxSayB8/4uX956aFwMEqjB41CliyCABwxhlnoH2e\nSVHTezb53NChJwJrVqF30TEIhUbHhd1X2wiEZyEvLy/m/g7lB4FFC9ClSxeEQhOM0+FBk49aOnTo\niFDozJhz4dp1wPZyDBo8CKGJAyPnV1dW4/np83BNSV88ddUoAJJv/475+vljVR4AYNCg4xE64zjd\n68XFxcCWzRhQXIxQ6ATT5zBKIxQKRX73H9AfodBQdFz+DXCkLhJUkdFQXk6mbV8K7NuLESNG4JRB\nPXDzjBlxYdzEr+XTL2cDqAeRfvkLhULIL5O+j06dDb4Pg/JxxhlnAl9O90xmt3lrhp9zBJUAKhlj\ni+TjdyEphr1E1BsA5P/7fJRBF7Xm2bKvLvaa6uKIR2bE36BirWzSaMcRWRCJWg35NO5rkH/3vr0K\nox/n24Vs0baDeM9kl62gDQEZjSsfbpQ2dFHKTOmeWox4ZAY+WOF8B7FkrGfwy3xULW8iTFRtZY9O\nZqZSmTTDN0XAGNsDYAcRDZFPTQKwHsDHACbL5yYD+MgvGYxl8ygiuZwG6X03NrdGFBQvRpY5bW0M\nW/YddiQHM/it5r3llZGVwVbx7KxuwL3vrIoPYFBXBGUI20qO0t1S/oY37vdfmBSCYDzPpKdkndrs\n86wbMpvv0i2THPcFDb+thn4GYBoRrQYwGsDvATwJ4Dwi2gzgXPk4oXj1ghTLiSAtHLn3nVW45Pm5\nOCibNvKg9Ai0liAvzd2Kc5+eE+M3yAq9Rpyb/HGbt4l+N+t31WJp+UHu8Lxl8aOVOw1bn8lQetr3\nfLSlDZ+v2e1pfhtFpT3/ztIdGHj/59hVbb9nziVtZE9Q6aFbWtvw1pLtlh5HA1QtWOKrImCMrWSM\nlTDGRjLGrmCMHWKMVTHGJjHGBjPGzmWM8X81nslldi1+cZXRx6oE4X3h5QeOmO6E5AUr5b1+64/y\n7ylrJL/idOuTVbtQ8sRXppZVZnEm5nuITYVs9NYONzZj2qIKTyqxi577Fle9uMD2M1tV5nf/b6Vh\n6zMZG9No+fOXG3HXtOVYV+WdGwazPLz9v0tR8oS0uOyjldIkeNn+OpM7DNKw07uQz/93QQXue28N\npi0yd3SYQnogM1cWezYyFKls+GK8duoCPPHZBsONv/cfbkLd0cQXHz1zWjUvzd2GA3VNWL79UOTc\nnprGyBZ++nFGn8PPlpGxDX3s+WUVhwzCAQ9/tA4PfrAWi7d53yYxXP/AYv+7SoMjjN1k1u2yN7yo\nmJfurGtD8ZTP8P5yp3Me6nJjMDQEYOb6vThQ597dhF4K2mS1YarlBpFVrztIIwVWZKYi0Lwgp68r\nOjTEF76u0byVfsrvvsJPv643DeMH0ZXF/Fz2t7mY/MriuPMR5cgzSeABVkpYkeN7/5hvGEapUBod\nOhXTHw7ju9cLh3/KvY3NrWhwuPBNy2erd1sHUqE8x646KQ/fXx7vBsQORGT4Zj0dflKlpz0XTS/2\nWAlptQYhddRApioCg/MU81tVMAxu4C0QgYdzLFadP/ssnH/F6gEXcwSuA6QGboZulDps1GNf4sSH\np3sSv51hQECnrDh8oJghxQS8W700zHoigGouzULAFOoQZKYiMKs87Lw7CuBksRO8tG5QFKh6jwNX\n2WNx79el+9yn4ZKgvP4mD90k263II5vkyMffbj6AvbUuXbMYNVDcxWo7We33wTv/lEr1QkYqAq6K\nj2cCjq9hoEo3mCjyL604hEc+WmsYTm/44plljZowSqTRc242vrF6V1PeX6N73s5kcTLw0kReGxWP\nV13rOJ0JqM7vH7y8yDAcX1z+vz2lN08x54zMVmPDWhXrFNIDGaoIbL4gq6EkuwU2KDbuCsrH0NrG\n8OqCqCUET2W1an90TLryUD2+lTdrj5ks9kZMWwQlj63y0Chv/jxjI2bxetvUJPL4p+st49fydele\nLNpaZRSlJXrf1N5a55O5ZBCnNq1New9j7pYDcefX76rFht3G+0W/+E0ZPl+jPw8SN1lsMOxl1cBJ\nIT3gq4uJwGJuPsofj90eQSKxI1Pc5Dljuq1/q7rh+n8t1E3f1X4EbntbCXg5nrTuNXEo/qH+c0GH\nmPNNLa3IzzH3bTXHxJrLiFv+sxQAUP7kxbbvBVSTrqpzNQ3NmLl+L84bputOzDQe7e/YMNErN6jK\nnJqLnpOcLBo9z5NfSC7onzqzHQCrXd4QEyYyJGx8i3Q9iBWDARnRI6g/2oLnZm2OdJl5Xo+6XCgv\n9PWFFTGrdpXus5vX/d6yShyysfjLD6ysJHipqos+h3oC3U0X38muUP+et83Wgjo/4H1mbWVxuLEZ\nP31juWH4IQ/FTwb70fuxilPbUDAqM7f/d2nk91tLtmPxtoP4zjNz4ryZxsfPV5G2qArI20v191dW\ns7O6Ab9V95h0J4v179U66U2nHkFGKIK/frUZT8/chPflna2cauqHPlyLS56fGznOknPPLLqFW6si\nK0214SqqjuDed1bhp2/GfvhKi+7f87Zh+to9jmTlZXbpPvzotWUx55wW4Hoj00UbEe4/3BSzN6xR\nhfrQh7FzA3VNLTjc2IyNew/jsU/W4xf/W2k3aceYvX/rjXOUcFLIt5bswKc2TTe1rVnd3pzmVE29\ntLva6wv1F0XZN2fVGWzXcN97a3DNPxdg497DeG2BfroR+RqaMfYJ/d3IjHqbPPn2y7dWxux/rffq\ntGVOW19kcU5ApVCHIDMUwRF5AZdiVRHfAlaPZ/O/PeVjMWsZXDd1Ia56UX+Hs2a5h6LdCvMm2T7/\nsU/W48evayppj0vXK/O2xZ1zM7mrEJunfJQfOIJTfvdVzApaI1FeX7g9Ztewoy1tOOnRL9HcIt1w\nWLVoj3ehmJcta7tZ6GZ4ycnEbmW11CKf5tHuavaf11zmdbuMx/e37j9iLzEVWrcQPD0CbZAszh6B\n1ZoOLyb1vSIjFIEWnkIb62LCIEwkPr6vQNuQiJqfct1uOyyfTPEfpFEBt1NZxQwNccqsrLj+bHXU\nb75Z3pZX8S2+u+af/m41apovNmt4JwvLtLfwlEcliNGEqn3zUXvobWxz6MjRiDmwGcr4vxdoe2SA\nidVQxERWCvuvb+MbUWrMyt37yysx6MEvsJ2zDPtNRiiC+NdqVNHZ/WjNYtORQxMwi6NHERcHd0jO\n+HR9rcT+V1CPrdqJl7eXdVRuIanfg9vFeqnUPQf4dv5aUFYVc2x6i8MMsOplaK9GfHRxxp+t86A/\ne3MF593x6TpFf2jI/ARvNbFHtY5i6/46nPOXMKrkVezKMNamvfrefRubvVkhzktaK4JlFYfww+lH\nIq0eu07irIhMFjuY0FTLY1XZqf22WCmNnQ48MGphDNhb24gvNPMTm/bWxe3fYBiHJr74NOJPKnMD\nehP1XvLXrzaheIr+ZiL7ahvjhuqsKN1zOM7JH7fUBo0DM643sJRRsGrQLKs4hCv/buxyQ4rDUowY\n7L4lPRl31UTL7rYDzod/7BApXqRzTjmOW1Bmv9c2dc5WbN1/BF+ujzUJ1ovq41W7MPQ30w2VhB+k\ntSKYvlbSuitkj5xcLXiDMUM9KxQlPqcrKHl7BPe8bT1m7hSjoSEjG+tzn/6GK14r76N6LnyP6qyM\ntV/BaO+Pj+GvX20GoD+GO+73szD+D7Nspho/zxORxyB8pNelWZHrZK7A7j0PvL8m0vsyjNPkWvmB\nI1ivGVLSLraywmrTGZ4hIsB9D1n3fotIvZhLMmvgzJSVhdk6CK9Ja0VghFnFqzdmCAAn/3ZmXFil\nIv97uAxfruO37onOEcjHNkoz7zDSH77YwB+pBi90Taz30fgYW0wUAZm0zmzLYXJ/1ZEmwzDFUz6L\ndON5cDuEpbVRt3UvR9VkFqbNhvAE4FfvxrvDbrPQBPGWN3qBuMXwFe035pUfJYW1O2vQ3MoM41ou\ne8rl6R16RUYpgkVbD6KtjXnmJE79ntbY3BVMfb8fwx+fr+FTTHpFrY0xTytgvbh0ewTKHAHHxJ0X\naB2rab+71ZV23qkzOePnjRxEwnFP7Erv2EQbW+J7RkZ1EIO5Uqlv5suHLEcP6j16nndfnFMWE8Zo\nbs8J2w/W45Ln50ZWQ09buD3OekgZ3g2UIiCiu4moM0m8TETLiej8RAjnNR+v2oWp3241rXhtrSxW\nFZ+cLPOsjJk8lX9Gh4b40/SiYvzNh2vx0UpjN8HMA6s2Iymr649i/pYDaNV5jialQkpQ+bfKyn2H\n+Yf8XPcI5Id2YgqqrTAs1y5oZG1qjn/hMW6ZbVghLdytP8nJ06p2lIU+tBNmrtd37aFdUOYE7eLR\nWaX78F+DNRXZCWym8yR1C2OsFsD5ALoB+AGSsL2kV2y12MUoMmbL8bLVYXKyzW/4dvMBQy+GbYxh\nBufQ0rwtVXh65iausHowxvDawgrcLS+40g3jgbsvPcUHALf8ZwlueGmR7vi8sgYgZkFZAocLtGnZ\nMfO28k/Di6M5Aq4wqopdc01PKcdO2GvOe6CoE9naNUPvPcVZRGmvu5BdL73aRn2X34HqESCaLxcB\neI0xtg4Ja7N5j5tVoGbhcy0UQcwwhMY8kwFxq3uNuP2/S/HcrM38Qmo4UBfbItEra2t31tr2R68l\ndrI4erBxj2QJ0axTy+r1dtyqJB6/Ukbf22sLK7h3wXK79qKppRUPfrAGtQ7y3aIzaonu3rsc62jU\nWG4QpDn2amTItjEBx/3x7jM0DTibaVph1AtMpCLgcTq3jIi+BDAQwP1E1AlAcJbE2YTBvPWm/J6v\nsdXWI6ZHkJWF7VX1KMjNQq/OBfrhI76JtJNRiWv2bj8oLWDpVGD86r/v0n0woJ0sjp43W0Snlwuu\n1xG4uHfD7lr8nNO23emQnXLXh/K+u3k59mt1py6jFfQUQezGTNYVodXjG7ppSDJ6jQEjJaUd0nWS\njp2GzZb9dThufwcc17Oj7fTswlPqbgUwBcApjLF6ALkAbvZVKo/QK5wb9xyOr4gdukxWF4jcbMKZ\nf5qNcb/XNz3cduAIGjSLRJS09Co7N61+M5QWbs+O+Y7jePrLjZZhrMxH9d6Nbo/A9aS1mYWYdeS8\nPaP4ZPgEjxs757pLm5K1FVxsmrFndXsEKto079LN1pwKepVpMrx16vYING8hfmjIN3FiePKLUpzz\nFz5zbbfwKILTAGxkjFUT0fcBPATAvolMQFizs8aiR2BrtjhCjsXMjnpcP7pyV/qhVwG6mQcwI/J8\nFPPPFs99vcUyTOzaB4Y1lTW49+1Vps/84Afxm+K4HRoyq+SMVlCr0VsBaxaXFrutdb9byuUHjqBM\n46tHbcqrmJLGuljhmCw2CbOgrIpraIjXZUhMum5XFkc+B+N81/Ya3Lwh3TmJAHSOeBTBPwDUE9Eo\nAPcCKAPwX1+lShHUhSfXwRT/Qx9KFV9SJkQTmSaAW15dgveWV+KIPEnMO5TiNm/M0qluaI6ZqNP7\nIHknBp0qLCMDAltxaJLearIqN/TncNy51rboSK8yJGo2WaxXaZq9p+v/tTDeBFPWBPO3HNCdL0oU\n+nME0d+/emdVpBfthdVQ4vs8fPDUXi1MUruXA/gbY+wFAJ38FctfzHoEvCsagdiPWD1ZPOKRGTja\n0oYPV+ibaCp3hTdK7qb9tJU3SjuhaXLOB+hhZ7ETb9oKk19ZjJGPfml6v4UNQAStmE6zV9sjaGMs\nskLeDWaVl7oeViyIzMJ7YfreRMvnAAAgAElEQVRJRFi+/RBueGkR/swx1OgXitxGz/vOsspIgw2R\nsP414dc6WI/kBTyTxYeJ6H5IZqNnEFEWpHmCwGNUONVujrXhjGx6rVAXjrqmFpzw0Bfc9yajR5DY\nlone2D9nj8BlyjwKT8+1hQLvUI3R89itM7RDJl9VtOCNGcYb1Uhp20tDS0ubzjoCm4v6mkzyEIjv\n+WQT4cBhqaX9zcb9uP/CE3lE9Rz1oz368Tp0a59nWtHvrG7Aoq18bs2t0lNobWOoaWhGl3a5WFVZ\n7ThuN/D0CK4F0ARpPcEeAH0B/MlXqRKMF5NUvNYleuk5aZ3rTWLOl1crmqYtf5DJtM+3k75bOfVs\n5LUsLjf+sLkVgfbYaY9AowkONfn/onSthgzcfBhNFq/aYa8CUz9m6Z7DjrbX5MFqL4rp5dHv6D/z\ny/HMV5uM/UMBOPtPYbyncgJpF7265tlZmzHqsS/R1uZ+Rb9TLBWBXPlPA9CFiC4B0MgY45ojIKJy\nIlpDRCuJaKl8rpCIZhLRZvl/N1dP4AFO896t2Z6CE0VwsY5P9htesjb71JqxedHN3VPTiP8tNt7g\nRO/peCpo6V57eaP1jtra6u7L4rXRj9v3WeNMTiHOjl0jnpUzNt20TeWyvl/ffNQ4fiflXitHfm5s\nxjp1sGb1eNf8cwHOM3GUuOOwvfkJK2d9RiivdYWJwuT9JvyAx8XENQAWA7gawDUAFhHRVTbSOJsx\nNpoxViIfTwEwizE2GMAs+Ti5OMx/5xOEmmMH0VQecuZuOjJX7GGZm/zKYkx5f43hdV1TUc7vye4U\nwS/eil0x7XYdglOrId4huHjTRHvuIrxArQiiVmX6Q0NeyaPtaflZCW5WNQ7iN/GJD+/HFICSjpl7\nbSszXj/hmSN4ENIagn0AQEQ9AXwF4F2HaV4OICT/fhVAGMB9DuPyBPcOFRykaeCCwW+U3b+8TLPK\nYqN4vfzl7gW5FNRtBcM7NKT9hp1+005W3Lod2tTzBKtexxK31sGjijLWn5E3cVoRp7AVWVTnkmXO\n2cZYZFvdRMOjCLIUJSBTBX6vpQzAl0TEAPyTMTYVQBFjTDGD2AOgSO9GIroDwB0AUFRUhHA4zJlk\nlModfO4BSkslq4VPVVsk8rBj+w7usIRooZs3bz465UWvtfI2jzUoedLUwlDNMZYcDocxY53UImls\nbEQ4HEZVlbO9FNQcPWquCJYsWRoXZsnSpVxxb6twNnmv0NTE70p69arVceeqDx3iunfFihWor8iO\nHO/eI/mO2rRxI8L1WyPn11dJ5rMLtlbhnpe/RIfc2FqnpTk2n6R8M6+ZSktLEa4r071WIedfefk2\nhMP6VmzLV0R7UatXrwZ2x1YLc+fOjfzevn07DtbY3z3rtU/DMcfr1m9AO1UyZVu3wgktrbGyGNUT\nyvmamtiedFtbGwBCqyqeusP6/sjq6vg2ZdJj9x5ry6/wN9/iD7Pi11IostfV1TmqB3ngUQTTiWgG\ngDfl42sBfM4Z/+mMsZ1E1AvATCIqVV9kjDFZScQhK42pAFBSUsJCoRBnklG+rVsPVGyzDHfCkCHA\nujW2W3H9+vcDtvEV4KwsinT9TptwGgrb5wEzZMsiIkdNIiVPrpu6AAs5LBkWNR4DaRkIkJ+fj1Ao\nhNfKlwD7+U1m9cjPzwOOGle4Y8eWIG/1YkBVKT+2gE8BDeg/ACizXsBmRE5uHmChqBRGjRoFLI2d\nZ+nZoztwwDp/Tho1ChOO7wFMl3Y+69WrF7BrF4YMGYLQuP6RcHlbDgBLpDTe39yM314+HFi/LnK9\nXUEB0BitrPLy8gCYr24eMnQoQiX9ImmrGTBgALB1C4qLByIUGqwbZsRJI4EliwEAI0eORGhIr5hw\nEydOBGZJ+3H0798fNTtrgCprwwQ1v1sU+75PPHEoOubnAsulBkH/AcXAZvur6bOzsgFVJR4KhfCj\n15YCiB2CUb6Vv22YD1SrlDtlAWDIyckBWqXWeJfOnYDaeDPOjh07AoedzWXMqbRu6U9Q5bOe7OFw\nGE7qQR4sFQFj7FdE9D0AE+VTUxljH/BEzhjbKf/fR0QfABgHYC8R9WaM7Sai3gDc1UKm6Xsbzg1m\nzq7cjg3yKAEA+Ec42mpM5HCkm6E3t+sd7OStnpy8QzVGQw5/nF6K4h4dMP647lzxaIclvBql+OtX\nmwxXq6uHz5pbGX6jsZvXZqEXBgba/HJaHrXv7PgHPnc/1m7wfH67wEjmHAHXEA9j7D3G2D3yH5cS\nIKIOsoM6EFEHSG6s1wL4GMBkOdhkAB/ZF9tbEjFHoB5rnl26L6ELuvTYU9uIhqOtnoyHWkXh5lET\nsY4gGjb+HG+lp/2IlcND9c24bqr5HsNqeCenY+B4RNM6RnXtq/V78drC2OE4vyrA2NXL3qRhVZlq\nX2fd0XjrrqYEbxyvkEyrIcMeAREdhsEKbEijOp0t4i4C8IH8IeUAeIMxNp2IlgB4m4huBVAByRIp\nqej5uPGarCwAcvm67701GHqMVfb5z49fX2bpPtsLvi7d53iFsNuVxXbu16uMeOvluqaWmIVpvAqo\nWWPeGmduypH+ih2H0LdbO670rNBrFMWbj3qQjibSep39KfxAm26Lzmsq3aO/afy+w/zzTU5wOFXo\nCYaKgDHmyo0EY2wrgFE656sATHITd2CwUUdpba8bk9TqUPPt5v04Z6juXL0trCorNw709Cxa7GDn\ndv1WD1+1d9e05RjTv2s0LgNFsKQ8dvJZuyLXybDLm4t34M3F/IYLWt5QrQF5e2n8Yik/rIa0ufPy\nXOu5PN14vGpEczzTQQvrOLforfBOFGm9Z3EyzEKN0LYsC3Kz9QMmECuPqUFAu5+rXWx1t13alK/Y\nHl0stOOg/jqPZ76KVYpNOvsFqznY6K4M83wDRlszKjz5RanpdScwxhJqprloaxVq6puN0wxAVRHI\nHoHAmiUmrgm0aO3Rg7AxR45H20R5tcJaD9c9ApeTxWYLgMxYw+k8rLbB3Jpk/q7k2JWrceNSwQiv\n6l3eeK6duhBjB3TzbGc0Pwj0ymKBMcu38/tX0db7ZRZ7J/PixoWvZ4rAx4+rxaWLCDuTxXpBjcaL\n7fLYJ+uwcGv8rnevzHM2JJJMPHndSajzTN1YBEBBGE10J2IYOa17BEk2zImhtjG2Zad1heCU3322\nwfG9Xg0N+ZnPCZ0j8PE5/j2vHId8HmNOFJ6YjyZBEwSgrjfFqNFSeagBg3r5u10lj6+hK2UHcTVE\nVEtEh4nI2aoKgees2M638lUPR6aKOuypdb862YhETqD5XTXxbHvpdCgqUSwtP2Rrzw4jktFIO3K0\nNW6yXiEISsKo95uIUWSeHsFTAC5ljDlveiaJAAzD+46b7ykniwKfR26Hhuzg94Kh2Rv9cbWcSMxc\ndtshQJ31wGDUI0jEJ8qjCPamohLIFNzUXXtrG7G7xr/WvBckchvDdKycgjQ8qsYzuTyKx89dx3gx\nmiNIhGxmC8qulH8uJaK3AHwIaYMaAABj7H2fZXNNUD8CL3HnvsFDQXzC7RyBHTKhvAQFhsSaj6YC\nRlZDye4RXKr6XQ/JRYQCAxB4RZAOOPRHlzaYbSPpPRmc0QkmaGU6CErJq+1OnWC2svhm/5MXWJFN\nhJagfTUJJJGKIIOzOeF4NzLkTUzV9dYT+X5jVP78XKejwGM19CoRdVUddyOiV/wVS6BgZdmT7pWX\n060BndCcCmNl6UK6F1wPSUSPgMeQfCRjLLJyijF2CMAY/0QSqLFa9JXu39NKm5uiu+Hnb65IWFqZ\nDgMwf0v8Aju7aJ32pTLJfBIeRZCl3mCeiAqR5gvRgkSWlSJIkBwCgZfUNjTjJYeO5tKVZ7/S35gn\nKOsI/gJgARG9Ix9fDeD3/okkUGM2NMQYM182LxAEFO1KewEwd4v+rm9JNR9VYIz9l4iWAjhHPnUl\nY2y9v2J5g98LhBJBtkkh8Ns/ukDgF8napD0VSbb5qCQE0WuMsR8AWK9zTuAzZkNDaaDnBD4T1CKS\nqI1o0oGgTBYPVx8QUTaAsf6I4y1BWC3oFq88hAoEQUL0CPhJqvkoEd0vb1c5UuVs7jCkzeaTvs8w\nD+kwNKTsW3Bcjw5JlkQg8I5ErhhPdZLaI2CM/UHervJPjLHOjLFO8l93xtj9/osmAKKTxTkJ2FtY\nIEgU6dBISxSBmCNgjN0vm48OBlCgOj/HT8EEEooi0Ptu/jO/PLHCCASCxBME81Eiug3A3QD6AlgJ\nYDyABYhaEQl8JKIIdK69+E2Z7j3d2ufiUACWzAsERoj+AD+BcDEBSQmcAqCCMXY2pFXFiVvu6YJ0\nKGyK+aidLReDsB+yIBgEdQRGTBHwExSroUbGWCMAEFE+Y6wUwBB/xRIoRMxHbXw4Qg8Igo6YI+An\nEHMEACplp3MfAphJRIcAVPgrlkBB2VbYzmeTDmazgvTGTg830wnKyuLvyj8fJaLZALoAmO6rVB6R\nDmUtO0vSBHY+HKEGBMngyjHH4v0VO7nCzvPA4VymkIjvmWdoCER0MhH9HMBIAJWMsaO8CRBRNhGt\nIKJP5eOBRLSIiLYQ0VtElOdM9MxAsRq1o9TEHIFAIZFFQfRE/SEQcwRE9DCAVwF0B9ADwL+J6CEb\nadwNQL3n8R8BPMMYGwTgEIBbbcSVcUSthmz0CMT3KJBJZK9YLIL3h6BYDd0I4BTG2COMsUcgmY9y\n+Rkior4ALgbwknxMkMxO35WDvArgCrtCZxJK677Nxv4s4nsUKHi1gxcPoifqE0FYRwBgF6SFZI3y\ncT4AvoFA4K8Afg2gk3zcHUA1Y0xxNFIJ4Fi9G4noDgB3AEBRURHC4TBnklF27kp975xZTTUAgJ3V\nDdz3NDWl/nMLgHY5QINLlzzl5du9EYaD3Xt2JyytTGLe3Llon0uoq6tzVA/yYKgIiOh5SMYqNQDW\nEdFM+fg8AIutIiaiSwDsY4wtI6KQXcEYY1MBTAWAkpISFgrZjgKzqtcC21PbwOm04cdjwa5Ntu5p\n164AaORXHIJgckzXDth24IirOHoc0weoSIwyOLZPb6ByR0LSyiTOOON0dCrIRTgchpN6kAezHsFS\n+f8yAB+ozoc5454I4DIiughSj6IzgGcBdCWiHLlX0Bf8vYuMpCCXaz4/BtFFTw+8GHNvaBbunr3i\n7kmD8ews/V3E/CSp5qOMsVfdRCw7prsfAOQewf8xxm6Udzq7CsD/AEyGj55MEzk+6hcFudnJFkGQ\nJHKy7DcCtDQmUBGkg7m2GecNK0qOIkhAGmZuqN+W/68hotXaPxdp3gfgHiLaAmnO4GUXcaUdQwtj\nX4mT1n1Ti2gFpgNm25Ty0thsw8ogTbjt9IG+xNunaztf4rUi2XsW3y3/v8RtIoyxMOQhJcbYVgDj\n3MaZrozqmYPSg9FlGmaFgEi/FdbUknkffzriQYcAX5fucx8JJ0HpEZjt6mfGqofPx6jHvzS8npsk\nV/BJNR9ljO2W/1fo/fkuWYaifeVmPYJTigt1zzelYCtQ7MRmny7tcpMtQhyT+vMYIvqL0xY0WSje\nZC2YC8qCsiuJaDMR1ah2Kqv1XzT3vL4wcaZzXqF96Wb1Y6uBC0fFHcW5JxZ5JZbvvH7bqckWIXAk\noiXoJQwMPxiWjx+ddVxS5XBqLGF1Xzq3VXg6n08BuIwx1kW1U1lnvwXLVLRljUD4/vj++N13R8SF\nNVIEylmzcn3G4B7OBPSQc4b2ivwWlk7xpFqWBGZoyGG+Wd2XLMUciB4BgL2MsQ3WwQReoC2MRMAT\nV5yEG08dEBfWUBHIX6RZ+QmCX5gHLjox8jtVW1tFnfN9izvVsiTSAEmy5H71CJL1yQTFxcRS2Tnc\n9fIw0ZVEdKXvkgkAmBfONTtrTO81nWh2KpCHqOVzo5ievPIkD6Rxxhd3n+lf5AmuedwqteD0CNJM\nEQSkR9AZQD2A8wFcKv+5tiRKBM9fPybZIthGr0dgF+WDTHbLzAr1h+emR+DUSsQu//zB2LhzfqX8\n+OXDA/72gotzRWB+PWlDQwlIg2c/gpsTIIcvnDpQ36omKIzq1xWrdsTu+mlmNWRkLqolIA0zS9TP\netSFyWt2AppMZ57QE98Zfkzceb+SvnBEb7y/PLGL7t1WdEFZwLnjUL2j+7KI0LkgB7WN+g6ektcj\nSOLQEBH9Wv7/PBE9p/3zXTIvCHiTqnfngrhz2nfurEfAHN+bSNTyHTka//HxLqjyYuGVU/xqJSbj\n3SW7vHRt74057IIyZ5veEAG3nWFs8eSkp0EEvHfnBEfyROJwdTcfZkNDygTxUkj+hrR/gcftR5qX\n48GKHhP0ylWc1ZAqEO8YrBIs6JY4avlCJ/QyCWmO28e8K3S8ZRjDPXZ9zGKr5/J67UVtQ3PcueF9\n+A0EI0OSFmJ1LtAfiHj4kmHcaZnRYsdnuwqrlreT3O7ZMR9jB3RzJE8k3WTOETDGPpH/v6r3579o\nyWdg9w6+xq+rCDx46TwKI2g6ws04v9sewc8nDY45vnliMfe9fuUjwbri8XrIQM9B3TPXjua+n1ea\nK8boep73jFaf1lM6ye5mE2E65vMtvkvq0JBKiBIi+oCIlnvkayhhuM2/Y7rED914id4L1p5Rt0Rt\nP0/AKnstVs9j2ArX4PUcwdlD+HsnfmUxA08F4O2Y/EUn9Y47Z0vJcgYd0acLf5wOaHXYIwDMG1FO\nKuQXbjjZ8Nrc+862HZ9f8KikaQB+BWANgJTyXeD2I33m2tFYVnEIt/93qXVgB+jJZ/bdEex9+gHX\nA54NXbm1GopfzW0cX8mAblhacchVerxYqgGP52b1Kjo7w0/KUKzVHd065KEgN8s3h3hG62t48HrC\ne8Ig44WbOdn+Dj3bgUeS/Yyxjxlj21LN15DbLlVhhzycN8y9m4ZOnF1APdpcfO1Gz//gRScGQkl4\n1ZB30yMo6pwfN5ekF53hFIFP3XbGEj98p5dcIifi7RT1jU9cYHiNRw+cOrAwEKvrgwKPIniEiF4S\nC8q8R6/lqa1Y1L1cryqddnnZjto9Xk9OemVxw7tng5574iNNrbYsteLCcqXMT4+Oeaq4zWO38w55\nKnS9IGZ7IlxT0jfmmLd48g75mWHWa+PpEeTlZOH4nh1dy8HLN78K4W83BHddE48iuBnAaAAXIMUW\nlAUFo2LJYzXk5pMx+lScxjn+uO646bR4VxdOcatXBvfqiN9cMgzjONeL6OX3sV3b6fp30qIMGYQ0\n8wdettrPOqFnXHpewZPXepVrjonr5ZP7x1rD+NF3aJ+nr+TN0uJRBOpG1TUlffH77/q7On1A9w7o\n4GJkwG94FMEpjLESxthkxtjN8t8tvkvmAV60PPxErzBrv0WjoaG+3aw3yfDDxYSnH7vLyLq0y8Wt\npw90NXzx2m3j4npaZvl251nHY+lD50bDepgjz98wBpFM4Vk4yBi3MuXpTbqeI+DtEYA/32b/XwiP\nXBpvVmrWI7hK01PRgxCtH07s3Rk3nNqfSx4vCVL9xKMI5hORNwa+CcZJNvsxbmj0wvU+PO13Z3Rv\nPscahyDMA+gxul9XfHDXBMvKwNKuW77sRA88eukwLH3oXPTqVKDTI4hHeQ1ZWYQeHaM+ebzsEZA2\nPo64eYcLrz+lH0dc0v9ffWdI5JwdJRuZLOaSO/6cXkkv6lyAmycOxLe/PhvXqZ6BCOjeIU/nDuAW\nDvNf9WPpifvjs6zXltjFy96+1/AogvEAVhLRRtl0dE2qmI96RQeD7qlbdHsEmmOjXm4uh8WBHxOZ\nXsT5yKXDMKZ/N888jjqR6YcTB0Yq9Pg5Am/z7diu7XC6ifWIXroM1gpOallbc/qgHnjk0uGmYcYV\nF0bSU89TuNk3+Z7zTnB8r5Z+he3x2OXRZyAi/M3QNJOv96P3aSkK3w+DHm25ai/Pbf1wQrH3idmE\n53EvADAYUadzl8j/0xK9SsCt5ja8n2eOgOlf42mpmYVIphtqJe0guMIG4uWwZzUU/V2iWUH65JUn\n4fOfn4F5U87B45ebV8RA7CIyxjgmi3Usi/T2rWiXl21pYvv2j0+LpKdufNjqEfg8ka4dDjqpr/56\nBLvFSquAAX/8V2ljzMnOQvmTF+P74xM/LKXFUhGk8laVyRqC064YNJJD32oo9thojsCVBY9OnLzL\n4L2YLFYkt2zxJukFOs3ad++cgJm/jLqlLupSgGGyiwaeJyECTj2uOwCgHacllFZZlAyInzjXe55R\nOpWoUvbU2W5WzozqSh4F5gRt5dwxPwev3xq/sx3P68si7xePpTLBWdHgA04sL5y+fvXG1l//31lc\ncvAMDRlVhlwtNR/K8nEGJnev3TqOOw7lG0u0W1/+j9vYaig+JN9EM49SIxD+dNVIzPjFmejC4YCN\nSd2GGPT8Y+nl802nFceHk4VXP6u2J6EoqNvPGKhzv6XIruBdOMj3nqNh/HL1wh9n8pVOWisCJ9gZ\nFlBzu8proZtNxbXpGyWtjN12aZeLnp30NxQxrGh9KOWFBhN3ppiIcdFJ8S6f42/35wOykz288ws8\n/m+IpDURQ47pxJ++5livBc/7PEo47bzUxicuQOlvpb83bj8VN5zaP2Z3OSNpjD+b6JVLRsa7tbAD\njwm20X2JdpttVF6D0PlIb0Wges8v3HCyLU+KbtC+cFtDQ5rjNtVXea3KakLpEdQ0NOP7OttYAvYK\n2KOXDkcvA4XCgxPrErNb3ExSGqfrbTi9sEZGP07cHqgrqt8YeOb00kIn8j40BTY/JxsFudLfmP7d\n8PvvnqTvJ4ti//Ogfi43Q4HqNHnSVw8NJaoeDkKFb0RaK4IeHfNRUpSN9+6cgItH9saJva0VgRfv\niveFX61j76ytHNX1xy/OjVphqBf6eNGyOalvF1d+0+34DYpWGMb32HWtcW2JtXkkb4x2xofjJ5pJ\n9zfP85glq15spqBnj6+3aIm356S8Q6euepx8OwTgw59MxLPX8Xs51UtTPX/A87xB2r0vCJKktSLI\nyiL8dEyBLX/gXkwS8cZQUlwY5zMlbo5A9dvuJKZhcMa4FrPZIc+BvZ1ZcmZ10TlDe8VFkG2yAjYS\nJ6dyIQB3avYo4N2OwMg+nUsRaGLr09V60aA6vcmnDdAfouN8r1FFwJtP+hHfevpAXHTSMZisMw8R\nJxJJ60ouHx3rmvrN28fj/bs4GiZyROoeqboc/8FgP+vcnKxoGVPf4KOBgtFrCMLEdForAi1671g7\nueb0lSjv8v/OPyHuxZoVLavJRqPKa4xqeT+PaaMaoxafmwJpZxOf6EIwk/RMMi2idFRhjuthvXcE\nb0uXCLjvgqEY0L09V9iYY4MS1K299RyKNq4nrog3BY2Bxb4zI59LSoj37pwQsRYyU4qOewRyQl3b\n5+HvN47lmvA2yq/Tju8e58LCDCPrJiPrq0dVq5V1G0Wqs+cNK8KvLxiiE0rii7vPiPw2bXTa+Lwu\nHdUHn//8DDx11Uj+m1zgmyIgogIiWkxEq4hoHRE9Jp8fSESLiGgLEb1FRA5mGT2UU+fc1WP74rdW\nH6EGI3t/K+I2q1fdPapfV3xXtYmH+qPvq2otar/bX18wBFeP7RtpOWvH7802y3CKHUXAs5OVWas0\nVyetWyYOxMU6/vR541SjvAN1cGN/UeaKXKFfYXt8YNHC1d7aPs/aNw0ZHsQzdkA3DC6SJqL1nidq\nPupME5x2HN+q/JhvRd0Yd5Cm8q6MegRGdO+Yr9uA0pPhXzeV4K7QIMO4ilUbWJkNrxpOFuuce+aa\nURjWpzOu4Rjy9AI/ewRNAM5hjI2C7LSOiMYD+COAZxhjgwAcAnCrjzJYoud58k9Xj8IPxkcnYO2O\nwccVRBv2ysphYYc8fPSTieiudmegCve9sfHzC8oWgKcUF+JPV4+KVM5natxmNBlsFO+mg6qnCP5y\n9SjH8ZkqAmUYSCVwVhZhosXqXd76zU1P3WzS8vhe5t4u7fbIGBCTB27Hvd3sD/HgRSfiYgcWQF4N\niqh9+9vNR7fmo27tGvTSSvReBb6lxiTq5MNc+Y8BOAfAu/L5VwFc4ZcMcTLp1MjxH4+zohlbAWiH\nhoxrIDupq6ONaeXLNdzNEwfii7vPwCnF5t44G3W2JNTGbxe9OQKjikGp5M0qHrPhiWM5xs71sNvS\nLSlWdfM1tx7XowOu1Nly0awSsspeJ9mvzkPDBV6cL1YJ5WQPDJ5hIAV17G7Hx5XPICYfYtIyexZv\n5gN4FahVsETu/aDFV7+oRJQNaaP7QQBeAFAGoJox1iIHqQSgu4EpEd0B4A4AKCoqQjgcdiRDXV1d\n5N69e5rirre2xVaKVVUH4tJqtRhKGdw1CxUV2wEAW7dtQzi8M+Z6m0GtFg6H4yqnxsYGAISjR4/G\nyXGkmcXcq1BeUQEAqKgox97cXdi7UTq/ep+UzVUHD8bEs3lrOQ4ciU03HA7jUKP5cx46dNDwPcyf\nOyfu3Lffxp8DgKVLl+Hglmy0tMU+zwuT2mPNgVa8uKoJBw4c0G3B//qUAtDhSgBATXV1jDybdsRv\nvq5mx87oezErT8uWLcWBzdm4oDtD+xPz8PqGo6iuiU3r4RIAiJ47s28OwuEwDqrycPWq1WjdGR2j\nbmgxr3i++SZsWDEuXrwYAHBMe8KeeimelpYWMJX+3bF9O8LhPXH37t+3NyLnHvkbKC0tjQkTDodR\nWXkUALClrCzmvBEbK6P5vbG0FOG6spjvzYh169ahtVX67ubPm4eOeRQT35he2ZZxKNe3VkvxNDVF\nv+2FCxdEfm/YEPuc6vt37ZLu2bxpE8IN2wAA5RVSHmzbti0uLSO+nfONYVj18caDrbrX9tVLZaZ9\nNsPhNuM0efLWKb4qAsZYK4DRRNQVwAcAhtq4dyqAqQBQUlLCQqGQIxnC4TCUez/etxLYFVtJZ2dl\nA63RF9SjRw+EQiUxYbK++iJ2hxgVpb+9ANlZhGdmbgK2luG4gQMRCg0Gpn8WCUNEuuMSkWeaEQ3b\nvl07AI3Izc2F9plrGsn4YbMAABqOSURBVJqBWV9G75XTKB4wACjbguLiYoRCURPT1g17geVL0b2w\nENi/P3K+V+9j0VLdAOzfFyPL3tpGIDxL9zkBoFu3QoRCp8Y8m8I5Z58d8xwAcNaZZwFffhEXdtSY\nkzF2QDe0tLZFrivPmrd+L7BqKQoLu4P274vLtru+NwkLyqqAJQvRtWtXhEKnRa7tWrQdWLfGUP4+\nffoA27fHpBdB9Uxjx5ZgxLHSpGqXrVV4fcNCdOnSBaGQ/vjvxtNbkZuVhawswp6aaB6OHj0qZriq\nrqkF+GqGoXxnn312/ElZrlNPHQfM/Qbt27cH6o8AALKys5GXk4UjzVIFWjxgAEKhIXHvp1evIoRC\n0qYon+1fBeysxNAhQ4G1Ud+RoVAIC+o3AOVbMXDg8cDGUv18UrFv6Y5IHEOGDkWopF/M9wYAc06q\nx02vLEJ5VX3k3PDhw5G9fhXQ2orTT5+IrvJE+r4lUnzH9+uNUMhgWFF+NiWN3nsOAwvnAFk5AKSG\nz4TTJkTewYknDgVWr4qLJhQKYXrVaqByB044YQhCshvqZUc3AmVbMHDgQGDLpvg80Cn7Z4dCwIzP\nI2Hf6HsAN7y0KO7e9tsOAoujSkq5tuNgPTBnNnLz8oDmo/Fpymjz1ksSslMCY6yaiGYDOA1AVyLK\nkXsFfQHsNL87sdjtnCmWGg4NYGyhTePHZx0vKQflOqf0/QvbY1d1Q+SYx6W1XvpOwzOToSGld8yT\nZ3bz1ckcKM/QRX5OtNVvFtyPjj9xDA3xDjkoLhwcrUsxuKV/9/Y4pbgwRhGo0SuzdvJJsQo6quq1\n27UGdWo9+v5dE7Bie3VcGTHbp9iKIUWdYockE4SfVkM95Z4AiKgdgPMAbAAwG8BVcrDJAD7ySwYe\nTjpW34Oh3zxwUbRzNKZ/V657tB/IlAuH4g9XnmRrB7R3f3wabpk4UDec0Qd42ag+hjJ9d1Au/nfH\neMPreigjQnry2bVld5KuFbrWJE6UiPbYY03AmMZaxiAcr6K/M3Q8ri3pZ2r/byiL2TxYnEm0SoG5\nzJOCXOnZ1MOMMXMEJu/N1OkcR9qDenXErTrbnxrGyRHpjF+eid/5vFuaHn5OTfcGMFveu2AJgJmM\nsU8B3AfgHiLaAqA7gJd9lMGQnCzCe3dOwEs/jB0G0ntZD14c9avixOuneh6g/MmLUf7kxbjjzOiC\npQ/umhiZAI2Y8NlOha+glRQXyi2/aOCIaZzB/VoLJbX/9MICwnjZY2acPAYyKO4WdFvb8qk2Bpw/\nzNzfkN2K1mgjEy3qSs12L8hecNfkcpRHPUWgV3F3LsjFH68a6WhLRbUJpRaznqrd/P3hhOKYRWKK\ntVqMCw8X78xP/0NGYgVgPZmvVkOrGWNjGGMjGWMjGGOPy+e3MsbGMcYGMcauZozFz+AmiLEDuqFz\nQay1g5GnxlH9pFa73davXZTU9VorRsMUbhrPU38wFj+fNFhOm69EPnrZcKx65HzcfsZAnNYnWml8\n+JOJmHZb1C2wsbzGAis9AsYYnr1e3+2A0cdq5UKkY4HzkVBHWaw1TXahJjrJlbNSDiWZWOyKak1+\nn3tiEQAgP5dv6MopY/p3jbjP1mNAj9iFeTGK1uC8EY9eNhzXj4v671dcvt8tl2EpTr6HtNOL0Q3D\nlQp/nMnUBxm1slj93p3WnU5MvOykZWo+6uJerrRtRNClXS4evHhYTA9pdL+uMZOjhj0CE0WgVoTq\nsXfdsJoERvfrarrVKK/CVIdT3DxE3FpYYTZH4OIF9epcgE9+enqcy4RclRG7tmgqw568Q0NOOa6H\n+fqIH51pb9tHO/mkbO6iDHfmZJHtRWrq8Md0LgAA9OpUwC+ES4SLiSRivI+wxX2G8cWfG3FsZ8Nr\nTjCcfA3U7qfWmI3Vu1nUBOhvzKLQvaPx0NBrt45DN9kWXi3esV3bYflvzsNdIb7KTN0atdMDOHtI\nvFM5LSf17RLjRoKxWOeD2vSaWiRrOL01Hl6USd6ny84i3Y1wPIWiaanlOsVk4lUvD248dQBe/P7J\nug4h45K0XVb1wydfDWSwIjDCcFGOzXi+/fXZ+O8t8bsnGRFRTKZWJ+ZSOKlDYxb32L/dFGNfR8a1\n0PjjCnHjqf3xRxMfK8P7dEFBbhZ+ds7guGs/PSfeFUBeThaeu34MrjrZ+OM+Y3BP9C/U9y1U2CHP\nUautHede18Xd2+PfN/Nv7KPAAGSregRaERVXImqXHMnyutni1IGRTaQeQfQZB5jNXSjz1arwWVmE\nC0b09qWVbjk0lESNkBDz0UxA+xL7FbY3XMFrGo/8X6/HYmyOaTsZg2X13pTEEcd2xtqdtYbx9exo\nvO9BTnaWpdVEl3a5KP3thbrX9Ibussjc8kmLV37xta1gJ9l7Zt8cXHeWsVLMMbEaUupeV9uaeoR6\nMtcoe131UuR7s7KsVZ2y6dGUC0/E3j17bJUNNd70B4IxWZxRioAZ/FbjtMV03rBj8MLsMoSGRMeS\nnbzgdjmEXp3yYyyVEoVX5XHareOxZX+d7rVPfnp6ZLFW4PDQBbleL8JJ2bplRD5CRs70tOajmuiV\nnpcTBf/y5BJs2qv/Dp1gtDGPV/0EZd5JO0egx6i+0oR7YYc83Dwi39Brqx0WPzAJtY0t1gF1CMLe\nCBmlCHhQxlWNMGq1jO7XFeVPXhxzzskLziZg8YPn6l6z+z3ztLCMPEG6oUv7XEN3vCf5PVbsAV5U\nTnZ6dG5Q75Ud5+NKaSXrpGv1jJNOLMIk2erIC04o6oTN++oiaXdpl4u6phbPq8BenQqSUrH26lyA\nXhb7Xlkr5OQphIyaI1Bv2mFUSR6q1/dX4+Qj9qvL59a5mBHJdHplxd9vPJlvoxIA7915WsxxIisG\nJ+/AqeJhYOhnMK8BqHoEqnN+lEkeY4WnrhqJHqohwTdvH49HLh2GTgXxzuqcvK/CDnl48sqT8Oot\n4xJWn9rNS6MhOjE0lGB+9Z0h6NetHR79ZL1hmPacE3w8OHm/5i4KDOzyHcSlF8aLLrLXPHf9GPTo\nmIcJx/Mv2x9rYjlkhpffo9478eN7f+KKEVi09SB2VjcYDg1l+aTg7Si9Dvk5OPW4Qny2ejcAyfXE\nzZoV7m57YtfJ6wtqG/UbcwW5WWhs9n4vDl6GHtMJv7lkGDbsrsV5w6K9rQDogczqERTkZuP7qn0G\ntPz56lF45lqrvVMZ/n7jyVzpeW154EtrTvX15froA/3akn546nv2d1u6bFQfW0rAC9xMWnYuyEHn\nghw8cmn8ZvNKefCqscGYtHnNJbK7b21DIerKIwhVTeIwetqbZNcZnvn+slmF52Rn4dbTB+LPV4/C\nd4abr5pPNBnVIwDMP4qrdDZ7idyn+n2RxU5YCl43xIyi88IPjt+YmYMmkkFds7Cl2mBjnkimOK8q\ncrKzsPrR7+hey84iTLlwKCYN7YXzntF30W0HRUplSC9Xs2+zMk+RrVPmvVhHoKy76N2Fb/FVosqc\n0TeuXqwYRIT5aAJJZF7baYkpZdPsDkOXDVCsQ4zv/ddNJb5sUZlq3D+uAGeceZbutUSUjR+fFb8w\nzW3FdNfZg9DQ3BrX21U8p6sbJOoysuHxC3Diw9Mdp3vO0F742w1jLH1CabEyz3VbIRrebjPenp3y\nsf+wsQccMzlH9euK7wznm2wPgl7KPEWQwr1kRXStf6TodeOHU49JxpLcYvjnq0cZbtzjFdqd3Yz2\nV+7brT2Wb69Gu9zU+CyUCrVjfg4euXR45PxX95yJ5dursWSbtCGRUZnnXfBmBBHhkpH8NviKY7oe\nButI/F6Bb5dZ956F+ib7a4EA4KOfTPRGiASRGiXeQ4gIpw4sxM0Ti23dN3FQDyzfXo2eHe37IDl1\nYCEWbTtoGoan7GZlER6/fDhO1/o792hBGQA8fvlw9O3WDrf8Z6n9SB1gNhyXaP5w5Um4YMQxGNbH\nwg4w4Azq1QmDenXC4ogi0BkaSkID4O5zB2NscTfLvaXd4pWVWOeCXMNGlx8ks42acYoAAN760WnW\ngTT84twTcE1JP1NzPT2+vvcsFHUuwPBHjHemUmPVmrnJxFe8nZaQUetLG38Hl63GIUWdsHHvYVdx\nJIoO+Tnc8z+pgNLTivXflLzqJjc7C2cP4XTe5zG8LsiTQRDmLDJSETghO4tilIDB7pNxHNfT3DOj\nF/hRjm6eWIwV26vxl2tGofJQg/UNBnzys9MNV5UK/EUxH/XRGMwXXM8RaO6fde9ZKGyfhxfnSHsx\ne9UbCkIF7hVCEThk8QPn4kiTsyXlevBMFnuJ1cemHnN2owiMxuMTSdCnhZxWTFZ3KfrXrUfXROHX\ncNXxcmPMiyGjtY99ByM4e/d2EVZDKUjPTvno2cnYeZpA4DdWLVIzL6/p1JrV4meF2jE/J7IwLdXc\nv5uR/OaawDOclP90rRAm8W4kk8I8e5354kfl1ap7BCnSOXCF3y5FvI4/CApFKII0wInbZDuFuZ3s\neqIwwBNuWtQVXlHnxO02lUguH32s6XX9yeLg0qeLtBvcwB7GewjwYLm5lMt69/XbTsW1Jf0i34VX\nJNMLqRgaSiP8+t5PKe6G3313hGO/7clByoyLT+qN31wS7+4hSPjVK0u1yeKzh/bCG7edivEm+x/z\nYPQZ3Hhqf3yxdje+Z7JBEQ9jB3Qz9K6bqqRIERGYYVSRnNy/Gwpys3BnKH7XrnEDJcds/btbm8MS\nEW48dYCup8igc9noPjiG0wVCuvGD8cUAgDH94yut5A9G6DNhUA/XTvKMVuD3K2yPb351duDKQxCG\nZ0WPII3Qdi27ts8z3Mnr5onFOG9Yke11EalCx3yp2x6E3bmSxemDe+jskSEThNrHJ1L1jQuroQCg\nONBKRZx80kSUtkoAAB67bASKe3RI2gKmoJIi0wWuyIRn9BqhCACsevh85GQnt/R40UATH0CULu1z\n8YtzT0i2GIa8dus4VNc342dvrkhK4zx9+wOp53a7d5cC3Hr6QFx7Sr+kySAUAaRKI5VJ415+2nLG\n4J7YcbA+4ekGYX9cQSxElHSDBt8mi4moHxHNJqL1RLSOiO6WzxcS0Uwi2iz/T6/pd4FAIEgx/LQa\nagFwL2NsGIDxAH5CRMMATAEwizE2GMAs+VggECSAjgXSIICyLWmn/Bz07dYumSIJAoBvQ0OMsd0A\ndsu/DxPRBgDHArgcQEgO9iqAMID7/JJDIEhHju3qrPK+e9JgdG2XG7GlX/nI+WKwSJCYOQIiKgYw\nBsAiAEWykgCAPQD4tvHJEJzMcwVhibrAPk7nNBc/OAnt85x9ugW52fiRape07Aw2rxVE8V0REFFH\nAO8B+AVjrFY9o88YY0SkW4sR0R0A7gCAoqIihMNhR+nX1dU5vtcPjGRpapK2xDtypN62vJWV0r1l\nZWUIt253I55tgpa/VgRJ3gMN0l6SjY2NhjIFSV4egiTv8O5ZlrIESV4r/JTVV0VARLmQlMA0xtj7\n8um9RNSbMbabiHoD2Kd3L2NsKoCpAFBSUsJCoZAjGcLhMJze6yUTtyxE9w75CIXG6F7Pnz8LaGpE\nhw7tbcsbrl0HVJRj8KBBCJ0+0ANpbaQdkPzlJUjyVh6qB76ZjYKCAkOZgiQvD0GRd+GYRnRtnxuZ\nCzEiKPLy4KesvikCkpr+LwPYwBh7WnXpYwCTATwp///ILxmCxLTbxidbBEFAceI0UGBO0NxIBB0/\newQTAfwAwBoiWimfewCSAnibiG4FUAHgGh9lyChSbB1NxpNqC58E6YufVkNzYez2Y5Jf6aYqbiZ8\nRYtSIBC4QXgfTSNE+1IgEDhBKAKBIEkoilv05wTJRvgaEgiSRO8uBbhl4kBcNy55zsYEAkAogrRA\ntChTEyLCw5cGe/c0QWYghoYChptxfmGFIhAInCAUQcBw0roXRkMCgcANQhGkEaJDIBAInCAUQUDo\nIDsRc1KXC6dzAoHADWKyOCD85+Zx+HTNLnRDpeM4RIdAIBA4QfQIAkL/7u1xV2iQo3vFHIFAIHCD\nUARpQF6O9BpzssXrFAgE9hFDQ2nAPeedgLzsrMiuUwKBQGAHoQjSgE4Fubj/ohOTLYZAIEhRxFiC\nQCAQZDhCEQgEAkGGIxSBQCAQZDhCEQgEAkGGIxSBQCAQZDhCEQgEAkGGIxSBQCAQZDhCEQgEAkGG\nQywFHNUQ0X4AFQ5v7wHggIfi+I2Q11+EvP4i5PUPJ7IOYIz1tAqUEorADUS0lDFWkmw5eBHy+ouQ\n11+EvP7hp6xiaEggEAgyHKEIBAKBIMPJBEUwNdkC2ETI6y9CXn8R8vqHb7Km/RyBQCAQCMzJhB6B\nQCAQCEwQikAgEAgynLRWBER0ARFtJKItRDQlAPL0I6LZRLSeiNYR0d3y+UIimklEm+X/3eTzRETP\nyfKvJqKTkyR3NhGtIKJP5eOBRLRIlustIsqTz+fLx1vk68VJkLUrEb1LRKVEtIGITgty/hLRL+Wy\nsJaI3iSigiDlLxG9QkT7iGit6pzt/CSiyXL4zUQ0OcHy/kkuD6uJ6AMi6qq6dr8s70Yi+o7qfELq\nDj15VdfuJSJGRD3kY//ylzGWln8AsgGUATgOQB6AVQCGJVmm3gBOln93ArAJwDAATwGYIp+fAuCP\n8u+LAHwBgACMB7AoSXLfA+ANAJ/Kx28DuE7+/SKAO+XfdwF4Uf59HYC3kiDrqwBuk3/nAega1PwF\ncCyAbQDaqfL1h0HKXwBnAjgZwFrVOVv5CaAQwFb5fzf5d7cEyns+gBz59x9V8g6T64V8AAPl+iI7\nkXWHnrzy+X4AZkBaSNvD7/xNWKFP9B+A0wDMUB3fD+D+ZMulkfEjAOcB2Aigt3yuN4CN8u9/Arhe\nFT4SLoEy9gUwC8A5AD6VC+EB1YcVyWe54J4m/86Rw1ECZe0iV6ykOR/I/IWkCHbIH3COnL/fCVr+\nAijWVKy28hPA9QD+qTofE85veTXXvgtgmvw7pk5Q8jfRdYeevADeBTAKQDmiisC3/E3noSHlI1Oo\nlM8FArlbPwbAIgBFjLHd8qU9AIrk30F4hr8C+DWANvm4O4BqxliLjkwReeXrNXL4RDEQwH4A/5aH\nsl4iog4IaP4yxnYC+DOA7QB2Q8qvZQhu/irYzc8glGOFWyC1qoGAyktElwPYyRhbpbnkm7zprAgC\nCxF1BPAegF8wxmrV15ik0gNh00tElwDYxxhblmxZOMmB1M3+B2NsDIAjkIYuIgQsf7sBuBySAusD\noAOAC5IqlE2ClJ9WENGDAFoATEu2LEYQUXsADwB4OJHpprMi2AlpnE2hr3wuqRBRLiQlMI0x9r58\nei8R9Zav9wawTz6f7GeYCOAyIioH8D9Iw0PPAuhKRDk6MkXkla93AVCVQHkrAVQyxhbJx+9CUgxB\nzd9zAWxjjO1njDUDeB9Sngc1fxXs5mey8xlE9EMAlwC4UVZeMJErmfIeD6lhsEr+7voCWE5Ex5jI\n5VredFYESwAMli0w8iBNrn2cTIGIiAC8DGADY+xp1aWPASgz/ZMhzR0o52+SrQXGA6hRdcl9hzF2\nP2OsL2OsGFL+fc0YuxHAbABXGcirPMdVcviEtRYZY3sA7CCiIfKpSQDWI6D5C2lIaDwRtZfLhiJv\nIPNXhd38nAHgfCLqJveCzpfPJQQiugDS8OZljLF61aWPAVwnW2MNBDAYwGIkse5gjK1hjPVijBXL\n310lJAOTPfAzf/2aAAnCH6RZ9k2QLAAeDIA8p0PqRq8GsFL+uwjSOO8sAJsBfAWgUA5PAF6Q5V8D\noCSJsocQtRo6DtIHswXAOwDy5fMF8vEW+fpxSZBzNIClch5/CMmKIrD5C+AxAKUA1gJ4DZIFS2Dy\nF8CbkOYvmiFVSrc6yU9IY/Nb5L+bEyzvFkhj6Mo396Iq/IOyvBsBXKg6n5C6Q09ezfVyRCeLfctf\n4WJCIBAIMpx0HhoSCAQCAQdCEQgEAkGGIxSBQCAQZDhCEQgEAkGGIxSBQCAQZDhCEQhSCiK6zMob\nJBH1IaJ35d8/JKK/2UzjAY4w/yGiq6zC+QURhYkoJTZdFwQfoQgEKQVj7GPG2JMWYXYxxtxU0paK\nIJVRrVoWCAAIRSAICERULPuM/w8RbSKiaUR0LhHNk32sj5PDRVr4ctjniGg+EW1VWuhyXGr/7v3k\nFvRmInpEleaHRLSMpP0A7pDPPQmgHRGtJKJp8rmbZP/vq4joNVW8Z2rT1nmmDUT0LzmNL4monXwt\n0qInoh6yOwHl+T4kyc9/ORH9lIjukZ3oLSSiQlUSP5DlXKvKnw4k+bhfLN9zuSrej4noa0iLwQSC\nCEIRCILEIAB/ATBU/rsB0mrs/4NxK723HOYSAEY9hXEAvgdgJICrVUMqtzDGxgIoAfBzIurOGJsC\noIExNpoxdiMRDQfwEIBzGGOjANxtM+3BAF5gjA0HUC3LYcUIAFcCOAXA/7d396BRBGEYx/8vaBEL\nG1PbCZaKYBACRkFrIUI6ewuthZRilZA6WKQTCxEEGyFooiAJoiRGTrGKvUaRQKKo91i8I1k2H1xi\nisA8v2pub/Z2djnuvZ3jnrkDrClD9OaAa41+RySdItcpmCrbRsnoibPABWAsMoEVMnfpqqTzPYzB\nKuJCYAfJsjJrpQt0gKfKv76/IzPbt/JIUlfSezbikNumJa1IWieD3QbL9psR8RaYJ0O7Tmyx70Xg\ngaQvAJK+7vLYy5IWS/vNDufRNCNpVdJnMmr6cdnevg73y5heAEcjV966DNyKiEVgloylOF76T7fG\nbwZkbK/ZQfGz0e42HnfZ/r3a3Ce26dPOUVFEDJHpn+ckrUXELPmhuRu9HLvZ5w/QV9q/2fgi1j5u\nr9dh03mVcQxL+th8IiIGyFhus018R2A1uBS5zm4fcAV4SUY4fytF4CS59N8/vyLjwgGekdNJxyDX\n692nMX0CzpT2Xn/YHgGIiEEyifI7mTp5o6SZEhGn/3OcVgEXAqvBK3INiCXgoaTXwBPgUER8IOf3\n5xv97wJLEXFPUoecp39eppEm2B/jwPWIWAD69/gaP8r+k2TKJsBt4DA5/k55bLYjp4+amVXOdwRm\nZpVzITAzq5wLgZlZ5VwIzMwq50JgZlY5FwIzs8q5EJiZVe4vz6T9qmGfCxQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13c7afb00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16656: with minibatch training loss = 0.529 and accuracy of 0.84\n",
      "Iteration 16657: with minibatch training loss = 0.837 and accuracy of 0.7\n",
      "Iteration 16658: with minibatch training loss = 0.557 and accuracy of 0.84\n",
      "Iteration 16659: with minibatch training loss = 0.592 and accuracy of 0.83\n",
      "Iteration 16660: with minibatch training loss = 0.516 and accuracy of 0.88\n",
      "Iteration 16661: with minibatch training loss = 0.655 and accuracy of 0.8\n",
      "Iteration 16662: with minibatch training loss = 0.415 and accuracy of 0.88\n",
      "Iteration 16663: with minibatch training loss = 0.525 and accuracy of 0.84\n",
      "Iteration 16664: with minibatch training loss = 0.526 and accuracy of 0.84\n",
      "Iteration 16665: with minibatch training loss = 0.689 and accuracy of 0.83\n",
      "Iteration 16666: with minibatch training loss = 0.565 and accuracy of 0.83\n",
      "Iteration 16667: with minibatch training loss = 0.693 and accuracy of 0.78\n",
      "Iteration 16668: with minibatch training loss = 0.617 and accuracy of 0.81\n",
      "Iteration 16669: with minibatch training loss = 0.779 and accuracy of 0.8\n",
      "Iteration 16670: with minibatch training loss = 0.615 and accuracy of 0.81\n",
      "Iteration 16671: with minibatch training loss = 0.517 and accuracy of 0.86\n",
      "Iteration 16672: with minibatch training loss = 0.833 and accuracy of 0.73\n",
      "Iteration 16673: with minibatch training loss = 0.734 and accuracy of 0.8\n",
      "Iteration 16674: with minibatch training loss = 0.719 and accuracy of 0.78\n",
      "Iteration 16675: with minibatch training loss = 0.598 and accuracy of 0.81\n",
      "Iteration 16676: with minibatch training loss = 0.457 and accuracy of 0.84\n",
      "Iteration 16677: with minibatch training loss = 0.695 and accuracy of 0.75\n",
      "Iteration 16678: with minibatch training loss = 0.464 and accuracy of 0.86\n",
      "Iteration 16679: with minibatch training loss = 0.647 and accuracy of 0.8\n",
      "Iteration 16680: with minibatch training loss = 0.505 and accuracy of 0.84\n",
      "Iteration 16681: with minibatch training loss = 0.835 and accuracy of 0.75\n",
      "Iteration 16682: with minibatch training loss = 0.446 and accuracy of 0.91\n",
      "Iteration 16683: with minibatch training loss = 0.624 and accuracy of 0.8\n",
      "Iteration 16684: with minibatch training loss = 0.733 and accuracy of 0.8\n",
      "Iteration 16685: with minibatch training loss = 0.552 and accuracy of 0.81\n",
      "Iteration 16686: with minibatch training loss = 0.981 and accuracy of 0.72\n",
      "Iteration 16687: with minibatch training loss = 0.982 and accuracy of 0.66\n",
      "Iteration 16688: with minibatch training loss = 0.628 and accuracy of 0.8\n",
      "Iteration 16689: with minibatch training loss = 0.65 and accuracy of 0.8\n",
      "Iteration 16690: with minibatch training loss = 0.734 and accuracy of 0.8\n",
      "Iteration 16691: with minibatch training loss = 0.592 and accuracy of 0.8\n",
      "Iteration 16692: with minibatch training loss = 0.706 and accuracy of 0.78\n",
      "Iteration 16693: with minibatch training loss = 0.711 and accuracy of 0.77\n",
      "Iteration 16694: with minibatch training loss = 0.657 and accuracy of 0.8\n",
      "Iteration 16695: with minibatch training loss = 0.567 and accuracy of 0.81\n",
      "Iteration 16696: with minibatch training loss = 0.599 and accuracy of 0.83\n",
      "Iteration 16697: with minibatch training loss = 0.903 and accuracy of 0.72\n",
      "Iteration 16698: with minibatch training loss = 0.436 and accuracy of 0.84\n",
      "Iteration 16699: with minibatch training loss = 0.847 and accuracy of 0.73\n",
      "Iteration 16700: with minibatch training loss = 0.921 and accuracy of 0.69\n",
      "Iteration 16701: with minibatch training loss = 0.864 and accuracy of 0.7\n",
      "Iteration 16702: with minibatch training loss = 0.389 and accuracy of 0.88\n",
      "Iteration 16703: with minibatch training loss = 0.947 and accuracy of 0.69\n",
      "Iteration 16704: with minibatch training loss = 0.931 and accuracy of 0.69\n",
      "Iteration 16705: with minibatch training loss = 0.821 and accuracy of 0.78\n",
      "Iteration 16706: with minibatch training loss = 0.46 and accuracy of 0.86\n",
      "Iteration 16707: with minibatch training loss = 0.85 and accuracy of 0.75\n",
      "Iteration 16708: with minibatch training loss = 0.617 and accuracy of 0.83\n",
      "Iteration 16709: with minibatch training loss = 0.585 and accuracy of 0.8\n",
      "Iteration 16710: with minibatch training loss = 0.738 and accuracy of 0.77\n",
      "Iteration 16711: with minibatch training loss = 0.585 and accuracy of 0.8\n",
      "Iteration 16712: with minibatch training loss = 0.835 and accuracy of 0.78\n",
      "Iteration 16713: with minibatch training loss = 0.435 and accuracy of 0.89\n",
      "Iteration 16714: with minibatch training loss = 0.609 and accuracy of 0.8\n",
      "Iteration 16715: with minibatch training loss = 0.56 and accuracy of 0.83\n",
      "Iteration 16716: with minibatch training loss = 0.601 and accuracy of 0.83\n",
      "Iteration 16717: with minibatch training loss = 0.563 and accuracy of 0.83\n",
      "Iteration 16718: with minibatch training loss = 0.968 and accuracy of 0.73\n",
      "Iteration 16719: with minibatch training loss = 0.685 and accuracy of 0.77\n",
      "Iteration 16720: with minibatch training loss = 0.632 and accuracy of 0.81\n",
      "Iteration 16721: with minibatch training loss = 0.543 and accuracy of 0.83\n",
      "Iteration 16722: with minibatch training loss = 0.587 and accuracy of 0.83\n",
      "Iteration 16723: with minibatch training loss = 0.584 and accuracy of 0.83\n",
      "Iteration 16724: with minibatch training loss = 0.682 and accuracy of 0.75\n",
      "Iteration 16725: with minibatch training loss = 0.535 and accuracy of 0.81\n",
      "Iteration 16726: with minibatch training loss = 0.622 and accuracy of 0.78\n",
      "Iteration 16727: with minibatch training loss = 0.459 and accuracy of 0.84\n",
      "Iteration 16728: with minibatch training loss = 0.703 and accuracy of 0.83\n",
      "Iteration 16729: with minibatch training loss = 0.732 and accuracy of 0.77\n",
      "Iteration 16730: with minibatch training loss = 0.495 and accuracy of 0.83\n",
      "Iteration 16731: with minibatch training loss = 0.614 and accuracy of 0.8\n",
      "Iteration 16732: with minibatch training loss = 0.703 and accuracy of 0.78\n",
      "Iteration 16733: with minibatch training loss = 0.727 and accuracy of 0.78\n",
      "Iteration 16734: with minibatch training loss = 0.791 and accuracy of 0.75\n",
      "Iteration 16735: with minibatch training loss = 0.643 and accuracy of 0.8\n",
      "Iteration 16736: with minibatch training loss = 0.663 and accuracy of 0.8\n",
      "Iteration 16737: with minibatch training loss = 0.633 and accuracy of 0.83\n",
      "Iteration 16738: with minibatch training loss = 0.737 and accuracy of 0.77\n",
      "Iteration 16739: with minibatch training loss = 0.618 and accuracy of 0.81\n",
      "Iteration 16740: with minibatch training loss = 0.701 and accuracy of 0.8\n",
      "Iteration 16741: with minibatch training loss = 0.629 and accuracy of 0.83\n",
      "Iteration 16742: with minibatch training loss = 0.513 and accuracy of 0.86\n",
      "Iteration 16743: with minibatch training loss = 0.547 and accuracy of 0.86\n",
      "Iteration 16744: with minibatch training loss = 0.569 and accuracy of 0.84\n",
      "Iteration 16745: with minibatch training loss = 0.591 and accuracy of 0.84\n",
      "Iteration 16746: with minibatch training loss = 0.746 and accuracy of 0.8\n",
      "Iteration 16747: with minibatch training loss = 0.711 and accuracy of 0.84\n",
      "Iteration 16748: with minibatch training loss = 0.661 and accuracy of 0.75\n",
      "Iteration 16749: with minibatch training loss = 0.867 and accuracy of 0.7\n",
      "Iteration 16750: with minibatch training loss = 0.514 and accuracy of 0.81\n",
      "Iteration 16751: with minibatch training loss = 0.717 and accuracy of 0.77\n",
      "Iteration 16752: with minibatch training loss = 0.509 and accuracy of 0.84\n",
      "Iteration 16753: with minibatch training loss = 0.737 and accuracy of 0.78\n",
      "Iteration 16754: with minibatch training loss = 0.392 and accuracy of 0.89\n",
      "Iteration 16755: with minibatch training loss = 0.791 and accuracy of 0.75\n",
      "Iteration 16756: with minibatch training loss = 0.687 and accuracy of 0.78\n",
      "Iteration 16757: with minibatch training loss = 0.507 and accuracy of 0.83\n",
      "Iteration 16758: with minibatch training loss = 0.605 and accuracy of 0.8\n",
      "Iteration 16759: with minibatch training loss = 0.635 and accuracy of 0.83\n",
      "Iteration 16760: with minibatch training loss = 1.01 and accuracy of 0.64\n",
      "Iteration 16761: with minibatch training loss = 0.544 and accuracy of 0.8\n",
      "Iteration 16762: with minibatch training loss = 0.668 and accuracy of 0.8\n",
      "Iteration 16763: with minibatch training loss = 0.533 and accuracy of 0.81\n",
      "Iteration 16764: with minibatch training loss = 0.631 and accuracy of 0.81\n",
      "Iteration 16765: with minibatch training loss = 0.335 and accuracy of 0.89\n",
      "Iteration 16766: with minibatch training loss = 0.803 and accuracy of 0.72\n",
      "Iteration 16767: with minibatch training loss = 0.734 and accuracy of 0.78\n",
      "Iteration 16768: with minibatch training loss = 0.654 and accuracy of 0.83\n",
      "Iteration 16769: with minibatch training loss = 0.451 and accuracy of 0.86\n",
      "Iteration 16770: with minibatch training loss = 0.598 and accuracy of 0.81\n",
      "Iteration 16771: with minibatch training loss = 0.655 and accuracy of 0.81\n",
      "Iteration 16772: with minibatch training loss = 0.221 and accuracy of 0.95\n",
      "Iteration 16773: with minibatch training loss = 0.985 and accuracy of 0.67\n",
      "Iteration 16774: with minibatch training loss = 0.438 and accuracy of 0.89\n",
      "Iteration 16775: with minibatch training loss = 0.542 and accuracy of 0.83\n",
      "Iteration 16776: with minibatch training loss = 0.623 and accuracy of 0.8\n",
      "Iteration 16777: with minibatch training loss = 0.666 and accuracy of 0.83\n",
      "Iteration 16778: with minibatch training loss = 0.66 and accuracy of 0.78\n",
      "Iteration 16779: with minibatch training loss = 0.68 and accuracy of 0.8\n",
      "Iteration 16780: with minibatch training loss = 0.561 and accuracy of 0.83\n",
      "Iteration 16781: with minibatch training loss = 0.677 and accuracy of 0.78\n",
      "Iteration 16782: with minibatch training loss = 0.943 and accuracy of 0.69\n",
      "Iteration 16783: with minibatch training loss = 0.439 and accuracy of 0.88\n",
      "Iteration 16784: with minibatch training loss = 0.543 and accuracy of 0.84\n",
      "Iteration 16785: with minibatch training loss = 0.556 and accuracy of 0.83\n",
      "Iteration 16786: with minibatch training loss = 0.446 and accuracy of 0.86\n",
      "Iteration 16787: with minibatch training loss = 0.708 and accuracy of 0.8\n",
      "Iteration 16788: with minibatch training loss = 0.389 and accuracy of 0.89\n",
      "Iteration 16789: with minibatch training loss = 0.602 and accuracy of 0.8\n",
      "Iteration 16790: with minibatch training loss = 0.839 and accuracy of 0.69\n",
      "Iteration 16791: with minibatch training loss = 0.852 and accuracy of 0.7\n",
      "Iteration 16792: with minibatch training loss = 0.808 and accuracy of 0.73\n",
      "Iteration 16793: with minibatch training loss = 0.711 and accuracy of 0.78\n",
      "Iteration 16794: with minibatch training loss = 0.867 and accuracy of 0.73\n",
      "Iteration 16795: with minibatch training loss = 0.714 and accuracy of 0.78\n",
      "Iteration 16796: with minibatch training loss = 0.621 and accuracy of 0.81\n",
      "Iteration 16797: with minibatch training loss = 0.462 and accuracy of 0.88\n",
      "Iteration 16798: with minibatch training loss = 0.761 and accuracy of 0.73\n",
      "Iteration 16799: with minibatch training loss = 0.637 and accuracy of 0.8\n",
      "Iteration 16800: with minibatch training loss = 0.797 and accuracy of 0.77\n",
      "Iteration 16801: with minibatch training loss = 0.543 and accuracy of 0.78\n",
      "Iteration 16802: with minibatch training loss = 0.513 and accuracy of 0.84\n",
      "Iteration 16803: with minibatch training loss = 0.758 and accuracy of 0.78\n",
      "Iteration 16804: with minibatch training loss = 0.702 and accuracy of 0.8\n",
      "Iteration 16805: with minibatch training loss = 0.492 and accuracy of 0.84\n",
      "Iteration 16806: with minibatch training loss = 0.845 and accuracy of 0.75\n",
      "Iteration 16807: with minibatch training loss = 0.557 and accuracy of 0.83\n",
      "Iteration 16808: with minibatch training loss = 0.884 and accuracy of 0.73\n",
      "Iteration 16809: with minibatch training loss = 0.631 and accuracy of 0.81\n",
      "Iteration 16810: with minibatch training loss = 0.81 and accuracy of 0.72\n",
      "Iteration 16811: with minibatch training loss = 0.786 and accuracy of 0.75\n",
      "Iteration 16812: with minibatch training loss = 0.538 and accuracy of 0.83\n",
      "Iteration 16813: with minibatch training loss = 0.512 and accuracy of 0.86\n",
      "Iteration 16814: with minibatch training loss = 0.646 and accuracy of 0.78\n",
      "Iteration 16815: with minibatch training loss = 0.541 and accuracy of 0.83\n",
      "Iteration 16816: with minibatch training loss = 0.454 and accuracy of 0.86\n",
      "Iteration 16817: with minibatch training loss = 0.697 and accuracy of 0.8\n",
      "Iteration 16818: with minibatch training loss = 0.535 and accuracy of 0.84\n",
      "Iteration 16819: with minibatch training loss = 0.695 and accuracy of 0.77\n",
      "Iteration 16820: with minibatch training loss = 0.704 and accuracy of 0.77\n",
      "Iteration 16821: with minibatch training loss = 0.948 and accuracy of 0.7\n",
      "Iteration 16822: with minibatch training loss = 0.736 and accuracy of 0.75\n",
      "Iteration 16823: with minibatch training loss = 0.714 and accuracy of 0.8\n",
      "Iteration 16824: with minibatch training loss = 0.678 and accuracy of 0.83\n",
      "Iteration 16825: with minibatch training loss = 0.794 and accuracy of 0.72\n",
      "Iteration 16826: with minibatch training loss = 0.701 and accuracy of 0.78\n",
      "Iteration 16827: with minibatch training loss = 0.504 and accuracy of 0.81\n",
      "Iteration 16828: with minibatch training loss = 0.547 and accuracy of 0.88\n",
      "Iteration 16829: with minibatch training loss = 0.582 and accuracy of 0.83\n",
      "Iteration 16830: with minibatch training loss = 0.76 and accuracy of 0.77\n",
      "Iteration 16831: with minibatch training loss = 0.71 and accuracy of 0.78\n",
      "Iteration 16832: with minibatch training loss = 0.707 and accuracy of 0.75\n",
      "Iteration 16833: with minibatch training loss = 0.591 and accuracy of 0.8\n",
      "Iteration 16834: with minibatch training loss = 0.683 and accuracy of 0.78\n",
      "Iteration 16835: with minibatch training loss = 0.743 and accuracy of 0.78\n",
      "Iteration 16836: with minibatch training loss = 0.651 and accuracy of 0.81\n",
      "Iteration 16837: with minibatch training loss = 0.652 and accuracy of 0.81\n",
      "Iteration 16838: with minibatch training loss = 0.6 and accuracy of 0.81\n",
      "Iteration 16839: with minibatch training loss = 0.835 and accuracy of 0.73\n",
      "Iteration 16840: with minibatch training loss = 0.255 and accuracy of 0.92\n",
      "Iteration 16841: with minibatch training loss = 0.809 and accuracy of 0.75\n",
      "Iteration 16842: with minibatch training loss = 0.533 and accuracy of 0.83\n",
      "Iteration 16843: with minibatch training loss = 0.366 and accuracy of 0.88\n",
      "Iteration 16844: with minibatch training loss = 0.778 and accuracy of 0.77\n",
      "Iteration 16845: with minibatch training loss = 0.798 and accuracy of 0.75\n",
      "Iteration 16846: with minibatch training loss = 0.746 and accuracy of 0.77\n",
      "Iteration 16847: with minibatch training loss = 0.685 and accuracy of 0.78\n",
      "Iteration 16848: with minibatch training loss = 0.801 and accuracy of 0.75\n",
      "Iteration 16849: with minibatch training loss = 0.669 and accuracy of 0.8\n",
      "Iteration 16850: with minibatch training loss = 0.718 and accuracy of 0.75\n",
      "Iteration 16851: with minibatch training loss = 0.736 and accuracy of 0.81\n",
      "Iteration 16852: with minibatch training loss = 0.541 and accuracy of 0.86\n",
      "Iteration 16853: with minibatch training loss = 0.697 and accuracy of 0.77\n",
      "Iteration 16854: with minibatch training loss = 0.575 and accuracy of 0.83\n",
      "Iteration 16855: with minibatch training loss = 0.749 and accuracy of 0.8\n",
      "Iteration 16856: with minibatch training loss = 0.631 and accuracy of 0.8\n",
      "Iteration 16857: with minibatch training loss = 0.759 and accuracy of 0.78\n",
      "Iteration 16858: with minibatch training loss = 0.583 and accuracy of 0.83\n",
      "Iteration 16859: with minibatch training loss = 0.697 and accuracy of 0.78\n",
      "Iteration 16860: with minibatch training loss = 0.658 and accuracy of 0.8\n",
      "Iteration 16861: with minibatch training loss = 0.57 and accuracy of 0.84\n",
      "Iteration 16862: with minibatch training loss = 0.592 and accuracy of 0.81\n",
      "Iteration 16863: with minibatch training loss = 0.524 and accuracy of 0.84\n",
      "Iteration 16864: with minibatch training loss = 0.923 and accuracy of 0.72\n",
      "Iteration 16865: with minibatch training loss = 0.657 and accuracy of 0.78\n",
      "Iteration 16866: with minibatch training loss = 0.664 and accuracy of 0.8\n",
      "Iteration 16867: with minibatch training loss = 0.773 and accuracy of 0.78\n",
      "Iteration 16868: with minibatch training loss = 0.884 and accuracy of 0.72\n",
      "Iteration 16869: with minibatch training loss = 0.597 and accuracy of 0.81\n",
      "Iteration 16870: with minibatch training loss = 0.555 and accuracy of 0.8\n",
      "Iteration 16871: with minibatch training loss = 0.658 and accuracy of 0.81\n",
      "Iteration 16872: with minibatch training loss = 0.877 and accuracy of 0.7\n",
      "Iteration 16873: with minibatch training loss = 0.543 and accuracy of 0.81\n",
      "Iteration 16874: with minibatch training loss = 0.834 and accuracy of 0.72\n",
      "Iteration 16875: with minibatch training loss = 0.769 and accuracy of 0.75\n",
      "Iteration 16876: with minibatch training loss = 0.55 and accuracy of 0.86\n",
      "Iteration 16877: with minibatch training loss = 0.69 and accuracy of 0.77\n",
      "Iteration 16878: with minibatch training loss = 0.795 and accuracy of 0.77\n",
      "Iteration 16879: with minibatch training loss = 0.824 and accuracy of 0.78\n",
      "Iteration 16880: with minibatch training loss = 0.558 and accuracy of 0.86\n",
      "Iteration 16881: with minibatch training loss = 0.734 and accuracy of 0.8\n",
      "Iteration 16882: with minibatch training loss = 0.613 and accuracy of 0.8\n",
      "Iteration 16883: with minibatch training loss = 0.616 and accuracy of 0.78\n",
      "Iteration 16884: with minibatch training loss = 0.645 and accuracy of 0.81\n",
      "Iteration 16885: with minibatch training loss = 0.656 and accuracy of 0.81\n",
      "Iteration 16886: with minibatch training loss = 0.624 and accuracy of 0.81\n",
      "Iteration 16887: with minibatch training loss = 0.582 and accuracy of 0.83\n",
      "Iteration 16888: with minibatch training loss = 0.881 and accuracy of 0.7\n",
      "Iteration 16889: with minibatch training loss = 0.73 and accuracy of 0.73\n",
      "Iteration 16890: with minibatch training loss = 0.716 and accuracy of 0.78\n",
      "Iteration 16891: with minibatch training loss = 0.877 and accuracy of 0.72\n",
      "Iteration 16892: with minibatch training loss = 0.522 and accuracy of 0.83\n",
      "Iteration 16893: with minibatch training loss = 0.918 and accuracy of 0.7\n",
      "Iteration 16894: with minibatch training loss = 0.381 and accuracy of 0.89\n",
      "Iteration 16895: with minibatch training loss = 0.683 and accuracy of 0.78\n",
      "Iteration 16896: with minibatch training loss = 0.592 and accuracy of 0.81\n",
      "Iteration 16897: with minibatch training loss = 0.539 and accuracy of 0.81\n",
      "Iteration 16898: with minibatch training loss = 0.479 and accuracy of 0.86\n",
      "Iteration 16899: with minibatch training loss = 0.52 and accuracy of 0.84\n",
      "Iteration 16900: with minibatch training loss = 0.788 and accuracy of 0.75\n",
      "Iteration 16901: with minibatch training loss = 0.529 and accuracy of 0.84\n",
      "Iteration 16902: with minibatch training loss = 0.797 and accuracy of 0.73\n",
      "Iteration 16903: with minibatch training loss = 0.993 and accuracy of 0.72\n",
      "Iteration 16904: with minibatch training loss = 0.305 and accuracy of 0.91\n",
      "Iteration 16905: with minibatch training loss = 0.743 and accuracy of 0.75\n",
      "Iteration 16906: with minibatch training loss = 0.767 and accuracy of 0.75\n",
      "Iteration 16907: with minibatch training loss = 0.605 and accuracy of 0.8\n",
      "Iteration 16908: with minibatch training loss = 0.85 and accuracy of 0.7\n",
      "Iteration 16909: with minibatch training loss = 0.57 and accuracy of 0.84\n",
      "Iteration 16910: with minibatch training loss = 0.502 and accuracy of 0.84\n",
      "Iteration 16911: with minibatch training loss = 0.788 and accuracy of 0.72\n",
      "Iteration 16912: with minibatch training loss = 0.645 and accuracy of 0.81\n",
      "Iteration 16913: with minibatch training loss = 0.625 and accuracy of 0.78\n",
      "Iteration 16914: with minibatch training loss = 0.596 and accuracy of 0.84\n",
      "Iteration 16915: with minibatch training loss = 0.544 and accuracy of 0.81\n",
      "Iteration 16916: with minibatch training loss = 0.736 and accuracy of 0.78\n",
      "Iteration 16917: with minibatch training loss = 0.813 and accuracy of 0.77\n",
      "Iteration 16918: with minibatch training loss = 0.8 and accuracy of 0.72\n",
      "Iteration 16919: with minibatch training loss = 0.518 and accuracy of 0.84\n",
      "Iteration 16920: with minibatch training loss = 0.628 and accuracy of 0.8\n",
      "Iteration 16921: with minibatch training loss = 0.658 and accuracy of 0.8\n",
      "Iteration 16922: with minibatch training loss = 0.602 and accuracy of 0.83\n",
      "Iteration 16923: with minibatch training loss = 0.741 and accuracy of 0.78\n",
      "Iteration 16924: with minibatch training loss = 0.338 and accuracy of 0.91\n",
      "Iteration 16925: with minibatch training loss = 1.01 and accuracy of 0.67\n",
      "Iteration 16926: with minibatch training loss = 0.304 and accuracy of 0.89\n",
      "Iteration 16927: with minibatch training loss = 0.372 and accuracy of 0.92\n",
      "Iteration 16928: with minibatch training loss = 0.71 and accuracy of 0.8\n",
      "Iteration 16929: with minibatch training loss = 0.553 and accuracy of 0.86\n",
      "Iteration 16930: with minibatch training loss = 0.439 and accuracy of 0.84\n",
      "Iteration 16931: with minibatch training loss = 0.91 and accuracy of 0.77\n",
      "Iteration 16932: with minibatch training loss = 0.998 and accuracy of 0.7\n",
      "Iteration 16933: with minibatch training loss = 0.599 and accuracy of 0.8\n",
      "Iteration 16934: with minibatch training loss = 0.528 and accuracy of 0.84\n",
      "Iteration 16935: with minibatch training loss = 0.568 and accuracy of 0.8\n",
      "Iteration 16936: with minibatch training loss = 0.646 and accuracy of 0.83\n",
      "Iteration 16937: with minibatch training loss = 0.541 and accuracy of 0.83\n",
      "Iteration 16938: with minibatch training loss = 0.597 and accuracy of 0.83\n",
      "Iteration 16939: with minibatch training loss = 0.437 and accuracy of 0.84\n",
      "Iteration 16940: with minibatch training loss = 0.586 and accuracy of 0.8\n",
      "Iteration 16941: with minibatch training loss = 0.52 and accuracy of 0.84\n",
      "Iteration 16942: with minibatch training loss = 0.376 and accuracy of 0.89\n",
      "Iteration 16943: with minibatch training loss = 0.534 and accuracy of 0.83\n",
      "Iteration 16944: with minibatch training loss = 0.682 and accuracy of 0.8\n",
      "Iteration 16945: with minibatch training loss = 0.42 and accuracy of 0.88\n",
      "Iteration 16946: with minibatch training loss = 0.326 and accuracy of 0.91\n",
      "Iteration 16947: with minibatch training loss = 0.64 and accuracy of 0.81\n",
      "Iteration 16948: with minibatch training loss = 0.734 and accuracy of 0.75\n",
      "Iteration 16949: with minibatch training loss = 0.736 and accuracy of 0.8\n",
      "Iteration 16950: with minibatch training loss = 0.866 and accuracy of 0.73\n",
      "Iteration 16951: with minibatch training loss = 0.56 and accuracy of 0.81\n",
      "Iteration 16952: with minibatch training loss = 0.592 and accuracy of 0.83\n",
      "Iteration 16953: with minibatch training loss = 0.782 and accuracy of 0.78\n",
      "Iteration 16954: with minibatch training loss = 0.578 and accuracy of 0.81\n",
      "Iteration 16955: with minibatch training loss = 0.557 and accuracy of 0.84\n",
      "Iteration 16956: with minibatch training loss = 0.65 and accuracy of 0.8\n",
      "Iteration 16957: with minibatch training loss = 0.789 and accuracy of 0.75\n",
      "Iteration 16958: with minibatch training loss = 0.776 and accuracy of 0.75\n",
      "Iteration 16959: with minibatch training loss = 0.87 and accuracy of 0.73\n",
      "Iteration 16960: with minibatch training loss = 0.666 and accuracy of 0.77\n",
      "Iteration 16961: with minibatch training loss = 0.632 and accuracy of 0.83\n",
      "Iteration 16962: with minibatch training loss = 0.858 and accuracy of 0.7\n",
      "Iteration 16963: with minibatch training loss = 0.711 and accuracy of 0.8\n",
      "Iteration 16964: with minibatch training loss = 0.719 and accuracy of 0.78\n",
      "Iteration 16965: with minibatch training loss = 0.714 and accuracy of 0.77\n",
      "Iteration 16966: with minibatch training loss = 0.7 and accuracy of 0.78\n",
      "Iteration 16967: with minibatch training loss = 0.542 and accuracy of 0.83\n",
      "Iteration 16968: with minibatch training loss = 0.687 and accuracy of 0.8\n",
      "Iteration 16969: with minibatch training loss = 0.6 and accuracy of 0.81\n",
      "Iteration 16970: with minibatch training loss = 0.716 and accuracy of 0.8\n",
      "Iteration 16971: with minibatch training loss = 0.592 and accuracy of 0.83\n",
      "Iteration 16972: with minibatch training loss = 0.571 and accuracy of 0.83\n",
      "Iteration 16973: with minibatch training loss = 0.694 and accuracy of 0.81\n",
      "Iteration 16974: with minibatch training loss = 0.599 and accuracy of 0.84\n",
      "Iteration 16975: with minibatch training loss = 0.686 and accuracy of 0.8\n",
      "Iteration 16976: with minibatch training loss = 0.781 and accuracy of 0.75\n",
      "Iteration 16977: with minibatch training loss = 0.891 and accuracy of 0.72\n",
      "Iteration 16978: with minibatch training loss = 0.658 and accuracy of 0.78\n",
      "Iteration 16979: with minibatch training loss = 0.606 and accuracy of 0.83\n",
      "Iteration 16980: with minibatch training loss = 0.675 and accuracy of 0.77\n",
      "Iteration 16981: with minibatch training loss = 0.414 and accuracy of 0.88\n",
      "Iteration 16982: with minibatch training loss = 0.784 and accuracy of 0.77\n",
      "Iteration 16983: with minibatch training loss = 0.533 and accuracy of 0.83\n",
      "Iteration 16984: with minibatch training loss = 0.578 and accuracy of 0.78\n",
      "Iteration 16985: with minibatch training loss = 0.672 and accuracy of 0.78\n",
      "Iteration 16986: with minibatch training loss = 0.98 and accuracy of 0.67\n",
      "Iteration 16987: with minibatch training loss = 0.409 and accuracy of 0.89\n",
      "Iteration 16988: with minibatch training loss = 0.606 and accuracy of 0.81\n",
      "Iteration 16989: with minibatch training loss = 0.473 and accuracy of 0.89\n",
      "Iteration 16990: with minibatch training loss = 0.57 and accuracy of 0.83\n",
      "Iteration 16991: with minibatch training loss = 0.65 and accuracy of 0.78\n",
      "Iteration 16992: with minibatch training loss = 0.773 and accuracy of 0.77\n",
      "Iteration 16993: with minibatch training loss = 0.534 and accuracy of 0.84\n",
      "Iteration 16994: with minibatch training loss = 0.67 and accuracy of 0.77\n",
      "Iteration 16995: with minibatch training loss = 0.494 and accuracy of 0.84\n",
      "Iteration 16996: with minibatch training loss = 0.818 and accuracy of 0.78\n",
      "Iteration 16997: with minibatch training loss = 0.481 and accuracy of 0.88\n",
      "Iteration 16998: with minibatch training loss = 0.772 and accuracy of 0.77\n",
      "Iteration 16999: with minibatch training loss = 0.756 and accuracy of 0.75\n",
      "Iteration 17000: with minibatch training loss = 1 and accuracy of 0.72\n",
      "Iteration 17001: with minibatch training loss = 0.578 and accuracy of 0.83\n",
      "Iteration 17002: with minibatch training loss = 0.718 and accuracy of 0.78\n",
      "Iteration 17003: with minibatch training loss = 0.488 and accuracy of 0.86\n",
      "Iteration 17004: with minibatch training loss = 0.806 and accuracy of 0.75\n",
      "Iteration 17005: with minibatch training loss = 0.545 and accuracy of 0.83\n",
      "Iteration 17006: with minibatch training loss = 0.724 and accuracy of 0.75\n",
      "Iteration 17007: with minibatch training loss = 0.612 and accuracy of 0.8\n",
      "Iteration 17008: with minibatch training loss = 0.625 and accuracy of 0.81\n",
      "Iteration 17009: with minibatch training loss = 0.73 and accuracy of 0.73\n",
      "Iteration 17010: with minibatch training loss = 0.493 and accuracy of 0.86\n",
      "Iteration 17011: with minibatch training loss = 0.734 and accuracy of 0.78\n",
      "Iteration 17012: with minibatch training loss = 0.738 and accuracy of 0.75\n",
      "Iteration 17013: with minibatch training loss = 0.843 and accuracy of 0.72\n",
      "Iteration 17014: with minibatch training loss = 0.391 and accuracy of 0.88\n",
      "Iteration 17015: with minibatch training loss = 0.48 and accuracy of 0.84\n",
      "Iteration 17016: with minibatch training loss = 0.677 and accuracy of 0.8\n",
      "Iteration 17017: with minibatch training loss = 0.69 and accuracy of 0.77\n",
      "Iteration 17018: with minibatch training loss = 0.676 and accuracy of 0.8\n",
      "Iteration 17019: with minibatch training loss = 1.05 and accuracy of 0.67\n",
      "Iteration 17020: with minibatch training loss = 0.79 and accuracy of 0.77\n",
      "Iteration 17021: with minibatch training loss = 0.449 and accuracy of 0.86\n",
      "Iteration 17022: with minibatch training loss = 0.549 and accuracy of 0.84\n",
      "Iteration 17023: with minibatch training loss = 0.834 and accuracy of 0.69\n",
      "Iteration 17024: with minibatch training loss = 0.529 and accuracy of 0.84\n",
      "Iteration 17025: with minibatch training loss = 0.7 and accuracy of 0.8\n",
      "Iteration 17026: with minibatch training loss = 0.513 and accuracy of 0.83\n",
      "Iteration 17027: with minibatch training loss = 0.686 and accuracy of 0.8\n",
      "Iteration 17028: with minibatch training loss = 0.474 and accuracy of 0.88\n",
      "Iteration 17029: with minibatch training loss = 0.69 and accuracy of 0.77\n",
      "Iteration 17030: with minibatch training loss = 0.667 and accuracy of 0.8\n",
      "Iteration 17031: with minibatch training loss = 0.336 and accuracy of 0.89\n",
      "Iteration 17032: with minibatch training loss = 0.474 and accuracy of 0.83\n",
      "Iteration 17033: with minibatch training loss = 0.892 and accuracy of 0.73\n",
      "Iteration 17034: with minibatch training loss = 0.816 and accuracy of 0.77\n",
      "Iteration 17035: with minibatch training loss = 0.759 and accuracy of 0.75\n",
      "Iteration 17036: with minibatch training loss = 0.536 and accuracy of 0.81\n",
      "Iteration 17037: with minibatch training loss = 0.844 and accuracy of 0.73\n",
      "Iteration 17038: with minibatch training loss = 0.877 and accuracy of 0.72\n",
      "Iteration 17039: with minibatch training loss = 0.651 and accuracy of 0.77\n",
      "Iteration 17040: with minibatch training loss = 0.432 and accuracy of 0.84\n",
      "Iteration 17041: with minibatch training loss = 0.532 and accuracy of 0.84\n",
      "Iteration 17042: with minibatch training loss = 0.661 and accuracy of 0.77\n",
      "Iteration 17043: with minibatch training loss = 0.674 and accuracy of 0.78\n",
      "Iteration 17044: with minibatch training loss = 0.462 and accuracy of 0.84\n",
      "Iteration 17045: with minibatch training loss = 0.763 and accuracy of 0.75\n",
      "Iteration 17046: with minibatch training loss = 0.429 and accuracy of 0.88\n",
      "Iteration 17047: with minibatch training loss = 0.723 and accuracy of 0.78\n",
      "Iteration 17048: with minibatch training loss = 0.526 and accuracy of 0.84\n",
      "Iteration 17049: with minibatch training loss = 0.58 and accuracy of 0.83\n",
      "Iteration 17050: with minibatch training loss = 0.615 and accuracy of 0.81\n",
      "Iteration 17051: with minibatch training loss = 0.357 and accuracy of 0.89\n",
      "Iteration 17052: with minibatch training loss = 0.63 and accuracy of 0.78\n",
      "Iteration 17053: with minibatch training loss = 0.837 and accuracy of 0.77\n",
      "Iteration 17054: with minibatch training loss = 0.592 and accuracy of 0.81\n",
      "Iteration 17055: with minibatch training loss = 0.52 and accuracy of 0.84\n",
      "Iteration 17056: with minibatch training loss = 0.729 and accuracy of 0.75\n",
      "Iteration 17057: with minibatch training loss = 0.668 and accuracy of 0.81\n",
      "Iteration 17058: with minibatch training loss = 0.546 and accuracy of 0.81\n",
      "Iteration 17059: with minibatch training loss = 0.599 and accuracy of 0.8\n",
      "Iteration 17060: with minibatch training loss = 0.553 and accuracy of 0.8\n",
      "Iteration 17061: with minibatch training loss = 0.792 and accuracy of 0.75\n",
      "Iteration 17062: with minibatch training loss = 0.771 and accuracy of 0.78\n",
      "Iteration 17063: with minibatch training loss = 0.68 and accuracy of 0.8\n",
      "Iteration 17064: with minibatch training loss = 0.561 and accuracy of 0.81\n",
      "Iteration 17065: with minibatch training loss = 0.839 and accuracy of 0.72\n",
      "Iteration 17066: with minibatch training loss = 0.682 and accuracy of 0.77\n",
      "Iteration 17067: with minibatch training loss = 0.72 and accuracy of 0.8\n",
      "Iteration 17068: with minibatch training loss = 0.453 and accuracy of 0.86\n",
      "Iteration 17069: with minibatch training loss = 0.581 and accuracy of 0.8\n",
      "Iteration 17070: with minibatch training loss = 0.638 and accuracy of 0.83\n",
      "Iteration 17071: with minibatch training loss = 0.661 and accuracy of 0.8\n",
      "Iteration 17072: with minibatch training loss = 0.509 and accuracy of 0.88\n",
      "Iteration 17073: with minibatch training loss = 0.631 and accuracy of 0.86\n",
      "Iteration 17074: with minibatch training loss = 0.622 and accuracy of 0.78\n",
      "Iteration 17075: with minibatch training loss = 0.672 and accuracy of 0.77\n",
      "Iteration 17076: with minibatch training loss = 0.619 and accuracy of 0.8\n",
      "Iteration 17077: with minibatch training loss = 1.08 and accuracy of 0.62\n",
      "Iteration 17078: with minibatch training loss = 0.524 and accuracy of 0.86\n",
      "Iteration 17079: with minibatch training loss = 0.891 and accuracy of 0.67\n",
      "Iteration 17080: with minibatch training loss = 0.515 and accuracy of 0.86\n",
      "Iteration 17081: with minibatch training loss = 0.746 and accuracy of 0.75\n",
      "Iteration 17082: with minibatch training loss = 0.416 and accuracy of 0.88\n",
      "Iteration 17083: with minibatch training loss = 0.546 and accuracy of 0.81\n",
      "Iteration 17084: with minibatch training loss = 0.514 and accuracy of 0.86\n",
      "Iteration 17085: with minibatch training loss = 0.578 and accuracy of 0.81\n",
      "Iteration 17086: with minibatch training loss = 0.821 and accuracy of 0.73\n",
      "Iteration 17087: with minibatch training loss = 0.658 and accuracy of 0.8\n",
      "Iteration 17088: with minibatch training loss = 0.707 and accuracy of 0.8\n",
      "Iteration 17089: with minibatch training loss = 0.7 and accuracy of 0.73\n",
      "Iteration 17090: with minibatch training loss = 0.445 and accuracy of 0.84\n",
      "Iteration 17091: with minibatch training loss = 0.637 and accuracy of 0.8\n",
      "Iteration 17092: with minibatch training loss = 0.71 and accuracy of 0.78\n",
      "Iteration 17093: with minibatch training loss = 0.79 and accuracy of 0.75\n",
      "Iteration 17094: with minibatch training loss = 0.529 and accuracy of 0.83\n",
      "Iteration 17095: with minibatch training loss = 0.338 and accuracy of 0.91\n",
      "Iteration 17096: with minibatch training loss = 0.423 and accuracy of 0.88\n",
      "Iteration 17097: with minibatch training loss = 0.394 and accuracy of 0.88\n",
      "Iteration 17098: with minibatch training loss = 0.7 and accuracy of 0.78\n",
      "Iteration 17099: with minibatch training loss = 0.532 and accuracy of 0.81\n",
      "Iteration 17100: with minibatch training loss = 0.908 and accuracy of 0.7\n",
      "Iteration 17101: with minibatch training loss = 0.446 and accuracy of 0.88\n",
      "Iteration 17102: with minibatch training loss = 0.567 and accuracy of 0.8\n",
      "Iteration 17103: with minibatch training loss = 0.803 and accuracy of 0.75\n",
      "Iteration 17104: with minibatch training loss = 0.494 and accuracy of 0.86\n",
      "Iteration 17105: with minibatch training loss = 0.681 and accuracy of 0.78\n",
      "Iteration 17106: with minibatch training loss = 0.61 and accuracy of 0.84\n",
      "Iteration 17107: with minibatch training loss = 0.646 and accuracy of 0.77\n",
      "Iteration 17108: with minibatch training loss = 0.728 and accuracy of 0.78\n",
      "Iteration 17109: with minibatch training loss = 0.585 and accuracy of 0.81\n",
      "Iteration 17110: with minibatch training loss = 0.813 and accuracy of 0.78\n",
      "Iteration 17111: with minibatch training loss = 0.715 and accuracy of 0.77\n",
      "Iteration 17112: with minibatch training loss = 0.613 and accuracy of 0.81\n",
      "Iteration 17113: with minibatch training loss = 0.683 and accuracy of 0.77\n",
      "Iteration 17114: with minibatch training loss = 0.62 and accuracy of 0.81\n",
      "Iteration 17115: with minibatch training loss = 0.515 and accuracy of 0.83\n",
      "Iteration 17116: with minibatch training loss = 0.48 and accuracy of 0.84\n",
      "Iteration 17117: with minibatch training loss = 0.728 and accuracy of 0.83\n",
      "Iteration 17118: with minibatch training loss = 0.888 and accuracy of 0.7\n",
      "Iteration 17119: with minibatch training loss = 0.477 and accuracy of 0.88\n",
      "Iteration 17120: with minibatch training loss = 0.499 and accuracy of 0.84\n",
      "Iteration 17121: with minibatch training loss = 0.638 and accuracy of 0.81\n",
      "Iteration 17122: with minibatch training loss = 0.557 and accuracy of 0.83\n",
      "Iteration 17123: with minibatch training loss = 0.527 and accuracy of 0.83\n",
      "Iteration 17124: with minibatch training loss = 0.729 and accuracy of 0.73\n",
      "Iteration 17125: with minibatch training loss = 0.59 and accuracy of 0.8\n",
      "Iteration 17126: with minibatch training loss = 0.72 and accuracy of 0.78\n",
      "Iteration 17127: with minibatch training loss = 0.689 and accuracy of 0.73\n",
      "Iteration 17128: with minibatch training loss = 0.663 and accuracy of 0.78\n",
      "Iteration 17129: with minibatch training loss = 0.647 and accuracy of 0.78\n",
      "Iteration 17130: with minibatch training loss = 0.65 and accuracy of 0.8\n",
      "Iteration 17131: with minibatch training loss = 0.56 and accuracy of 0.83\n",
      "Iteration 17132: with minibatch training loss = 0.493 and accuracy of 0.84\n",
      "Iteration 17133: with minibatch training loss = 0.569 and accuracy of 0.81\n",
      "Iteration 17134: with minibatch training loss = 0.631 and accuracy of 0.78\n",
      "Iteration 17135: with minibatch training loss = 0.467 and accuracy of 0.89\n",
      "Iteration 17136: with minibatch training loss = 0.558 and accuracy of 0.83\n",
      "Iteration 17137: with minibatch training loss = 0.525 and accuracy of 0.8\n",
      "Iteration 17138: with minibatch training loss = 0.673 and accuracy of 0.78\n",
      "Iteration 17139: with minibatch training loss = 0.84 and accuracy of 0.73\n",
      "Iteration 17140: with minibatch training loss = 0.809 and accuracy of 0.8\n",
      "Iteration 17141: with minibatch training loss = 0.652 and accuracy of 0.83\n",
      "Iteration 17142: with minibatch training loss = 0.661 and accuracy of 0.78\n",
      "Iteration 17143: with minibatch training loss = 0.846 and accuracy of 0.72\n",
      "Iteration 17144: with minibatch training loss = 0.87 and accuracy of 0.73\n",
      "Iteration 17145: with minibatch training loss = 0.694 and accuracy of 0.83\n",
      "Iteration 17146: with minibatch training loss = 0.398 and accuracy of 0.88\n",
      "Iteration 17147: with minibatch training loss = 0.331 and accuracy of 0.91\n",
      "Iteration 17148: with minibatch training loss = 0.559 and accuracy of 0.83\n",
      "Iteration 17149: with minibatch training loss = 0.492 and accuracy of 0.81\n",
      "Iteration 17150: with minibatch training loss = 0.468 and accuracy of 0.86\n",
      "Iteration 17151: with minibatch training loss = 0.775 and accuracy of 0.75\n",
      "Iteration 17152: with minibatch training loss = 0.557 and accuracy of 0.84\n",
      "Iteration 17153: with minibatch training loss = 0.839 and accuracy of 0.73\n",
      "Iteration 17154: with minibatch training loss = 0.823 and accuracy of 0.72\n",
      "Iteration 17155: with minibatch training loss = 0.833 and accuracy of 0.75\n",
      "Iteration 17156: with minibatch training loss = 0.908 and accuracy of 0.7\n",
      "Iteration 17157: with minibatch training loss = 0.565 and accuracy of 0.81\n",
      "Iteration 17158: with minibatch training loss = 0.714 and accuracy of 0.78\n",
      "Iteration 17159: with minibatch training loss = 0.602 and accuracy of 0.83\n",
      "Iteration 17160: with minibatch training loss = 0.917 and accuracy of 0.72\n",
      "Iteration 17161: with minibatch training loss = 0.609 and accuracy of 0.78\n",
      "Iteration 17162: with minibatch training loss = 0.733 and accuracy of 0.77\n",
      "Iteration 17163: with minibatch training loss = 0.714 and accuracy of 0.75\n",
      "Iteration 17164: with minibatch training loss = 0.665 and accuracy of 0.78\n",
      "Iteration 17165: with minibatch training loss = 0.688 and accuracy of 0.78\n",
      "Iteration 17166: with minibatch training loss = 0.866 and accuracy of 0.75\n",
      "Iteration 17167: with minibatch training loss = 0.809 and accuracy of 0.75\n",
      "Iteration 17168: with minibatch training loss = 0.918 and accuracy of 0.69\n",
      "Iteration 17169: with minibatch training loss = 0.462 and accuracy of 0.86\n",
      "Iteration 17170: with minibatch training loss = 0.773 and accuracy of 0.75\n",
      "Iteration 17171: with minibatch training loss = 0.711 and accuracy of 0.78\n",
      "Iteration 17172: with minibatch training loss = 0.565 and accuracy of 0.84\n",
      "Iteration 17173: with minibatch training loss = 0.676 and accuracy of 0.78\n",
      "Iteration 17174: with minibatch training loss = 0.497 and accuracy of 0.89\n",
      "Iteration 17175: with minibatch training loss = 0.324 and accuracy of 0.94\n",
      "Iteration 17176: with minibatch training loss = 0.612 and accuracy of 0.83\n",
      "Iteration 17177: with minibatch training loss = 1.03 and accuracy of 0.66\n",
      "Iteration 17178: with minibatch training loss = 0.602 and accuracy of 0.84\n",
      "Iteration 17179: with minibatch training loss = 0.376 and accuracy of 0.88\n",
      "Iteration 17180: with minibatch training loss = 0.551 and accuracy of 0.83\n",
      "Iteration 17181: with minibatch training loss = 0.583 and accuracy of 0.81\n",
      "Iteration 17182: with minibatch training loss = 0.651 and accuracy of 0.81\n",
      "Iteration 17183: with minibatch training loss = 0.584 and accuracy of 0.81\n",
      "Iteration 17184: with minibatch training loss = 0.559 and accuracy of 0.83\n",
      "Iteration 17185: with minibatch training loss = 0.852 and accuracy of 0.72\n",
      "Iteration 17186: with minibatch training loss = 0.586 and accuracy of 0.83\n",
      "Iteration 17187: with minibatch training loss = 0.607 and accuracy of 0.81\n",
      "Iteration 17188: with minibatch training loss = 0.676 and accuracy of 0.8\n",
      "Iteration 17189: with minibatch training loss = 0.752 and accuracy of 0.75\n",
      "Iteration 17190: with minibatch training loss = 0.58 and accuracy of 0.83\n",
      "Iteration 17191: with minibatch training loss = 0.536 and accuracy of 0.84\n",
      "Iteration 17192: with minibatch training loss = 0.773 and accuracy of 0.73\n",
      "Iteration 17193: with minibatch training loss = 0.848 and accuracy of 0.75\n",
      "Iteration 17194: with minibatch training loss = 0.573 and accuracy of 0.83\n",
      "Iteration 17195: with minibatch training loss = 0.757 and accuracy of 0.75\n",
      "Iteration 17196: with minibatch training loss = 0.557 and accuracy of 0.86\n",
      "Iteration 17197: with minibatch training loss = 0.396 and accuracy of 0.88\n",
      "Iteration 17198: with minibatch training loss = 0.502 and accuracy of 0.88\n",
      "Iteration 17199: with minibatch training loss = 0.652 and accuracy of 0.83\n",
      "Iteration 17200: with minibatch training loss = 0.679 and accuracy of 0.77\n",
      "Iteration 17201: with minibatch training loss = 0.588 and accuracy of 0.83\n",
      "Iteration 17202: with minibatch training loss = 0.614 and accuracy of 0.83\n",
      "Iteration 17203: with minibatch training loss = 0.622 and accuracy of 0.83\n",
      "Iteration 17204: with minibatch training loss = 0.875 and accuracy of 0.73\n",
      "Iteration 17205: with minibatch training loss = 0.758 and accuracy of 0.78\n",
      "Iteration 17206: with minibatch training loss = 0.566 and accuracy of 0.8\n",
      "Iteration 17207: with minibatch training loss = 0.669 and accuracy of 0.84\n",
      "Iteration 17208: with minibatch training loss = 0.463 and accuracy of 0.86\n",
      "Iteration 17209: with minibatch training loss = 0.26 and accuracy of 0.94\n",
      "Iteration 17210: with minibatch training loss = 0.902 and accuracy of 0.73\n",
      "Iteration 17211: with minibatch training loss = 0.859 and accuracy of 0.75\n",
      "Iteration 17212: with minibatch training loss = 0.395 and accuracy of 0.88\n",
      "Iteration 17213: with minibatch training loss = 0.584 and accuracy of 0.81\n",
      "Iteration 17214: with minibatch training loss = 0.832 and accuracy of 0.72\n",
      "Iteration 17215: with minibatch training loss = 0.42 and accuracy of 0.89\n",
      "Iteration 17216: with minibatch training loss = 0.615 and accuracy of 0.81\n",
      "Iteration 17217: with minibatch training loss = 0.526 and accuracy of 0.88\n",
      "Iteration 17218: with minibatch training loss = 0.847 and accuracy of 0.75\n",
      "Iteration 17219: with minibatch training loss = 0.671 and accuracy of 0.75\n",
      "Iteration 17220: with minibatch training loss = 0.357 and accuracy of 0.91\n",
      "Iteration 17221: with minibatch training loss = 0.82 and accuracy of 0.77\n",
      "Iteration 17222: with minibatch training loss = 0.514 and accuracy of 0.83\n",
      "Iteration 17223: with minibatch training loss = 0.594 and accuracy of 0.81\n",
      "Iteration 17224: with minibatch training loss = 0.555 and accuracy of 0.81\n",
      "Iteration 17225: with minibatch training loss = 0.579 and accuracy of 0.83\n",
      "Iteration 17226: with minibatch training loss = 0.733 and accuracy of 0.77\n",
      "Iteration 17227: with minibatch training loss = 0.603 and accuracy of 0.8\n",
      "Iteration 17228: with minibatch training loss = 0.534 and accuracy of 0.84\n",
      "Iteration 17229: with minibatch training loss = 0.787 and accuracy of 0.8\n",
      "Iteration 17230: with minibatch training loss = 0.642 and accuracy of 0.81\n",
      "Iteration 17231: with minibatch training loss = 0.707 and accuracy of 0.81\n",
      "Iteration 17232: with minibatch training loss = 0.401 and accuracy of 0.89\n",
      "Iteration 17233: with minibatch training loss = 0.64 and accuracy of 0.8\n",
      "Iteration 17234: with minibatch training loss = 0.487 and accuracy of 0.84\n",
      "Iteration 17235: with minibatch training loss = 0.714 and accuracy of 0.77\n",
      "Iteration 17236: with minibatch training loss = 0.491 and accuracy of 0.88\n",
      "Iteration 17237: with minibatch training loss = 0.386 and accuracy of 0.89\n",
      "Iteration 17238: with minibatch training loss = 0.431 and accuracy of 0.88\n",
      "Iteration 17239: with minibatch training loss = 0.845 and accuracy of 0.7\n",
      "Iteration 17240: with minibatch training loss = 0.386 and accuracy of 0.88\n",
      "Iteration 17241: with minibatch training loss = 0.586 and accuracy of 0.81\n",
      "Iteration 17242: with minibatch training loss = 0.537 and accuracy of 0.81\n",
      "Iteration 17243: with minibatch training loss = 0.733 and accuracy of 0.81\n",
      "Iteration 17244: with minibatch training loss = 0.622 and accuracy of 0.81\n",
      "Iteration 17245: with minibatch training loss = 0.646 and accuracy of 0.78\n",
      "Iteration 17246: with minibatch training loss = 0.548 and accuracy of 0.83\n",
      "Iteration 17247: with minibatch training loss = 0.664 and accuracy of 0.77\n",
      "Iteration 17248: with minibatch training loss = 0.483 and accuracy of 0.89\n",
      "Iteration 17249: with minibatch training loss = 0.586 and accuracy of 0.83\n",
      "Iteration 17250: with minibatch training loss = 0.701 and accuracy of 0.8\n",
      "Iteration 17251: with minibatch training loss = 0.796 and accuracy of 0.78\n",
      "Iteration 17252: with minibatch training loss = 0.582 and accuracy of 0.78\n",
      "Iteration 17253: with minibatch training loss = 0.598 and accuracy of 0.83\n",
      "Iteration 17254: with minibatch training loss = 0.951 and accuracy of 0.69\n",
      "Iteration 17255: with minibatch training loss = 0.597 and accuracy of 0.83\n",
      "Iteration 17256: with minibatch training loss = 0.345 and accuracy of 0.89\n",
      "Iteration 17257: with minibatch training loss = 0.525 and accuracy of 0.88\n",
      "Iteration 17258: with minibatch training loss = 0.454 and accuracy of 0.88\n",
      "Iteration 17259: with minibatch training loss = 0.677 and accuracy of 0.77\n",
      "Iteration 17260: with minibatch training loss = 0.479 and accuracy of 0.84\n",
      "Iteration 17261: with minibatch training loss = 0.751 and accuracy of 0.78\n",
      "Iteration 17262: with minibatch training loss = 0.798 and accuracy of 0.72\n",
      "Iteration 17263: with minibatch training loss = 0.527 and accuracy of 0.86\n",
      "Iteration 17264: with minibatch training loss = 0.782 and accuracy of 0.73\n",
      "Iteration 17265: with minibatch training loss = 0.478 and accuracy of 0.88\n",
      "Iteration 17266: with minibatch training loss = 0.659 and accuracy of 0.78\n",
      "Iteration 17267: with minibatch training loss = 0.645 and accuracy of 0.81\n",
      "Iteration 17268: with minibatch training loss = 0.933 and accuracy of 0.67\n",
      "Iteration 17269: with minibatch training loss = 0.365 and accuracy of 0.88\n",
      "Iteration 17270: with minibatch training loss = 0.752 and accuracy of 0.73\n",
      "Iteration 17271: with minibatch training loss = 0.53 and accuracy of 0.84\n",
      "Iteration 17272: with minibatch training loss = 0.596 and accuracy of 0.83\n",
      "Iteration 17273: with minibatch training loss = 0.78 and accuracy of 0.77\n",
      "Iteration 17274: with minibatch training loss = 0.538 and accuracy of 0.83\n",
      "Iteration 17275: with minibatch training loss = 0.91 and accuracy of 0.72\n",
      "Iteration 17276: with minibatch training loss = 0.832 and accuracy of 0.77\n",
      "Iteration 17277: with minibatch training loss = 0.68 and accuracy of 0.8\n",
      "Iteration 17278: with minibatch training loss = 0.562 and accuracy of 0.81\n",
      "Iteration 17279: with minibatch training loss = 0.478 and accuracy of 0.84\n",
      "Iteration 17280: with minibatch training loss = 0.644 and accuracy of 0.81\n",
      "Iteration 17281: with minibatch training loss = 0.465 and accuracy of 0.86\n",
      "Iteration 17282: with minibatch training loss = 0.61 and accuracy of 0.83\n",
      "Iteration 17283: with minibatch training loss = 0.387 and accuracy of 0.92\n",
      "Iteration 17284: with minibatch training loss = 0.538 and accuracy of 0.84\n",
      "Iteration 17285: with minibatch training loss = 0.525 and accuracy of 0.83\n",
      "Iteration 17286: with minibatch training loss = 0.884 and accuracy of 0.73\n",
      "Iteration 17287: with minibatch training loss = 0.539 and accuracy of 0.86\n",
      "Iteration 17288: with minibatch training loss = 0.613 and accuracy of 0.8\n",
      "Iteration 17289: with minibatch training loss = 0.751 and accuracy of 0.8\n",
      "Iteration 17290: with minibatch training loss = 0.682 and accuracy of 0.77\n",
      "Iteration 17291: with minibatch training loss = 0.718 and accuracy of 0.78\n",
      "Iteration 17292: with minibatch training loss = 0.591 and accuracy of 0.8\n",
      "Iteration 17293: with minibatch training loss = 0.539 and accuracy of 0.83\n",
      "Iteration 17294: with minibatch training loss = 0.531 and accuracy of 0.86\n",
      "Iteration 17295: with minibatch training loss = 0.562 and accuracy of 0.83\n",
      "Iteration 17296: with minibatch training loss = 0.529 and accuracy of 0.83\n",
      "Iteration 17297: with minibatch training loss = 0.616 and accuracy of 0.81\n",
      "Iteration 17298: with minibatch training loss = 0.605 and accuracy of 0.81\n",
      "Iteration 17299: with minibatch training loss = 0.816 and accuracy of 0.75\n",
      "Iteration 17300: with minibatch training loss = 0.579 and accuracy of 0.81\n",
      "Iteration 17301: with minibatch training loss = 0.571 and accuracy of 0.84\n",
      "Iteration 17302: with minibatch training loss = 0.519 and accuracy of 0.84\n",
      "Iteration 17303: with minibatch training loss = 0.473 and accuracy of 0.84\n",
      "Iteration 17304: with minibatch training loss = 0.592 and accuracy of 0.8\n",
      "Iteration 17305: with minibatch training loss = 0.621 and accuracy of 0.83\n",
      "Iteration 17306: with minibatch training loss = 0.582 and accuracy of 0.81\n",
      "Iteration 17307: with minibatch training loss = 0.319 and accuracy of 0.91\n",
      "Iteration 17308: with minibatch training loss = 0.645 and accuracy of 0.78\n",
      "Iteration 17309: with minibatch training loss = 0.608 and accuracy of 0.83\n",
      "Iteration 17310: with minibatch training loss = 0.558 and accuracy of 0.84\n",
      "Iteration 17311: with minibatch training loss = 0.523 and accuracy of 0.83\n",
      "Iteration 17312: with minibatch training loss = 0.588 and accuracy of 0.83\n",
      "Iteration 17313: with minibatch training loss = 0.797 and accuracy of 0.73\n",
      "Iteration 17314: with minibatch training loss = 0.445 and accuracy of 0.88\n",
      "Iteration 17315: with minibatch training loss = 0.528 and accuracy of 0.84\n",
      "Iteration 17316: with minibatch training loss = 0.518 and accuracy of 0.86\n",
      "Iteration 17317: with minibatch training loss = 0.695 and accuracy of 0.8\n",
      "Iteration 17318: with minibatch training loss = 0.988 and accuracy of 0.73\n",
      "Iteration 17319: with minibatch training loss = 0.404 and accuracy of 0.88\n",
      "Iteration 17320: with minibatch training loss = 0.625 and accuracy of 0.78\n",
      "Iteration 17321: with minibatch training loss = 0.457 and accuracy of 0.89\n",
      "Iteration 17322: with minibatch training loss = 0.585 and accuracy of 0.84\n",
      "Iteration 17323: with minibatch training loss = 0.511 and accuracy of 0.81\n",
      "Iteration 17324: with minibatch training loss = 0.674 and accuracy of 0.83\n",
      "Iteration 17325: with minibatch training loss = 0.583 and accuracy of 0.83\n",
      "Iteration 17326: with minibatch training loss = 0.824 and accuracy of 0.73\n",
      "Iteration 17327: with minibatch training loss = 0.764 and accuracy of 0.78\n",
      "Iteration 17328: with minibatch training loss = 0.392 and accuracy of 0.89\n",
      "Iteration 17329: with minibatch training loss = 0.632 and accuracy of 0.81\n",
      "Iteration 17330: with minibatch training loss = 0.789 and accuracy of 0.73\n",
      "Iteration 17331: with minibatch training loss = 0.587 and accuracy of 0.88\n",
      "Iteration 17332: with minibatch training loss = 0.751 and accuracy of 0.8\n",
      "Iteration 17333: with minibatch training loss = 0.506 and accuracy of 0.84\n",
      "Iteration 17334: with minibatch training loss = 0.686 and accuracy of 0.78\n",
      "Iteration 17335: with minibatch training loss = 0.746 and accuracy of 0.78\n",
      "Iteration 17336: with minibatch training loss = 0.63 and accuracy of 0.78\n",
      "Iteration 17337: with minibatch training loss = 0.663 and accuracy of 0.8\n",
      "Iteration 17338: with minibatch training loss = 0.668 and accuracy of 0.78\n",
      "Iteration 17339: with minibatch training loss = 0.563 and accuracy of 0.81\n",
      "Iteration 17340: with minibatch training loss = 0.716 and accuracy of 0.8\n",
      "Iteration 17341: with minibatch training loss = 0.644 and accuracy of 0.8\n",
      "Iteration 17342: with minibatch training loss = 0.505 and accuracy of 0.81\n",
      "Iteration 17343: with minibatch training loss = 0.786 and accuracy of 0.75\n",
      "Iteration 17344: with minibatch training loss = 0.719 and accuracy of 0.77\n",
      "Iteration 17345: with minibatch training loss = 0.807 and accuracy of 0.75\n",
      "Iteration 17346: with minibatch training loss = 0.679 and accuracy of 0.8\n",
      "Iteration 17347: with minibatch training loss = 0.6 and accuracy of 0.83\n",
      "Iteration 17348: with minibatch training loss = 0.51 and accuracy of 0.83\n",
      "Iteration 17349: with minibatch training loss = 0.639 and accuracy of 0.81\n",
      "Iteration 17350: with minibatch training loss = 0.779 and accuracy of 0.75\n",
      "Iteration 17351: with minibatch training loss = 0.534 and accuracy of 0.83\n",
      "Iteration 17352: with minibatch training loss = 0.976 and accuracy of 0.69\n",
      "Iteration 17353: with minibatch training loss = 0.733 and accuracy of 0.75\n",
      "Iteration 17354: with minibatch training loss = 0.602 and accuracy of 0.83\n",
      "Iteration 17355: with minibatch training loss = 0.603 and accuracy of 0.83\n",
      "Iteration 17356: with minibatch training loss = 0.506 and accuracy of 0.84\n",
      "Iteration 17357: with minibatch training loss = 0.531 and accuracy of 0.83\n",
      "Iteration 17358: with minibatch training loss = 0.655 and accuracy of 0.8\n",
      "Iteration 17359: with minibatch training loss = 0.548 and accuracy of 0.84\n",
      "Iteration 17360: with minibatch training loss = 0.696 and accuracy of 0.8\n",
      "Iteration 17361: with minibatch training loss = 0.78 and accuracy of 0.77\n",
      "Iteration 17362: with minibatch training loss = 0.771 and accuracy of 0.72\n",
      "Iteration 17363: with minibatch training loss = 0.434 and accuracy of 0.91\n",
      "Iteration 17364: with minibatch training loss = 0.381 and accuracy of 0.89\n",
      "Iteration 17365: with minibatch training loss = 0.77 and accuracy of 0.75\n",
      "Iteration 17366: with minibatch training loss = 0.263 and accuracy of 0.92\n",
      "Iteration 17367: with minibatch training loss = 0.886 and accuracy of 0.75\n",
      "Iteration 17368: with minibatch training loss = 0.665 and accuracy of 0.78\n",
      "Iteration 17369: with minibatch training loss = 0.644 and accuracy of 0.78\n",
      "Iteration 17370: with minibatch training loss = 0.794 and accuracy of 0.77\n",
      "Iteration 17371: with minibatch training loss = 0.675 and accuracy of 0.77\n",
      "Iteration 17372: with minibatch training loss = 0.597 and accuracy of 0.8\n",
      "Iteration 17373: with minibatch training loss = 0.405 and accuracy of 0.88\n",
      "Iteration 17374: with minibatch training loss = 0.634 and accuracy of 0.77\n",
      "Iteration 17375: with minibatch training loss = 0.85 and accuracy of 0.81\n",
      "Iteration 17376: with minibatch training loss = 0.623 and accuracy of 0.8\n",
      "Iteration 17377: with minibatch training loss = 0.622 and accuracy of 0.8\n",
      "Iteration 17378: with minibatch training loss = 0.749 and accuracy of 0.75\n",
      "Iteration 17379: with minibatch training loss = 0.612 and accuracy of 0.81\n",
      "Iteration 17380: with minibatch training loss = 0.529 and accuracy of 0.84\n",
      "Iteration 17381: with minibatch training loss = 0.69 and accuracy of 0.81\n",
      "Iteration 17382: with minibatch training loss = 0.843 and accuracy of 0.73\n",
      "Iteration 17383: with minibatch training loss = 0.593 and accuracy of 0.81\n",
      "Iteration 17384: with minibatch training loss = 0.727 and accuracy of 0.78\n",
      "Iteration 17385: with minibatch training loss = 0.631 and accuracy of 0.77\n",
      "Iteration 17386: with minibatch training loss = 0.7 and accuracy of 0.78\n",
      "Iteration 17387: with minibatch training loss = 0.64 and accuracy of 0.81\n",
      "Iteration 17388: with minibatch training loss = 0.868 and accuracy of 0.73\n",
      "Iteration 17389: with minibatch training loss = 0.54 and accuracy of 0.84\n",
      "Iteration 17390: with minibatch training loss = 0.343 and accuracy of 0.92\n",
      "Iteration 17391: with minibatch training loss = 0.679 and accuracy of 0.78\n",
      "Iteration 17392: with minibatch training loss = 0.592 and accuracy of 0.78\n",
      "Iteration 17393: with minibatch training loss = 0.545 and accuracy of 0.83\n",
      "Iteration 17394: with minibatch training loss = 0.745 and accuracy of 0.8\n",
      "Iteration 17395: with minibatch training loss = 0.745 and accuracy of 0.75\n",
      "Iteration 17396: with minibatch training loss = 0.72 and accuracy of 0.8\n",
      "Iteration 17397: with minibatch training loss = 0.454 and accuracy of 0.83\n",
      "Iteration 17398: with minibatch training loss = 0.561 and accuracy of 0.81\n",
      "Iteration 17399: with minibatch training loss = 0.793 and accuracy of 0.77\n",
      "Iteration 17400: with minibatch training loss = 0.581 and accuracy of 0.83\n",
      "Iteration 17401: with minibatch training loss = 0.728 and accuracy of 0.78\n",
      "Iteration 17402: with minibatch training loss = 0.647 and accuracy of 0.81\n",
      "Iteration 17403: with minibatch training loss = 0.593 and accuracy of 0.83\n",
      "Iteration 17404: with minibatch training loss = 0.685 and accuracy of 0.77\n",
      "Iteration 17405: with minibatch training loss = 0.642 and accuracy of 0.8\n",
      "Iteration 17406: with minibatch training loss = 0.359 and accuracy of 0.89\n",
      "Iteration 17407: with minibatch training loss = 0.592 and accuracy of 0.83\n",
      "Iteration 17408: with minibatch training loss = 1.04 and accuracy of 0.69\n",
      "Iteration 17409: with minibatch training loss = 0.456 and accuracy of 0.86\n",
      "Iteration 17410: with minibatch training loss = 0.795 and accuracy of 0.75\n",
      "Iteration 17411: with minibatch training loss = 0.615 and accuracy of 0.8\n",
      "Iteration 17412: with minibatch training loss = 0.661 and accuracy of 0.78\n",
      "Iteration 17413: with minibatch training loss = 0.724 and accuracy of 0.78\n",
      "Iteration 17414: with minibatch training loss = 0.676 and accuracy of 0.77\n",
      "Iteration 17415: with minibatch training loss = 0.581 and accuracy of 0.83\n",
      "Iteration 17416: with minibatch training loss = 0.363 and accuracy of 0.91\n",
      "Iteration 17417: with minibatch training loss = 0.492 and accuracy of 0.83\n",
      "Iteration 17418: with minibatch training loss = 0.832 and accuracy of 0.72\n",
      "Iteration 17419: with minibatch training loss = 0.733 and accuracy of 0.75\n",
      "Iteration 17420: with minibatch training loss = 0.317 and accuracy of 0.91\n",
      "Iteration 17421: with minibatch training loss = 0.845 and accuracy of 0.73\n",
      "Iteration 17422: with minibatch training loss = 0.462 and accuracy of 0.86\n",
      "Iteration 17423: with minibatch training loss = 0.586 and accuracy of 0.81\n",
      "Iteration 17424: with minibatch training loss = 0.907 and accuracy of 0.7\n",
      "Iteration 17425: with minibatch training loss = 0.611 and accuracy of 0.83\n",
      "Iteration 17426: with minibatch training loss = 0.668 and accuracy of 0.77\n",
      "Iteration 17427: with minibatch training loss = 0.537 and accuracy of 0.83\n",
      "Iteration 17428: with minibatch training loss = 0.614 and accuracy of 0.78\n",
      "Iteration 17429: with minibatch training loss = 0.435 and accuracy of 0.86\n",
      "Iteration 17430: with minibatch training loss = 0.536 and accuracy of 0.83\n",
      "Iteration 17431: with minibatch training loss = 0.675 and accuracy of 0.8\n",
      "Iteration 17432: with minibatch training loss = 0.804 and accuracy of 0.78\n",
      "Iteration 17433: with minibatch training loss = 0.817 and accuracy of 0.72\n",
      "Iteration 17434: with minibatch training loss = 0.81 and accuracy of 0.75\n",
      "Iteration 17435: with minibatch training loss = 0.618 and accuracy of 0.78\n",
      "Iteration 17436: with minibatch training loss = 0.829 and accuracy of 0.77\n",
      "Iteration 17437: with minibatch training loss = 0.744 and accuracy of 0.78\n",
      "Iteration 17438: with minibatch training loss = 0.814 and accuracy of 0.73\n",
      "Iteration 17439: with minibatch training loss = 0.699 and accuracy of 0.8\n",
      "Iteration 17440: with minibatch training loss = 0.674 and accuracy of 0.8\n",
      "Iteration 17441: with minibatch training loss = 0.769 and accuracy of 0.77\n",
      "Iteration 17442: with minibatch training loss = 0.478 and accuracy of 0.84\n",
      "Iteration 17443: with minibatch training loss = 0.639 and accuracy of 0.83\n",
      "Iteration 17444: with minibatch training loss = 0.433 and accuracy of 0.88\n",
      "Iteration 17445: with minibatch training loss = 0.577 and accuracy of 0.81\n",
      "Iteration 17446: with minibatch training loss = 0.579 and accuracy of 0.83\n",
      "Iteration 17447: with minibatch training loss = 0.678 and accuracy of 0.77\n",
      "Iteration 17448: with minibatch training loss = 0.736 and accuracy of 0.75\n",
      "Iteration 17449: with minibatch training loss = 0.592 and accuracy of 0.83\n",
      "Iteration 17450: with minibatch training loss = 0.69 and accuracy of 0.8\n",
      "Iteration 17451: with minibatch training loss = 0.267 and accuracy of 0.91\n",
      "Iteration 17452: with minibatch training loss = 0.492 and accuracy of 0.84\n",
      "Iteration 17453: with minibatch training loss = 0.63 and accuracy of 0.77\n",
      "Iteration 17454: with minibatch training loss = 0.629 and accuracy of 0.81\n",
      "Iteration 17455: with minibatch training loss = 0.581 and accuracy of 0.83\n",
      "Iteration 17456: with minibatch training loss = 0.704 and accuracy of 0.77\n",
      "Iteration 17457: with minibatch training loss = 0.487 and accuracy of 0.84\n",
      "Iteration 17458: with minibatch training loss = 0.583 and accuracy of 0.83\n",
      "Iteration 17459: with minibatch training loss = 0.766 and accuracy of 0.77\n",
      "Iteration 17460: with minibatch training loss = 0.624 and accuracy of 0.81\n",
      "Iteration 17461: with minibatch training loss = 0.584 and accuracy of 0.81\n",
      "Iteration 17462: with minibatch training loss = 0.697 and accuracy of 0.78\n",
      "Iteration 17463: with minibatch training loss = 0.613 and accuracy of 0.78\n",
      "Iteration 17464: with minibatch training loss = 0.645 and accuracy of 0.81\n",
      "Iteration 17465: with minibatch training loss = 0.797 and accuracy of 0.77\n",
      "Iteration 17466: with minibatch training loss = 0.694 and accuracy of 0.75\n",
      "Iteration 17467: with minibatch training loss = 0.622 and accuracy of 0.8\n",
      "Iteration 17468: with minibatch training loss = 0.516 and accuracy of 0.83\n",
      "Iteration 17469: with minibatch training loss = 0.538 and accuracy of 0.84\n",
      "Iteration 17470: with minibatch training loss = 0.693 and accuracy of 0.77\n",
      "Iteration 17471: with minibatch training loss = 0.268 and accuracy of 0.94\n",
      "Iteration 17472: with minibatch training loss = 0.648 and accuracy of 0.78\n",
      "Iteration 17473: with minibatch training loss = 0.584 and accuracy of 0.83\n",
      "Iteration 17474: with minibatch training loss = 0.828 and accuracy of 0.78\n",
      "Iteration 17475: with minibatch training loss = 0.612 and accuracy of 0.81\n",
      "Iteration 17476: with minibatch training loss = 0.779 and accuracy of 0.78\n",
      "Iteration 17477: with minibatch training loss = 0.523 and accuracy of 0.83\n",
      "Iteration 17478: with minibatch training loss = 0.634 and accuracy of 0.78\n",
      "Iteration 17479: with minibatch training loss = 0.416 and accuracy of 0.88\n",
      "Iteration 17480: with minibatch training loss = 0.811 and accuracy of 0.75\n",
      "Iteration 17481: with minibatch training loss = 0.772 and accuracy of 0.77\n",
      "Iteration 17482: with minibatch training loss = 0.714 and accuracy of 0.75\n",
      "Iteration 17483: with minibatch training loss = 0.634 and accuracy of 0.77\n",
      "Iteration 17484: with minibatch training loss = 0.512 and accuracy of 0.83\n",
      "Iteration 17485: with minibatch training loss = 0.843 and accuracy of 0.75\n",
      "Iteration 17486: with minibatch training loss = 0.511 and accuracy of 0.83\n",
      "Iteration 17487: with minibatch training loss = 0.947 and accuracy of 0.73\n",
      "Iteration 17488: with minibatch training loss = 0.902 and accuracy of 0.73\n",
      "Iteration 17489: with minibatch training loss = 0.875 and accuracy of 0.73\n",
      "Iteration 17490: with minibatch training loss = 0.662 and accuracy of 0.81\n",
      "Iteration 17491: with minibatch training loss = 0.715 and accuracy of 0.75\n",
      "Iteration 17492: with minibatch training loss = 0.83 and accuracy of 0.72\n",
      "Iteration 17493: with minibatch training loss = 0.866 and accuracy of 0.72\n",
      "Iteration 17494: with minibatch training loss = 0.659 and accuracy of 0.78\n",
      "Iteration 17495: with minibatch training loss = 0.557 and accuracy of 0.86\n",
      "Iteration 17496: with minibatch training loss = 0.593 and accuracy of 0.78\n",
      "Iteration 17497: with minibatch training loss = 0.724 and accuracy of 0.81\n",
      "Iteration 17498: with minibatch training loss = 0.873 and accuracy of 0.7\n",
      "Iteration 17499: with minibatch training loss = 0.384 and accuracy of 0.88\n",
      "Iteration 17500: with minibatch training loss = 0.703 and accuracy of 0.78\n",
      "Iteration 17501: with minibatch training loss = 0.682 and accuracy of 0.78\n",
      "Iteration 17502: with minibatch training loss = 0.568 and accuracy of 0.78\n",
      "Iteration 17503: with minibatch training loss = 0.813 and accuracy of 0.72\n",
      "Iteration 17504: with minibatch training loss = 0.689 and accuracy of 0.83\n",
      "Iteration 17505: with minibatch training loss = 0.594 and accuracy of 0.84\n",
      "Iteration 17506: with minibatch training loss = 0.595 and accuracy of 0.81\n",
      "Iteration 17507: with minibatch training loss = 0.453 and accuracy of 0.88\n",
      "Iteration 17508: with minibatch training loss = 0.655 and accuracy of 0.83\n",
      "Iteration 17509: with minibatch training loss = 0.473 and accuracy of 0.89\n",
      "Iteration 17510: with minibatch training loss = 0.662 and accuracy of 0.78\n",
      "Iteration 17511: with minibatch training loss = 0.625 and accuracy of 0.8\n",
      "Iteration 17512: with minibatch training loss = 0.772 and accuracy of 0.77\n",
      "Iteration 17513: with minibatch training loss = 0.559 and accuracy of 0.86\n",
      "Iteration 17514: with minibatch training loss = 0.608 and accuracy of 0.78\n",
      "Iteration 17515: with minibatch training loss = 0.654 and accuracy of 0.78\n",
      "Iteration 17516: with minibatch training loss = 0.813 and accuracy of 0.72\n",
      "Iteration 17517: with minibatch training loss = 0.665 and accuracy of 0.78\n",
      "Iteration 17518: with minibatch training loss = 0.929 and accuracy of 0.7\n",
      "Iteration 17519: with minibatch training loss = 0.662 and accuracy of 0.8\n",
      "Iteration 17520: with minibatch training loss = 0.548 and accuracy of 0.81\n",
      "Iteration 17521: with minibatch training loss = 0.596 and accuracy of 0.83\n",
      "Iteration 17522: with minibatch training loss = 0.66 and accuracy of 0.78\n",
      "Iteration 17523: with minibatch training loss = 0.783 and accuracy of 0.83\n",
      "Iteration 17524: with minibatch training loss = 0.778 and accuracy of 0.77\n",
      "Iteration 17525: with minibatch training loss = 0.541 and accuracy of 0.81\n",
      "Iteration 17526: with minibatch training loss = 0.649 and accuracy of 0.77\n",
      "Iteration 17527: with minibatch training loss = 0.538 and accuracy of 0.84\n",
      "Iteration 17528: with minibatch training loss = 0.588 and accuracy of 0.81\n",
      "Iteration 17529: with minibatch training loss = 0.578 and accuracy of 0.83\n",
      "Iteration 17530: with minibatch training loss = 0.48 and accuracy of 0.84\n",
      "Iteration 17531: with minibatch training loss = 0.603 and accuracy of 0.81\n",
      "Iteration 17532: with minibatch training loss = 0.669 and accuracy of 0.78\n",
      "Iteration 17533: with minibatch training loss = 0.501 and accuracy of 0.83\n",
      "Iteration 17534: with minibatch training loss = 0.638 and accuracy of 0.8\n",
      "Iteration 17535: with minibatch training loss = 0.616 and accuracy of 0.78\n",
      "Iteration 17536: with minibatch training loss = 0.537 and accuracy of 0.86\n",
      "Iteration 17537: with minibatch training loss = 0.991 and accuracy of 0.7\n",
      "Iteration 17538: with minibatch training loss = 0.688 and accuracy of 0.77\n",
      "Iteration 17539: with minibatch training loss = 0.797 and accuracy of 0.77\n",
      "Iteration 17540: with minibatch training loss = 0.934 and accuracy of 0.7\n",
      "Iteration 17541: with minibatch training loss = 0.269 and accuracy of 0.92\n",
      "Iteration 17542: with minibatch training loss = 0.759 and accuracy of 0.77\n",
      "Iteration 17543: with minibatch training loss = 0.833 and accuracy of 0.73\n",
      "Iteration 17544: with minibatch training loss = 0.529 and accuracy of 0.83\n",
      "Iteration 17545: with minibatch training loss = 0.817 and accuracy of 0.75\n",
      "Iteration 17546: with minibatch training loss = 0.73 and accuracy of 0.78\n",
      "Iteration 17547: with minibatch training loss = 0.728 and accuracy of 0.75\n",
      "Iteration 17548: with minibatch training loss = 0.749 and accuracy of 0.75\n",
      "Iteration 17549: with minibatch training loss = 0.586 and accuracy of 0.83\n",
      "Iteration 17550: with minibatch training loss = 0.75 and accuracy of 0.75\n",
      "Iteration 17551: with minibatch training loss = 0.621 and accuracy of 0.83\n",
      "Iteration 17552: with minibatch training loss = 0.592 and accuracy of 0.8\n",
      "Iteration 17553: with minibatch training loss = 0.766 and accuracy of 0.77\n",
      "Iteration 17554: with minibatch training loss = 0.586 and accuracy of 0.83\n",
      "Iteration 17555: with minibatch training loss = 0.412 and accuracy of 0.86\n",
      "Iteration 17556: with minibatch training loss = 0.62 and accuracy of 0.81\n",
      "Iteration 17557: with minibatch training loss = 0.546 and accuracy of 0.84\n",
      "Iteration 17558: with minibatch training loss = 0.676 and accuracy of 0.78\n",
      "Iteration 17559: with minibatch training loss = 0.759 and accuracy of 0.75\n",
      "Iteration 17560: with minibatch training loss = 0.754 and accuracy of 0.78\n",
      "Iteration 17561: with minibatch training loss = 0.881 and accuracy of 0.7\n",
      "Iteration 17562: with minibatch training loss = 0.764 and accuracy of 0.81\n",
      "Iteration 17563: with minibatch training loss = 0.732 and accuracy of 0.75\n",
      "Iteration 17564: with minibatch training loss = 0.414 and accuracy of 0.88\n",
      "Iteration 17565: with minibatch training loss = 0.687 and accuracy of 0.77\n",
      "Iteration 17566: with minibatch training loss = 0.563 and accuracy of 0.83\n",
      "Iteration 17567: with minibatch training loss = 0.816 and accuracy of 0.8\n",
      "Iteration 17568: with minibatch training loss = 0.45 and accuracy of 0.86\n",
      "Iteration 17569: with minibatch training loss = 0.664 and accuracy of 0.77\n",
      "Iteration 17570: with minibatch training loss = 0.85 and accuracy of 0.69\n",
      "Iteration 17571: with minibatch training loss = 0.66 and accuracy of 0.8\n",
      "Iteration 17572: with minibatch training loss = 0.544 and accuracy of 0.84\n",
      "Iteration 17573: with minibatch training loss = 0.67 and accuracy of 0.77\n",
      "Iteration 17574: with minibatch training loss = 0.406 and accuracy of 0.89\n",
      "Iteration 17575: with minibatch training loss = 0.591 and accuracy of 0.81\n",
      "Iteration 17576: with minibatch training loss = 0.859 and accuracy of 0.67\n",
      "Iteration 17577: with minibatch training loss = 0.451 and accuracy of 0.84\n",
      "Iteration 17578: with minibatch training loss = 0.635 and accuracy of 0.81\n",
      "Iteration 17579: with minibatch training loss = 0.538 and accuracy of 0.86\n",
      "Iteration 17580: with minibatch training loss = 0.628 and accuracy of 0.78\n",
      "Iteration 17581: with minibatch training loss = 0.61 and accuracy of 0.78\n",
      "Iteration 17582: with minibatch training loss = 1.07 and accuracy of 0.67\n",
      "Iteration 17583: with minibatch training loss = 0.679 and accuracy of 0.78\n",
      "Iteration 17584: with minibatch training loss = 0.793 and accuracy of 0.75\n",
      "Iteration 17585: with minibatch training loss = 0.56 and accuracy of 0.84\n",
      "Iteration 17586: with minibatch training loss = 0.525 and accuracy of 0.83\n",
      "Iteration 17587: with minibatch training loss = 0.584 and accuracy of 0.8\n",
      "Iteration 17588: with minibatch training loss = 0.614 and accuracy of 0.8\n",
      "Iteration 17589: with minibatch training loss = 0.719 and accuracy of 0.75\n",
      "Iteration 17590: with minibatch training loss = 0.507 and accuracy of 0.84\n",
      "Iteration 17591: with minibatch training loss = 0.304 and accuracy of 0.92\n",
      "Iteration 17592: with minibatch training loss = 0.36 and accuracy of 0.92\n",
      "Iteration 17593: with minibatch training loss = 0.597 and accuracy of 0.81\n",
      "Iteration 17594: with minibatch training loss = 0.785 and accuracy of 0.73\n",
      "Iteration 17595: with minibatch training loss = 0.455 and accuracy of 0.84\n",
      "Iteration 17596: with minibatch training loss = 0.482 and accuracy of 0.84\n",
      "Iteration 17597: with minibatch training loss = 0.495 and accuracy of 0.84\n",
      "Iteration 17598: with minibatch training loss = 0.849 and accuracy of 0.7\n",
      "Iteration 17599: with minibatch training loss = 0.673 and accuracy of 0.83\n",
      "Iteration 17600: with minibatch training loss = 1.06 and accuracy of 0.73\n",
      "Iteration 17601: with minibatch training loss = 0.816 and accuracy of 0.78\n",
      "Iteration 17602: with minibatch training loss = 0.504 and accuracy of 0.86\n",
      "Iteration 17603: with minibatch training loss = 0.688 and accuracy of 0.81\n",
      "Iteration 17604: with minibatch training loss = 0.739 and accuracy of 0.77\n",
      "Iteration 17605: with minibatch training loss = 0.891 and accuracy of 0.69\n",
      "Iteration 17606: with minibatch training loss = 0.651 and accuracy of 0.81\n",
      "Iteration 17607: with minibatch training loss = 0.65 and accuracy of 0.81\n",
      "Iteration 17608: with minibatch training loss = 0.469 and accuracy of 0.84\n",
      "Iteration 17609: with minibatch training loss = 0.544 and accuracy of 0.86\n",
      "Iteration 17610: with minibatch training loss = 0.557 and accuracy of 0.81\n",
      "Iteration 17611: with minibatch training loss = 0.663 and accuracy of 0.78\n",
      "Iteration 17612: with minibatch training loss = 0.767 and accuracy of 0.73\n",
      "Iteration 17613: with minibatch training loss = 0.579 and accuracy of 0.81\n",
      "Iteration 17614: with minibatch training loss = 0.459 and accuracy of 0.86\n",
      "Iteration 17615: with minibatch training loss = 0.87 and accuracy of 0.75\n",
      "Iteration 17616: with minibatch training loss = 0.642 and accuracy of 0.8\n",
      "Iteration 17617: with minibatch training loss = 0.712 and accuracy of 0.75\n",
      "Iteration 17618: with minibatch training loss = 0.735 and accuracy of 0.77\n",
      "Iteration 17619: with minibatch training loss = 0.803 and accuracy of 0.73\n",
      "Iteration 17620: with minibatch training loss = 0.688 and accuracy of 0.81\n",
      "Iteration 17621: with minibatch training loss = 0.885 and accuracy of 0.7\n",
      "Iteration 17622: with minibatch training loss = 0.641 and accuracy of 0.8\n",
      "Iteration 17623: with minibatch training loss = 0.808 and accuracy of 0.77\n",
      "Iteration 17624: with minibatch training loss = 0.686 and accuracy of 0.77\n",
      "Iteration 17625: with minibatch training loss = 0.452 and accuracy of 0.88\n",
      "Iteration 17626: with minibatch training loss = 1.06 and accuracy of 0.64\n",
      "Iteration 17627: with minibatch training loss = 0.603 and accuracy of 0.8\n",
      "Iteration 17628: with minibatch training loss = 0.606 and accuracy of 0.84\n",
      "Iteration 17629: with minibatch training loss = 0.612 and accuracy of 0.83\n",
      "Iteration 17630: with minibatch training loss = 0.616 and accuracy of 0.81\n",
      "Iteration 17631: with minibatch training loss = 0.453 and accuracy of 0.83\n",
      "Iteration 17632: with minibatch training loss = 0.801 and accuracy of 0.72\n",
      "Iteration 17633: with minibatch training loss = 0.65 and accuracy of 0.78\n",
      "Iteration 17634: with minibatch training loss = 0.547 and accuracy of 0.86\n",
      "Iteration 17635: with minibatch training loss = 0.444 and accuracy of 0.88\n",
      "Iteration 17636: with minibatch training loss = 0.799 and accuracy of 0.73\n",
      "Iteration 17637: with minibatch training loss = 0.475 and accuracy of 0.86\n",
      "Iteration 17638: with minibatch training loss = 0.467 and accuracy of 0.86\n",
      "Iteration 17639: with minibatch training loss = 0.767 and accuracy of 0.73\n",
      "Iteration 17640: with minibatch training loss = 0.562 and accuracy of 0.83\n",
      "Iteration 17641: with minibatch training loss = 0.591 and accuracy of 0.83\n",
      "Iteration 17642: with minibatch training loss = 0.67 and accuracy of 0.81\n",
      "Iteration 17643: with minibatch training loss = 0.687 and accuracy of 0.8\n",
      "Iteration 17644: with minibatch training loss = 0.628 and accuracy of 0.83\n",
      "Iteration 17645: with minibatch training loss = 0.846 and accuracy of 0.73\n",
      "Iteration 17646: with minibatch training loss = 0.736 and accuracy of 0.75\n",
      "Iteration 17647: with minibatch training loss = 0.683 and accuracy of 0.8\n",
      "Iteration 17648: with minibatch training loss = 0.695 and accuracy of 0.77\n",
      "Iteration 17649: with minibatch training loss = 0.671 and accuracy of 0.77\n",
      "Iteration 17650: with minibatch training loss = 0.586 and accuracy of 0.81\n",
      "Iteration 17651: with minibatch training loss = 0.477 and accuracy of 0.88\n",
      "Iteration 17652: with minibatch training loss = 0.683 and accuracy of 0.78\n",
      "Iteration 17653: with minibatch training loss = 0.668 and accuracy of 0.81\n",
      "Iteration 17654: with minibatch training loss = 0.479 and accuracy of 0.86\n",
      "Iteration 17655: with minibatch training loss = 0.545 and accuracy of 0.81\n",
      "Iteration 17656: with minibatch training loss = 0.828 and accuracy of 0.75\n",
      "Iteration 17657: with minibatch training loss = 0.661 and accuracy of 0.78\n",
      "Iteration 17658: with minibatch training loss = 0.751 and accuracy of 0.75\n",
      "Iteration 17659: with minibatch training loss = 0.724 and accuracy of 0.75\n",
      "Iteration 17660: with minibatch training loss = 0.75 and accuracy of 0.77\n",
      "Iteration 17661: with minibatch training loss = 0.476 and accuracy of 0.83\n",
      "Iteration 17662: with minibatch training loss = 0.342 and accuracy of 0.88\n",
      "Iteration 17663: with minibatch training loss = 0.752 and accuracy of 0.77\n",
      "Iteration 17664: with minibatch training loss = 0.655 and accuracy of 0.8\n",
      "Iteration 17665: with minibatch training loss = 0.509 and accuracy of 0.84\n",
      "Iteration 17666: with minibatch training loss = 0.601 and accuracy of 0.86\n",
      "Iteration 17667: with minibatch training loss = 0.832 and accuracy of 0.73\n",
      "Iteration 17668: with minibatch training loss = 0.57 and accuracy of 0.83\n",
      "Iteration 17669: with minibatch training loss = 0.556 and accuracy of 0.86\n",
      "Iteration 17670: with minibatch training loss = 0.713 and accuracy of 0.78\n",
      "Iteration 17671: with minibatch training loss = 0.585 and accuracy of 0.8\n",
      "Iteration 17672: with minibatch training loss = 0.84 and accuracy of 0.73\n",
      "Iteration 17673: with minibatch training loss = 0.614 and accuracy of 0.77\n",
      "Iteration 17674: with minibatch training loss = 0.51 and accuracy of 0.83\n",
      "Iteration 17675: with minibatch training loss = 0.603 and accuracy of 0.83\n",
      "Iteration 17676: with minibatch training loss = 0.661 and accuracy of 0.78\n",
      "Iteration 17677: with minibatch training loss = 0.487 and accuracy of 0.86\n",
      "Iteration 17678: with minibatch training loss = 0.606 and accuracy of 0.81\n",
      "Iteration 17679: with minibatch training loss = 0.459 and accuracy of 0.86\n",
      "Iteration 17680: with minibatch training loss = 0.493 and accuracy of 0.86\n",
      "Iteration 17681: with minibatch training loss = 0.786 and accuracy of 0.75\n",
      "Iteration 17682: with minibatch training loss = 0.59 and accuracy of 0.81\n",
      "Iteration 17683: with minibatch training loss = 0.6 and accuracy of 0.81\n",
      "Iteration 17684: with minibatch training loss = 0.629 and accuracy of 0.81\n",
      "Iteration 17685: with minibatch training loss = 0.652 and accuracy of 0.8\n",
      "Iteration 17686: with minibatch training loss = 0.626 and accuracy of 0.8\n",
      "Iteration 17687: with minibatch training loss = 0.618 and accuracy of 0.8\n",
      "Iteration 17688: with minibatch training loss = 0.603 and accuracy of 0.81\n",
      "Iteration 17689: with minibatch training loss = 0.397 and accuracy of 0.89\n",
      "Iteration 17690: with minibatch training loss = 0.497 and accuracy of 0.83\n",
      "Iteration 17691: with minibatch training loss = 0.956 and accuracy of 0.67\n",
      "Iteration 17692: with minibatch training loss = 0.531 and accuracy of 0.86\n",
      "Iteration 17693: with minibatch training loss = 0.619 and accuracy of 0.78\n",
      "Iteration 17694: with minibatch training loss = 0.521 and accuracy of 0.81\n",
      "Iteration 17695: with minibatch training loss = 0.779 and accuracy of 0.77\n",
      "Iteration 17696: with minibatch training loss = 0.789 and accuracy of 0.75\n",
      "Iteration 17697: with minibatch training loss = 0.452 and accuracy of 0.84\n",
      "Iteration 17698: with minibatch training loss = 0.595 and accuracy of 0.83\n",
      "Iteration 17699: with minibatch training loss = 0.805 and accuracy of 0.75\n",
      "Iteration 17700: with minibatch training loss = 0.461 and accuracy of 0.84\n",
      "Iteration 17701: with minibatch training loss = 0.644 and accuracy of 0.8\n",
      "Iteration 17702: with minibatch training loss = 0.643 and accuracy of 0.83\n",
      "Iteration 17703: with minibatch training loss = 0.73 and accuracy of 0.73\n",
      "Iteration 17704: with minibatch training loss = 0.686 and accuracy of 0.81\n",
      "Iteration 17705: with minibatch training loss = 0.776 and accuracy of 0.75\n",
      "Iteration 17706: with minibatch training loss = 0.6 and accuracy of 0.83\n",
      "Iteration 17707: with minibatch training loss = 0.567 and accuracy of 0.84\n",
      "Iteration 17708: with minibatch training loss = 0.637 and accuracy of 0.81\n",
      "Iteration 17709: with minibatch training loss = 0.741 and accuracy of 0.75\n",
      "Iteration 17710: with minibatch training loss = 0.605 and accuracy of 0.84\n",
      "Iteration 17711: with minibatch training loss = 0.475 and accuracy of 0.88\n",
      "Iteration 17712: with minibatch training loss = 0.591 and accuracy of 0.8\n",
      "Iteration 17713: with minibatch training loss = 0.518 and accuracy of 0.86\n",
      "Iteration 17714: with minibatch training loss = 0.562 and accuracy of 0.83\n",
      "Iteration 17715: with minibatch training loss = 0.626 and accuracy of 0.78\n",
      "Iteration 17716: with minibatch training loss = 0.777 and accuracy of 0.77\n",
      "Iteration 17717: with minibatch training loss = 0.473 and accuracy of 0.86\n",
      "Iteration 17718: with minibatch training loss = 0.507 and accuracy of 0.84\n",
      "Iteration 17719: with minibatch training loss = 0.728 and accuracy of 0.78\n",
      "Iteration 17720: with minibatch training loss = 0.831 and accuracy of 0.7\n",
      "Iteration 17721: with minibatch training loss = 0.841 and accuracy of 0.73\n",
      "Iteration 17722: with minibatch training loss = 0.516 and accuracy of 0.83\n",
      "Iteration 17723: with minibatch training loss = 0.579 and accuracy of 0.83\n",
      "Iteration 17724: with minibatch training loss = 0.787 and accuracy of 0.78\n",
      "Iteration 17725: with minibatch training loss = 0.499 and accuracy of 0.88\n",
      "Iteration 17726: with minibatch training loss = 0.319 and accuracy of 0.91\n",
      "Iteration 17727: with minibatch training loss = 0.592 and accuracy of 0.8\n",
      "Iteration 17728: with minibatch training loss = 0.648 and accuracy of 0.8\n",
      "Iteration 17729: with minibatch training loss = 0.831 and accuracy of 0.72\n",
      "Iteration 17730: with minibatch training loss = 0.403 and accuracy of 0.89\n",
      "Iteration 17731: with minibatch training loss = 0.766 and accuracy of 0.73\n",
      "Iteration 17732: with minibatch training loss = 0.481 and accuracy of 0.84\n",
      "Iteration 17733: with minibatch training loss = 0.485 and accuracy of 0.83\n",
      "Iteration 17734: with minibatch training loss = 0.725 and accuracy of 0.8\n",
      "Iteration 17735: with minibatch training loss = 0.651 and accuracy of 0.81\n",
      "Iteration 17736: with minibatch training loss = 0.695 and accuracy of 0.73\n",
      "Iteration 17737: with minibatch training loss = 0.769 and accuracy of 0.75\n",
      "Iteration 17738: with minibatch training loss = 0.696 and accuracy of 0.8\n",
      "Iteration 17739: with minibatch training loss = 0.775 and accuracy of 0.73\n",
      "Iteration 17740: with minibatch training loss = 0.46 and accuracy of 0.88\n",
      "Iteration 17741: with minibatch training loss = 0.739 and accuracy of 0.73\n",
      "Iteration 17742: with minibatch training loss = 0.724 and accuracy of 0.75\n",
      "Iteration 17743: with minibatch training loss = 0.517 and accuracy of 0.83\n",
      "Iteration 17744: with minibatch training loss = 0.76 and accuracy of 0.75\n",
      "Iteration 17745: with minibatch training loss = 0.763 and accuracy of 0.81\n",
      "Iteration 17746: with minibatch training loss = 0.711 and accuracy of 0.77\n",
      "Iteration 17747: with minibatch training loss = 0.476 and accuracy of 0.88\n",
      "Iteration 17748: with minibatch training loss = 0.813 and accuracy of 0.72\n",
      "Iteration 17749: with minibatch training loss = 0.642 and accuracy of 0.8\n",
      "Iteration 17750: with minibatch training loss = 0.483 and accuracy of 0.86\n",
      "Iteration 17751: with minibatch training loss = 0.894 and accuracy of 0.73\n",
      "Iteration 17752: with minibatch training loss = 0.779 and accuracy of 0.77\n",
      "Iteration 17753: with minibatch training loss = 0.584 and accuracy of 0.81\n",
      "Iteration 17754: with minibatch training loss = 0.46 and accuracy of 0.86\n",
      "Iteration 17755: with minibatch training loss = 0.815 and accuracy of 0.75\n",
      "Iteration 17756: with minibatch training loss = 0.689 and accuracy of 0.83\n",
      "Iteration 17757: with minibatch training loss = 0.641 and accuracy of 0.8\n",
      "Iteration 17758: with minibatch training loss = 0.825 and accuracy of 0.77\n",
      "Iteration 17759: with minibatch training loss = 0.643 and accuracy of 0.78\n",
      "Iteration 17760: with minibatch training loss = 0.701 and accuracy of 0.8\n",
      "Iteration 17761: with minibatch training loss = 0.833 and accuracy of 0.72\n",
      "Iteration 17762: with minibatch training loss = 0.737 and accuracy of 0.78\n",
      "Iteration 17763: with minibatch training loss = 0.635 and accuracy of 0.77\n",
      "Iteration 17764: with minibatch training loss = 0.675 and accuracy of 0.78\n",
      "Iteration 17765: with minibatch training loss = 0.485 and accuracy of 0.86\n",
      "Iteration 17766: with minibatch training loss = 0.164 and accuracy of 0.95\n",
      "Iteration 17767: with minibatch training loss = 0.462 and accuracy of 0.88\n",
      "Iteration 17768: with minibatch training loss = 0.694 and accuracy of 0.8\n",
      "Iteration 17769: with minibatch training loss = 0.647 and accuracy of 0.8\n",
      "Iteration 17770: with minibatch training loss = 0.426 and accuracy of 0.88\n",
      "Iteration 17771: with minibatch training loss = 0.484 and accuracy of 0.86\n",
      "Iteration 17772: with minibatch training loss = 0.554 and accuracy of 0.83\n",
      "Iteration 17773: with minibatch training loss = 0.967 and accuracy of 0.67\n",
      "Iteration 17774: with minibatch training loss = 0.837 and accuracy of 0.72\n",
      "Iteration 17775: with minibatch training loss = 0.585 and accuracy of 0.83\n",
      "Iteration 17776: with minibatch training loss = 0.403 and accuracy of 0.88\n",
      "Iteration 17777: with minibatch training loss = 0.498 and accuracy of 0.89\n",
      "Iteration 17778: with minibatch training loss = 0.749 and accuracy of 0.77\n",
      "Iteration 17779: with minibatch training loss = 0.71 and accuracy of 0.77\n",
      "Iteration 17780: with minibatch training loss = 0.772 and accuracy of 0.73\n",
      "Iteration 17781: with minibatch training loss = 0.565 and accuracy of 0.81\n",
      "Iteration 17782: with minibatch training loss = 0.649 and accuracy of 0.81\n",
      "Iteration 17783: with minibatch training loss = 0.794 and accuracy of 0.72\n",
      "Iteration 17784: with minibatch training loss = 0.634 and accuracy of 0.8\n",
      "Iteration 17785: with minibatch training loss = 0.619 and accuracy of 0.8\n",
      "Iteration 17786: with minibatch training loss = 0.621 and accuracy of 0.8\n",
      "Iteration 17787: with minibatch training loss = 0.497 and accuracy of 0.84\n",
      "Iteration 17788: with minibatch training loss = 0.653 and accuracy of 0.78\n",
      "Iteration 17789: with minibatch training loss = 0.518 and accuracy of 0.83\n",
      "Iteration 17790: with minibatch training loss = 0.63 and accuracy of 0.78\n",
      "Iteration 17791: with minibatch training loss = 0.698 and accuracy of 0.78\n",
      "Iteration 17792: with minibatch training loss = 0.56 and accuracy of 0.81\n",
      "Iteration 17793: with minibatch training loss = 0.422 and accuracy of 0.88\n",
      "Iteration 17794: with minibatch training loss = 0.664 and accuracy of 0.8\n",
      "Iteration 17795: with minibatch training loss = 0.707 and accuracy of 0.77\n",
      "Iteration 17796: with minibatch training loss = 0.787 and accuracy of 0.77\n",
      "Iteration 17797: with minibatch training loss = 0.803 and accuracy of 0.73\n",
      "Iteration 17798: with minibatch training loss = 0.653 and accuracy of 0.78\n",
      "Iteration 17799: with minibatch training loss = 0.537 and accuracy of 0.84\n",
      "Iteration 17800: with minibatch training loss = 0.301 and accuracy of 0.91\n",
      "Iteration 17801: with minibatch training loss = 0.64 and accuracy of 0.81\n",
      "Iteration 17802: with minibatch training loss = 0.768 and accuracy of 0.75\n",
      "Iteration 17803: with minibatch training loss = 0.626 and accuracy of 0.8\n",
      "Iteration 17804: with minibatch training loss = 1.11 and accuracy of 0.69\n",
      "Iteration 17805: with minibatch training loss = 0.552 and accuracy of 0.83\n",
      "Iteration 17806: with minibatch training loss = 0.547 and accuracy of 0.81\n",
      "Iteration 17807: with minibatch training loss = 0.646 and accuracy of 0.77\n",
      "Iteration 17808: with minibatch training loss = 0.513 and accuracy of 0.81\n",
      "Iteration 17809: with minibatch training loss = 0.848 and accuracy of 0.72\n",
      "Iteration 17810: with minibatch training loss = 0.471 and accuracy of 0.89\n",
      "Iteration 17811: with minibatch training loss = 0.955 and accuracy of 0.66\n",
      "Iteration 17812: with minibatch training loss = 0.749 and accuracy of 0.77\n",
      "Iteration 17813: with minibatch training loss = 0.629 and accuracy of 0.78\n",
      "Iteration 17814: with minibatch training loss = 0.529 and accuracy of 0.84\n",
      "Iteration 17815: with minibatch training loss = 0.869 and accuracy of 0.73\n",
      "Iteration 17816: with minibatch training loss = 0.68 and accuracy of 0.78\n",
      "Iteration 17817: with minibatch training loss = 0.498 and accuracy of 0.84\n",
      "Iteration 17818: with minibatch training loss = 0.635 and accuracy of 0.83\n",
      "Iteration 17819: with minibatch training loss = 0.637 and accuracy of 0.81\n",
      "Iteration 17820: with minibatch training loss = 0.556 and accuracy of 0.84\n",
      "Iteration 17821: with minibatch training loss = 0.485 and accuracy of 0.84\n",
      "Iteration 17822: with minibatch training loss = 0.739 and accuracy of 0.77\n",
      "Iteration 17823: with minibatch training loss = 0.531 and accuracy of 0.84\n",
      "Iteration 17824: with minibatch training loss = 0.633 and accuracy of 0.78\n",
      "Iteration 17825: with minibatch training loss = 0.565 and accuracy of 0.81\n",
      "Iteration 17826: with minibatch training loss = 0.466 and accuracy of 0.84\n",
      "Iteration 17827: with minibatch training loss = 0.894 and accuracy of 0.72\n",
      "Iteration 17828: with minibatch training loss = 0.545 and accuracy of 0.83\n",
      "Iteration 17829: with minibatch training loss = 0.38 and accuracy of 0.88\n",
      "Iteration 17830: with minibatch training loss = 0.751 and accuracy of 0.73\n",
      "Iteration 17831: with minibatch training loss = 0.747 and accuracy of 0.75\n",
      "Iteration 17832: with minibatch training loss = 0.514 and accuracy of 0.86\n",
      "Iteration 17833: with minibatch training loss = 0.506 and accuracy of 0.81\n",
      "Iteration 17834: with minibatch training loss = 0.585 and accuracy of 0.8\n",
      "Iteration 17835: with minibatch training loss = 0.782 and accuracy of 0.77\n",
      "Iteration 17836: with minibatch training loss = 0.473 and accuracy of 0.84\n",
      "Iteration 17837: with minibatch training loss = 0.507 and accuracy of 0.84\n",
      "Iteration 17838: with minibatch training loss = 0.662 and accuracy of 0.8\n",
      "Iteration 17839: with minibatch training loss = 0.877 and accuracy of 0.7\n",
      "Iteration 17840: with minibatch training loss = 0.594 and accuracy of 0.83\n",
      "Iteration 17841: with minibatch training loss = 0.619 and accuracy of 0.81\n",
      "Iteration 17842: with minibatch training loss = 0.495 and accuracy of 0.84\n",
      "Iteration 17843: with minibatch training loss = 0.739 and accuracy of 0.78\n",
      "Iteration 17844: with minibatch training loss = 0.64 and accuracy of 0.78\n",
      "Iteration 17845: with minibatch training loss = 0.579 and accuracy of 0.81\n",
      "Iteration 17846: with minibatch training loss = 0.717 and accuracy of 0.8\n",
      "Iteration 17847: with minibatch training loss = 0.456 and accuracy of 0.83\n",
      "Iteration 17848: with minibatch training loss = 0.551 and accuracy of 0.81\n",
      "Iteration 17849: with minibatch training loss = 0.409 and accuracy of 0.91\n",
      "Iteration 17850: with minibatch training loss = 0.684 and accuracy of 0.8\n",
      "Iteration 17851: with minibatch training loss = 0.737 and accuracy of 0.75\n",
      "Iteration 17852: with minibatch training loss = 0.772 and accuracy of 0.77\n",
      "Iteration 17853: with minibatch training loss = 0.402 and accuracy of 0.88\n",
      "Iteration 17854: with minibatch training loss = 0.869 and accuracy of 0.67\n",
      "Iteration 17855: with minibatch training loss = 0.43 and accuracy of 0.86\n",
      "Iteration 17856: with minibatch training loss = 0.623 and accuracy of 0.83\n",
      "Iteration 17857: with minibatch training loss = 0.74 and accuracy of 0.77\n",
      "Iteration 17858: with minibatch training loss = 0.745 and accuracy of 0.77\n",
      "Iteration 17859: with minibatch training loss = 0.796 and accuracy of 0.75\n",
      "Iteration 17860: with minibatch training loss = 0.55 and accuracy of 0.83\n",
      "Iteration 17861: with minibatch training loss = 0.614 and accuracy of 0.78\n",
      "Iteration 17862: with minibatch training loss = 0.632 and accuracy of 0.81\n",
      "Iteration 17863: with minibatch training loss = 0.567 and accuracy of 0.8\n",
      "Iteration 17864: with minibatch training loss = 0.595 and accuracy of 0.78\n",
      "Iteration 17865: with minibatch training loss = 0.619 and accuracy of 0.8\n",
      "Iteration 17866: with minibatch training loss = 0.705 and accuracy of 0.77\n",
      "Iteration 17867: with minibatch training loss = 0.7 and accuracy of 0.77\n",
      "Iteration 17868: with minibatch training loss = 0.782 and accuracy of 0.75\n",
      "Iteration 17869: with minibatch training loss = 0.411 and accuracy of 0.86\n",
      "Iteration 17870: with minibatch training loss = 0.581 and accuracy of 0.8\n",
      "Iteration 17871: with minibatch training loss = 0.525 and accuracy of 0.81\n",
      "Iteration 17872: with minibatch training loss = 0.613 and accuracy of 0.84\n",
      "Iteration 17873: with minibatch training loss = 0.664 and accuracy of 0.8\n",
      "Iteration 17874: with minibatch training loss = 0.396 and accuracy of 0.86\n",
      "Iteration 17875: with minibatch training loss = 0.392 and accuracy of 0.92\n",
      "Iteration 17876: with minibatch training loss = 0.589 and accuracy of 0.83\n",
      "Iteration 17877: with minibatch training loss = 0.741 and accuracy of 0.78\n",
      "Iteration 17878: with minibatch training loss = 0.736 and accuracy of 0.75\n",
      "Iteration 17879: with minibatch training loss = 0.583 and accuracy of 0.81\n",
      "Iteration 17880: with minibatch training loss = 0.6 and accuracy of 0.81\n",
      "Iteration 17881: with minibatch training loss = 0.456 and accuracy of 0.84\n",
      "Iteration 17882: with minibatch training loss = 0.815 and accuracy of 0.8\n",
      "Iteration 17883: with minibatch training loss = 0.607 and accuracy of 0.8\n",
      "Iteration 17884: with minibatch training loss = 0.866 and accuracy of 0.72\n",
      "Iteration 17885: with minibatch training loss = 0.393 and accuracy of 0.88\n",
      "Iteration 17886: with minibatch training loss = 0.756 and accuracy of 0.75\n",
      "Iteration 17887: with minibatch training loss = 0.684 and accuracy of 0.77\n",
      "Iteration 17888: with minibatch training loss = 0.353 and accuracy of 0.89\n",
      "Iteration 17889: with minibatch training loss = 0.577 and accuracy of 0.78\n",
      "Iteration 17890: with minibatch training loss = 0.682 and accuracy of 0.77\n",
      "Iteration 17891: with minibatch training loss = 0.801 and accuracy of 0.7\n",
      "Iteration 17892: with minibatch training loss = 0.678 and accuracy of 0.78\n",
      "Iteration 17893: with minibatch training loss = 0.646 and accuracy of 0.78\n",
      "Iteration 17894: with minibatch training loss = 0.7 and accuracy of 0.73\n",
      "Iteration 17895: with minibatch training loss = 0.505 and accuracy of 0.84\n",
      "Iteration 17896: with minibatch training loss = 0.47 and accuracy of 0.86\n",
      "Iteration 17897: with minibatch training loss = 0.753 and accuracy of 0.75\n",
      "Iteration 17898: with minibatch training loss = 0.442 and accuracy of 0.86\n",
      "Iteration 17899: with minibatch training loss = 0.645 and accuracy of 0.77\n",
      "Iteration 17900: with minibatch training loss = 0.427 and accuracy of 0.86\n",
      "Iteration 17901: with minibatch training loss = 0.598 and accuracy of 0.84\n",
      "Iteration 17902: with minibatch training loss = 0.609 and accuracy of 0.8\n",
      "Iteration 17903: with minibatch training loss = 0.571 and accuracy of 0.83\n",
      "Iteration 17904: with minibatch training loss = 0.496 and accuracy of 0.86\n",
      "Iteration 17905: with minibatch training loss = 0.647 and accuracy of 0.78\n",
      "Iteration 17906: with minibatch training loss = 0.498 and accuracy of 0.86\n",
      "Iteration 17907: with minibatch training loss = 0.91 and accuracy of 0.75\n",
      "Iteration 17908: with minibatch training loss = 0.558 and accuracy of 0.83\n",
      "Iteration 17909: with minibatch training loss = 0.631 and accuracy of 0.81\n",
      "Iteration 17910: with minibatch training loss = 0.897 and accuracy of 0.7\n",
      "Iteration 17911: with minibatch training loss = 0.503 and accuracy of 0.86\n",
      "Iteration 17912: with minibatch training loss = 0.7 and accuracy of 0.78\n",
      "Iteration 17913: with minibatch training loss = 0.65 and accuracy of 0.78\n",
      "Iteration 17914: with minibatch training loss = 0.69 and accuracy of 0.75\n",
      "Iteration 17915: with minibatch training loss = 0.543 and accuracy of 0.84\n",
      "Iteration 17916: with minibatch training loss = 0.685 and accuracy of 0.8\n",
      "Iteration 17917: with minibatch training loss = 0.72 and accuracy of 0.77\n",
      "Iteration 17918: with minibatch training loss = 0.937 and accuracy of 0.72\n",
      "Iteration 17919: with minibatch training loss = 0.773 and accuracy of 0.8\n",
      "Iteration 17920: with minibatch training loss = 0.435 and accuracy of 0.89\n",
      "Iteration 17921: with minibatch training loss = 0.705 and accuracy of 0.78\n",
      "Iteration 17922: with minibatch training loss = 0.617 and accuracy of 0.81\n",
      "Iteration 17923: with minibatch training loss = 0.501 and accuracy of 0.83\n",
      "Iteration 17924: with minibatch training loss = 0.641 and accuracy of 0.78\n",
      "Iteration 17925: with minibatch training loss = 0.709 and accuracy of 0.78\n",
      "Iteration 17926: with minibatch training loss = 0.735 and accuracy of 0.8\n",
      "Iteration 17927: with minibatch training loss = 0.728 and accuracy of 0.77\n",
      "Iteration 17928: with minibatch training loss = 0.767 and accuracy of 0.8\n",
      "Iteration 17929: with minibatch training loss = 0.743 and accuracy of 0.77\n",
      "Iteration 17930: with minibatch training loss = 0.557 and accuracy of 0.83\n",
      "Iteration 17931: with minibatch training loss = 0.703 and accuracy of 0.77\n",
      "Iteration 17932: with minibatch training loss = 0.573 and accuracy of 0.8\n",
      "Iteration 17933: with minibatch training loss = 0.737 and accuracy of 0.78\n",
      "Iteration 17934: with minibatch training loss = 0.954 and accuracy of 0.69\n",
      "Iteration 17935: with minibatch training loss = 0.311 and accuracy of 0.89\n",
      "Iteration 17936: with minibatch training loss = 0.578 and accuracy of 0.83\n",
      "Iteration 17937: with minibatch training loss = 0.779 and accuracy of 0.75\n",
      "Iteration 17938: with minibatch training loss = 0.918 and accuracy of 0.69\n",
      "Iteration 17939: with minibatch training loss = 0.435 and accuracy of 0.88\n",
      "Iteration 17940: with minibatch training loss = 0.733 and accuracy of 0.78\n",
      "Iteration 17941: with minibatch training loss = 0.624 and accuracy of 0.78\n",
      "Iteration 17942: with minibatch training loss = 0.716 and accuracy of 0.75\n",
      "Iteration 17943: with minibatch training loss = 0.514 and accuracy of 0.84\n",
      "Iteration 17944: with minibatch training loss = 0.872 and accuracy of 0.75\n",
      "Iteration 17945: with minibatch training loss = 0.679 and accuracy of 0.8\n",
      "Iteration 17946: with minibatch training loss = 0.973 and accuracy of 0.66\n",
      "Iteration 17947: with minibatch training loss = 0.797 and accuracy of 0.73\n",
      "Iteration 17948: with minibatch training loss = 0.674 and accuracy of 0.78\n",
      "Iteration 17949: with minibatch training loss = 0.751 and accuracy of 0.81\n",
      "Iteration 17950: with minibatch training loss = 0.687 and accuracy of 0.8\n",
      "Iteration 17951: with minibatch training loss = 0.667 and accuracy of 0.77\n",
      "Iteration 17952: with minibatch training loss = 0.706 and accuracy of 0.8\n",
      "Iteration 17953: with minibatch training loss = 0.882 and accuracy of 0.73\n",
      "Iteration 17954: with minibatch training loss = 0.544 and accuracy of 0.84\n",
      "Iteration 17955: with minibatch training loss = 0.625 and accuracy of 0.8\n",
      "Iteration 17956: with minibatch training loss = 0.662 and accuracy of 0.75\n",
      "Iteration 17957: with minibatch training loss = 0.642 and accuracy of 0.78\n",
      "Iteration 17958: with minibatch training loss = 0.863 and accuracy of 0.73\n",
      "Iteration 17959: with minibatch training loss = 0.407 and accuracy of 0.91\n",
      "Iteration 17960: with minibatch training loss = 0.62 and accuracy of 0.81\n",
      "Iteration 17961: with minibatch training loss = 0.649 and accuracy of 0.8\n",
      "Iteration 17962: with minibatch training loss = 0.71 and accuracy of 0.78\n",
      "Iteration 17963: with minibatch training loss = 0.978 and accuracy of 0.67\n",
      "Iteration 17964: with minibatch training loss = 0.571 and accuracy of 0.84\n",
      "Iteration 17965: with minibatch training loss = 0.575 and accuracy of 0.81\n",
      "Iteration 17966: with minibatch training loss = 0.809 and accuracy of 0.77\n",
      "Iteration 17967: with minibatch training loss = 0.502 and accuracy of 0.84\n",
      "Iteration 17968: with minibatch training loss = 0.582 and accuracy of 0.81\n",
      "Iteration 17969: with minibatch training loss = 0.571 and accuracy of 0.81\n",
      "Iteration 17970: with minibatch training loss = 0.585 and accuracy of 0.8\n",
      "Iteration 17971: with minibatch training loss = 0.486 and accuracy of 0.86\n",
      "Iteration 17972: with minibatch training loss = 0.619 and accuracy of 0.8\n",
      "Iteration 17973: with minibatch training loss = 0.589 and accuracy of 0.83\n",
      "Iteration 17974: with minibatch training loss = 0.63 and accuracy of 0.78\n",
      "Iteration 17975: with minibatch training loss = 0.753 and accuracy of 0.77\n",
      "Iteration 17976: with minibatch training loss = 0.277 and accuracy of 0.92\n",
      "Iteration 17977: with minibatch training loss = 0.643 and accuracy of 0.75\n",
      "Iteration 17978: with minibatch training loss = 0.602 and accuracy of 0.83\n",
      "Iteration 17979: with minibatch training loss = 0.602 and accuracy of 0.8\n",
      "Iteration 17980: with minibatch training loss = 0.706 and accuracy of 0.8\n",
      "Iteration 17981: with minibatch training loss = 0.995 and accuracy of 0.69\n",
      "Iteration 17982: with minibatch training loss = 0.69 and accuracy of 0.78\n",
      "Iteration 17983: with minibatch training loss = 0.601 and accuracy of 0.83\n",
      "Iteration 17984: with minibatch training loss = 0.716 and accuracy of 0.77\n",
      "Iteration 17985: with minibatch training loss = 0.683 and accuracy of 0.78\n",
      "Iteration 17986: with minibatch training loss = 0.637 and accuracy of 0.81\n",
      "Iteration 17987: with minibatch training loss = 0.772 and accuracy of 0.77\n",
      "Iteration 17988: with minibatch training loss = 0.532 and accuracy of 0.83\n",
      "Iteration 17989: with minibatch training loss = 0.564 and accuracy of 0.88\n",
      "Iteration 17990: with minibatch training loss = 0.756 and accuracy of 0.8\n",
      "Iteration 17991: with minibatch training loss = 0.814 and accuracy of 0.73\n",
      "Iteration 17992: with minibatch training loss = 0.771 and accuracy of 0.73\n",
      "Iteration 17993: with minibatch training loss = 0.776 and accuracy of 0.72\n",
      "Iteration 17994: with minibatch training loss = 0.662 and accuracy of 0.81\n",
      "Iteration 17995: with minibatch training loss = 0.663 and accuracy of 0.77\n",
      "Iteration 17996: with minibatch training loss = 0.676 and accuracy of 0.78\n",
      "Iteration 17997: with minibatch training loss = 0.721 and accuracy of 0.73\n",
      "Iteration 17998: with minibatch training loss = 0.827 and accuracy of 0.72\n",
      "Iteration 17999: with minibatch training loss = 0.764 and accuracy of 0.75\n",
      "Iteration 18000: with minibatch training loss = 0.915 and accuracy of 0.69\n",
      "Iteration 18001: with minibatch training loss = 0.748 and accuracy of 0.75\n",
      "Iteration 18002: with minibatch training loss = 0.606 and accuracy of 0.8\n",
      "Iteration 18003: with minibatch training loss = 0.55 and accuracy of 0.86\n",
      "Iteration 18004: with minibatch training loss = 0.595 and accuracy of 0.78\n",
      "Iteration 18005: with minibatch training loss = 0.662 and accuracy of 0.78\n",
      "Iteration 18006: with minibatch training loss = 0.732 and accuracy of 0.8\n",
      "Iteration 18007: with minibatch training loss = 0.585 and accuracy of 0.81\n",
      "Iteration 18008: with minibatch training loss = 0.681 and accuracy of 0.78\n",
      "Iteration 18009: with minibatch training loss = 0.869 and accuracy of 0.73\n",
      "Iteration 18010: with minibatch training loss = 0.662 and accuracy of 0.78\n",
      "Iteration 18011: with minibatch training loss = 0.71 and accuracy of 0.78\n",
      "Iteration 18012: with minibatch training loss = 0.617 and accuracy of 0.8\n",
      "Iteration 18013: with minibatch training loss = 0.407 and accuracy of 0.86\n",
      "Iteration 18014: with minibatch training loss = 0.716 and accuracy of 0.77\n",
      "Iteration 18015: with minibatch training loss = 0.684 and accuracy of 0.77\n",
      "Iteration 18016: with minibatch training loss = 1 and accuracy of 0.64\n",
      "Iteration 18017: with minibatch training loss = 0.64 and accuracy of 0.77\n",
      "Iteration 18018: with minibatch training loss = 0.446 and accuracy of 0.86\n",
      "Iteration 18019: with minibatch training loss = 0.722 and accuracy of 0.78\n",
      "Iteration 18020: with minibatch training loss = 0.667 and accuracy of 0.8\n",
      "Iteration 18021: with minibatch training loss = 0.593 and accuracy of 0.84\n",
      "Iteration 18022: with minibatch training loss = 0.571 and accuracy of 0.84\n",
      "Iteration 18023: with minibatch training loss = 0.857 and accuracy of 0.69\n",
      "Iteration 18024: with minibatch training loss = 0.604 and accuracy of 0.78\n",
      "Iteration 18025: with minibatch training loss = 0.693 and accuracy of 0.77\n",
      "Iteration 18026: with minibatch training loss = 0.376 and accuracy of 0.89\n",
      "Iteration 18027: with minibatch training loss = 0.586 and accuracy of 0.81\n",
      "Iteration 18028: with minibatch training loss = 0.583 and accuracy of 0.83\n",
      "Iteration 18029: with minibatch training loss = 0.48 and accuracy of 0.88\n",
      "Iteration 18030: with minibatch training loss = 0.645 and accuracy of 0.78\n",
      "Iteration 18031: with minibatch training loss = 0.694 and accuracy of 0.77\n",
      "Iteration 18032: with minibatch training loss = 0.714 and accuracy of 0.78\n",
      "Iteration 18033: with minibatch training loss = 0.66 and accuracy of 0.81\n",
      "Iteration 18034: with minibatch training loss = 0.623 and accuracy of 0.77\n",
      "Iteration 18035: with minibatch training loss = 0.605 and accuracy of 0.86\n",
      "Iteration 18036: with minibatch training loss = 0.67 and accuracy of 0.81\n",
      "Iteration 18037: with minibatch training loss = 0.744 and accuracy of 0.73\n",
      "Iteration 18038: with minibatch training loss = 0.914 and accuracy of 0.77\n",
      "Iteration 18039: with minibatch training loss = 0.787 and accuracy of 0.77\n",
      "Iteration 18040: with minibatch training loss = 0.735 and accuracy of 0.77\n",
      "Iteration 18041: with minibatch training loss = 0.739 and accuracy of 0.77\n",
      "Iteration 18042: with minibatch training loss = 0.721 and accuracy of 0.8\n",
      "Validation loss: 0.20565814\n",
      "Epoch 13, Overall loss = 0.644 and accuracy of 0.799\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWs6BwMys5f4Dc78MEYDb6dUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11ffe0c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18044: with minibatch training loss = 0.452 and accuracy of 0.86\n",
      "Iteration 18045: with minibatch training loss = 0.676 and accuracy of 0.78\n",
      "Iteration 18046: with minibatch training loss = 0.668 and accuracy of 0.78\n",
      "Iteration 18047: with minibatch training loss = 0.689 and accuracy of 0.77\n",
      "Iteration 18048: with minibatch training loss = 0.746 and accuracy of 0.8\n",
      "Iteration 18049: with minibatch training loss = 0.954 and accuracy of 0.69\n",
      "Iteration 18050: with minibatch training loss = 0.543 and accuracy of 0.84\n",
      "Iteration 18051: with minibatch training loss = 0.603 and accuracy of 0.8\n",
      "Iteration 18052: with minibatch training loss = 0.592 and accuracy of 0.83\n",
      "Iteration 18053: with minibatch training loss = 0.626 and accuracy of 0.8\n",
      "Iteration 18054: with minibatch training loss = 0.636 and accuracy of 0.81\n",
      "Iteration 18055: with minibatch training loss = 0.652 and accuracy of 0.8\n",
      "Iteration 18056: with minibatch training loss = 0.529 and accuracy of 0.83\n",
      "Iteration 18057: with minibatch training loss = 0.68 and accuracy of 0.81\n",
      "Iteration 18058: with minibatch training loss = 0.801 and accuracy of 0.73\n",
      "Iteration 18059: with minibatch training loss = 0.59 and accuracy of 0.78\n",
      "Iteration 18060: with minibatch training loss = 0.776 and accuracy of 0.75\n",
      "Iteration 18061: with minibatch training loss = 0.675 and accuracy of 0.78\n",
      "Iteration 18062: with minibatch training loss = 0.64 and accuracy of 0.86\n",
      "Iteration 18063: with minibatch training loss = 0.47 and accuracy of 0.84\n",
      "Iteration 18064: with minibatch training loss = 0.759 and accuracy of 0.73\n",
      "Iteration 18065: with minibatch training loss = 0.661 and accuracy of 0.78\n",
      "Iteration 18066: with minibatch training loss = 0.529 and accuracy of 0.83\n",
      "Iteration 18067: with minibatch training loss = 0.608 and accuracy of 0.8\n",
      "Iteration 18068: with minibatch training loss = 0.624 and accuracy of 0.81\n",
      "Iteration 18069: with minibatch training loss = 0.776 and accuracy of 0.72\n",
      "Iteration 18070: with minibatch training loss = 0.492 and accuracy of 0.86\n",
      "Iteration 18071: with minibatch training loss = 0.556 and accuracy of 0.83\n",
      "Iteration 18072: with minibatch training loss = 0.74 and accuracy of 0.75\n",
      "Iteration 18073: with minibatch training loss = 0.743 and accuracy of 0.73\n",
      "Iteration 18074: with minibatch training loss = 0.647 and accuracy of 0.81\n",
      "Iteration 18075: with minibatch training loss = 0.7 and accuracy of 0.77\n",
      "Iteration 18076: with minibatch training loss = 0.642 and accuracy of 0.78\n",
      "Iteration 18077: with minibatch training loss = 0.738 and accuracy of 0.72\n",
      "Iteration 18078: with minibatch training loss = 0.67 and accuracy of 0.81\n",
      "Iteration 18079: with minibatch training loss = 0.656 and accuracy of 0.8\n",
      "Iteration 18080: with minibatch training loss = 0.555 and accuracy of 0.81\n",
      "Iteration 18081: with minibatch training loss = 0.641 and accuracy of 0.81\n",
      "Iteration 18082: with minibatch training loss = 0.73 and accuracy of 0.77\n",
      "Iteration 18083: with minibatch training loss = 0.904 and accuracy of 0.7\n",
      "Iteration 18084: with minibatch training loss = 0.808 and accuracy of 0.78\n",
      "Iteration 18085: with minibatch training loss = 0.766 and accuracy of 0.77\n",
      "Iteration 18086: with minibatch training loss = 0.445 and accuracy of 0.86\n",
      "Iteration 18087: with minibatch training loss = 0.623 and accuracy of 0.8\n",
      "Iteration 18088: with minibatch training loss = 0.735 and accuracy of 0.78\n",
      "Iteration 18089: with minibatch training loss = 0.722 and accuracy of 0.77\n",
      "Iteration 18090: with minibatch training loss = 0.493 and accuracy of 0.86\n",
      "Iteration 18091: with minibatch training loss = 0.68 and accuracy of 0.78\n",
      "Iteration 18092: with minibatch training loss = 0.656 and accuracy of 0.8\n",
      "Iteration 18093: with minibatch training loss = 0.654 and accuracy of 0.8\n",
      "Iteration 18094: with minibatch training loss = 0.566 and accuracy of 0.81\n",
      "Iteration 18095: with minibatch training loss = 0.526 and accuracy of 0.83\n",
      "Iteration 18096: with minibatch training loss = 0.566 and accuracy of 0.8\n",
      "Iteration 18097: with minibatch training loss = 0.644 and accuracy of 0.8\n",
      "Iteration 18098: with minibatch training loss = 0.556 and accuracy of 0.86\n",
      "Iteration 18099: with minibatch training loss = 0.798 and accuracy of 0.75\n",
      "Iteration 18100: with minibatch training loss = 0.53 and accuracy of 0.86\n",
      "Iteration 18101: with minibatch training loss = 0.708 and accuracy of 0.78\n",
      "Iteration 18102: with minibatch training loss = 0.508 and accuracy of 0.84\n",
      "Iteration 18103: with minibatch training loss = 0.57 and accuracy of 0.8\n",
      "Iteration 18104: with minibatch training loss = 0.637 and accuracy of 0.8\n",
      "Iteration 18105: with minibatch training loss = 0.553 and accuracy of 0.83\n",
      "Iteration 18106: with minibatch training loss = 0.417 and accuracy of 0.91\n",
      "Iteration 18107: with minibatch training loss = 0.599 and accuracy of 0.81\n",
      "Iteration 18108: with minibatch training loss = 0.544 and accuracy of 0.81\n",
      "Iteration 18109: with minibatch training loss = 0.588 and accuracy of 0.83\n",
      "Iteration 18110: with minibatch training loss = 0.823 and accuracy of 0.73\n",
      "Iteration 18111: with minibatch training loss = 0.362 and accuracy of 0.89\n",
      "Iteration 18112: with minibatch training loss = 0.753 and accuracy of 0.77\n",
      "Iteration 18113: with minibatch training loss = 0.773 and accuracy of 0.72\n",
      "Iteration 18114: with minibatch training loss = 0.675 and accuracy of 0.78\n",
      "Iteration 18115: with minibatch training loss = 0.495 and accuracy of 0.86\n",
      "Iteration 18116: with minibatch training loss = 0.747 and accuracy of 0.83\n",
      "Iteration 18117: with minibatch training loss = 0.833 and accuracy of 0.73\n",
      "Iteration 18118: with minibatch training loss = 0.5 and accuracy of 0.84\n",
      "Iteration 18119: with minibatch training loss = 0.684 and accuracy of 0.78\n",
      "Iteration 18120: with minibatch training loss = 0.509 and accuracy of 0.88\n",
      "Iteration 18121: with minibatch training loss = 0.465 and accuracy of 0.88\n",
      "Iteration 18122: with minibatch training loss = 0.655 and accuracy of 0.8\n",
      "Iteration 18123: with minibatch training loss = 0.499 and accuracy of 0.86\n",
      "Iteration 18124: with minibatch training loss = 0.789 and accuracy of 0.75\n",
      "Iteration 18125: with minibatch training loss = 0.572 and accuracy of 0.86\n",
      "Iteration 18126: with minibatch training loss = 0.459 and accuracy of 0.86\n",
      "Iteration 18127: with minibatch training loss = 0.632 and accuracy of 0.78\n",
      "Iteration 18128: with minibatch training loss = 0.642 and accuracy of 0.8\n",
      "Iteration 18129: with minibatch training loss = 0.555 and accuracy of 0.81\n",
      "Iteration 18130: with minibatch training loss = 0.572 and accuracy of 0.84\n",
      "Iteration 18131: with minibatch training loss = 0.767 and accuracy of 0.8\n",
      "Iteration 18132: with minibatch training loss = 0.717 and accuracy of 0.78\n",
      "Iteration 18133: with minibatch training loss = 0.461 and accuracy of 0.86\n",
      "Iteration 18134: with minibatch training loss = 0.606 and accuracy of 0.81\n",
      "Iteration 18135: with minibatch training loss = 0.427 and accuracy of 0.84\n",
      "Iteration 18136: with minibatch training loss = 0.81 and accuracy of 0.75\n",
      "Iteration 18137: with minibatch training loss = 0.714 and accuracy of 0.78\n",
      "Iteration 18138: with minibatch training loss = 0.513 and accuracy of 0.81\n",
      "Iteration 18139: with minibatch training loss = 0.592 and accuracy of 0.81\n",
      "Iteration 18140: with minibatch training loss = 0.821 and accuracy of 0.72\n",
      "Iteration 18141: with minibatch training loss = 0.387 and accuracy of 0.91\n",
      "Iteration 18142: with minibatch training loss = 0.639 and accuracy of 0.83\n",
      "Iteration 18143: with minibatch training loss = 0.518 and accuracy of 0.81\n",
      "Iteration 18144: with minibatch training loss = 0.809 and accuracy of 0.72\n",
      "Iteration 18145: with minibatch training loss = 0.576 and accuracy of 0.86\n",
      "Iteration 18146: with minibatch training loss = 0.621 and accuracy of 0.83\n",
      "Iteration 18147: with minibatch training loss = 0.529 and accuracy of 0.84\n",
      "Iteration 18148: with minibatch training loss = 0.804 and accuracy of 0.77\n",
      "Iteration 18149: with minibatch training loss = 0.963 and accuracy of 0.7\n",
      "Iteration 18150: with minibatch training loss = 0.73 and accuracy of 0.8\n",
      "Iteration 18151: with minibatch training loss = 0.46 and accuracy of 0.83\n",
      "Iteration 18152: with minibatch training loss = 0.618 and accuracy of 0.81\n",
      "Iteration 18153: with minibatch training loss = 0.588 and accuracy of 0.81\n",
      "Iteration 18154: with minibatch training loss = 0.575 and accuracy of 0.81\n",
      "Iteration 18155: with minibatch training loss = 0.786 and accuracy of 0.78\n",
      "Iteration 18156: with minibatch training loss = 0.846 and accuracy of 0.78\n",
      "Iteration 18157: with minibatch training loss = 0.865 and accuracy of 0.72\n",
      "Iteration 18158: with minibatch training loss = 0.477 and accuracy of 0.89\n",
      "Iteration 18159: with minibatch training loss = 0.781 and accuracy of 0.77\n",
      "Iteration 18160: with minibatch training loss = 0.772 and accuracy of 0.77\n",
      "Iteration 18161: with minibatch training loss = 0.579 and accuracy of 0.84\n",
      "Iteration 18162: with minibatch training loss = 0.615 and accuracy of 0.81\n",
      "Iteration 18163: with minibatch training loss = 0.365 and accuracy of 0.91\n",
      "Iteration 18164: with minibatch training loss = 0.732 and accuracy of 0.77\n",
      "Iteration 18165: with minibatch training loss = 0.63 and accuracy of 0.84\n",
      "Iteration 18166: with minibatch training loss = 0.441 and accuracy of 0.86\n",
      "Iteration 18167: with minibatch training loss = 0.665 and accuracy of 0.81\n",
      "Iteration 18168: with minibatch training loss = 0.624 and accuracy of 0.83\n",
      "Iteration 18169: with minibatch training loss = 0.582 and accuracy of 0.83\n",
      "Iteration 18170: with minibatch training loss = 0.805 and accuracy of 0.73\n",
      "Iteration 18171: with minibatch training loss = 0.561 and accuracy of 0.8\n",
      "Iteration 18172: with minibatch training loss = 0.515 and accuracy of 0.88\n",
      "Iteration 18173: with minibatch training loss = 0.493 and accuracy of 0.84\n",
      "Iteration 18174: with minibatch training loss = 0.612 and accuracy of 0.81\n",
      "Iteration 18175: with minibatch training loss = 0.581 and accuracy of 0.81\n",
      "Iteration 18176: with minibatch training loss = 0.624 and accuracy of 0.77\n",
      "Iteration 18177: with minibatch training loss = 0.459 and accuracy of 0.88\n",
      "Iteration 18178: with minibatch training loss = 0.642 and accuracy of 0.77\n",
      "Iteration 18179: with minibatch training loss = 0.473 and accuracy of 0.84\n",
      "Iteration 18180: with minibatch training loss = 0.509 and accuracy of 0.84\n",
      "Iteration 18181: with minibatch training loss = 0.524 and accuracy of 0.84\n",
      "Iteration 18182: with minibatch training loss = 0.942 and accuracy of 0.69\n",
      "Iteration 18183: with minibatch training loss = 0.64 and accuracy of 0.77\n",
      "Iteration 18184: with minibatch training loss = 0.387 and accuracy of 0.88\n",
      "Iteration 18185: with minibatch training loss = 0.543 and accuracy of 0.86\n",
      "Iteration 18186: with minibatch training loss = 0.513 and accuracy of 0.84\n",
      "Iteration 18187: with minibatch training loss = 0.675 and accuracy of 0.77\n",
      "Iteration 18188: with minibatch training loss = 0.71 and accuracy of 0.77\n",
      "Iteration 18189: with minibatch training loss = 0.73 and accuracy of 0.75\n",
      "Iteration 18190: with minibatch training loss = 0.382 and accuracy of 0.88\n",
      "Iteration 18191: with minibatch training loss = 0.654 and accuracy of 0.8\n",
      "Iteration 18192: with minibatch training loss = 0.717 and accuracy of 0.75\n",
      "Iteration 18193: with minibatch training loss = 0.464 and accuracy of 0.86\n",
      "Iteration 18194: with minibatch training loss = 0.682 and accuracy of 0.78\n",
      "Iteration 18195: with minibatch training loss = 0.681 and accuracy of 0.78\n",
      "Iteration 18196: with minibatch training loss = 0.553 and accuracy of 0.84\n",
      "Iteration 18197: with minibatch training loss = 0.546 and accuracy of 0.84\n",
      "Iteration 18198: with minibatch training loss = 0.497 and accuracy of 0.86\n",
      "Iteration 18199: with minibatch training loss = 0.661 and accuracy of 0.8\n",
      "Iteration 18200: with minibatch training loss = 0.633 and accuracy of 0.78\n",
      "Iteration 18201: with minibatch training loss = 0.587 and accuracy of 0.81\n",
      "Iteration 18202: with minibatch training loss = 0.508 and accuracy of 0.83\n",
      "Iteration 18203: with minibatch training loss = 0.467 and accuracy of 0.86\n",
      "Iteration 18204: with minibatch training loss = 0.619 and accuracy of 0.8\n",
      "Iteration 18205: with minibatch training loss = 0.846 and accuracy of 0.78\n",
      "Iteration 18206: with minibatch training loss = 0.603 and accuracy of 0.8\n",
      "Iteration 18207: with minibatch training loss = 0.776 and accuracy of 0.75\n",
      "Iteration 18208: with minibatch training loss = 0.568 and accuracy of 0.81\n",
      "Iteration 18209: with minibatch training loss = 1 and accuracy of 0.69\n",
      "Iteration 18210: with minibatch training loss = 0.405 and accuracy of 0.89\n",
      "Iteration 18211: with minibatch training loss = 0.641 and accuracy of 0.77\n",
      "Iteration 18212: with minibatch training loss = 0.823 and accuracy of 0.75\n",
      "Iteration 18213: with minibatch training loss = 0.689 and accuracy of 0.81\n",
      "Iteration 18214: with minibatch training loss = 0.615 and accuracy of 0.8\n",
      "Iteration 18215: with minibatch training loss = 0.612 and accuracy of 0.8\n",
      "Iteration 18216: with minibatch training loss = 0.802 and accuracy of 0.77\n",
      "Iteration 18217: with minibatch training loss = 0.631 and accuracy of 0.8\n",
      "Iteration 18218: with minibatch training loss = 0.791 and accuracy of 0.73\n",
      "Iteration 18219: with minibatch training loss = 0.688 and accuracy of 0.8\n",
      "Iteration 18220: with minibatch training loss = 0.528 and accuracy of 0.88\n",
      "Iteration 18221: with minibatch training loss = 0.31 and accuracy of 0.89\n",
      "Iteration 18222: with minibatch training loss = 0.979 and accuracy of 0.67\n",
      "Iteration 18223: with minibatch training loss = 0.505 and accuracy of 0.86\n",
      "Iteration 18224: with minibatch training loss = 0.812 and accuracy of 0.77\n",
      "Iteration 18225: with minibatch training loss = 0.722 and accuracy of 0.75\n",
      "Iteration 18226: with minibatch training loss = 0.473 and accuracy of 0.83\n",
      "Iteration 18227: with minibatch training loss = 1.01 and accuracy of 0.67\n",
      "Iteration 18228: with minibatch training loss = 0.34 and accuracy of 0.89\n",
      "Iteration 18229: with minibatch training loss = 0.526 and accuracy of 0.86\n",
      "Iteration 18230: with minibatch training loss = 0.616 and accuracy of 0.81\n",
      "Iteration 18231: with minibatch training loss = 0.584 and accuracy of 0.81\n",
      "Iteration 18232: with minibatch training loss = 0.727 and accuracy of 0.77\n",
      "Iteration 18233: with minibatch training loss = 0.585 and accuracy of 0.83\n",
      "Iteration 18234: with minibatch training loss = 0.539 and accuracy of 0.86\n",
      "Iteration 18235: with minibatch training loss = 0.876 and accuracy of 0.7\n",
      "Iteration 18236: with minibatch training loss = 0.808 and accuracy of 0.77\n",
      "Iteration 18237: with minibatch training loss = 0.679 and accuracy of 0.8\n",
      "Iteration 18238: with minibatch training loss = 0.577 and accuracy of 0.81\n",
      "Iteration 18239: with minibatch training loss = 0.719 and accuracy of 0.78\n",
      "Iteration 18240: with minibatch training loss = 0.606 and accuracy of 0.8\n",
      "Iteration 18241: with minibatch training loss = 0.586 and accuracy of 0.78\n",
      "Iteration 18242: with minibatch training loss = 0.959 and accuracy of 0.7\n",
      "Iteration 18243: with minibatch training loss = 0.872 and accuracy of 0.75\n",
      "Iteration 18244: with minibatch training loss = 0.679 and accuracy of 0.77\n",
      "Iteration 18245: with minibatch training loss = 0.8 and accuracy of 0.75\n",
      "Iteration 18246: with minibatch training loss = 0.63 and accuracy of 0.8\n",
      "Iteration 18247: with minibatch training loss = 0.618 and accuracy of 0.8\n",
      "Iteration 18248: with minibatch training loss = 0.524 and accuracy of 0.88\n",
      "Iteration 18249: with minibatch training loss = 0.648 and accuracy of 0.77\n",
      "Iteration 18250: with minibatch training loss = 0.679 and accuracy of 0.81\n",
      "Iteration 18251: with minibatch training loss = 0.437 and accuracy of 0.91\n",
      "Iteration 18252: with minibatch training loss = 0.722 and accuracy of 0.77\n",
      "Iteration 18253: with minibatch training loss = 0.839 and accuracy of 0.75\n",
      "Iteration 18254: with minibatch training loss = 0.641 and accuracy of 0.8\n",
      "Iteration 18255: with minibatch training loss = 0.446 and accuracy of 0.84\n",
      "Iteration 18256: with minibatch training loss = 0.601 and accuracy of 0.83\n",
      "Iteration 18257: with minibatch training loss = 0.512 and accuracy of 0.83\n",
      "Iteration 18258: with minibatch training loss = 0.633 and accuracy of 0.84\n",
      "Iteration 18259: with minibatch training loss = 0.776 and accuracy of 0.72\n",
      "Iteration 18260: with minibatch training loss = 0.735 and accuracy of 0.77\n",
      "Iteration 18261: with minibatch training loss = 0.525 and accuracy of 0.84\n",
      "Iteration 18262: with minibatch training loss = 0.639 and accuracy of 0.8\n",
      "Iteration 18263: with minibatch training loss = 0.679 and accuracy of 0.81\n",
      "Iteration 18264: with minibatch training loss = 0.573 and accuracy of 0.81\n",
      "Iteration 18265: with minibatch training loss = 0.753 and accuracy of 0.75\n",
      "Iteration 18266: with minibatch training loss = 0.338 and accuracy of 0.89\n",
      "Iteration 18267: with minibatch training loss = 0.692 and accuracy of 0.75\n",
      "Iteration 18268: with minibatch training loss = 0.707 and accuracy of 0.78\n",
      "Iteration 18269: with minibatch training loss = 0.853 and accuracy of 0.73\n",
      "Iteration 18270: with minibatch training loss = 0.594 and accuracy of 0.8\n",
      "Iteration 18271: with minibatch training loss = 0.605 and accuracy of 0.81\n",
      "Iteration 18272: with minibatch training loss = 0.806 and accuracy of 0.77\n",
      "Iteration 18273: with minibatch training loss = 0.583 and accuracy of 0.8\n",
      "Iteration 18274: with minibatch training loss = 0.565 and accuracy of 0.8\n",
      "Iteration 18275: with minibatch training loss = 0.882 and accuracy of 0.78\n",
      "Iteration 18276: with minibatch training loss = 0.417 and accuracy of 0.89\n",
      "Iteration 18277: with minibatch training loss = 0.578 and accuracy of 0.83\n",
      "Iteration 18278: with minibatch training loss = 0.563 and accuracy of 0.83\n",
      "Iteration 18279: with minibatch training loss = 0.629 and accuracy of 0.78\n",
      "Iteration 18280: with minibatch training loss = 0.427 and accuracy of 0.84\n",
      "Iteration 18281: with minibatch training loss = 0.649 and accuracy of 0.78\n",
      "Iteration 18282: with minibatch training loss = 0.489 and accuracy of 0.83\n",
      "Iteration 18283: with minibatch training loss = 0.513 and accuracy of 0.81\n",
      "Iteration 18284: with minibatch training loss = 0.474 and accuracy of 0.81\n",
      "Iteration 18285: with minibatch training loss = 0.796 and accuracy of 0.75\n",
      "Iteration 18286: with minibatch training loss = 0.686 and accuracy of 0.8\n",
      "Iteration 18287: with minibatch training loss = 0.461 and accuracy of 0.84\n",
      "Iteration 18288: with minibatch training loss = 0.577 and accuracy of 0.81\n",
      "Iteration 18289: with minibatch training loss = 0.636 and accuracy of 0.78\n",
      "Iteration 18290: with minibatch training loss = 0.641 and accuracy of 0.81\n",
      "Iteration 18291: with minibatch training loss = 0.815 and accuracy of 0.77\n",
      "Iteration 18292: with minibatch training loss = 0.496 and accuracy of 0.84\n",
      "Iteration 18293: with minibatch training loss = 0.81 and accuracy of 0.73\n",
      "Iteration 18294: with minibatch training loss = 0.735 and accuracy of 0.73\n",
      "Iteration 18295: with minibatch training loss = 0.448 and accuracy of 0.86\n",
      "Iteration 18296: with minibatch training loss = 0.443 and accuracy of 0.84\n",
      "Iteration 18297: with minibatch training loss = 0.548 and accuracy of 0.84\n",
      "Iteration 18298: with minibatch training loss = 0.563 and accuracy of 0.81\n",
      "Iteration 18299: with minibatch training loss = 0.613 and accuracy of 0.8\n",
      "Iteration 18300: with minibatch training loss = 0.639 and accuracy of 0.81\n",
      "Iteration 18301: with minibatch training loss = 0.552 and accuracy of 0.83\n",
      "Iteration 18302: with minibatch training loss = 0.684 and accuracy of 0.77\n",
      "Iteration 18303: with minibatch training loss = 0.44 and accuracy of 0.86\n",
      "Iteration 18304: with minibatch training loss = 0.707 and accuracy of 0.78\n",
      "Iteration 18305: with minibatch training loss = 0.727 and accuracy of 0.77\n",
      "Iteration 18306: with minibatch training loss = 0.709 and accuracy of 0.75\n",
      "Iteration 18307: with minibatch training loss = 0.59 and accuracy of 0.81\n",
      "Iteration 18308: with minibatch training loss = 0.585 and accuracy of 0.8\n",
      "Iteration 18309: with minibatch training loss = 0.723 and accuracy of 0.77\n",
      "Iteration 18310: with minibatch training loss = 0.627 and accuracy of 0.78\n",
      "Iteration 18311: with minibatch training loss = 0.523 and accuracy of 0.86\n",
      "Iteration 18312: with minibatch training loss = 0.656 and accuracy of 0.77\n",
      "Iteration 18313: with minibatch training loss = 0.62 and accuracy of 0.81\n",
      "Iteration 18314: with minibatch training loss = 0.613 and accuracy of 0.8\n",
      "Iteration 18315: with minibatch training loss = 0.507 and accuracy of 0.89\n",
      "Iteration 18316: with minibatch training loss = 0.478 and accuracy of 0.86\n",
      "Iteration 18317: with minibatch training loss = 0.719 and accuracy of 0.81\n",
      "Iteration 18318: with minibatch training loss = 0.617 and accuracy of 0.8\n",
      "Iteration 18319: with minibatch training loss = 0.78 and accuracy of 0.8\n",
      "Iteration 18320: with minibatch training loss = 0.443 and accuracy of 0.84\n",
      "Iteration 18321: with minibatch training loss = 0.94 and accuracy of 0.73\n",
      "Iteration 18322: with minibatch training loss = 0.447 and accuracy of 0.86\n",
      "Iteration 18323: with minibatch training loss = 0.644 and accuracy of 0.78\n",
      "Iteration 18324: with minibatch training loss = 0.721 and accuracy of 0.77\n",
      "Iteration 18325: with minibatch training loss = 0.601 and accuracy of 0.77\n",
      "Iteration 18326: with minibatch training loss = 0.747 and accuracy of 0.75\n",
      "Iteration 18327: with minibatch training loss = 0.712 and accuracy of 0.77\n",
      "Iteration 18328: with minibatch training loss = 0.6 and accuracy of 0.81\n",
      "Iteration 18329: with minibatch training loss = 0.423 and accuracy of 0.91\n",
      "Iteration 18330: with minibatch training loss = 0.643 and accuracy of 0.78\n",
      "Iteration 18331: with minibatch training loss = 0.743 and accuracy of 0.77\n",
      "Iteration 18332: with minibatch training loss = 0.462 and accuracy of 0.88\n",
      "Iteration 18333: with minibatch training loss = 0.573 and accuracy of 0.81\n",
      "Iteration 18334: with minibatch training loss = 0.431 and accuracy of 0.88\n",
      "Iteration 18335: with minibatch training loss = 0.616 and accuracy of 0.81\n",
      "Iteration 18336: with minibatch training loss = 0.639 and accuracy of 0.78\n",
      "Iteration 18337: with minibatch training loss = 0.821 and accuracy of 0.83\n",
      "Iteration 18338: with minibatch training loss = 0.532 and accuracy of 0.81\n",
      "Iteration 18339: with minibatch training loss = 0.903 and accuracy of 0.72\n",
      "Iteration 18340: with minibatch training loss = 0.616 and accuracy of 0.83\n",
      "Iteration 18341: with minibatch training loss = 0.584 and accuracy of 0.81\n",
      "Iteration 18342: with minibatch training loss = 0.477 and accuracy of 0.84\n",
      "Iteration 18343: with minibatch training loss = 0.672 and accuracy of 0.78\n",
      "Iteration 18344: with minibatch training loss = 0.674 and accuracy of 0.8\n",
      "Iteration 18345: with minibatch training loss = 0.54 and accuracy of 0.81\n",
      "Iteration 18346: with minibatch training loss = 0.719 and accuracy of 0.77\n",
      "Iteration 18347: with minibatch training loss = 0.378 and accuracy of 0.89\n",
      "Iteration 18348: with minibatch training loss = 0.763 and accuracy of 0.73\n",
      "Iteration 18349: with minibatch training loss = 0.494 and accuracy of 0.83\n",
      "Iteration 18350: with minibatch training loss = 0.505 and accuracy of 0.84\n",
      "Iteration 18351: with minibatch training loss = 0.382 and accuracy of 0.89\n",
      "Iteration 18352: with minibatch training loss = 0.514 and accuracy of 0.84\n",
      "Iteration 18353: with minibatch training loss = 0.772 and accuracy of 0.77\n",
      "Iteration 18354: with minibatch training loss = 0.399 and accuracy of 0.86\n",
      "Iteration 18355: with minibatch training loss = 0.463 and accuracy of 0.88\n",
      "Iteration 18356: with minibatch training loss = 0.635 and accuracy of 0.83\n",
      "Iteration 18357: with minibatch training loss = 0.436 and accuracy of 0.88\n",
      "Iteration 18358: with minibatch training loss = 0.64 and accuracy of 0.8\n",
      "Iteration 18359: with minibatch training loss = 0.652 and accuracy of 0.81\n",
      "Iteration 18360: with minibatch training loss = 0.692 and accuracy of 0.78\n",
      "Iteration 18361: with minibatch training loss = 0.734 and accuracy of 0.81\n",
      "Iteration 18362: with minibatch training loss = 0.711 and accuracy of 0.8\n",
      "Iteration 18363: with minibatch training loss = 0.436 and accuracy of 0.88\n",
      "Iteration 18364: with minibatch training loss = 0.844 and accuracy of 0.75\n",
      "Iteration 18365: with minibatch training loss = 0.887 and accuracy of 0.7\n",
      "Iteration 18366: with minibatch training loss = 0.657 and accuracy of 0.8\n",
      "Iteration 18367: with minibatch training loss = 0.426 and accuracy of 0.88\n",
      "Iteration 18368: with minibatch training loss = 0.585 and accuracy of 0.81\n",
      "Iteration 18369: with minibatch training loss = 0.689 and accuracy of 0.77\n",
      "Iteration 18370: with minibatch training loss = 0.538 and accuracy of 0.84\n",
      "Iteration 18371: with minibatch training loss = 0.416 and accuracy of 0.88\n",
      "Iteration 18372: with minibatch training loss = 0.58 and accuracy of 0.8\n",
      "Iteration 18373: with minibatch training loss = 0.656 and accuracy of 0.78\n",
      "Iteration 18374: with minibatch training loss = 0.595 and accuracy of 0.81\n",
      "Iteration 18375: with minibatch training loss = 0.523 and accuracy of 0.83\n",
      "Iteration 18376: with minibatch training loss = 0.546 and accuracy of 0.84\n",
      "Iteration 18377: with minibatch training loss = 0.454 and accuracy of 0.88\n",
      "Iteration 18378: with minibatch training loss = 0.666 and accuracy of 0.81\n",
      "Iteration 18379: with minibatch training loss = 0.543 and accuracy of 0.83\n",
      "Iteration 18380: with minibatch training loss = 0.555 and accuracy of 0.84\n",
      "Iteration 18381: with minibatch training loss = 0.868 and accuracy of 0.72\n",
      "Iteration 18382: with minibatch training loss = 0.568 and accuracy of 0.8\n",
      "Iteration 18383: with minibatch training loss = 0.813 and accuracy of 0.8\n",
      "Iteration 18384: with minibatch training loss = 0.802 and accuracy of 0.77\n",
      "Iteration 18385: with minibatch training loss = 0.414 and accuracy of 0.88\n",
      "Iteration 18386: with minibatch training loss = 0.547 and accuracy of 0.83\n",
      "Iteration 18387: with minibatch training loss = 0.589 and accuracy of 0.81\n",
      "Iteration 18388: with minibatch training loss = 0.507 and accuracy of 0.86\n",
      "Iteration 18389: with minibatch training loss = 0.842 and accuracy of 0.72\n",
      "Iteration 18390: with minibatch training loss = 0.539 and accuracy of 0.86\n",
      "Iteration 18391: with minibatch training loss = 0.69 and accuracy of 0.78\n",
      "Iteration 18392: with minibatch training loss = 0.832 and accuracy of 0.72\n",
      "Iteration 18393: with minibatch training loss = 0.312 and accuracy of 0.91\n",
      "Iteration 18394: with minibatch training loss = 0.567 and accuracy of 0.81\n",
      "Iteration 18395: with minibatch training loss = 0.65 and accuracy of 0.8\n",
      "Iteration 18396: with minibatch training loss = 0.585 and accuracy of 0.81\n",
      "Iteration 18397: with minibatch training loss = 0.529 and accuracy of 0.83\n",
      "Iteration 18398: with minibatch training loss = 0.532 and accuracy of 0.81\n",
      "Iteration 18399: with minibatch training loss = 0.728 and accuracy of 0.77\n",
      "Iteration 18400: with minibatch training loss = 0.558 and accuracy of 0.84\n",
      "Iteration 18401: with minibatch training loss = 0.839 and accuracy of 0.77\n",
      "Iteration 18402: with minibatch training loss = 0.44 and accuracy of 0.84\n",
      "Iteration 18403: with minibatch training loss = 0.582 and accuracy of 0.83\n",
      "Iteration 18404: with minibatch training loss = 0.714 and accuracy of 0.77\n",
      "Iteration 18405: with minibatch training loss = 0.696 and accuracy of 0.78\n",
      "Iteration 18406: with minibatch training loss = 0.463 and accuracy of 0.84\n",
      "Iteration 18407: with minibatch training loss = 0.33 and accuracy of 0.89\n",
      "Iteration 18408: with minibatch training loss = 0.596 and accuracy of 0.83\n",
      "Iteration 18409: with minibatch training loss = 0.504 and accuracy of 0.81\n",
      "Iteration 18410: with minibatch training loss = 0.854 and accuracy of 0.73\n",
      "Iteration 18411: with minibatch training loss = 0.619 and accuracy of 0.83\n",
      "Iteration 18412: with minibatch training loss = 0.53 and accuracy of 0.83\n",
      "Iteration 18413: with minibatch training loss = 0.746 and accuracy of 0.72\n",
      "Iteration 18414: with minibatch training loss = 0.52 and accuracy of 0.81\n",
      "Iteration 18415: with minibatch training loss = 0.74 and accuracy of 0.77\n",
      "Iteration 18416: with minibatch training loss = 0.685 and accuracy of 0.78\n",
      "Iteration 18417: with minibatch training loss = 0.59 and accuracy of 0.83\n",
      "Iteration 18418: with minibatch training loss = 0.688 and accuracy of 0.8\n",
      "Iteration 18419: with minibatch training loss = 0.755 and accuracy of 0.75\n",
      "Iteration 18420: with minibatch training loss = 0.6 and accuracy of 0.8\n",
      "Iteration 18421: with minibatch training loss = 1.02 and accuracy of 0.67\n",
      "Iteration 18422: with minibatch training loss = 0.575 and accuracy of 0.81\n",
      "Iteration 18423: with minibatch training loss = 0.577 and accuracy of 0.8\n",
      "Iteration 18424: with minibatch training loss = 0.541 and accuracy of 0.84\n",
      "Iteration 18425: with minibatch training loss = 0.506 and accuracy of 0.86\n",
      "Iteration 18426: with minibatch training loss = 0.663 and accuracy of 0.77\n",
      "Iteration 18427: with minibatch training loss = 0.745 and accuracy of 0.73\n",
      "Iteration 18428: with minibatch training loss = 0.68 and accuracy of 0.77\n",
      "Iteration 18429: with minibatch training loss = 0.695 and accuracy of 0.78\n",
      "Iteration 18430: with minibatch training loss = 0.722 and accuracy of 0.81\n",
      "Iteration 18431: with minibatch training loss = 0.701 and accuracy of 0.78\n",
      "Iteration 18432: with minibatch training loss = 0.615 and accuracy of 0.8\n",
      "Iteration 18433: with minibatch training loss = 0.839 and accuracy of 0.7\n",
      "Iteration 18434: with minibatch training loss = 0.926 and accuracy of 0.72\n",
      "Iteration 18435: with minibatch training loss = 0.466 and accuracy of 0.83\n",
      "Iteration 18436: with minibatch training loss = 0.452 and accuracy of 0.84\n",
      "Iteration 18437: with minibatch training loss = 0.478 and accuracy of 0.84\n",
      "Iteration 18438: with minibatch training loss = 0.783 and accuracy of 0.73\n",
      "Iteration 18439: with minibatch training loss = 0.302 and accuracy of 0.91\n",
      "Iteration 18440: with minibatch training loss = 0.55 and accuracy of 0.83\n",
      "Iteration 18441: with minibatch training loss = 0.726 and accuracy of 0.77\n",
      "Iteration 18442: with minibatch training loss = 0.738 and accuracy of 0.8\n",
      "Iteration 18443: with minibatch training loss = 0.669 and accuracy of 0.78\n",
      "Iteration 18444: with minibatch training loss = 0.557 and accuracy of 0.81\n",
      "Iteration 18445: with minibatch training loss = 0.865 and accuracy of 0.73\n",
      "Iteration 18446: with minibatch training loss = 0.855 and accuracy of 0.69\n",
      "Iteration 18447: with minibatch training loss = 0.543 and accuracy of 0.83\n",
      "Iteration 18448: with minibatch training loss = 0.696 and accuracy of 0.8\n",
      "Iteration 18449: with minibatch training loss = 0.432 and accuracy of 0.83\n",
      "Iteration 18450: with minibatch training loss = 0.509 and accuracy of 0.86\n",
      "Iteration 18451: with minibatch training loss = 0.68 and accuracy of 0.8\n",
      "Iteration 18452: with minibatch training loss = 0.511 and accuracy of 0.84\n",
      "Iteration 18453: with minibatch training loss = 0.578 and accuracy of 0.81\n",
      "Iteration 18454: with minibatch training loss = 0.533 and accuracy of 0.84\n",
      "Iteration 18455: with minibatch training loss = 0.59 and accuracy of 0.8\n",
      "Iteration 18456: with minibatch training loss = 0.4 and accuracy of 0.86\n",
      "Iteration 18457: with minibatch training loss = 0.603 and accuracy of 0.8\n",
      "Iteration 18458: with minibatch training loss = 0.605 and accuracy of 0.81\n",
      "Iteration 18459: with minibatch training loss = 0.803 and accuracy of 0.77\n",
      "Iteration 18460: with minibatch training loss = 0.609 and accuracy of 0.83\n",
      "Iteration 18461: with minibatch training loss = 0.598 and accuracy of 0.81\n",
      "Iteration 18462: with minibatch training loss = 0.537 and accuracy of 0.83\n",
      "Iteration 18463: with minibatch training loss = 0.673 and accuracy of 0.8\n",
      "Iteration 18464: with minibatch training loss = 0.607 and accuracy of 0.78\n",
      "Iteration 18465: with minibatch training loss = 0.803 and accuracy of 0.75\n",
      "Iteration 18466: with minibatch training loss = 0.554 and accuracy of 0.81\n",
      "Iteration 18467: with minibatch training loss = 0.751 and accuracy of 0.75\n",
      "Iteration 18468: with minibatch training loss = 0.97 and accuracy of 0.75\n",
      "Iteration 18469: with minibatch training loss = 0.566 and accuracy of 0.83\n",
      "Iteration 18470: with minibatch training loss = 0.614 and accuracy of 0.8\n",
      "Iteration 18471: with minibatch training loss = 0.759 and accuracy of 0.77\n",
      "Iteration 18472: with minibatch training loss = 0.513 and accuracy of 0.83\n",
      "Iteration 18473: with minibatch training loss = 0.439 and accuracy of 0.86\n",
      "Iteration 18474: with minibatch training loss = 0.346 and accuracy of 0.91\n",
      "Iteration 18475: with minibatch training loss = 0.454 and accuracy of 0.86\n",
      "Iteration 18476: with minibatch training loss = 0.77 and accuracy of 0.73\n",
      "Iteration 18477: with minibatch training loss = 0.518 and accuracy of 0.83\n",
      "Iteration 18478: with minibatch training loss = 0.66 and accuracy of 0.78\n",
      "Iteration 18479: with minibatch training loss = 0.7 and accuracy of 0.81\n",
      "Iteration 18480: with minibatch training loss = 0.61 and accuracy of 0.8\n",
      "Iteration 18481: with minibatch training loss = 0.714 and accuracy of 0.77\n",
      "Iteration 18482: with minibatch training loss = 0.818 and accuracy of 0.72\n",
      "Iteration 18483: with minibatch training loss = 0.514 and accuracy of 0.84\n",
      "Iteration 18484: with minibatch training loss = 0.585 and accuracy of 0.8\n",
      "Iteration 18485: with minibatch training loss = 0.428 and accuracy of 0.88\n",
      "Iteration 18486: with minibatch training loss = 0.503 and accuracy of 0.84\n",
      "Iteration 18487: with minibatch training loss = 0.634 and accuracy of 0.8\n",
      "Iteration 18488: with minibatch training loss = 0.718 and accuracy of 0.78\n",
      "Iteration 18489: with minibatch training loss = 0.77 and accuracy of 0.77\n",
      "Iteration 18490: with minibatch training loss = 0.259 and accuracy of 0.94\n",
      "Iteration 18491: with minibatch training loss = 0.463 and accuracy of 0.84\n",
      "Iteration 18492: with minibatch training loss = 0.437 and accuracy of 0.89\n",
      "Iteration 18493: with minibatch training loss = 0.262 and accuracy of 0.94\n",
      "Iteration 18494: with minibatch training loss = 0.466 and accuracy of 0.84\n",
      "Iteration 18495: with minibatch training loss = 0.482 and accuracy of 0.86\n",
      "Iteration 18496: with minibatch training loss = 0.607 and accuracy of 0.81\n",
      "Iteration 18497: with minibatch training loss = 0.545 and accuracy of 0.81\n",
      "Iteration 18498: with minibatch training loss = 0.715 and accuracy of 0.8\n",
      "Iteration 18499: with minibatch training loss = 0.519 and accuracy of 0.83\n",
      "Iteration 18500: with minibatch training loss = 0.816 and accuracy of 0.73\n",
      "Iteration 18501: with minibatch training loss = 0.695 and accuracy of 0.81\n",
      "Iteration 18502: with minibatch training loss = 0.695 and accuracy of 0.8\n",
      "Iteration 18503: with minibatch training loss = 0.797 and accuracy of 0.75\n",
      "Iteration 18504: with minibatch training loss = 0.813 and accuracy of 0.7\n",
      "Iteration 18505: with minibatch training loss = 0.765 and accuracy of 0.78\n",
      "Iteration 18506: with minibatch training loss = 0.838 and accuracy of 0.72\n",
      "Iteration 18507: with minibatch training loss = 0.606 and accuracy of 0.77\n",
      "Iteration 18508: with minibatch training loss = 0.715 and accuracy of 0.78\n",
      "Iteration 18509: with minibatch training loss = 0.718 and accuracy of 0.77\n",
      "Iteration 18510: with minibatch training loss = 0.653 and accuracy of 0.81\n",
      "Iteration 18511: with minibatch training loss = 0.557 and accuracy of 0.81\n",
      "Iteration 18512: with minibatch training loss = 0.961 and accuracy of 0.67\n",
      "Iteration 18513: with minibatch training loss = 0.678 and accuracy of 0.75\n",
      "Iteration 18514: with minibatch training loss = 0.55 and accuracy of 0.81\n",
      "Iteration 18515: with minibatch training loss = 0.798 and accuracy of 0.78\n",
      "Iteration 18516: with minibatch training loss = 0.517 and accuracy of 0.81\n",
      "Iteration 18517: with minibatch training loss = 0.501 and accuracy of 0.86\n",
      "Iteration 18518: with minibatch training loss = 0.493 and accuracy of 0.86\n",
      "Iteration 18519: with minibatch training loss = 0.825 and accuracy of 0.73\n",
      "Iteration 18520: with minibatch training loss = 0.501 and accuracy of 0.84\n",
      "Iteration 18521: with minibatch training loss = 0.684 and accuracy of 0.78\n",
      "Iteration 18522: with minibatch training loss = 0.628 and accuracy of 0.81\n",
      "Iteration 18523: with minibatch training loss = 0.865 and accuracy of 0.75\n",
      "Iteration 18524: with minibatch training loss = 0.432 and accuracy of 0.88\n",
      "Iteration 18525: with minibatch training loss = 0.616 and accuracy of 0.77\n",
      "Iteration 18526: with minibatch training loss = 0.435 and accuracy of 0.91\n",
      "Iteration 18527: with minibatch training loss = 0.651 and accuracy of 0.8\n",
      "Iteration 18528: with minibatch training loss = 0.643 and accuracy of 0.83\n",
      "Iteration 18529: with minibatch training loss = 0.731 and accuracy of 0.73\n",
      "Iteration 18530: with minibatch training loss = 0.745 and accuracy of 0.81\n",
      "Iteration 18531: with minibatch training loss = 0.801 and accuracy of 0.73\n",
      "Iteration 18532: with minibatch training loss = 0.816 and accuracy of 0.72\n",
      "Iteration 18533: with minibatch training loss = 0.696 and accuracy of 0.78\n",
      "Iteration 18534: with minibatch training loss = 0.542 and accuracy of 0.83\n",
      "Iteration 18535: with minibatch training loss = 0.687 and accuracy of 0.75\n",
      "Iteration 18536: with minibatch training loss = 0.634 and accuracy of 0.8\n",
      "Iteration 18537: with minibatch training loss = 0.448 and accuracy of 0.84\n",
      "Iteration 18538: with minibatch training loss = 0.399 and accuracy of 0.89\n",
      "Iteration 18539: with minibatch training loss = 0.331 and accuracy of 0.89\n",
      "Iteration 18540: with minibatch training loss = 0.543 and accuracy of 0.81\n",
      "Iteration 18541: with minibatch training loss = 0.466 and accuracy of 0.84\n",
      "Iteration 18542: with minibatch training loss = 0.737 and accuracy of 0.77\n",
      "Iteration 18543: with minibatch training loss = 0.514 and accuracy of 0.84\n",
      "Iteration 18544: with minibatch training loss = 0.699 and accuracy of 0.78\n",
      "Iteration 18545: with minibatch training loss = 0.541 and accuracy of 0.84\n",
      "Iteration 18546: with minibatch training loss = 0.743 and accuracy of 0.73\n",
      "Iteration 18547: with minibatch training loss = 0.408 and accuracy of 0.88\n",
      "Iteration 18548: with minibatch training loss = 0.595 and accuracy of 0.88\n",
      "Iteration 18549: with minibatch training loss = 0.532 and accuracy of 0.83\n",
      "Iteration 18550: with minibatch training loss = 0.751 and accuracy of 0.75\n",
      "Iteration 18551: with minibatch training loss = 0.676 and accuracy of 0.8\n",
      "Iteration 18552: with minibatch training loss = 0.574 and accuracy of 0.84\n",
      "Iteration 18553: with minibatch training loss = 0.486 and accuracy of 0.86\n",
      "Iteration 18554: with minibatch training loss = 0.379 and accuracy of 0.89\n",
      "Iteration 18555: with minibatch training loss = 0.617 and accuracy of 0.78\n",
      "Iteration 18556: with minibatch training loss = 0.594 and accuracy of 0.8\n",
      "Iteration 18557: with minibatch training loss = 0.81 and accuracy of 0.72\n",
      "Iteration 18558: with minibatch training loss = 0.507 and accuracy of 0.83\n",
      "Iteration 18559: with minibatch training loss = 0.734 and accuracy of 0.83\n",
      "Iteration 18560: with minibatch training loss = 0.569 and accuracy of 0.88\n",
      "Iteration 18561: with minibatch training loss = 0.729 and accuracy of 0.8\n",
      "Iteration 18562: with minibatch training loss = 0.939 and accuracy of 0.72\n",
      "Iteration 18563: with minibatch training loss = 0.651 and accuracy of 0.78\n",
      "Iteration 18564: with minibatch training loss = 0.645 and accuracy of 0.77\n",
      "Iteration 18565: with minibatch training loss = 0.601 and accuracy of 0.81\n",
      "Iteration 18566: with minibatch training loss = 0.683 and accuracy of 0.8\n",
      "Iteration 18567: with minibatch training loss = 0.94 and accuracy of 0.69\n",
      "Iteration 18568: with minibatch training loss = 0.699 and accuracy of 0.77\n",
      "Iteration 18569: with minibatch training loss = 0.37 and accuracy of 0.89\n",
      "Iteration 18570: with minibatch training loss = 0.779 and accuracy of 0.75\n",
      "Iteration 18571: with minibatch training loss = 0.67 and accuracy of 0.75\n",
      "Iteration 18572: with minibatch training loss = 0.667 and accuracy of 0.77\n",
      "Iteration 18573: with minibatch training loss = 0.508 and accuracy of 0.83\n",
      "Iteration 18574: with minibatch training loss = 0.604 and accuracy of 0.8\n",
      "Iteration 18575: with minibatch training loss = 0.603 and accuracy of 0.81\n",
      "Iteration 18576: with minibatch training loss = 0.486 and accuracy of 0.84\n",
      "Iteration 18577: with minibatch training loss = 0.58 and accuracy of 0.8\n",
      "Iteration 18578: with minibatch training loss = 0.71 and accuracy of 0.75\n",
      "Iteration 18579: with minibatch training loss = 0.75 and accuracy of 0.75\n",
      "Iteration 18580: with minibatch training loss = 0.81 and accuracy of 0.73\n",
      "Iteration 18581: with minibatch training loss = 0.549 and accuracy of 0.84\n",
      "Iteration 18582: with minibatch training loss = 0.524 and accuracy of 0.81\n",
      "Iteration 18583: with minibatch training loss = 0.717 and accuracy of 0.75\n",
      "Iteration 18584: with minibatch training loss = 0.645 and accuracy of 0.83\n",
      "Iteration 18585: with minibatch training loss = 0.264 and accuracy of 0.92\n",
      "Iteration 18586: with minibatch training loss = 0.511 and accuracy of 0.84\n",
      "Iteration 18587: with minibatch training loss = 0.594 and accuracy of 0.8\n",
      "Iteration 18588: with minibatch training loss = 0.817 and accuracy of 0.73\n",
      "Iteration 18589: with minibatch training loss = 0.603 and accuracy of 0.84\n",
      "Iteration 18590: with minibatch training loss = 0.774 and accuracy of 0.72\n",
      "Iteration 18591: with minibatch training loss = 0.798 and accuracy of 0.78\n",
      "Iteration 18592: with minibatch training loss = 0.844 and accuracy of 0.73\n",
      "Iteration 18593: with minibatch training loss = 0.438 and accuracy of 0.86\n",
      "Iteration 18594: with minibatch training loss = 0.77 and accuracy of 0.75\n",
      "Iteration 18595: with minibatch training loss = 0.939 and accuracy of 0.69\n",
      "Iteration 18596: with minibatch training loss = 0.739 and accuracy of 0.75\n",
      "Iteration 18597: with minibatch training loss = 0.591 and accuracy of 0.81\n",
      "Iteration 18598: with minibatch training loss = 0.863 and accuracy of 0.72\n",
      "Iteration 18599: with minibatch training loss = 0.485 and accuracy of 0.84\n",
      "Iteration 18600: with minibatch training loss = 0.622 and accuracy of 0.81\n",
      "Iteration 18601: with minibatch training loss = 0.691 and accuracy of 0.77\n",
      "Iteration 18602: with minibatch training loss = 0.712 and accuracy of 0.77\n",
      "Iteration 18603: with minibatch training loss = 0.602 and accuracy of 0.8\n",
      "Iteration 18604: with minibatch training loss = 0.631 and accuracy of 0.78\n",
      "Iteration 18605: with minibatch training loss = 0.74 and accuracy of 0.78\n",
      "Iteration 18606: with minibatch training loss = 0.551 and accuracy of 0.86\n",
      "Iteration 18607: with minibatch training loss = 0.721 and accuracy of 0.75\n",
      "Iteration 18608: with minibatch training loss = 0.76 and accuracy of 0.75\n",
      "Iteration 18609: with minibatch training loss = 0.869 and accuracy of 0.72\n",
      "Iteration 18610: with minibatch training loss = 0.577 and accuracy of 0.83\n",
      "Iteration 18611: with minibatch training loss = 0.672 and accuracy of 0.77\n",
      "Iteration 18612: with minibatch training loss = 0.676 and accuracy of 0.75\n",
      "Iteration 18613: with minibatch training loss = 0.391 and accuracy of 0.88\n",
      "Iteration 18614: with minibatch training loss = 0.579 and accuracy of 0.8\n",
      "Iteration 18615: with minibatch training loss = 0.563 and accuracy of 0.83\n",
      "Iteration 18616: with minibatch training loss = 0.522 and accuracy of 0.81\n",
      "Iteration 18617: with minibatch training loss = 0.674 and accuracy of 0.86\n",
      "Iteration 18618: with minibatch training loss = 0.601 and accuracy of 0.83\n",
      "Iteration 18619: with minibatch training loss = 0.534 and accuracy of 0.84\n",
      "Iteration 18620: with minibatch training loss = 0.663 and accuracy of 0.78\n",
      "Iteration 18621: with minibatch training loss = 0.629 and accuracy of 0.81\n",
      "Iteration 18622: with minibatch training loss = 0.563 and accuracy of 0.81\n",
      "Iteration 18623: with minibatch training loss = 0.568 and accuracy of 0.84\n",
      "Iteration 18624: with minibatch training loss = 0.608 and accuracy of 0.8\n",
      "Iteration 18625: with minibatch training loss = 0.682 and accuracy of 0.78\n",
      "Iteration 18626: with minibatch training loss = 0.628 and accuracy of 0.78\n",
      "Iteration 18627: with minibatch training loss = 0.488 and accuracy of 0.83\n",
      "Iteration 18628: with minibatch training loss = 0.686 and accuracy of 0.8\n",
      "Iteration 18629: with minibatch training loss = 0.689 and accuracy of 0.78\n",
      "Iteration 18630: with minibatch training loss = 0.622 and accuracy of 0.8\n",
      "Iteration 18631: with minibatch training loss = 0.781 and accuracy of 0.75\n",
      "Iteration 18632: with minibatch training loss = 0.725 and accuracy of 0.73\n",
      "Iteration 18633: with minibatch training loss = 0.755 and accuracy of 0.75\n",
      "Iteration 18634: with minibatch training loss = 0.688 and accuracy of 0.8\n",
      "Iteration 18635: with minibatch training loss = 0.731 and accuracy of 0.75\n",
      "Iteration 18636: with minibatch training loss = 0.517 and accuracy of 0.83\n",
      "Iteration 18637: with minibatch training loss = 0.535 and accuracy of 0.83\n",
      "Iteration 18638: with minibatch training loss = 0.673 and accuracy of 0.8\n",
      "Iteration 18639: with minibatch training loss = 0.839 and accuracy of 0.75\n",
      "Iteration 18640: with minibatch training loss = 0.592 and accuracy of 0.8\n",
      "Iteration 18641: with minibatch training loss = 0.528 and accuracy of 0.84\n",
      "Iteration 18642: with minibatch training loss = 0.561 and accuracy of 0.83\n",
      "Iteration 18643: with minibatch training loss = 0.73 and accuracy of 0.8\n",
      "Iteration 18644: with minibatch training loss = 0.641 and accuracy of 0.81\n",
      "Iteration 18645: with minibatch training loss = 0.597 and accuracy of 0.8\n",
      "Iteration 18646: with minibatch training loss = 0.547 and accuracy of 0.81\n",
      "Iteration 18647: with minibatch training loss = 0.59 and accuracy of 0.83\n",
      "Iteration 18648: with minibatch training loss = 0.479 and accuracy of 0.86\n",
      "Iteration 18649: with minibatch training loss = 0.568 and accuracy of 0.83\n",
      "Iteration 18650: with minibatch training loss = 0.665 and accuracy of 0.77\n",
      "Iteration 18651: with minibatch training loss = 0.509 and accuracy of 0.83\n",
      "Iteration 18652: with minibatch training loss = 0.501 and accuracy of 0.83\n",
      "Iteration 18653: with minibatch training loss = 0.635 and accuracy of 0.83\n",
      "Iteration 18654: with minibatch training loss = 0.582 and accuracy of 0.8\n",
      "Iteration 18655: with minibatch training loss = 0.594 and accuracy of 0.8\n",
      "Iteration 18656: with minibatch training loss = 0.507 and accuracy of 0.83\n",
      "Iteration 18657: with minibatch training loss = 0.561 and accuracy of 0.81\n",
      "Iteration 18658: with minibatch training loss = 0.742 and accuracy of 0.73\n",
      "Iteration 18659: with minibatch training loss = 0.573 and accuracy of 0.81\n",
      "Iteration 18660: with minibatch training loss = 0.48 and accuracy of 0.86\n",
      "Iteration 18661: with minibatch training loss = 0.67 and accuracy of 0.78\n",
      "Iteration 18662: with minibatch training loss = 0.654 and accuracy of 0.78\n",
      "Iteration 18663: with minibatch training loss = 0.933 and accuracy of 0.69\n",
      "Iteration 18664: with minibatch training loss = 0.609 and accuracy of 0.8\n",
      "Iteration 18665: with minibatch training loss = 0.666 and accuracy of 0.8\n",
      "Iteration 18666: with minibatch training loss = 0.759 and accuracy of 0.75\n",
      "Iteration 18667: with minibatch training loss = 0.639 and accuracy of 0.8\n",
      "Iteration 18668: with minibatch training loss = 0.612 and accuracy of 0.8\n",
      "Iteration 18669: with minibatch training loss = 0.634 and accuracy of 0.83\n",
      "Iteration 18670: with minibatch training loss = 0.536 and accuracy of 0.83\n",
      "Iteration 18671: with minibatch training loss = 0.582 and accuracy of 0.83\n",
      "Iteration 18672: with minibatch training loss = 0.72 and accuracy of 0.77\n",
      "Iteration 18673: with minibatch training loss = 0.468 and accuracy of 0.83\n",
      "Iteration 18674: with minibatch training loss = 0.562 and accuracy of 0.88\n",
      "Iteration 18675: with minibatch training loss = 0.61 and accuracy of 0.83\n",
      "Iteration 18676: with minibatch training loss = 0.612 and accuracy of 0.8\n",
      "Iteration 18677: with minibatch training loss = 0.576 and accuracy of 0.83\n",
      "Iteration 18678: with minibatch training loss = 0.737 and accuracy of 0.77\n",
      "Iteration 18679: with minibatch training loss = 0.746 and accuracy of 0.78\n",
      "Iteration 18680: with minibatch training loss = 0.718 and accuracy of 0.78\n",
      "Iteration 18681: with minibatch training loss = 0.585 and accuracy of 0.8\n",
      "Iteration 18682: with minibatch training loss = 0.697 and accuracy of 0.8\n",
      "Iteration 18683: with minibatch training loss = 0.533 and accuracy of 0.84\n",
      "Iteration 18684: with minibatch training loss = 0.764 and accuracy of 0.72\n",
      "Iteration 18685: with minibatch training loss = 0.601 and accuracy of 0.81\n",
      "Iteration 18686: with minibatch training loss = 0.492 and accuracy of 0.83\n",
      "Iteration 18687: with minibatch training loss = 0.471 and accuracy of 0.86\n",
      "Iteration 18688: with minibatch training loss = 0.486 and accuracy of 0.83\n",
      "Iteration 18689: with minibatch training loss = 0.413 and accuracy of 0.91\n",
      "Iteration 18690: with minibatch training loss = 0.408 and accuracy of 0.88\n",
      "Iteration 18691: with minibatch training loss = 0.662 and accuracy of 0.77\n",
      "Iteration 18692: with minibatch training loss = 0.766 and accuracy of 0.75\n",
      "Iteration 18693: with minibatch training loss = 0.59 and accuracy of 0.83\n",
      "Iteration 18694: with minibatch training loss = 0.773 and accuracy of 0.72\n",
      "Iteration 18695: with minibatch training loss = 0.535 and accuracy of 0.86\n",
      "Iteration 18696: with minibatch training loss = 0.786 and accuracy of 0.7\n",
      "Iteration 18697: with minibatch training loss = 0.686 and accuracy of 0.78\n",
      "Iteration 18698: with minibatch training loss = 0.641 and accuracy of 0.81\n",
      "Iteration 18699: with minibatch training loss = 0.731 and accuracy of 0.75\n",
      "Iteration 18700: with minibatch training loss = 0.925 and accuracy of 0.7\n",
      "Iteration 18701: with minibatch training loss = 0.723 and accuracy of 0.77\n",
      "Iteration 18702: with minibatch training loss = 0.643 and accuracy of 0.8\n",
      "Iteration 18703: with minibatch training loss = 0.574 and accuracy of 0.8\n",
      "Iteration 18704: with minibatch training loss = 0.79 and accuracy of 0.77\n",
      "Iteration 18705: with minibatch training loss = 0.476 and accuracy of 0.83\n",
      "Iteration 18706: with minibatch training loss = 0.536 and accuracy of 0.86\n",
      "Iteration 18707: with minibatch training loss = 0.602 and accuracy of 0.8\n",
      "Iteration 18708: with minibatch training loss = 0.441 and accuracy of 0.88\n",
      "Iteration 18709: with minibatch training loss = 0.669 and accuracy of 0.78\n",
      "Iteration 18710: with minibatch training loss = 0.651 and accuracy of 0.77\n",
      "Iteration 18711: with minibatch training loss = 0.747 and accuracy of 0.75\n",
      "Iteration 18712: with minibatch training loss = 0.639 and accuracy of 0.81\n",
      "Iteration 18713: with minibatch training loss = 0.661 and accuracy of 0.78\n",
      "Iteration 18714: with minibatch training loss = 0.584 and accuracy of 0.83\n",
      "Iteration 18715: with minibatch training loss = 0.739 and accuracy of 0.77\n",
      "Iteration 18716: with minibatch training loss = 0.565 and accuracy of 0.78\n",
      "Iteration 18717: with minibatch training loss = 0.716 and accuracy of 0.78\n",
      "Iteration 18718: with minibatch training loss = 0.683 and accuracy of 0.77\n",
      "Iteration 18719: with minibatch training loss = 0.869 and accuracy of 0.73\n",
      "Iteration 18720: with minibatch training loss = 0.533 and accuracy of 0.88\n",
      "Iteration 18721: with minibatch training loss = 0.732 and accuracy of 0.75\n",
      "Iteration 18722: with minibatch training loss = 0.499 and accuracy of 0.86\n",
      "Iteration 18723: with minibatch training loss = 0.835 and accuracy of 0.73\n",
      "Iteration 18724: with minibatch training loss = 0.603 and accuracy of 0.84\n",
      "Iteration 18725: with minibatch training loss = 0.738 and accuracy of 0.77\n",
      "Iteration 18726: with minibatch training loss = 0.645 and accuracy of 0.78\n",
      "Iteration 18727: with minibatch training loss = 0.61 and accuracy of 0.81\n",
      "Iteration 18728: with minibatch training loss = 0.542 and accuracy of 0.81\n",
      "Iteration 18729: with minibatch training loss = 0.63 and accuracy of 0.84\n",
      "Iteration 18730: with minibatch training loss = 0.726 and accuracy of 0.77\n",
      "Iteration 18731: with minibatch training loss = 0.692 and accuracy of 0.75\n",
      "Iteration 18732: with minibatch training loss = 0.54 and accuracy of 0.84\n",
      "Iteration 18733: with minibatch training loss = 0.935 and accuracy of 0.69\n",
      "Iteration 18734: with minibatch training loss = 0.741 and accuracy of 0.75\n",
      "Iteration 18735: with minibatch training loss = 0.572 and accuracy of 0.83\n",
      "Iteration 18736: with minibatch training loss = 0.589 and accuracy of 0.83\n",
      "Iteration 18737: with minibatch training loss = 0.617 and accuracy of 0.8\n",
      "Iteration 18738: with minibatch training loss = 0.568 and accuracy of 0.78\n",
      "Iteration 18739: with minibatch training loss = 0.715 and accuracy of 0.78\n",
      "Iteration 18740: with minibatch training loss = 0.604 and accuracy of 0.8\n",
      "Iteration 18741: with minibatch training loss = 0.835 and accuracy of 0.72\n",
      "Iteration 18742: with minibatch training loss = 0.737 and accuracy of 0.8\n",
      "Iteration 18743: with minibatch training loss = 0.607 and accuracy of 0.83\n",
      "Iteration 18744: with minibatch training loss = 0.552 and accuracy of 0.83\n",
      "Iteration 18745: with minibatch training loss = 0.821 and accuracy of 0.72\n",
      "Iteration 18746: with minibatch training loss = 0.701 and accuracy of 0.75\n",
      "Iteration 18747: with minibatch training loss = 0.696 and accuracy of 0.8\n",
      "Iteration 18748: with minibatch training loss = 0.854 and accuracy of 0.7\n",
      "Iteration 18749: with minibatch training loss = 0.631 and accuracy of 0.78\n",
      "Iteration 18750: with minibatch training loss = 0.406 and accuracy of 0.86\n",
      "Iteration 18751: with minibatch training loss = 0.538 and accuracy of 0.83\n",
      "Iteration 18752: with minibatch training loss = 0.602 and accuracy of 0.81\n",
      "Iteration 18753: with minibatch training loss = 0.773 and accuracy of 0.75\n",
      "Iteration 18754: with minibatch training loss = 0.467 and accuracy of 0.88\n",
      "Iteration 18755: with minibatch training loss = 0.68 and accuracy of 0.77\n",
      "Iteration 18756: with minibatch training loss = 0.602 and accuracy of 0.8\n",
      "Iteration 18757: with minibatch training loss = 0.561 and accuracy of 0.8\n",
      "Iteration 18758: with minibatch training loss = 0.456 and accuracy of 0.86\n",
      "Iteration 18759: with minibatch training loss = 0.469 and accuracy of 0.83\n",
      "Iteration 18760: with minibatch training loss = 0.767 and accuracy of 0.75\n",
      "Iteration 18761: with minibatch training loss = 0.764 and accuracy of 0.77\n",
      "Iteration 18762: with minibatch training loss = 0.63 and accuracy of 0.81\n",
      "Iteration 18763: with minibatch training loss = 0.735 and accuracy of 0.78\n",
      "Iteration 18764: with minibatch training loss = 0.667 and accuracy of 0.84\n",
      "Iteration 18765: with minibatch training loss = 0.618 and accuracy of 0.78\n",
      "Iteration 18766: with minibatch training loss = 0.539 and accuracy of 0.8\n",
      "Iteration 18767: with minibatch training loss = 0.689 and accuracy of 0.75\n",
      "Iteration 18768: with minibatch training loss = 0.509 and accuracy of 0.83\n",
      "Iteration 18769: with minibatch training loss = 0.698 and accuracy of 0.8\n",
      "Iteration 18770: with minibatch training loss = 0.56 and accuracy of 0.81\n",
      "Iteration 18771: with minibatch training loss = 0.551 and accuracy of 0.81\n",
      "Iteration 18772: with minibatch training loss = 0.736 and accuracy of 0.73\n",
      "Iteration 18773: with minibatch training loss = 0.691 and accuracy of 0.75\n",
      "Iteration 18774: with minibatch training loss = 0.681 and accuracy of 0.75\n",
      "Iteration 18775: with minibatch training loss = 0.613 and accuracy of 0.8\n",
      "Iteration 18776: with minibatch training loss = 0.677 and accuracy of 0.8\n",
      "Iteration 18777: with minibatch training loss = 0.531 and accuracy of 0.84\n",
      "Iteration 18778: with minibatch training loss = 0.439 and accuracy of 0.89\n",
      "Iteration 18779: with minibatch training loss = 0.914 and accuracy of 0.7\n",
      "Iteration 18780: with minibatch training loss = 0.68 and accuracy of 0.81\n",
      "Iteration 18781: with minibatch training loss = 0.688 and accuracy of 0.77\n",
      "Iteration 18782: with minibatch training loss = 1 and accuracy of 0.66\n",
      "Iteration 18783: with minibatch training loss = 0.573 and accuracy of 0.84\n",
      "Iteration 18784: with minibatch training loss = 0.605 and accuracy of 0.81\n",
      "Iteration 18785: with minibatch training loss = 0.67 and accuracy of 0.8\n",
      "Iteration 18786: with minibatch training loss = 0.594 and accuracy of 0.83\n",
      "Iteration 18787: with minibatch training loss = 0.731 and accuracy of 0.75\n",
      "Iteration 18788: with minibatch training loss = 0.727 and accuracy of 0.75\n",
      "Iteration 18789: with minibatch training loss = 0.9 and accuracy of 0.72\n",
      "Iteration 18790: with minibatch training loss = 0.571 and accuracy of 0.83\n",
      "Iteration 18791: with minibatch training loss = 0.742 and accuracy of 0.75\n",
      "Iteration 18792: with minibatch training loss = 0.634 and accuracy of 0.8\n",
      "Iteration 18793: with minibatch training loss = 0.553 and accuracy of 0.83\n",
      "Iteration 18794: with minibatch training loss = 0.545 and accuracy of 0.81\n",
      "Iteration 18795: with minibatch training loss = 0.366 and accuracy of 0.89\n",
      "Iteration 18796: with minibatch training loss = 0.624 and accuracy of 0.8\n",
      "Iteration 18797: with minibatch training loss = 0.484 and accuracy of 0.86\n",
      "Iteration 18798: with minibatch training loss = 0.828 and accuracy of 0.72\n",
      "Iteration 18799: with minibatch training loss = 0.54 and accuracy of 0.84\n",
      "Iteration 18800: with minibatch training loss = 0.561 and accuracy of 0.81\n",
      "Iteration 18801: with minibatch training loss = 0.42 and accuracy of 0.86\n",
      "Iteration 18802: with minibatch training loss = 0.728 and accuracy of 0.78\n",
      "Iteration 18803: with minibatch training loss = 0.844 and accuracy of 0.72\n",
      "Iteration 18804: with minibatch training loss = 0.751 and accuracy of 0.77\n",
      "Iteration 18805: with minibatch training loss = 0.594 and accuracy of 0.8\n",
      "Iteration 18806: with minibatch training loss = 0.466 and accuracy of 0.86\n",
      "Iteration 18807: with minibatch training loss = 0.594 and accuracy of 0.78\n",
      "Iteration 18808: with minibatch training loss = 0.772 and accuracy of 0.77\n",
      "Iteration 18809: with minibatch training loss = 0.871 and accuracy of 0.73\n",
      "Iteration 18810: with minibatch training loss = 0.729 and accuracy of 0.77\n",
      "Iteration 18811: with minibatch training loss = 0.622 and accuracy of 0.81\n",
      "Iteration 18812: with minibatch training loss = 0.861 and accuracy of 0.72\n",
      "Iteration 18813: with minibatch training loss = 0.483 and accuracy of 0.84\n",
      "Iteration 18814: with minibatch training loss = 0.625 and accuracy of 0.8\n",
      "Iteration 18815: with minibatch training loss = 0.391 and accuracy of 0.88\n",
      "Iteration 18816: with minibatch training loss = 0.433 and accuracy of 0.86\n",
      "Iteration 18817: with minibatch training loss = 0.567 and accuracy of 0.83\n",
      "Iteration 18818: with minibatch training loss = 0.617 and accuracy of 0.8\n",
      "Iteration 18819: with minibatch training loss = 0.398 and accuracy of 0.88\n",
      "Iteration 18820: with minibatch training loss = 0.8 and accuracy of 0.73\n",
      "Iteration 18821: with minibatch training loss = 0.632 and accuracy of 0.8\n",
      "Iteration 18822: with minibatch training loss = 0.667 and accuracy of 0.83\n",
      "Iteration 18823: with minibatch training loss = 0.558 and accuracy of 0.83\n",
      "Iteration 18824: with minibatch training loss = 0.723 and accuracy of 0.77\n",
      "Iteration 18825: with minibatch training loss = 0.523 and accuracy of 0.84\n",
      "Iteration 18826: with minibatch training loss = 0.614 and accuracy of 0.81\n",
      "Iteration 18827: with minibatch training loss = 0.54 and accuracy of 0.81\n",
      "Iteration 18828: with minibatch training loss = 0.557 and accuracy of 0.81\n",
      "Iteration 18829: with minibatch training loss = 0.696 and accuracy of 0.81\n",
      "Iteration 18830: with minibatch training loss = 0.709 and accuracy of 0.75\n",
      "Iteration 18831: with minibatch training loss = 0.658 and accuracy of 0.83\n",
      "Iteration 18832: with minibatch training loss = 0.547 and accuracy of 0.8\n",
      "Iteration 18833: with minibatch training loss = 0.742 and accuracy of 0.75\n",
      "Iteration 18834: with minibatch training loss = 0.461 and accuracy of 0.88\n",
      "Iteration 18835: with minibatch training loss = 0.518 and accuracy of 0.84\n",
      "Iteration 18836: with minibatch training loss = 0.814 and accuracy of 0.73\n",
      "Iteration 18837: with minibatch training loss = 0.407 and accuracy of 0.86\n",
      "Iteration 18838: with minibatch training loss = 0.669 and accuracy of 0.77\n",
      "Iteration 18839: with minibatch training loss = 0.621 and accuracy of 0.8\n",
      "Iteration 18840: with minibatch training loss = 0.601 and accuracy of 0.81\n",
      "Iteration 18841: with minibatch training loss = 0.763 and accuracy of 0.75\n",
      "Iteration 18842: with minibatch training loss = 0.778 and accuracy of 0.77\n",
      "Iteration 18843: with minibatch training loss = 0.81 and accuracy of 0.73\n",
      "Iteration 18844: with minibatch training loss = 0.552 and accuracy of 0.8\n",
      "Iteration 18845: with minibatch training loss = 0.651 and accuracy of 0.78\n",
      "Iteration 18846: with minibatch training loss = 0.445 and accuracy of 0.84\n",
      "Iteration 18847: with minibatch training loss = 0.649 and accuracy of 0.78\n",
      "Iteration 18848: with minibatch training loss = 0.72 and accuracy of 0.78\n",
      "Iteration 18849: with minibatch training loss = 0.996 and accuracy of 0.7\n",
      "Iteration 18850: with minibatch training loss = 0.851 and accuracy of 0.72\n",
      "Iteration 18851: with minibatch training loss = 0.521 and accuracy of 0.8\n",
      "Iteration 18852: with minibatch training loss = 0.496 and accuracy of 0.83\n",
      "Iteration 18853: with minibatch training loss = 0.694 and accuracy of 0.77\n",
      "Iteration 18854: with minibatch training loss = 0.661 and accuracy of 0.78\n",
      "Iteration 18855: with minibatch training loss = 0.732 and accuracy of 0.77\n",
      "Iteration 18856: with minibatch training loss = 0.674 and accuracy of 0.78\n",
      "Iteration 18857: with minibatch training loss = 0.534 and accuracy of 0.84\n",
      "Iteration 18858: with minibatch training loss = 0.761 and accuracy of 0.75\n",
      "Iteration 18859: with minibatch training loss = 0.433 and accuracy of 0.84\n",
      "Iteration 18860: with minibatch training loss = 0.65 and accuracy of 0.8\n",
      "Iteration 18861: with minibatch training loss = 1.07 and accuracy of 0.62\n",
      "Iteration 18862: with minibatch training loss = 0.613 and accuracy of 0.8\n",
      "Iteration 18863: with minibatch training loss = 0.584 and accuracy of 0.8\n",
      "Iteration 18864: with minibatch training loss = 0.7 and accuracy of 0.78\n",
      "Iteration 18865: with minibatch training loss = 0.586 and accuracy of 0.8\n",
      "Iteration 18866: with minibatch training loss = 0.386 and accuracy of 0.88\n",
      "Iteration 18867: with minibatch training loss = 0.569 and accuracy of 0.81\n",
      "Iteration 18868: with minibatch training loss = 0.436 and accuracy of 0.86\n",
      "Iteration 18869: with minibatch training loss = 0.399 and accuracy of 0.88\n",
      "Iteration 18870: with minibatch training loss = 0.353 and accuracy of 0.88\n",
      "Iteration 18871: with minibatch training loss = 0.602 and accuracy of 0.8\n",
      "Iteration 18872: with minibatch training loss = 0.505 and accuracy of 0.83\n",
      "Iteration 18873: with minibatch training loss = 0.695 and accuracy of 0.78\n",
      "Iteration 18874: with minibatch training loss = 0.497 and accuracy of 0.86\n",
      "Iteration 18875: with minibatch training loss = 0.652 and accuracy of 0.77\n",
      "Iteration 18876: with minibatch training loss = 0.727 and accuracy of 0.77\n",
      "Iteration 18877: with minibatch training loss = 0.859 and accuracy of 0.73\n",
      "Iteration 18878: with minibatch training loss = 0.803 and accuracy of 0.77\n",
      "Iteration 18879: with minibatch training loss = 0.402 and accuracy of 0.88\n",
      "Iteration 18880: with minibatch training loss = 0.698 and accuracy of 0.75\n",
      "Iteration 18881: with minibatch training loss = 0.701 and accuracy of 0.77\n",
      "Iteration 18882: with minibatch training loss = 0.37 and accuracy of 0.88\n",
      "Iteration 18883: with minibatch training loss = 0.555 and accuracy of 0.83\n",
      "Iteration 18884: with minibatch training loss = 0.752 and accuracy of 0.73\n",
      "Iteration 18885: with minibatch training loss = 0.961 and accuracy of 0.69\n",
      "Iteration 18886: with minibatch training loss = 0.657 and accuracy of 0.77\n",
      "Iteration 18887: with minibatch training loss = 0.479 and accuracy of 0.86\n",
      "Iteration 18888: with minibatch training loss = 0.507 and accuracy of 0.83\n",
      "Iteration 18889: with minibatch training loss = 0.763 and accuracy of 0.77\n",
      "Iteration 18890: with minibatch training loss = 0.58 and accuracy of 0.78\n",
      "Iteration 18891: with minibatch training loss = 0.631 and accuracy of 0.83\n",
      "Iteration 18892: with minibatch training loss = 0.628 and accuracy of 0.8\n",
      "Iteration 18893: with minibatch training loss = 0.621 and accuracy of 0.78\n",
      "Iteration 18894: with minibatch training loss = 0.367 and accuracy of 0.89\n",
      "Iteration 18895: with minibatch training loss = 0.559 and accuracy of 0.81\n",
      "Iteration 18896: with minibatch training loss = 0.414 and accuracy of 0.88\n",
      "Iteration 18897: with minibatch training loss = 0.559 and accuracy of 0.81\n",
      "Iteration 18898: with minibatch training loss = 0.549 and accuracy of 0.81\n",
      "Iteration 18899: with minibatch training loss = 0.619 and accuracy of 0.81\n",
      "Iteration 18900: with minibatch training loss = 0.401 and accuracy of 0.86\n",
      "Iteration 18901: with minibatch training loss = 0.388 and accuracy of 0.89\n",
      "Iteration 18902: with minibatch training loss = 0.72 and accuracy of 0.73\n",
      "Iteration 18903: with minibatch training loss = 0.33 and accuracy of 0.91\n",
      "Iteration 18904: with minibatch training loss = 0.595 and accuracy of 0.78\n",
      "Iteration 18905: with minibatch training loss = 0.537 and accuracy of 0.86\n",
      "Iteration 18906: with minibatch training loss = 0.79 and accuracy of 0.73\n",
      "Iteration 18907: with minibatch training loss = 0.693 and accuracy of 0.8\n",
      "Iteration 18908: with minibatch training loss = 0.735 and accuracy of 0.75\n",
      "Iteration 18909: with minibatch training loss = 0.505 and accuracy of 0.83\n",
      "Iteration 18910: with minibatch training loss = 0.569 and accuracy of 0.83\n",
      "Iteration 18911: with minibatch training loss = 0.704 and accuracy of 0.8\n",
      "Iteration 18912: with minibatch training loss = 0.523 and accuracy of 0.83\n",
      "Iteration 18913: with minibatch training loss = 0.73 and accuracy of 0.73\n",
      "Iteration 18914: with minibatch training loss = 0.495 and accuracy of 0.81\n",
      "Iteration 18915: with minibatch training loss = 0.492 and accuracy of 0.83\n",
      "Iteration 18916: with minibatch training loss = 0.672 and accuracy of 0.77\n",
      "Iteration 18917: with minibatch training loss = 0.789 and accuracy of 0.77\n",
      "Iteration 18918: with minibatch training loss = 0.334 and accuracy of 0.89\n",
      "Iteration 18919: with minibatch training loss = 0.364 and accuracy of 0.88\n",
      "Iteration 18920: with minibatch training loss = 0.759 and accuracy of 0.73\n",
      "Iteration 18921: with minibatch training loss = 0.596 and accuracy of 0.81\n",
      "Iteration 18922: with minibatch training loss = 0.628 and accuracy of 0.83\n",
      "Iteration 18923: with minibatch training loss = 0.634 and accuracy of 0.8\n",
      "Iteration 18924: with minibatch training loss = 0.768 and accuracy of 0.73\n",
      "Iteration 18925: with minibatch training loss = 0.594 and accuracy of 0.83\n",
      "Iteration 18926: with minibatch training loss = 0.436 and accuracy of 0.91\n",
      "Iteration 18927: with minibatch training loss = 0.599 and accuracy of 0.8\n",
      "Iteration 18928: with minibatch training loss = 0.416 and accuracy of 0.89\n",
      "Iteration 18929: with minibatch training loss = 0.552 and accuracy of 0.84\n",
      "Iteration 18930: with minibatch training loss = 0.557 and accuracy of 0.81\n",
      "Iteration 18931: with minibatch training loss = 0.589 and accuracy of 0.81\n",
      "Iteration 18932: with minibatch training loss = 0.753 and accuracy of 0.72\n",
      "Iteration 18933: with minibatch training loss = 0.749 and accuracy of 0.77\n",
      "Iteration 18934: with minibatch training loss = 0.716 and accuracy of 0.77\n",
      "Iteration 18935: with minibatch training loss = 0.591 and accuracy of 0.8\n",
      "Iteration 18936: with minibatch training loss = 0.742 and accuracy of 0.78\n",
      "Iteration 18937: with minibatch training loss = 0.555 and accuracy of 0.83\n",
      "Iteration 18938: with minibatch training loss = 0.72 and accuracy of 0.78\n",
      "Iteration 18939: with minibatch training loss = 0.658 and accuracy of 0.8\n",
      "Iteration 18940: with minibatch training loss = 0.464 and accuracy of 0.88\n",
      "Iteration 18941: with minibatch training loss = 0.331 and accuracy of 0.91\n",
      "Iteration 18942: with minibatch training loss = 0.69 and accuracy of 0.77\n",
      "Iteration 18943: with minibatch training loss = 0.551 and accuracy of 0.84\n",
      "Iteration 18944: with minibatch training loss = 0.38 and accuracy of 0.89\n",
      "Iteration 18945: with minibatch training loss = 0.816 and accuracy of 0.72\n",
      "Iteration 18946: with minibatch training loss = 0.452 and accuracy of 0.83\n",
      "Iteration 18947: with minibatch training loss = 0.524 and accuracy of 0.81\n",
      "Iteration 18948: with minibatch training loss = 0.705 and accuracy of 0.78\n",
      "Iteration 18949: with minibatch training loss = 0.557 and accuracy of 0.81\n",
      "Iteration 18950: with minibatch training loss = 0.621 and accuracy of 0.8\n",
      "Iteration 18951: with minibatch training loss = 0.578 and accuracy of 0.78\n",
      "Iteration 18952: with minibatch training loss = 0.563 and accuracy of 0.83\n",
      "Iteration 18953: with minibatch training loss = 0.6 and accuracy of 0.81\n",
      "Iteration 18954: with minibatch training loss = 0.75 and accuracy of 0.73\n",
      "Iteration 18955: with minibatch training loss = 0.714 and accuracy of 0.77\n",
      "Iteration 18956: with minibatch training loss = 0.722 and accuracy of 0.77\n",
      "Iteration 18957: with minibatch training loss = 0.833 and accuracy of 0.72\n",
      "Iteration 18958: with minibatch training loss = 0.533 and accuracy of 0.83\n",
      "Iteration 18959: with minibatch training loss = 0.667 and accuracy of 0.77\n",
      "Iteration 18960: with minibatch training loss = 0.432 and accuracy of 0.84\n",
      "Iteration 18961: with minibatch training loss = 0.58 and accuracy of 0.8\n",
      "Iteration 18962: with minibatch training loss = 0.26 and accuracy of 0.91\n",
      "Iteration 18963: with minibatch training loss = 0.728 and accuracy of 0.77\n",
      "Iteration 18964: with minibatch training loss = 0.496 and accuracy of 0.84\n",
      "Iteration 18965: with minibatch training loss = 0.616 and accuracy of 0.81\n",
      "Iteration 18966: with minibatch training loss = 0.753 and accuracy of 0.78\n",
      "Iteration 18967: with minibatch training loss = 0.671 and accuracy of 0.8\n",
      "Iteration 18968: with minibatch training loss = 0.753 and accuracy of 0.81\n",
      "Iteration 18969: with minibatch training loss = 0.66 and accuracy of 0.78\n",
      "Iteration 18970: with minibatch training loss = 0.54 and accuracy of 0.84\n",
      "Iteration 18971: with minibatch training loss = 0.711 and accuracy of 0.78\n",
      "Iteration 18972: with minibatch training loss = 0.707 and accuracy of 0.78\n",
      "Iteration 18973: with minibatch training loss = 0.719 and accuracy of 0.75\n",
      "Iteration 18974: with minibatch training loss = 0.469 and accuracy of 0.84\n",
      "Iteration 18975: with minibatch training loss = 0.501 and accuracy of 0.83\n",
      "Iteration 18976: with minibatch training loss = 0.634 and accuracy of 0.77\n",
      "Iteration 18977: with minibatch training loss = 0.257 and accuracy of 0.92\n",
      "Iteration 18978: with minibatch training loss = 0.538 and accuracy of 0.81\n",
      "Iteration 18979: with minibatch training loss = 0.602 and accuracy of 0.81\n",
      "Iteration 18980: with minibatch training loss = 0.717 and accuracy of 0.77\n",
      "Iteration 18981: with minibatch training loss = 0.655 and accuracy of 0.77\n",
      "Iteration 18982: with minibatch training loss = 0.42 and accuracy of 0.88\n",
      "Iteration 18983: with minibatch training loss = 0.752 and accuracy of 0.73\n",
      "Iteration 18984: with minibatch training loss = 0.436 and accuracy of 0.88\n",
      "Iteration 18985: with minibatch training loss = 0.6 and accuracy of 0.81\n",
      "Iteration 18986: with minibatch training loss = 0.459 and accuracy of 0.84\n",
      "Iteration 18987: with minibatch training loss = 0.45 and accuracy of 0.86\n",
      "Iteration 18988: with minibatch training loss = 0.637 and accuracy of 0.8\n",
      "Iteration 18989: with minibatch training loss = 0.648 and accuracy of 0.84\n",
      "Iteration 18990: with minibatch training loss = 0.833 and accuracy of 0.75\n",
      "Iteration 18991: with minibatch training loss = 0.666 and accuracy of 0.78\n",
      "Iteration 18992: with minibatch training loss = 0.495 and accuracy of 0.84\n",
      "Iteration 18993: with minibatch training loss = 0.514 and accuracy of 0.83\n",
      "Iteration 18994: with minibatch training loss = 0.984 and accuracy of 0.7\n",
      "Iteration 18995: with minibatch training loss = 0.582 and accuracy of 0.83\n",
      "Iteration 18996: with minibatch training loss = 0.554 and accuracy of 0.81\n",
      "Iteration 18997: with minibatch training loss = 0.557 and accuracy of 0.84\n",
      "Iteration 18998: with minibatch training loss = 0.59 and accuracy of 0.81\n",
      "Iteration 18999: with minibatch training loss = 0.731 and accuracy of 0.77\n",
      "Iteration 19000: with minibatch training loss = 0.631 and accuracy of 0.78\n",
      "Iteration 19001: with minibatch training loss = 0.605 and accuracy of 0.78\n",
      "Iteration 19002: with minibatch training loss = 0.435 and accuracy of 0.86\n",
      "Iteration 19003: with minibatch training loss = 0.715 and accuracy of 0.8\n",
      "Iteration 19004: with minibatch training loss = 0.486 and accuracy of 0.88\n",
      "Iteration 19005: with minibatch training loss = 0.56 and accuracy of 0.84\n",
      "Iteration 19006: with minibatch training loss = 0.931 and accuracy of 0.72\n",
      "Iteration 19007: with minibatch training loss = 0.902 and accuracy of 0.77\n",
      "Iteration 19008: with minibatch training loss = 0.726 and accuracy of 0.77\n",
      "Iteration 19009: with minibatch training loss = 0.748 and accuracy of 0.8\n",
      "Iteration 19010: with minibatch training loss = 0.481 and accuracy of 0.84\n",
      "Iteration 19011: with minibatch training loss = 0.515 and accuracy of 0.84\n",
      "Iteration 19012: with minibatch training loss = 0.602 and accuracy of 0.81\n",
      "Iteration 19013: with minibatch training loss = 0.652 and accuracy of 0.77\n",
      "Iteration 19014: with minibatch training loss = 0.822 and accuracy of 0.73\n",
      "Iteration 19015: with minibatch training loss = 0.469 and accuracy of 0.86\n",
      "Iteration 19016: with minibatch training loss = 0.634 and accuracy of 0.81\n",
      "Iteration 19017: with minibatch training loss = 0.491 and accuracy of 0.83\n",
      "Iteration 19018: with minibatch training loss = 0.562 and accuracy of 0.83\n",
      "Iteration 19019: with minibatch training loss = 0.716 and accuracy of 0.77\n",
      "Iteration 19020: with minibatch training loss = 0.834 and accuracy of 0.72\n",
      "Iteration 19021: with minibatch training loss = 0.528 and accuracy of 0.83\n",
      "Iteration 19022: with minibatch training loss = 0.383 and accuracy of 0.89\n",
      "Iteration 19023: with minibatch training loss = 0.496 and accuracy of 0.86\n",
      "Iteration 19024: with minibatch training loss = 0.615 and accuracy of 0.81\n",
      "Iteration 19025: with minibatch training loss = 0.696 and accuracy of 0.8\n",
      "Iteration 19026: with minibatch training loss = 0.687 and accuracy of 0.8\n",
      "Iteration 19027: with minibatch training loss = 0.61 and accuracy of 0.8\n",
      "Iteration 19028: with minibatch training loss = 0.723 and accuracy of 0.78\n",
      "Iteration 19029: with minibatch training loss = 0.673 and accuracy of 0.8\n",
      "Iteration 19030: with minibatch training loss = 0.49 and accuracy of 0.88\n",
      "Iteration 19031: with minibatch training loss = 0.913 and accuracy of 0.69\n",
      "Iteration 19032: with minibatch training loss = 0.734 and accuracy of 0.78\n",
      "Iteration 19033: with minibatch training loss = 0.756 and accuracy of 0.77\n",
      "Iteration 19034: with minibatch training loss = 0.453 and accuracy of 0.84\n",
      "Iteration 19035: with minibatch training loss = 0.498 and accuracy of 0.84\n",
      "Iteration 19036: with minibatch training loss = 0.422 and accuracy of 0.88\n",
      "Iteration 19037: with minibatch training loss = 0.706 and accuracy of 0.75\n",
      "Iteration 19038: with minibatch training loss = 0.577 and accuracy of 0.83\n",
      "Iteration 19039: with minibatch training loss = 0.752 and accuracy of 0.72\n",
      "Iteration 19040: with minibatch training loss = 0.598 and accuracy of 0.81\n",
      "Iteration 19041: with minibatch training loss = 0.39 and accuracy of 0.91\n",
      "Iteration 19042: with minibatch training loss = 0.667 and accuracy of 0.8\n",
      "Iteration 19043: with minibatch training loss = 0.69 and accuracy of 0.8\n",
      "Iteration 19044: with minibatch training loss = 0.595 and accuracy of 0.84\n",
      "Iteration 19045: with minibatch training loss = 0.446 and accuracy of 0.88\n",
      "Iteration 19046: with minibatch training loss = 0.767 and accuracy of 0.72\n",
      "Iteration 19047: with minibatch training loss = 0.664 and accuracy of 0.81\n",
      "Iteration 19048: with minibatch training loss = 0.528 and accuracy of 0.84\n",
      "Iteration 19049: with minibatch training loss = 0.633 and accuracy of 0.78\n",
      "Iteration 19050: with minibatch training loss = 0.367 and accuracy of 0.86\n",
      "Iteration 19051: with minibatch training loss = 0.675 and accuracy of 0.78\n",
      "Iteration 19052: with minibatch training loss = 0.608 and accuracy of 0.8\n",
      "Iteration 19053: with minibatch training loss = 0.558 and accuracy of 0.83\n",
      "Iteration 19054: with minibatch training loss = 0.54 and accuracy of 0.81\n",
      "Iteration 19055: with minibatch training loss = 0.458 and accuracy of 0.84\n",
      "Iteration 19056: with minibatch training loss = 0.464 and accuracy of 0.88\n",
      "Iteration 19057: with minibatch training loss = 0.614 and accuracy of 0.81\n",
      "Iteration 19058: with minibatch training loss = 0.778 and accuracy of 0.72\n",
      "Iteration 19059: with minibatch training loss = 0.646 and accuracy of 0.78\n",
      "Iteration 19060: with minibatch training loss = 0.668 and accuracy of 0.77\n",
      "Iteration 19061: with minibatch training loss = 0.606 and accuracy of 0.81\n",
      "Iteration 19062: with minibatch training loss = 0.514 and accuracy of 0.81\n",
      "Iteration 19063: with minibatch training loss = 0.768 and accuracy of 0.72\n",
      "Iteration 19064: with minibatch training loss = 0.501 and accuracy of 0.83\n",
      "Iteration 19065: with minibatch training loss = 0.869 and accuracy of 0.72\n",
      "Iteration 19066: with minibatch training loss = 0.931 and accuracy of 0.72\n",
      "Iteration 19067: with minibatch training loss = 0.669 and accuracy of 0.81\n",
      "Iteration 19068: with minibatch training loss = 0.562 and accuracy of 0.84\n",
      "Iteration 19069: with minibatch training loss = 0.356 and accuracy of 0.89\n",
      "Iteration 19070: with minibatch training loss = 0.648 and accuracy of 0.8\n",
      "Iteration 19071: with minibatch training loss = 0.724 and accuracy of 0.75\n",
      "Iteration 19072: with minibatch training loss = 0.557 and accuracy of 0.84\n",
      "Iteration 19073: with minibatch training loss = 0.423 and accuracy of 0.88\n",
      "Iteration 19074: with minibatch training loss = 0.773 and accuracy of 0.77\n",
      "Iteration 19075: with minibatch training loss = 0.723 and accuracy of 0.78\n",
      "Iteration 19076: with minibatch training loss = 0.447 and accuracy of 0.86\n",
      "Iteration 19077: with minibatch training loss = 0.649 and accuracy of 0.81\n",
      "Iteration 19078: with minibatch training loss = 0.535 and accuracy of 0.8\n",
      "Iteration 19079: with minibatch training loss = 0.49 and accuracy of 0.83\n",
      "Iteration 19080: with minibatch training loss = 0.707 and accuracy of 0.78\n",
      "Iteration 19081: with minibatch training loss = 0.584 and accuracy of 0.83\n",
      "Iteration 19082: with minibatch training loss = 0.977 and accuracy of 0.67\n",
      "Iteration 19083: with minibatch training loss = 0.678 and accuracy of 0.78\n",
      "Iteration 19084: with minibatch training loss = 0.426 and accuracy of 0.88\n",
      "Iteration 19085: with minibatch training loss = 0.6 and accuracy of 0.84\n",
      "Iteration 19086: with minibatch training loss = 0.637 and accuracy of 0.78\n",
      "Iteration 19087: with minibatch training loss = 0.628 and accuracy of 0.78\n",
      "Iteration 19088: with minibatch training loss = 0.516 and accuracy of 0.81\n",
      "Iteration 19089: with minibatch training loss = 0.665 and accuracy of 0.75\n",
      "Iteration 19090: with minibatch training loss = 0.502 and accuracy of 0.86\n",
      "Iteration 19091: with minibatch training loss = 0.461 and accuracy of 0.83\n",
      "Iteration 19092: with minibatch training loss = 0.689 and accuracy of 0.78\n",
      "Iteration 19093: with minibatch training loss = 0.557 and accuracy of 0.84\n",
      "Iteration 19094: with minibatch training loss = 0.781 and accuracy of 0.78\n",
      "Iteration 19095: with minibatch training loss = 0.861 and accuracy of 0.77\n",
      "Iteration 19096: with minibatch training loss = 0.436 and accuracy of 0.86\n",
      "Iteration 19097: with minibatch training loss = 0.826 and accuracy of 0.73\n",
      "Iteration 19098: with minibatch training loss = 0.531 and accuracy of 0.83\n",
      "Iteration 19099: with minibatch training loss = 0.745 and accuracy of 0.78\n",
      "Iteration 19100: with minibatch training loss = 0.505 and accuracy of 0.83\n",
      "Iteration 19101: with minibatch training loss = 0.678 and accuracy of 0.77\n",
      "Iteration 19102: with minibatch training loss = 0.596 and accuracy of 0.8\n",
      "Iteration 19103: with minibatch training loss = 0.734 and accuracy of 0.77\n",
      "Iteration 19104: with minibatch training loss = 0.855 and accuracy of 0.69\n",
      "Iteration 19105: with minibatch training loss = 0.603 and accuracy of 0.8\n",
      "Iteration 19106: with minibatch training loss = 0.544 and accuracy of 0.84\n",
      "Iteration 19107: with minibatch training loss = 0.653 and accuracy of 0.78\n",
      "Iteration 19108: with minibatch training loss = 0.592 and accuracy of 0.83\n",
      "Iteration 19109: with minibatch training loss = 0.924 and accuracy of 0.7\n",
      "Iteration 19110: with minibatch training loss = 0.779 and accuracy of 0.77\n",
      "Iteration 19111: with minibatch training loss = 0.473 and accuracy of 0.88\n",
      "Iteration 19112: with minibatch training loss = 0.691 and accuracy of 0.78\n",
      "Iteration 19113: with minibatch training loss = 0.6 and accuracy of 0.8\n",
      "Iteration 19114: with minibatch training loss = 0.485 and accuracy of 0.83\n",
      "Iteration 19115: with minibatch training loss = 0.619 and accuracy of 0.83\n",
      "Iteration 19116: with minibatch training loss = 0.51 and accuracy of 0.83\n",
      "Iteration 19117: with minibatch training loss = 0.613 and accuracy of 0.81\n",
      "Iteration 19118: with minibatch training loss = 0.631 and accuracy of 0.83\n",
      "Iteration 19119: with minibatch training loss = 0.604 and accuracy of 0.8\n",
      "Iteration 19120: with minibatch training loss = 0.613 and accuracy of 0.83\n",
      "Iteration 19121: with minibatch training loss = 0.536 and accuracy of 0.8\n",
      "Iteration 19122: with minibatch training loss = 0.717 and accuracy of 0.78\n",
      "Iteration 19123: with minibatch training loss = 0.587 and accuracy of 0.83\n",
      "Iteration 19124: with minibatch training loss = 0.599 and accuracy of 0.78\n",
      "Iteration 19125: with minibatch training loss = 0.706 and accuracy of 0.78\n",
      "Iteration 19126: with minibatch training loss = 0.746 and accuracy of 0.75\n",
      "Iteration 19127: with minibatch training loss = 0.483 and accuracy of 0.84\n",
      "Iteration 19128: with minibatch training loss = 0.875 and accuracy of 0.73\n",
      "Iteration 19129: with minibatch training loss = 0.66 and accuracy of 0.78\n",
      "Iteration 19130: with minibatch training loss = 0.463 and accuracy of 0.86\n",
      "Iteration 19131: with minibatch training loss = 0.676 and accuracy of 0.83\n",
      "Iteration 19132: with minibatch training loss = 0.335 and accuracy of 0.91\n",
      "Iteration 19133: with minibatch training loss = 0.618 and accuracy of 0.83\n",
      "Iteration 19134: with minibatch training loss = 0.485 and accuracy of 0.84\n",
      "Iteration 19135: with minibatch training loss = 0.513 and accuracy of 0.83\n",
      "Iteration 19136: with minibatch training loss = 0.677 and accuracy of 0.8\n",
      "Iteration 19137: with minibatch training loss = 0.724 and accuracy of 0.75\n",
      "Iteration 19138: with minibatch training loss = 0.683 and accuracy of 0.77\n",
      "Iteration 19139: with minibatch training loss = 0.696 and accuracy of 0.81\n",
      "Iteration 19140: with minibatch training loss = 0.708 and accuracy of 0.73\n",
      "Iteration 19141: with minibatch training loss = 0.583 and accuracy of 0.86\n",
      "Iteration 19142: with minibatch training loss = 0.581 and accuracy of 0.78\n",
      "Iteration 19143: with minibatch training loss = 0.611 and accuracy of 0.81\n",
      "Iteration 19144: with minibatch training loss = 0.477 and accuracy of 0.86\n",
      "Iteration 19145: with minibatch training loss = 0.723 and accuracy of 0.77\n",
      "Iteration 19146: with minibatch training loss = 0.977 and accuracy of 0.69\n",
      "Iteration 19147: with minibatch training loss = 0.61 and accuracy of 0.77\n",
      "Iteration 19148: with minibatch training loss = 0.617 and accuracy of 0.81\n",
      "Iteration 19149: with minibatch training loss = 0.48 and accuracy of 0.83\n",
      "Iteration 19150: with minibatch training loss = 0.785 and accuracy of 0.7\n",
      "Iteration 19151: with minibatch training loss = 0.433 and accuracy of 0.86\n",
      "Iteration 19152: with minibatch training loss = 0.621 and accuracy of 0.81\n",
      "Iteration 19153: with minibatch training loss = 0.62 and accuracy of 0.78\n",
      "Iteration 19154: with minibatch training loss = 0.656 and accuracy of 0.8\n",
      "Iteration 19155: with minibatch training loss = 0.748 and accuracy of 0.78\n",
      "Iteration 19156: with minibatch training loss = 0.667 and accuracy of 0.78\n",
      "Iteration 19157: with minibatch training loss = 0.574 and accuracy of 0.81\n",
      "Iteration 19158: with minibatch training loss = 0.612 and accuracy of 0.8\n",
      "Iteration 19159: with minibatch training loss = 0.705 and accuracy of 0.77\n",
      "Iteration 19160: with minibatch training loss = 0.65 and accuracy of 0.78\n",
      "Iteration 19161: with minibatch training loss = 0.669 and accuracy of 0.78\n",
      "Iteration 19162: with minibatch training loss = 0.789 and accuracy of 0.78\n",
      "Iteration 19163: with minibatch training loss = 0.793 and accuracy of 0.7\n",
      "Iteration 19164: with minibatch training loss = 0.597 and accuracy of 0.8\n",
      "Iteration 19165: with minibatch training loss = 0.446 and accuracy of 0.86\n",
      "Iteration 19166: with minibatch training loss = 0.496 and accuracy of 0.86\n",
      "Iteration 19167: with minibatch training loss = 0.431 and accuracy of 0.86\n",
      "Iteration 19168: with minibatch training loss = 0.292 and accuracy of 0.92\n",
      "Iteration 19169: with minibatch training loss = 0.757 and accuracy of 0.75\n",
      "Iteration 19170: with minibatch training loss = 0.44 and accuracy of 0.88\n",
      "Iteration 19171: with minibatch training loss = 0.265 and accuracy of 0.91\n",
      "Iteration 19172: with minibatch training loss = 0.537 and accuracy of 0.81\n",
      "Iteration 19173: with minibatch training loss = 0.564 and accuracy of 0.83\n",
      "Iteration 19174: with minibatch training loss = 0.503 and accuracy of 0.81\n",
      "Iteration 19175: with minibatch training loss = 0.581 and accuracy of 0.83\n",
      "Iteration 19176: with minibatch training loss = 0.623 and accuracy of 0.78\n",
      "Iteration 19177: with minibatch training loss = 0.625 and accuracy of 0.83\n",
      "Iteration 19178: with minibatch training loss = 0.722 and accuracy of 0.73\n",
      "Iteration 19179: with minibatch training loss = 0.727 and accuracy of 0.75\n",
      "Iteration 19180: with minibatch training loss = 0.369 and accuracy of 0.89\n",
      "Iteration 19181: with minibatch training loss = 0.727 and accuracy of 0.73\n",
      "Iteration 19182: with minibatch training loss = 0.682 and accuracy of 0.84\n",
      "Iteration 19183: with minibatch training loss = 0.725 and accuracy of 0.77\n",
      "Iteration 19184: with minibatch training loss = 0.758 and accuracy of 0.78\n",
      "Iteration 19185: with minibatch training loss = 0.333 and accuracy of 0.92\n",
      "Iteration 19186: with minibatch training loss = 0.65 and accuracy of 0.75\n",
      "Iteration 19187: with minibatch training loss = 0.676 and accuracy of 0.81\n",
      "Iteration 19188: with minibatch training loss = 0.512 and accuracy of 0.84\n",
      "Iteration 19189: with minibatch training loss = 0.607 and accuracy of 0.78\n",
      "Iteration 19190: with minibatch training loss = 0.598 and accuracy of 0.81\n",
      "Iteration 19191: with minibatch training loss = 0.632 and accuracy of 0.78\n",
      "Iteration 19192: with minibatch training loss = 0.965 and accuracy of 0.7\n",
      "Iteration 19193: with minibatch training loss = 0.58 and accuracy of 0.83\n",
      "Iteration 19194: with minibatch training loss = 0.496 and accuracy of 0.86\n",
      "Iteration 19195: with minibatch training loss = 0.779 and accuracy of 0.73\n",
      "Iteration 19196: with minibatch training loss = 0.843 and accuracy of 0.72\n",
      "Iteration 19197: with minibatch training loss = 0.633 and accuracy of 0.8\n",
      "Iteration 19198: with minibatch training loss = 0.726 and accuracy of 0.78\n",
      "Iteration 19199: with minibatch training loss = 0.515 and accuracy of 0.83\n",
      "Iteration 19200: with minibatch training loss = 0.772 and accuracy of 0.73\n",
      "Iteration 19201: with minibatch training loss = 0.653 and accuracy of 0.81\n",
      "Iteration 19202: with minibatch training loss = 0.469 and accuracy of 0.84\n",
      "Iteration 19203: with minibatch training loss = 0.626 and accuracy of 0.8\n",
      "Iteration 19204: with minibatch training loss = 0.753 and accuracy of 0.77\n",
      "Iteration 19205: with minibatch training loss = 0.819 and accuracy of 0.77\n",
      "Iteration 19206: with minibatch training loss = 0.759 and accuracy of 0.75\n",
      "Iteration 19207: with minibatch training loss = 0.468 and accuracy of 0.86\n",
      "Iteration 19208: with minibatch training loss = 0.419 and accuracy of 0.91\n",
      "Iteration 19209: with minibatch training loss = 0.639 and accuracy of 0.78\n",
      "Iteration 19210: with minibatch training loss = 0.649 and accuracy of 0.78\n",
      "Iteration 19211: with minibatch training loss = 0.386 and accuracy of 0.88\n",
      "Iteration 19212: with minibatch training loss = 0.331 and accuracy of 0.89\n",
      "Iteration 19213: with minibatch training loss = 0.373 and accuracy of 0.92\n",
      "Iteration 19214: with minibatch training loss = 0.597 and accuracy of 0.8\n",
      "Iteration 19215: with minibatch training loss = 0.743 and accuracy of 0.77\n",
      "Iteration 19216: with minibatch training loss = 0.509 and accuracy of 0.83\n",
      "Iteration 19217: with minibatch training loss = 0.703 and accuracy of 0.8\n",
      "Iteration 19218: with minibatch training loss = 0.549 and accuracy of 0.8\n",
      "Iteration 19219: with minibatch training loss = 0.748 and accuracy of 0.75\n",
      "Iteration 19220: with minibatch training loss = 0.563 and accuracy of 0.81\n",
      "Iteration 19221: with minibatch training loss = 0.768 and accuracy of 0.72\n",
      "Iteration 19222: with minibatch training loss = 0.773 and accuracy of 0.75\n",
      "Iteration 19223: with minibatch training loss = 0.577 and accuracy of 0.8\n",
      "Iteration 19224: with minibatch training loss = 0.637 and accuracy of 0.78\n",
      "Iteration 19225: with minibatch training loss = 0.764 and accuracy of 0.73\n",
      "Iteration 19226: with minibatch training loss = 0.556 and accuracy of 0.8\n",
      "Iteration 19227: with minibatch training loss = 0.574 and accuracy of 0.81\n",
      "Iteration 19228: with minibatch training loss = 0.703 and accuracy of 0.8\n",
      "Iteration 19229: with minibatch training loss = 0.54 and accuracy of 0.83\n",
      "Iteration 19230: with minibatch training loss = 0.737 and accuracy of 0.73\n",
      "Iteration 19231: with minibatch training loss = 0.756 and accuracy of 0.72\n",
      "Iteration 19232: with minibatch training loss = 0.75 and accuracy of 0.8\n",
      "Iteration 19233: with minibatch training loss = 0.637 and accuracy of 0.8\n",
      "Iteration 19234: with minibatch training loss = 0.614 and accuracy of 0.83\n",
      "Iteration 19235: with minibatch training loss = 0.662 and accuracy of 0.77\n",
      "Iteration 19236: with minibatch training loss = 0.558 and accuracy of 0.84\n",
      "Iteration 19237: with minibatch training loss = 0.577 and accuracy of 0.81\n",
      "Iteration 19238: with minibatch training loss = 0.591 and accuracy of 0.83\n",
      "Iteration 19239: with minibatch training loss = 0.642 and accuracy of 0.78\n",
      "Iteration 19240: with minibatch training loss = 0.463 and accuracy of 0.88\n",
      "Iteration 19241: with minibatch training loss = 0.606 and accuracy of 0.8\n",
      "Iteration 19242: with minibatch training loss = 0.479 and accuracy of 0.84\n",
      "Iteration 19243: with minibatch training loss = 0.621 and accuracy of 0.8\n",
      "Iteration 19244: with minibatch training loss = 0.549 and accuracy of 0.83\n",
      "Iteration 19245: with minibatch training loss = 0.719 and accuracy of 0.75\n",
      "Iteration 19246: with minibatch training loss = 0.723 and accuracy of 0.77\n",
      "Iteration 19247: with minibatch training loss = 1.04 and accuracy of 0.67\n",
      "Iteration 19248: with minibatch training loss = 0.674 and accuracy of 0.77\n",
      "Iteration 19249: with minibatch training loss = 0.656 and accuracy of 0.8\n",
      "Iteration 19250: with minibatch training loss = 0.501 and accuracy of 0.84\n",
      "Iteration 19251: with minibatch training loss = 0.393 and accuracy of 0.88\n",
      "Iteration 19252: with minibatch training loss = 0.441 and accuracy of 0.88\n",
      "Iteration 19253: with minibatch training loss = 0.612 and accuracy of 0.83\n",
      "Iteration 19254: with minibatch training loss = 0.637 and accuracy of 0.8\n",
      "Iteration 19255: with minibatch training loss = 0.66 and accuracy of 0.77\n",
      "Iteration 19256: with minibatch training loss = 0.417 and accuracy of 0.89\n",
      "Iteration 19257: with minibatch training loss = 0.426 and accuracy of 0.89\n",
      "Iteration 19258: with minibatch training loss = 0.698 and accuracy of 0.77\n",
      "Iteration 19259: with minibatch training loss = 0.345 and accuracy of 0.89\n",
      "Iteration 19260: with minibatch training loss = 0.401 and accuracy of 0.92\n",
      "Iteration 19261: with minibatch training loss = 0.359 and accuracy of 0.91\n",
      "Iteration 19262: with minibatch training loss = 0.626 and accuracy of 0.78\n",
      "Iteration 19263: with minibatch training loss = 0.606 and accuracy of 0.83\n",
      "Iteration 19264: with minibatch training loss = 0.569 and accuracy of 0.81\n",
      "Iteration 19265: with minibatch training loss = 0.564 and accuracy of 0.83\n",
      "Iteration 19266: with minibatch training loss = 0.44 and accuracy of 0.84\n",
      "Iteration 19267: with minibatch training loss = 0.821 and accuracy of 0.77\n",
      "Iteration 19268: with minibatch training loss = 0.664 and accuracy of 0.8\n",
      "Iteration 19269: with minibatch training loss = 0.805 and accuracy of 0.75\n",
      "Iteration 19270: with minibatch training loss = 0.731 and accuracy of 0.73\n",
      "Iteration 19271: with minibatch training loss = 0.541 and accuracy of 0.84\n",
      "Iteration 19272: with minibatch training loss = 0.74 and accuracy of 0.78\n",
      "Iteration 19273: with minibatch training loss = 0.331 and accuracy of 0.89\n",
      "Iteration 19274: with minibatch training loss = 0.564 and accuracy of 0.83\n",
      "Iteration 19275: with minibatch training loss = 0.911 and accuracy of 0.77\n",
      "Iteration 19276: with minibatch training loss = 0.595 and accuracy of 0.84\n",
      "Iteration 19277: with minibatch training loss = 0.702 and accuracy of 0.77\n",
      "Iteration 19278: with minibatch training loss = 0.794 and accuracy of 0.73\n",
      "Iteration 19279: with minibatch training loss = 0.717 and accuracy of 0.75\n",
      "Iteration 19280: with minibatch training loss = 0.553 and accuracy of 0.84\n",
      "Iteration 19281: with minibatch training loss = 0.597 and accuracy of 0.81\n",
      "Iteration 19282: with minibatch training loss = 0.788 and accuracy of 0.75\n",
      "Iteration 19283: with minibatch training loss = 0.465 and accuracy of 0.84\n",
      "Iteration 19284: with minibatch training loss = 0.741 and accuracy of 0.77\n",
      "Iteration 19285: with minibatch training loss = 0.63 and accuracy of 0.78\n",
      "Iteration 19286: with minibatch training loss = 0.629 and accuracy of 0.81\n",
      "Iteration 19287: with minibatch training loss = 0.61 and accuracy of 0.8\n",
      "Iteration 19288: with minibatch training loss = 0.605 and accuracy of 0.83\n",
      "Iteration 19289: with minibatch training loss = 0.8 and accuracy of 0.75\n",
      "Iteration 19290: with minibatch training loss = 0.833 and accuracy of 0.73\n",
      "Iteration 19291: with minibatch training loss = 0.622 and accuracy of 0.8\n",
      "Iteration 19292: with minibatch training loss = 0.547 and accuracy of 0.84\n",
      "Iteration 19293: with minibatch training loss = 0.59 and accuracy of 0.8\n",
      "Iteration 19294: with minibatch training loss = 0.697 and accuracy of 0.78\n",
      "Iteration 19295: with minibatch training loss = 0.62 and accuracy of 0.78\n",
      "Iteration 19296: with minibatch training loss = 0.53 and accuracy of 0.83\n",
      "Iteration 19297: with minibatch training loss = 0.831 and accuracy of 0.7\n",
      "Iteration 19298: with minibatch training loss = 0.724 and accuracy of 0.75\n",
      "Iteration 19299: with minibatch training loss = 0.57 and accuracy of 0.8\n",
      "Iteration 19300: with minibatch training loss = 0.489 and accuracy of 0.86\n",
      "Iteration 19301: with minibatch training loss = 0.425 and accuracy of 0.84\n",
      "Iteration 19302: with minibatch training loss = 0.789 and accuracy of 0.75\n",
      "Iteration 19303: with minibatch training loss = 0.849 and accuracy of 0.75\n",
      "Iteration 19304: with minibatch training loss = 0.817 and accuracy of 0.69\n",
      "Iteration 19305: with minibatch training loss = 0.376 and accuracy of 0.86\n",
      "Iteration 19306: with minibatch training loss = 0.708 and accuracy of 0.8\n",
      "Iteration 19307: with minibatch training loss = 0.599 and accuracy of 0.81\n",
      "Iteration 19308: with minibatch training loss = 0.489 and accuracy of 0.84\n",
      "Iteration 19309: with minibatch training loss = 0.755 and accuracy of 0.75\n",
      "Iteration 19310: with minibatch training loss = 0.661 and accuracy of 0.8\n",
      "Iteration 19311: with minibatch training loss = 0.492 and accuracy of 0.86\n",
      "Iteration 19312: with minibatch training loss = 0.574 and accuracy of 0.8\n",
      "Iteration 19313: with minibatch training loss = 0.708 and accuracy of 0.81\n",
      "Iteration 19314: with minibatch training loss = 0.444 and accuracy of 0.88\n",
      "Iteration 19315: with minibatch training loss = 0.388 and accuracy of 0.84\n",
      "Iteration 19316: with minibatch training loss = 0.469 and accuracy of 0.84\n",
      "Iteration 19317: with minibatch training loss = 0.725 and accuracy of 0.81\n",
      "Iteration 19318: with minibatch training loss = 0.978 and accuracy of 0.67\n",
      "Iteration 19319: with minibatch training loss = 0.434 and accuracy of 0.86\n",
      "Iteration 19320: with minibatch training loss = 0.768 and accuracy of 0.75\n",
      "Iteration 19321: with minibatch training loss = 0.469 and accuracy of 0.86\n",
      "Iteration 19322: with minibatch training loss = 0.509 and accuracy of 0.83\n",
      "Iteration 19323: with minibatch training loss = 0.56 and accuracy of 0.8\n",
      "Iteration 19324: with minibatch training loss = 0.749 and accuracy of 0.73\n",
      "Iteration 19325: with minibatch training loss = 0.759 and accuracy of 0.77\n",
      "Iteration 19326: with minibatch training loss = 0.762 and accuracy of 0.73\n",
      "Iteration 19327: with minibatch training loss = 0.614 and accuracy of 0.8\n",
      "Iteration 19328: with minibatch training loss = 0.495 and accuracy of 0.84\n",
      "Iteration 19329: with minibatch training loss = 0.656 and accuracy of 0.77\n",
      "Iteration 19330: with minibatch training loss = 0.787 and accuracy of 0.75\n",
      "Iteration 19331: with minibatch training loss = 0.42 and accuracy of 0.86\n",
      "Iteration 19332: with minibatch training loss = 0.374 and accuracy of 0.89\n",
      "Iteration 19333: with minibatch training loss = 0.812 and accuracy of 0.73\n",
      "Iteration 19334: with minibatch training loss = 0.552 and accuracy of 0.81\n",
      "Iteration 19335: with minibatch training loss = 0.727 and accuracy of 0.77\n",
      "Iteration 19336: with minibatch training loss = 0.889 and accuracy of 0.7\n",
      "Iteration 19337: with minibatch training loss = 0.608 and accuracy of 0.8\n",
      "Iteration 19338: with minibatch training loss = 0.761 and accuracy of 0.77\n",
      "Iteration 19339: with minibatch training loss = 0.948 and accuracy of 0.69\n",
      "Iteration 19340: with minibatch training loss = 0.467 and accuracy of 0.88\n",
      "Iteration 19341: with minibatch training loss = 0.57 and accuracy of 0.81\n",
      "Iteration 19342: with minibatch training loss = 0.766 and accuracy of 0.77\n",
      "Iteration 19343: with minibatch training loss = 0.497 and accuracy of 0.84\n",
      "Iteration 19344: with minibatch training loss = 0.668 and accuracy of 0.77\n",
      "Iteration 19345: with minibatch training loss = 0.52 and accuracy of 0.84\n",
      "Iteration 19346: with minibatch training loss = 0.576 and accuracy of 0.86\n",
      "Iteration 19347: with minibatch training loss = 0.686 and accuracy of 0.77\n",
      "Iteration 19348: with minibatch training loss = 0.696 and accuracy of 0.75\n",
      "Iteration 19349: with minibatch training loss = 0.78 and accuracy of 0.77\n",
      "Iteration 19350: with minibatch training loss = 0.567 and accuracy of 0.8\n",
      "Iteration 19351: with minibatch training loss = 0.652 and accuracy of 0.83\n",
      "Iteration 19352: with minibatch training loss = 0.743 and accuracy of 0.73\n",
      "Iteration 19353: with minibatch training loss = 0.769 and accuracy of 0.77\n",
      "Iteration 19354: with minibatch training loss = 0.572 and accuracy of 0.83\n",
      "Iteration 19355: with minibatch training loss = 0.672 and accuracy of 0.75\n",
      "Iteration 19356: with minibatch training loss = 0.595 and accuracy of 0.83\n",
      "Iteration 19357: with minibatch training loss = 0.474 and accuracy of 0.86\n",
      "Iteration 19358: with minibatch training loss = 0.664 and accuracy of 0.78\n",
      "Iteration 19359: with minibatch training loss = 0.673 and accuracy of 0.78\n",
      "Iteration 19360: with minibatch training loss = 0.451 and accuracy of 0.88\n",
      "Iteration 19361: with minibatch training loss = 0.402 and accuracy of 0.89\n",
      "Iteration 19362: with minibatch training loss = 0.627 and accuracy of 0.8\n",
      "Iteration 19363: with minibatch training loss = 0.467 and accuracy of 0.88\n",
      "Iteration 19364: with minibatch training loss = 0.764 and accuracy of 0.72\n",
      "Iteration 19365: with minibatch training loss = 0.669 and accuracy of 0.78\n",
      "Iteration 19366: with minibatch training loss = 0.55 and accuracy of 0.83\n",
      "Iteration 19367: with minibatch training loss = 0.804 and accuracy of 0.7\n",
      "Iteration 19368: with minibatch training loss = 0.541 and accuracy of 0.84\n",
      "Iteration 19369: with minibatch training loss = 0.802 and accuracy of 0.77\n",
      "Iteration 19370: with minibatch training loss = 0.772 and accuracy of 0.73\n",
      "Iteration 19371: with minibatch training loss = 0.723 and accuracy of 0.83\n",
      "Iteration 19372: with minibatch training loss = 0.767 and accuracy of 0.72\n",
      "Iteration 19373: with minibatch training loss = 0.608 and accuracy of 0.78\n",
      "Iteration 19374: with minibatch training loss = 0.701 and accuracy of 0.78\n",
      "Iteration 19375: with minibatch training loss = 0.442 and accuracy of 0.89\n",
      "Iteration 19376: with minibatch training loss = 0.833 and accuracy of 0.77\n",
      "Iteration 19377: with minibatch training loss = 0.529 and accuracy of 0.84\n",
      "Iteration 19378: with minibatch training loss = 0.881 and accuracy of 0.72\n",
      "Iteration 19379: with minibatch training loss = 0.604 and accuracy of 0.78\n",
      "Iteration 19380: with minibatch training loss = 0.554 and accuracy of 0.81\n",
      "Iteration 19381: with minibatch training loss = 0.442 and accuracy of 0.84\n",
      "Iteration 19382: with minibatch training loss = 0.545 and accuracy of 0.83\n",
      "Iteration 19383: with minibatch training loss = 0.765 and accuracy of 0.75\n",
      "Iteration 19384: with minibatch training loss = 0.493 and accuracy of 0.86\n",
      "Iteration 19385: with minibatch training loss = 0.85 and accuracy of 0.73\n",
      "Iteration 19386: with minibatch training loss = 0.473 and accuracy of 0.88\n",
      "Iteration 19387: with minibatch training loss = 0.985 and accuracy of 0.67\n",
      "Iteration 19388: with minibatch training loss = 0.428 and accuracy of 0.84\n",
      "Iteration 19389: with minibatch training loss = 0.542 and accuracy of 0.81\n",
      "Iteration 19390: with minibatch training loss = 0.378 and accuracy of 0.89\n",
      "Iteration 19391: with minibatch training loss = 0.597 and accuracy of 0.8\n",
      "Iteration 19392: with minibatch training loss = 0.857 and accuracy of 0.66\n",
      "Iteration 19393: with minibatch training loss = 0.706 and accuracy of 0.75\n",
      "Iteration 19394: with minibatch training loss = 0.877 and accuracy of 0.72\n",
      "Iteration 19395: with minibatch training loss = 0.372 and accuracy of 0.89\n",
      "Iteration 19396: with minibatch training loss = 0.527 and accuracy of 0.84\n",
      "Iteration 19397: with minibatch training loss = 0.697 and accuracy of 0.78\n",
      "Iteration 19398: with minibatch training loss = 0.464 and accuracy of 0.83\n",
      "Iteration 19399: with minibatch training loss = 0.531 and accuracy of 0.83\n",
      "Iteration 19400: with minibatch training loss = 0.688 and accuracy of 0.77\n",
      "Iteration 19401: with minibatch training loss = 0.601 and accuracy of 0.81\n",
      "Iteration 19402: with minibatch training loss = 0.675 and accuracy of 0.81\n",
      "Iteration 19403: with minibatch training loss = 0.611 and accuracy of 0.81\n",
      "Iteration 19404: with minibatch training loss = 0.678 and accuracy of 0.77\n",
      "Iteration 19405: with minibatch training loss = 0.629 and accuracy of 0.8\n",
      "Iteration 19406: with minibatch training loss = 0.512 and accuracy of 0.81\n",
      "Iteration 19407: with minibatch training loss = 0.562 and accuracy of 0.81\n",
      "Iteration 19408: with minibatch training loss = 0.646 and accuracy of 0.81\n",
      "Iteration 19409: with minibatch training loss = 0.549 and accuracy of 0.8\n",
      "Iteration 19410: with minibatch training loss = 0.52 and accuracy of 0.84\n",
      "Iteration 19411: with minibatch training loss = 0.583 and accuracy of 0.84\n",
      "Iteration 19412: with minibatch training loss = 0.698 and accuracy of 0.78\n",
      "Iteration 19413: with minibatch training loss = 0.625 and accuracy of 0.78\n",
      "Iteration 19414: with minibatch training loss = 0.488 and accuracy of 0.86\n",
      "Iteration 19415: with minibatch training loss = 0.594 and accuracy of 0.84\n",
      "Iteration 19416: with minibatch training loss = 0.683 and accuracy of 0.8\n",
      "Iteration 19417: with minibatch training loss = 0.533 and accuracy of 0.83\n",
      "Iteration 19418: with minibatch training loss = 0.677 and accuracy of 0.78\n",
      "Iteration 19419: with minibatch training loss = 0.762 and accuracy of 0.75\n",
      "Iteration 19420: with minibatch training loss = 0.705 and accuracy of 0.75\n",
      "Iteration 19421: with minibatch training loss = 0.559 and accuracy of 0.86\n",
      "Iteration 19422: with minibatch training loss = 0.502 and accuracy of 0.83\n",
      "Iteration 19423: with minibatch training loss = 0.544 and accuracy of 0.84\n",
      "Iteration 19424: with minibatch training loss = 0.727 and accuracy of 0.75\n",
      "Iteration 19425: with minibatch training loss = 0.629 and accuracy of 0.77\n",
      "Iteration 19426: with minibatch training loss = 0.674 and accuracy of 0.81\n",
      "Iteration 19427: with minibatch training loss = 0.909 and accuracy of 0.7\n",
      "Iteration 19428: with minibatch training loss = 0.674 and accuracy of 0.81\n",
      "Iteration 19429: with minibatch training loss = 0.876 and accuracy of 0.67\n",
      "Iteration 19430: with minibatch training loss = 0.519 and accuracy of 0.86\n",
      "Validation loss: 0.2285283\n",
      "Epoch 14, Overall loss = 0.625 and accuracy of 0.801\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nziCQpOIMAkkqziCQpOL+AAIb2DEI8wJFAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11ffe37b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19432: with minibatch training loss = 0.649 and accuracy of 0.78\n",
      "Iteration 19433: with minibatch training loss = 0.489 and accuracy of 0.83\n",
      "Iteration 19434: with minibatch training loss = 0.439 and accuracy of 0.84\n",
      "Iteration 19435: with minibatch training loss = 0.543 and accuracy of 0.86\n",
      "Iteration 19436: with minibatch training loss = 0.455 and accuracy of 0.88\n",
      "Iteration 19437: with minibatch training loss = 0.884 and accuracy of 0.7\n",
      "Iteration 19438: with minibatch training loss = 0.747 and accuracy of 0.78\n",
      "Iteration 19439: with minibatch training loss = 0.363 and accuracy of 0.92\n",
      "Iteration 19440: with minibatch training loss = 0.775 and accuracy of 0.75\n",
      "Iteration 19441: with minibatch training loss = 0.435 and accuracy of 0.88\n",
      "Iteration 19442: with minibatch training loss = 0.73 and accuracy of 0.77\n",
      "Iteration 19443: with minibatch training loss = 0.821 and accuracy of 0.73\n",
      "Iteration 19444: with minibatch training loss = 0.731 and accuracy of 0.73\n",
      "Iteration 19445: with minibatch training loss = 0.986 and accuracy of 0.67\n",
      "Iteration 19446: with minibatch training loss = 0.606 and accuracy of 0.78\n",
      "Iteration 19447: with minibatch training loss = 0.557 and accuracy of 0.81\n",
      "Iteration 19448: with minibatch training loss = 0.558 and accuracy of 0.83\n",
      "Iteration 19449: with minibatch training loss = 0.779 and accuracy of 0.73\n",
      "Iteration 19450: with minibatch training loss = 0.692 and accuracy of 0.78\n",
      "Iteration 19451: with minibatch training loss = 0.329 and accuracy of 0.91\n",
      "Iteration 19452: with minibatch training loss = 0.796 and accuracy of 0.75\n",
      "Iteration 19453: with minibatch training loss = 0.428 and accuracy of 0.86\n",
      "Iteration 19454: with minibatch training loss = 0.64 and accuracy of 0.78\n",
      "Iteration 19455: with minibatch training loss = 0.923 and accuracy of 0.69\n",
      "Iteration 19456: with minibatch training loss = 0.501 and accuracy of 0.84\n",
      "Iteration 19457: with minibatch training loss = 0.472 and accuracy of 0.83\n",
      "Iteration 19458: with minibatch training loss = 0.569 and accuracy of 0.8\n",
      "Iteration 19459: with minibatch training loss = 0.628 and accuracy of 0.8\n",
      "Iteration 19460: with minibatch training loss = 0.517 and accuracy of 0.84\n",
      "Iteration 19461: with minibatch training loss = 0.693 and accuracy of 0.78\n",
      "Iteration 19462: with minibatch training loss = 0.498 and accuracy of 0.84\n",
      "Iteration 19463: with minibatch training loss = 0.54 and accuracy of 0.84\n",
      "Iteration 19464: with minibatch training loss = 0.575 and accuracy of 0.8\n",
      "Iteration 19465: with minibatch training loss = 0.609 and accuracy of 0.83\n",
      "Iteration 19466: with minibatch training loss = 0.51 and accuracy of 0.86\n",
      "Iteration 19467: with minibatch training loss = 0.511 and accuracy of 0.88\n",
      "Iteration 19468: with minibatch training loss = 0.668 and accuracy of 0.78\n",
      "Iteration 19469: with minibatch training loss = 0.817 and accuracy of 0.7\n",
      "Iteration 19470: with minibatch training loss = 0.407 and accuracy of 0.84\n",
      "Iteration 19471: with minibatch training loss = 0.935 and accuracy of 0.7\n",
      "Iteration 19472: with minibatch training loss = 0.645 and accuracy of 0.83\n",
      "Iteration 19473: with minibatch training loss = 0.672 and accuracy of 0.78\n",
      "Iteration 19474: with minibatch training loss = 0.631 and accuracy of 0.78\n",
      "Iteration 19475: with minibatch training loss = 0.579 and accuracy of 0.83\n",
      "Iteration 19476: with minibatch training loss = 0.743 and accuracy of 0.75\n",
      "Iteration 19477: with minibatch training loss = 0.583 and accuracy of 0.83\n",
      "Iteration 19478: with minibatch training loss = 0.561 and accuracy of 0.81\n",
      "Iteration 19479: with minibatch training loss = 0.53 and accuracy of 0.83\n",
      "Iteration 19480: with minibatch training loss = 0.512 and accuracy of 0.86\n",
      "Iteration 19481: with minibatch training loss = 0.681 and accuracy of 0.81\n",
      "Iteration 19482: with minibatch training loss = 0.661 and accuracy of 0.8\n",
      "Iteration 19483: with minibatch training loss = 0.869 and accuracy of 0.7\n",
      "Iteration 19484: with minibatch training loss = 0.838 and accuracy of 0.7\n",
      "Iteration 19485: with minibatch training loss = 0.827 and accuracy of 0.72\n",
      "Iteration 19486: with minibatch training loss = 0.745 and accuracy of 0.77\n",
      "Iteration 19487: with minibatch training loss = 0.751 and accuracy of 0.77\n",
      "Iteration 19488: with minibatch training loss = 0.353 and accuracy of 0.89\n",
      "Iteration 19489: with minibatch training loss = 0.62 and accuracy of 0.8\n",
      "Iteration 19490: with minibatch training loss = 0.75 and accuracy of 0.78\n",
      "Iteration 19491: with minibatch training loss = 0.797 and accuracy of 0.73\n",
      "Iteration 19492: with minibatch training loss = 0.668 and accuracy of 0.8\n",
      "Iteration 19493: with minibatch training loss = 0.504 and accuracy of 0.84\n",
      "Iteration 19494: with minibatch training loss = 0.44 and accuracy of 0.86\n",
      "Iteration 19495: with minibatch training loss = 0.518 and accuracy of 0.83\n",
      "Iteration 19496: with minibatch training loss = 0.726 and accuracy of 0.75\n",
      "Iteration 19497: with minibatch training loss = 0.542 and accuracy of 0.81\n",
      "Iteration 19498: with minibatch training loss = 0.549 and accuracy of 0.81\n",
      "Iteration 19499: with minibatch training loss = 0.578 and accuracy of 0.8\n",
      "Iteration 19500: with minibatch training loss = 0.833 and accuracy of 0.73\n",
      "Iteration 19501: with minibatch training loss = 0.447 and accuracy of 0.88\n",
      "Iteration 19502: with minibatch training loss = 0.698 and accuracy of 0.77\n",
      "Iteration 19503: with minibatch training loss = 0.487 and accuracy of 0.86\n",
      "Iteration 19504: with minibatch training loss = 0.566 and accuracy of 0.88\n",
      "Iteration 19505: with minibatch training loss = 0.447 and accuracy of 0.86\n",
      "Iteration 19506: with minibatch training loss = 0.46 and accuracy of 0.86\n",
      "Iteration 19507: with minibatch training loss = 0.672 and accuracy of 0.81\n",
      "Iteration 19508: with minibatch training loss = 0.842 and accuracy of 0.7\n",
      "Iteration 19509: with minibatch training loss = 0.555 and accuracy of 0.81\n",
      "Iteration 19510: with minibatch training loss = 0.639 and accuracy of 0.78\n",
      "Iteration 19511: with minibatch training loss = 0.63 and accuracy of 0.78\n",
      "Iteration 19512: with minibatch training loss = 0.438 and accuracy of 0.88\n",
      "Iteration 19513: with minibatch training loss = 0.876 and accuracy of 0.72\n",
      "Iteration 19514: with minibatch training loss = 0.773 and accuracy of 0.75\n",
      "Iteration 19515: with minibatch training loss = 0.616 and accuracy of 0.78\n",
      "Iteration 19516: with minibatch training loss = 0.684 and accuracy of 0.78\n",
      "Iteration 19517: with minibatch training loss = 0.69 and accuracy of 0.73\n",
      "Iteration 19518: with minibatch training loss = 0.685 and accuracy of 0.81\n",
      "Iteration 19519: with minibatch training loss = 0.732 and accuracy of 0.8\n",
      "Iteration 19520: with minibatch training loss = 0.598 and accuracy of 0.8\n",
      "Iteration 19521: with minibatch training loss = 0.755 and accuracy of 0.75\n",
      "Iteration 19522: with minibatch training loss = 0.537 and accuracy of 0.84\n",
      "Iteration 19523: with minibatch training loss = 0.465 and accuracy of 0.89\n",
      "Iteration 19524: with minibatch training loss = 0.505 and accuracy of 0.84\n",
      "Iteration 19525: with minibatch training loss = 0.77 and accuracy of 0.75\n",
      "Iteration 19526: with minibatch training loss = 0.412 and accuracy of 0.88\n",
      "Iteration 19527: with minibatch training loss = 0.738 and accuracy of 0.77\n",
      "Iteration 19528: with minibatch training loss = 0.492 and accuracy of 0.86\n",
      "Iteration 19529: with minibatch training loss = 0.516 and accuracy of 0.81\n",
      "Iteration 19530: with minibatch training loss = 0.608 and accuracy of 0.77\n",
      "Iteration 19531: with minibatch training loss = 0.811 and accuracy of 0.72\n",
      "Iteration 19532: with minibatch training loss = 0.826 and accuracy of 0.72\n",
      "Iteration 19533: with minibatch training loss = 0.528 and accuracy of 0.84\n",
      "Iteration 19534: with minibatch training loss = 0.528 and accuracy of 0.83\n",
      "Iteration 19535: with minibatch training loss = 0.55 and accuracy of 0.83\n",
      "Iteration 19536: with minibatch training loss = 0.806 and accuracy of 0.77\n",
      "Iteration 19537: with minibatch training loss = 0.59 and accuracy of 0.81\n",
      "Iteration 19538: with minibatch training loss = 0.705 and accuracy of 0.77\n",
      "Iteration 19539: with minibatch training loss = 0.662 and accuracy of 0.75\n",
      "Iteration 19540: with minibatch training loss = 0.742 and accuracy of 0.78\n",
      "Iteration 19541: with minibatch training loss = 0.464 and accuracy of 0.84\n",
      "Iteration 19542: with minibatch training loss = 0.763 and accuracy of 0.75\n",
      "Iteration 19543: with minibatch training loss = 0.747 and accuracy of 0.75\n",
      "Iteration 19544: with minibatch training loss = 0.586 and accuracy of 0.83\n",
      "Iteration 19545: with minibatch training loss = 0.556 and accuracy of 0.81\n",
      "Iteration 19546: with minibatch training loss = 0.737 and accuracy of 0.77\n",
      "Iteration 19547: with minibatch training loss = 0.416 and accuracy of 0.88\n",
      "Iteration 19548: with minibatch training loss = 0.745 and accuracy of 0.75\n",
      "Iteration 19549: with minibatch training loss = 0.52 and accuracy of 0.84\n",
      "Iteration 19550: with minibatch training loss = 0.609 and accuracy of 0.78\n",
      "Iteration 19551: with minibatch training loss = 0.431 and accuracy of 0.86\n",
      "Iteration 19552: with minibatch training loss = 0.718 and accuracy of 0.8\n",
      "Iteration 19553: with minibatch training loss = 0.599 and accuracy of 0.8\n",
      "Iteration 19554: with minibatch training loss = 0.472 and accuracy of 0.86\n",
      "Iteration 19555: with minibatch training loss = 0.354 and accuracy of 0.91\n",
      "Iteration 19556: with minibatch training loss = 0.889 and accuracy of 0.75\n",
      "Iteration 19557: with minibatch training loss = 0.659 and accuracy of 0.78\n",
      "Iteration 19558: with minibatch training loss = 0.605 and accuracy of 0.81\n",
      "Iteration 19559: with minibatch training loss = 0.326 and accuracy of 0.89\n",
      "Iteration 19560: with minibatch training loss = 0.738 and accuracy of 0.75\n",
      "Iteration 19561: with minibatch training loss = 0.676 and accuracy of 0.77\n",
      "Iteration 19562: with minibatch training loss = 0.758 and accuracy of 0.77\n",
      "Iteration 19563: with minibatch training loss = 0.477 and accuracy of 0.84\n",
      "Iteration 19564: with minibatch training loss = 0.56 and accuracy of 0.83\n",
      "Iteration 19565: with minibatch training loss = 0.793 and accuracy of 0.75\n",
      "Iteration 19566: with minibatch training loss = 0.561 and accuracy of 0.81\n",
      "Iteration 19567: with minibatch training loss = 0.502 and accuracy of 0.91\n",
      "Iteration 19568: with minibatch training loss = 0.459 and accuracy of 0.89\n",
      "Iteration 19569: with minibatch training loss = 0.68 and accuracy of 0.8\n",
      "Iteration 19570: with minibatch training loss = 0.775 and accuracy of 0.77\n",
      "Iteration 19571: with minibatch training loss = 0.561 and accuracy of 0.83\n",
      "Iteration 19572: with minibatch training loss = 0.603 and accuracy of 0.77\n",
      "Iteration 19573: with minibatch training loss = 0.703 and accuracy of 0.77\n",
      "Iteration 19574: with minibatch training loss = 0.479 and accuracy of 0.84\n",
      "Iteration 19575: with minibatch training loss = 1.04 and accuracy of 0.64\n",
      "Iteration 19576: with minibatch training loss = 0.612 and accuracy of 0.8\n",
      "Iteration 19577: with minibatch training loss = 0.835 and accuracy of 0.72\n",
      "Iteration 19578: with minibatch training loss = 0.733 and accuracy of 0.8\n",
      "Iteration 19579: with minibatch training loss = 0.665 and accuracy of 0.78\n",
      "Iteration 19580: with minibatch training loss = 0.54 and accuracy of 0.83\n",
      "Iteration 19581: with minibatch training loss = 0.82 and accuracy of 0.73\n",
      "Iteration 19582: with minibatch training loss = 0.707 and accuracy of 0.78\n",
      "Iteration 19583: with minibatch training loss = 0.755 and accuracy of 0.75\n",
      "Iteration 19584: with minibatch training loss = 0.784 and accuracy of 0.77\n",
      "Iteration 19585: with minibatch training loss = 0.794 and accuracy of 0.72\n",
      "Iteration 19586: with minibatch training loss = 0.646 and accuracy of 0.77\n",
      "Iteration 19587: with minibatch training loss = 0.771 and accuracy of 0.77\n",
      "Iteration 19588: with minibatch training loss = 0.683 and accuracy of 0.77\n",
      "Iteration 19589: with minibatch training loss = 0.675 and accuracy of 0.75\n",
      "Iteration 19590: with minibatch training loss = 0.3 and accuracy of 0.91\n",
      "Iteration 19591: with minibatch training loss = 0.397 and accuracy of 0.88\n",
      "Iteration 19592: with minibatch training loss = 0.614 and accuracy of 0.78\n",
      "Iteration 19593: with minibatch training loss = 0.489 and accuracy of 0.86\n",
      "Iteration 19594: with minibatch training loss = 0.867 and accuracy of 0.7\n",
      "Iteration 19595: with minibatch training loss = 0.617 and accuracy of 0.81\n",
      "Iteration 19596: with minibatch training loss = 0.512 and accuracy of 0.86\n",
      "Iteration 19597: with minibatch training loss = 0.851 and accuracy of 0.72\n",
      "Iteration 19598: with minibatch training loss = 0.684 and accuracy of 0.78\n",
      "Iteration 19599: with minibatch training loss = 0.872 and accuracy of 0.73\n",
      "Iteration 19600: with minibatch training loss = 0.372 and accuracy of 0.86\n",
      "Iteration 19601: with minibatch training loss = 0.545 and accuracy of 0.84\n",
      "Iteration 19602: with minibatch training loss = 0.851 and accuracy of 0.7\n",
      "Iteration 19603: with minibatch training loss = 0.866 and accuracy of 0.7\n",
      "Iteration 19604: with minibatch training loss = 0.841 and accuracy of 0.75\n",
      "Iteration 19605: with minibatch training loss = 0.919 and accuracy of 0.69\n",
      "Iteration 19606: with minibatch training loss = 0.507 and accuracy of 0.84\n",
      "Iteration 19607: with minibatch training loss = 0.677 and accuracy of 0.77\n",
      "Iteration 19608: with minibatch training loss = 0.6 and accuracy of 0.8\n",
      "Iteration 19609: with minibatch training loss = 0.531 and accuracy of 0.81\n",
      "Iteration 19610: with minibatch training loss = 0.609 and accuracy of 0.78\n",
      "Iteration 19611: with minibatch training loss = 0.686 and accuracy of 0.81\n",
      "Iteration 19612: with minibatch training loss = 0.674 and accuracy of 0.78\n",
      "Iteration 19613: with minibatch training loss = 0.475 and accuracy of 0.84\n",
      "Iteration 19614: with minibatch training loss = 0.377 and accuracy of 0.89\n",
      "Iteration 19615: with minibatch training loss = 0.549 and accuracy of 0.83\n",
      "Iteration 19616: with minibatch training loss = 0.464 and accuracy of 0.83\n",
      "Iteration 19617: with minibatch training loss = 0.68 and accuracy of 0.77\n",
      "Iteration 19618: with minibatch training loss = 0.719 and accuracy of 0.78\n",
      "Iteration 19619: with minibatch training loss = 0.442 and accuracy of 0.88\n",
      "Iteration 19620: with minibatch training loss = 0.866 and accuracy of 0.75\n",
      "Iteration 19621: with minibatch training loss = 0.641 and accuracy of 0.78\n",
      "Iteration 19622: with minibatch training loss = 0.709 and accuracy of 0.78\n",
      "Iteration 19623: with minibatch training loss = 0.683 and accuracy of 0.77\n",
      "Iteration 19624: with minibatch training loss = 0.654 and accuracy of 0.84\n",
      "Iteration 19625: with minibatch training loss = 0.795 and accuracy of 0.75\n",
      "Iteration 19626: with minibatch training loss = 0.386 and accuracy of 0.86\n",
      "Iteration 19627: with minibatch training loss = 0.803 and accuracy of 0.75\n",
      "Iteration 19628: with minibatch training loss = 0.571 and accuracy of 0.8\n",
      "Iteration 19629: with minibatch training loss = 0.465 and accuracy of 0.84\n",
      "Iteration 19630: with minibatch training loss = 0.438 and accuracy of 0.86\n",
      "Iteration 19631: with minibatch training loss = 0.779 and accuracy of 0.73\n",
      "Iteration 19632: with minibatch training loss = 0.841 and accuracy of 0.73\n",
      "Iteration 19633: with minibatch training loss = 0.564 and accuracy of 0.83\n",
      "Iteration 19634: with minibatch training loss = 0.827 and accuracy of 0.77\n",
      "Iteration 19635: with minibatch training loss = 0.687 and accuracy of 0.8\n",
      "Iteration 19636: with minibatch training loss = 0.596 and accuracy of 0.81\n",
      "Iteration 19637: with minibatch training loss = 0.678 and accuracy of 0.75\n",
      "Iteration 19638: with minibatch training loss = 0.71 and accuracy of 0.77\n",
      "Iteration 19639: with minibatch training loss = 0.494 and accuracy of 0.84\n",
      "Iteration 19640: with minibatch training loss = 0.864 and accuracy of 0.72\n",
      "Iteration 19641: with minibatch training loss = 0.719 and accuracy of 0.77\n",
      "Iteration 19642: with minibatch training loss = 0.738 and accuracy of 0.78\n",
      "Iteration 19643: with minibatch training loss = 0.515 and accuracy of 0.83\n",
      "Iteration 19644: with minibatch training loss = 0.466 and accuracy of 0.88\n",
      "Iteration 19645: with minibatch training loss = 0.765 and accuracy of 0.73\n",
      "Iteration 19646: with minibatch training loss = 0.619 and accuracy of 0.8\n",
      "Iteration 19647: with minibatch training loss = 0.661 and accuracy of 0.78\n",
      "Iteration 19648: with minibatch training loss = 0.957 and accuracy of 0.72\n",
      "Iteration 19649: with minibatch training loss = 0.683 and accuracy of 0.77\n",
      "Iteration 19650: with minibatch training loss = 0.421 and accuracy of 0.91\n",
      "Iteration 19651: with minibatch training loss = 0.767 and accuracy of 0.73\n",
      "Iteration 19652: with minibatch training loss = 0.761 and accuracy of 0.73\n",
      "Iteration 19653: with minibatch training loss = 0.612 and accuracy of 0.8\n",
      "Iteration 19654: with minibatch training loss = 0.542 and accuracy of 0.8\n",
      "Iteration 19655: with minibatch training loss = 0.551 and accuracy of 0.83\n",
      "Iteration 19656: with minibatch training loss = 0.626 and accuracy of 0.8\n",
      "Iteration 19657: with minibatch training loss = 0.446 and accuracy of 0.84\n",
      "Iteration 19658: with minibatch training loss = 0.562 and accuracy of 0.84\n",
      "Iteration 19659: with minibatch training loss = 0.923 and accuracy of 0.67\n",
      "Iteration 19660: with minibatch training loss = 1.02 and accuracy of 0.66\n",
      "Iteration 19661: with minibatch training loss = 0.525 and accuracy of 0.84\n",
      "Iteration 19662: with minibatch training loss = 0.716 and accuracy of 0.77\n",
      "Iteration 19663: with minibatch training loss = 0.397 and accuracy of 0.89\n",
      "Iteration 19664: with minibatch training loss = 0.762 and accuracy of 0.72\n",
      "Iteration 19665: with minibatch training loss = 0.831 and accuracy of 0.73\n",
      "Iteration 19666: with minibatch training loss = 0.516 and accuracy of 0.84\n",
      "Iteration 19667: with minibatch training loss = 0.475 and accuracy of 0.81\n",
      "Iteration 19668: with minibatch training loss = 0.439 and accuracy of 0.83\n",
      "Iteration 19669: with minibatch training loss = 0.853 and accuracy of 0.7\n",
      "Iteration 19670: with minibatch training loss = 0.385 and accuracy of 0.88\n",
      "Iteration 19671: with minibatch training loss = 0.534 and accuracy of 0.88\n",
      "Iteration 19672: with minibatch training loss = 0.539 and accuracy of 0.83\n",
      "Iteration 19673: with minibatch training loss = 0.635 and accuracy of 0.81\n",
      "Iteration 19674: with minibatch training loss = 0.617 and accuracy of 0.8\n",
      "Iteration 19675: with minibatch training loss = 0.488 and accuracy of 0.83\n",
      "Iteration 19676: with minibatch training loss = 0.409 and accuracy of 0.86\n",
      "Iteration 19677: with minibatch training loss = 0.665 and accuracy of 0.78\n",
      "Iteration 19678: with minibatch training loss = 0.509 and accuracy of 0.88\n",
      "Iteration 19679: with minibatch training loss = 0.618 and accuracy of 0.78\n",
      "Iteration 19680: with minibatch training loss = 0.607 and accuracy of 0.81\n",
      "Iteration 19681: with minibatch training loss = 0.714 and accuracy of 0.73\n",
      "Iteration 19682: with minibatch training loss = 0.667 and accuracy of 0.77\n",
      "Iteration 19683: with minibatch training loss = 0.682 and accuracy of 0.78\n",
      "Iteration 19684: with minibatch training loss = 0.445 and accuracy of 0.83\n",
      "Iteration 19685: with minibatch training loss = 0.62 and accuracy of 0.8\n",
      "Iteration 19686: with minibatch training loss = 0.657 and accuracy of 0.78\n",
      "Iteration 19687: with minibatch training loss = 0.468 and accuracy of 0.83\n",
      "Iteration 19688: with minibatch training loss = 0.581 and accuracy of 0.81\n",
      "Iteration 19689: with minibatch training loss = 0.558 and accuracy of 0.83\n",
      "Iteration 19690: with minibatch training loss = 0.734 and accuracy of 0.72\n",
      "Iteration 19691: with minibatch training loss = 0.616 and accuracy of 0.8\n",
      "Iteration 19692: with minibatch training loss = 0.738 and accuracy of 0.75\n",
      "Iteration 19693: with minibatch training loss = 0.679 and accuracy of 0.8\n",
      "Iteration 19694: with minibatch training loss = 0.379 and accuracy of 0.88\n",
      "Iteration 19695: with minibatch training loss = 0.675 and accuracy of 0.81\n",
      "Iteration 19696: with minibatch training loss = 0.506 and accuracy of 0.88\n",
      "Iteration 19697: with minibatch training loss = 0.77 and accuracy of 0.81\n",
      "Iteration 19698: with minibatch training loss = 0.44 and accuracy of 0.86\n",
      "Iteration 19699: with minibatch training loss = 0.778 and accuracy of 0.77\n",
      "Iteration 19700: with minibatch training loss = 0.45 and accuracy of 0.81\n",
      "Iteration 19701: with minibatch training loss = 0.624 and accuracy of 0.83\n",
      "Iteration 19702: with minibatch training loss = 0.712 and accuracy of 0.78\n",
      "Iteration 19703: with minibatch training loss = 0.571 and accuracy of 0.83\n",
      "Iteration 19704: with minibatch training loss = 0.708 and accuracy of 0.78\n",
      "Iteration 19705: with minibatch training loss = 0.677 and accuracy of 0.83\n",
      "Iteration 19706: with minibatch training loss = 0.684 and accuracy of 0.77\n",
      "Iteration 19707: with minibatch training loss = 0.534 and accuracy of 0.84\n",
      "Iteration 19708: with minibatch training loss = 0.616 and accuracy of 0.78\n",
      "Iteration 19709: with minibatch training loss = 0.53 and accuracy of 0.8\n",
      "Iteration 19710: with minibatch training loss = 0.749 and accuracy of 0.77\n",
      "Iteration 19711: with minibatch training loss = 0.656 and accuracy of 0.78\n",
      "Iteration 19712: with minibatch training loss = 0.815 and accuracy of 0.75\n",
      "Iteration 19713: with minibatch training loss = 0.423 and accuracy of 0.86\n",
      "Iteration 19714: with minibatch training loss = 0.891 and accuracy of 0.69\n",
      "Iteration 19715: with minibatch training loss = 0.66 and accuracy of 0.8\n",
      "Iteration 19716: with minibatch training loss = 0.553 and accuracy of 0.81\n",
      "Iteration 19717: with minibatch training loss = 0.791 and accuracy of 0.75\n",
      "Iteration 19718: with minibatch training loss = 0.883 and accuracy of 0.7\n",
      "Iteration 19719: with minibatch training loss = 0.603 and accuracy of 0.8\n",
      "Iteration 19720: with minibatch training loss = 0.635 and accuracy of 0.8\n",
      "Iteration 19721: with minibatch training loss = 0.505 and accuracy of 0.84\n",
      "Iteration 19722: with minibatch training loss = 0.644 and accuracy of 0.77\n",
      "Iteration 19723: with minibatch training loss = 0.632 and accuracy of 0.83\n",
      "Iteration 19724: with minibatch training loss = 0.679 and accuracy of 0.78\n",
      "Iteration 19725: with minibatch training loss = 0.93 and accuracy of 0.77\n",
      "Iteration 19726: with minibatch training loss = 0.728 and accuracy of 0.75\n",
      "Iteration 19727: with minibatch training loss = 0.688 and accuracy of 0.83\n",
      "Iteration 19728: with minibatch training loss = 0.826 and accuracy of 0.77\n",
      "Iteration 19729: with minibatch training loss = 0.675 and accuracy of 0.77\n",
      "Iteration 19730: with minibatch training loss = 0.381 and accuracy of 0.86\n",
      "Iteration 19731: with minibatch training loss = 0.576 and accuracy of 0.86\n",
      "Iteration 19732: with minibatch training loss = 0.542 and accuracy of 0.83\n",
      "Iteration 19733: with minibatch training loss = 0.671 and accuracy of 0.78\n",
      "Iteration 19734: with minibatch training loss = 0.602 and accuracy of 0.81\n",
      "Iteration 19735: with minibatch training loss = 0.633 and accuracy of 0.8\n",
      "Iteration 19736: with minibatch training loss = 0.778 and accuracy of 0.72\n",
      "Iteration 19737: with minibatch training loss = 0.643 and accuracy of 0.77\n",
      "Iteration 19738: with minibatch training loss = 0.712 and accuracy of 0.75\n",
      "Iteration 19739: with minibatch training loss = 0.621 and accuracy of 0.81\n",
      "Iteration 19740: with minibatch training loss = 0.509 and accuracy of 0.81\n",
      "Iteration 19741: with minibatch training loss = 0.518 and accuracy of 0.84\n",
      "Iteration 19742: with minibatch training loss = 0.694 and accuracy of 0.73\n",
      "Iteration 19743: with minibatch training loss = 0.803 and accuracy of 0.7\n",
      "Iteration 19744: with minibatch training loss = 0.668 and accuracy of 0.77\n",
      "Iteration 19745: with minibatch training loss = 0.947 and accuracy of 0.67\n",
      "Iteration 19746: with minibatch training loss = 0.488 and accuracy of 0.86\n",
      "Iteration 19747: with minibatch training loss = 0.74 and accuracy of 0.78\n",
      "Iteration 19748: with minibatch training loss = 0.427 and accuracy of 0.88\n",
      "Iteration 19749: with minibatch training loss = 0.444 and accuracy of 0.89\n",
      "Iteration 19750: with minibatch training loss = 0.353 and accuracy of 0.89\n",
      "Iteration 19751: with minibatch training loss = 0.255 and accuracy of 0.92\n",
      "Iteration 19752: with minibatch training loss = 0.814 and accuracy of 0.73\n",
      "Iteration 19753: with minibatch training loss = 0.572 and accuracy of 0.83\n",
      "Iteration 19754: with minibatch training loss = 0.537 and accuracy of 0.81\n",
      "Iteration 19755: with minibatch training loss = 0.5 and accuracy of 0.84\n",
      "Iteration 19756: with minibatch training loss = 0.622 and accuracy of 0.8\n",
      "Iteration 19757: with minibatch training loss = 0.699 and accuracy of 0.77\n",
      "Iteration 19758: with minibatch training loss = 0.404 and accuracy of 0.89\n",
      "Iteration 19759: with minibatch training loss = 0.644 and accuracy of 0.8\n",
      "Iteration 19760: with minibatch training loss = 0.364 and accuracy of 0.89\n",
      "Iteration 19761: with minibatch training loss = 0.377 and accuracy of 0.89\n",
      "Iteration 19762: with minibatch training loss = 0.878 and accuracy of 0.7\n",
      "Iteration 19763: with minibatch training loss = 0.695 and accuracy of 0.78\n",
      "Iteration 19764: with minibatch training loss = 0.563 and accuracy of 0.8\n",
      "Iteration 19765: with minibatch training loss = 0.639 and accuracy of 0.8\n",
      "Iteration 19766: with minibatch training loss = 0.425 and accuracy of 0.86\n",
      "Iteration 19767: with minibatch training loss = 0.903 and accuracy of 0.7\n",
      "Iteration 19768: with minibatch training loss = 0.707 and accuracy of 0.8\n",
      "Iteration 19769: with minibatch training loss = 0.427 and accuracy of 0.89\n",
      "Iteration 19770: with minibatch training loss = 0.717 and accuracy of 0.78\n",
      "Iteration 19771: with minibatch training loss = 0.858 and accuracy of 0.75\n",
      "Iteration 19772: with minibatch training loss = 0.877 and accuracy of 0.72\n",
      "Iteration 19773: with minibatch training loss = 0.59 and accuracy of 0.81\n",
      "Iteration 19774: with minibatch training loss = 0.727 and accuracy of 0.81\n",
      "Iteration 19775: with minibatch training loss = 0.355 and accuracy of 0.88\n",
      "Iteration 19776: with minibatch training loss = 0.494 and accuracy of 0.86\n",
      "Iteration 19777: with minibatch training loss = 0.723 and accuracy of 0.78\n",
      "Iteration 19778: with minibatch training loss = 0.554 and accuracy of 0.84\n",
      "Iteration 19779: with minibatch training loss = 0.388 and accuracy of 0.88\n",
      "Iteration 19780: with minibatch training loss = 0.908 and accuracy of 0.7\n",
      "Iteration 19781: with minibatch training loss = 0.666 and accuracy of 0.75\n",
      "Iteration 19782: with minibatch training loss = 0.665 and accuracy of 0.77\n",
      "Iteration 19783: with minibatch training loss = 0.524 and accuracy of 0.84\n",
      "Iteration 19784: with minibatch training loss = 0.554 and accuracy of 0.81\n",
      "Iteration 19785: with minibatch training loss = 0.439 and accuracy of 0.89\n",
      "Iteration 19786: with minibatch training loss = 0.66 and accuracy of 0.8\n",
      "Iteration 19787: with minibatch training loss = 0.548 and accuracy of 0.81\n",
      "Iteration 19788: with minibatch training loss = 0.667 and accuracy of 0.81\n",
      "Iteration 19789: with minibatch training loss = 0.603 and accuracy of 0.81\n",
      "Iteration 19790: with minibatch training loss = 0.521 and accuracy of 0.83\n",
      "Iteration 19791: with minibatch training loss = 0.527 and accuracy of 0.83\n",
      "Iteration 19792: with minibatch training loss = 0.597 and accuracy of 0.78\n",
      "Iteration 19793: with minibatch training loss = 0.765 and accuracy of 0.75\n",
      "Iteration 19794: with minibatch training loss = 0.599 and accuracy of 0.83\n",
      "Iteration 19795: with minibatch training loss = 0.756 and accuracy of 0.73\n",
      "Iteration 19796: with minibatch training loss = 0.795 and accuracy of 0.72\n",
      "Iteration 19797: with minibatch training loss = 0.546 and accuracy of 0.8\n",
      "Iteration 19798: with minibatch training loss = 0.523 and accuracy of 0.83\n",
      "Iteration 19799: with minibatch training loss = 0.731 and accuracy of 0.77\n",
      "Iteration 19800: with minibatch training loss = 0.567 and accuracy of 0.8\n",
      "Iteration 19801: with minibatch training loss = 0.798 and accuracy of 0.72\n",
      "Iteration 19802: with minibatch training loss = 0.628 and accuracy of 0.77\n",
      "Iteration 19803: with minibatch training loss = 0.647 and accuracy of 0.8\n",
      "Iteration 19804: with minibatch training loss = 0.581 and accuracy of 0.81\n",
      "Iteration 19805: with minibatch training loss = 0.63 and accuracy of 0.78\n",
      "Iteration 19806: with minibatch training loss = 0.592 and accuracy of 0.84\n",
      "Iteration 19807: with minibatch training loss = 0.397 and accuracy of 0.88\n",
      "Iteration 19808: with minibatch training loss = 0.673 and accuracy of 0.75\n",
      "Iteration 19809: with minibatch training loss = 0.772 and accuracy of 0.77\n",
      "Iteration 19810: with minibatch training loss = 0.762 and accuracy of 0.77\n",
      "Iteration 19811: with minibatch training loss = 0.511 and accuracy of 0.84\n",
      "Iteration 19812: with minibatch training loss = 0.498 and accuracy of 0.83\n",
      "Iteration 19813: with minibatch training loss = 0.649 and accuracy of 0.81\n",
      "Iteration 19814: with minibatch training loss = 0.738 and accuracy of 0.8\n",
      "Iteration 19815: with minibatch training loss = 0.722 and accuracy of 0.77\n",
      "Iteration 19816: with minibatch training loss = 0.829 and accuracy of 0.7\n",
      "Iteration 19817: with minibatch training loss = 0.771 and accuracy of 0.75\n",
      "Iteration 19818: with minibatch training loss = 0.539 and accuracy of 0.83\n",
      "Iteration 19819: with minibatch training loss = 0.683 and accuracy of 0.77\n",
      "Iteration 19820: with minibatch training loss = 0.588 and accuracy of 0.8\n",
      "Iteration 19821: with minibatch training loss = 0.48 and accuracy of 0.86\n",
      "Iteration 19822: with minibatch training loss = 0.74 and accuracy of 0.75\n",
      "Iteration 19823: with minibatch training loss = 0.545 and accuracy of 0.83\n",
      "Iteration 19824: with minibatch training loss = 0.56 and accuracy of 0.8\n",
      "Iteration 19825: with minibatch training loss = 0.69 and accuracy of 0.78\n",
      "Iteration 19826: with minibatch training loss = 0.566 and accuracy of 0.81\n",
      "Iteration 19827: with minibatch training loss = 0.468 and accuracy of 0.84\n",
      "Iteration 19828: with minibatch training loss = 0.608 and accuracy of 0.81\n",
      "Iteration 19829: with minibatch training loss = 0.501 and accuracy of 0.84\n",
      "Iteration 19830: with minibatch training loss = 0.548 and accuracy of 0.81\n",
      "Iteration 19831: with minibatch training loss = 0.58 and accuracy of 0.83\n",
      "Iteration 19832: with minibatch training loss = 0.77 and accuracy of 0.72\n",
      "Iteration 19833: with minibatch training loss = 0.495 and accuracy of 0.83\n",
      "Iteration 19834: with minibatch training loss = 0.496 and accuracy of 0.84\n",
      "Iteration 19835: with minibatch training loss = 0.615 and accuracy of 0.8\n",
      "Iteration 19836: with minibatch training loss = 0.606 and accuracy of 0.8\n",
      "Iteration 19837: with minibatch training loss = 0.689 and accuracy of 0.83\n",
      "Iteration 19838: with minibatch training loss = 0.636 and accuracy of 0.78\n",
      "Iteration 19839: with minibatch training loss = 0.69 and accuracy of 0.75\n",
      "Iteration 19840: with minibatch training loss = 0.667 and accuracy of 0.77\n",
      "Iteration 19841: with minibatch training loss = 0.59 and accuracy of 0.83\n",
      "Iteration 19842: with minibatch training loss = 0.777 and accuracy of 0.73\n",
      "Iteration 19843: with minibatch training loss = 0.617 and accuracy of 0.77\n",
      "Iteration 19844: with minibatch training loss = 0.494 and accuracy of 0.83\n",
      "Iteration 19845: with minibatch training loss = 0.593 and accuracy of 0.8\n",
      "Iteration 19846: with minibatch training loss = 0.574 and accuracy of 0.81\n",
      "Iteration 19847: with minibatch training loss = 0.658 and accuracy of 0.8\n",
      "Iteration 19848: with minibatch training loss = 0.508 and accuracy of 0.83\n",
      "Iteration 19849: with minibatch training loss = 1.06 and accuracy of 0.64\n",
      "Iteration 19850: with minibatch training loss = 0.668 and accuracy of 0.78\n",
      "Iteration 19851: with minibatch training loss = 0.687 and accuracy of 0.8\n",
      "Iteration 19852: with minibatch training loss = 0.735 and accuracy of 0.77\n",
      "Iteration 19853: with minibatch training loss = 0.385 and accuracy of 0.89\n",
      "Iteration 19854: with minibatch training loss = 0.426 and accuracy of 0.84\n",
      "Iteration 19855: with minibatch training loss = 0.529 and accuracy of 0.81\n",
      "Iteration 19856: with minibatch training loss = 0.556 and accuracy of 0.84\n",
      "Iteration 19857: with minibatch training loss = 0.567 and accuracy of 0.81\n",
      "Iteration 19858: with minibatch training loss = 0.397 and accuracy of 0.86\n",
      "Iteration 19859: with minibatch training loss = 0.517 and accuracy of 0.84\n",
      "Iteration 19860: with minibatch training loss = 0.767 and accuracy of 0.73\n",
      "Iteration 19861: with minibatch training loss = 0.648 and accuracy of 0.8\n",
      "Iteration 19862: with minibatch training loss = 0.74 and accuracy of 0.75\n",
      "Iteration 19863: with minibatch training loss = 0.982 and accuracy of 0.67\n",
      "Iteration 19864: with minibatch training loss = 0.598 and accuracy of 0.83\n",
      "Iteration 19865: with minibatch training loss = 0.626 and accuracy of 0.81\n",
      "Iteration 19866: with minibatch training loss = 0.672 and accuracy of 0.77\n",
      "Iteration 19867: with minibatch training loss = 0.552 and accuracy of 0.83\n",
      "Iteration 19868: with minibatch training loss = 0.457 and accuracy of 0.88\n",
      "Iteration 19869: with minibatch training loss = 0.672 and accuracy of 0.77\n",
      "Iteration 19870: with minibatch training loss = 0.618 and accuracy of 0.78\n",
      "Iteration 19871: with minibatch training loss = 0.668 and accuracy of 0.75\n",
      "Iteration 19872: with minibatch training loss = 0.573 and accuracy of 0.81\n",
      "Iteration 19873: with minibatch training loss = 0.519 and accuracy of 0.83\n",
      "Iteration 19874: with minibatch training loss = 0.699 and accuracy of 0.75\n",
      "Iteration 19875: with minibatch training loss = 0.642 and accuracy of 0.81\n",
      "Iteration 19876: with minibatch training loss = 0.58 and accuracy of 0.88\n",
      "Iteration 19877: with minibatch training loss = 0.502 and accuracy of 0.84\n",
      "Iteration 19878: with minibatch training loss = 0.622 and accuracy of 0.8\n",
      "Iteration 19879: with minibatch training loss = 0.587 and accuracy of 0.81\n",
      "Iteration 19880: with minibatch training loss = 0.584 and accuracy of 0.8\n",
      "Iteration 19881: with minibatch training loss = 0.588 and accuracy of 0.78\n",
      "Iteration 19882: with minibatch training loss = 0.502 and accuracy of 0.86\n",
      "Iteration 19883: with minibatch training loss = 0.735 and accuracy of 0.73\n",
      "Iteration 19884: with minibatch training loss = 0.872 and accuracy of 0.7\n",
      "Iteration 19885: with minibatch training loss = 0.464 and accuracy of 0.83\n",
      "Iteration 19886: with minibatch training loss = 0.71 and accuracy of 0.77\n",
      "Iteration 19887: with minibatch training loss = 0.449 and accuracy of 0.86\n",
      "Iteration 19888: with minibatch training loss = 0.615 and accuracy of 0.78\n",
      "Iteration 19889: with minibatch training loss = 0.519 and accuracy of 0.84\n",
      "Iteration 19890: with minibatch training loss = 0.682 and accuracy of 0.8\n",
      "Iteration 19891: with minibatch training loss = 0.627 and accuracy of 0.77\n",
      "Iteration 19892: with minibatch training loss = 0.6 and accuracy of 0.83\n",
      "Iteration 19893: with minibatch training loss = 0.663 and accuracy of 0.78\n",
      "Iteration 19894: with minibatch training loss = 0.823 and accuracy of 0.72\n",
      "Iteration 19895: with minibatch training loss = 0.662 and accuracy of 0.77\n",
      "Iteration 19896: with minibatch training loss = 0.487 and accuracy of 0.84\n",
      "Iteration 19897: with minibatch training loss = 0.663 and accuracy of 0.77\n",
      "Iteration 19898: with minibatch training loss = 0.705 and accuracy of 0.77\n",
      "Iteration 19899: with minibatch training loss = 0.46 and accuracy of 0.84\n",
      "Iteration 19900: with minibatch training loss = 0.458 and accuracy of 0.84\n",
      "Iteration 19901: with minibatch training loss = 0.587 and accuracy of 0.8\n",
      "Iteration 19902: with minibatch training loss = 0.597 and accuracy of 0.81\n",
      "Iteration 19903: with minibatch training loss = 0.649 and accuracy of 0.8\n",
      "Iteration 19904: with minibatch training loss = 0.642 and accuracy of 0.81\n",
      "Iteration 19905: with minibatch training loss = 0.671 and accuracy of 0.78\n",
      "Iteration 19906: with minibatch training loss = 0.508 and accuracy of 0.84\n",
      "Iteration 19907: with minibatch training loss = 0.593 and accuracy of 0.81\n",
      "Iteration 19908: with minibatch training loss = 0.483 and accuracy of 0.86\n",
      "Iteration 19909: with minibatch training loss = 0.459 and accuracy of 0.88\n",
      "Iteration 19910: with minibatch training loss = 0.224 and accuracy of 0.92\n",
      "Iteration 19911: with minibatch training loss = 0.597 and accuracy of 0.81\n",
      "Iteration 19912: with minibatch training loss = 0.63 and accuracy of 0.8\n",
      "Iteration 19913: with minibatch training loss = 0.569 and accuracy of 0.81\n",
      "Iteration 19914: with minibatch training loss = 0.665 and accuracy of 0.78\n",
      "Iteration 19915: with minibatch training loss = 0.564 and accuracy of 0.86\n",
      "Iteration 19916: with minibatch training loss = 0.542 and accuracy of 0.84\n",
      "Iteration 19917: with minibatch training loss = 0.636 and accuracy of 0.8\n",
      "Iteration 19918: with minibatch training loss = 0.452 and accuracy of 0.86\n",
      "Iteration 19919: with minibatch training loss = 0.51 and accuracy of 0.86\n",
      "Iteration 19920: with minibatch training loss = 0.928 and accuracy of 0.69\n",
      "Iteration 19921: with minibatch training loss = 0.486 and accuracy of 0.84\n",
      "Iteration 19922: with minibatch training loss = 0.439 and accuracy of 0.88\n",
      "Iteration 19923: with minibatch training loss = 0.708 and accuracy of 0.78\n",
      "Iteration 19924: with minibatch training loss = 0.457 and accuracy of 0.88\n",
      "Iteration 19925: with minibatch training loss = 0.371 and accuracy of 0.86\n",
      "Iteration 19926: with minibatch training loss = 0.665 and accuracy of 0.78\n",
      "Iteration 19927: with minibatch training loss = 0.804 and accuracy of 0.75\n",
      "Iteration 19928: with minibatch training loss = 0.575 and accuracy of 0.78\n",
      "Iteration 19929: with minibatch training loss = 0.63 and accuracy of 0.81\n",
      "Iteration 19930: with minibatch training loss = 0.464 and accuracy of 0.84\n",
      "Iteration 19931: with minibatch training loss = 0.444 and accuracy of 0.84\n",
      "Iteration 19932: with minibatch training loss = 0.595 and accuracy of 0.8\n",
      "Iteration 19933: with minibatch training loss = 0.583 and accuracy of 0.81\n",
      "Iteration 19934: with minibatch training loss = 0.646 and accuracy of 0.78\n",
      "Iteration 19935: with minibatch training loss = 0.694 and accuracy of 0.78\n",
      "Iteration 19936: with minibatch training loss = 0.662 and accuracy of 0.83\n",
      "Iteration 19937: with minibatch training loss = 0.529 and accuracy of 0.88\n",
      "Iteration 19938: with minibatch training loss = 0.676 and accuracy of 0.77\n",
      "Iteration 19939: with minibatch training loss = 0.594 and accuracy of 0.8\n",
      "Iteration 19940: with minibatch training loss = 0.578 and accuracy of 0.81\n",
      "Iteration 19941: with minibatch training loss = 0.605 and accuracy of 0.8\n",
      "Iteration 19942: with minibatch training loss = 0.524 and accuracy of 0.83\n",
      "Iteration 19943: with minibatch training loss = 0.676 and accuracy of 0.8\n",
      "Iteration 19944: with minibatch training loss = 0.742 and accuracy of 0.75\n",
      "Iteration 19945: with minibatch training loss = 0.833 and accuracy of 0.72\n",
      "Iteration 19946: with minibatch training loss = 0.582 and accuracy of 0.81\n",
      "Iteration 19947: with minibatch training loss = 0.576 and accuracy of 0.8\n",
      "Iteration 19948: with minibatch training loss = 0.781 and accuracy of 0.77\n",
      "Iteration 19949: with minibatch training loss = 0.564 and accuracy of 0.81\n",
      "Iteration 19950: with minibatch training loss = 0.607 and accuracy of 0.83\n",
      "Iteration 19951: with minibatch training loss = 0.67 and accuracy of 0.78\n",
      "Iteration 19952: with minibatch training loss = 0.651 and accuracy of 0.77\n",
      "Iteration 19953: with minibatch training loss = 0.747 and accuracy of 0.78\n",
      "Iteration 19954: with minibatch training loss = 0.628 and accuracy of 0.8\n",
      "Iteration 19955: with minibatch training loss = 0.404 and accuracy of 0.89\n",
      "Iteration 19956: with minibatch training loss = 0.561 and accuracy of 0.84\n",
      "Iteration 19957: with minibatch training loss = 0.694 and accuracy of 0.77\n",
      "Iteration 19958: with minibatch training loss = 0.837 and accuracy of 0.72\n",
      "Iteration 19959: with minibatch training loss = 0.674 and accuracy of 0.77\n",
      "Iteration 19960: with minibatch training loss = 0.811 and accuracy of 0.72\n",
      "Iteration 19961: with minibatch training loss = 0.687 and accuracy of 0.8\n",
      "Iteration 19962: with minibatch training loss = 0.642 and accuracy of 0.81\n",
      "Iteration 19963: with minibatch training loss = 0.483 and accuracy of 0.84\n",
      "Iteration 19964: with minibatch training loss = 0.691 and accuracy of 0.75\n",
      "Iteration 19965: with minibatch training loss = 0.398 and accuracy of 0.88\n",
      "Iteration 19966: with minibatch training loss = 0.663 and accuracy of 0.75\n",
      "Iteration 19967: with minibatch training loss = 0.505 and accuracy of 0.83\n",
      "Iteration 19968: with minibatch training loss = 0.601 and accuracy of 0.81\n",
      "Iteration 19969: with minibatch training loss = 0.532 and accuracy of 0.83\n",
      "Iteration 19970: with minibatch training loss = 0.518 and accuracy of 0.83\n",
      "Iteration 19971: with minibatch training loss = 0.511 and accuracy of 0.83\n",
      "Iteration 19972: with minibatch training loss = 0.516 and accuracy of 0.84\n",
      "Iteration 19973: with minibatch training loss = 0.699 and accuracy of 0.8\n",
      "Iteration 19974: with minibatch training loss = 0.633 and accuracy of 0.8\n",
      "Iteration 19975: with minibatch training loss = 0.399 and accuracy of 0.86\n",
      "Iteration 19976: with minibatch training loss = 0.51 and accuracy of 0.86\n",
      "Iteration 19977: with minibatch training loss = 0.671 and accuracy of 0.83\n",
      "Iteration 19978: with minibatch training loss = 0.802 and accuracy of 0.73\n",
      "Iteration 19979: with minibatch training loss = 0.799 and accuracy of 0.72\n",
      "Iteration 19980: with minibatch training loss = 0.608 and accuracy of 0.86\n",
      "Iteration 19981: with minibatch training loss = 0.658 and accuracy of 0.78\n",
      "Iteration 19982: with minibatch training loss = 0.364 and accuracy of 0.88\n",
      "Iteration 19983: with minibatch training loss = 0.766 and accuracy of 0.73\n",
      "Iteration 19984: with minibatch training loss = 0.753 and accuracy of 0.73\n",
      "Iteration 19985: with minibatch training loss = 0.49 and accuracy of 0.84\n",
      "Iteration 19986: with minibatch training loss = 0.73 and accuracy of 0.78\n",
      "Iteration 19987: with minibatch training loss = 0.332 and accuracy of 0.89\n",
      "Iteration 19988: with minibatch training loss = 0.45 and accuracy of 0.84\n",
      "Iteration 19989: with minibatch training loss = 0.513 and accuracy of 0.81\n",
      "Iteration 19990: with minibatch training loss = 0.954 and accuracy of 0.62\n",
      "Iteration 19991: with minibatch training loss = 0.656 and accuracy of 0.77\n",
      "Iteration 19992: with minibatch training loss = 0.481 and accuracy of 0.83\n",
      "Iteration 19993: with minibatch training loss = 0.698 and accuracy of 0.78\n",
      "Iteration 19994: with minibatch training loss = 0.854 and accuracy of 0.75\n",
      "Iteration 19995: with minibatch training loss = 0.384 and accuracy of 0.88\n",
      "Iteration 19996: with minibatch training loss = 0.602 and accuracy of 0.78\n",
      "Iteration 19997: with minibatch training loss = 0.827 and accuracy of 0.77\n",
      "Iteration 19998: with minibatch training loss = 0.69 and accuracy of 0.78\n",
      "Iteration 19999: with minibatch training loss = 0.546 and accuracy of 0.81\n",
      "Iteration 20000: with minibatch training loss = 0.672 and accuracy of 0.78\n",
      "Iteration 20001: with minibatch training loss = 0.876 and accuracy of 0.7\n",
      "Iteration 20002: with minibatch training loss = 0.404 and accuracy of 0.89\n",
      "Iteration 20003: with minibatch training loss = 0.484 and accuracy of 0.86\n",
      "Iteration 20004: with minibatch training loss = 0.62 and accuracy of 0.78\n",
      "Iteration 20005: with minibatch training loss = 0.847 and accuracy of 0.75\n",
      "Iteration 20006: with minibatch training loss = 0.636 and accuracy of 0.8\n",
      "Iteration 20007: with minibatch training loss = 0.651 and accuracy of 0.83\n",
      "Iteration 20008: with minibatch training loss = 0.716 and accuracy of 0.8\n",
      "Iteration 20009: with minibatch training loss = 0.628 and accuracy of 0.8\n",
      "Iteration 20010: with minibatch training loss = 0.569 and accuracy of 0.81\n",
      "Iteration 20011: with minibatch training loss = 0.542 and accuracy of 0.81\n",
      "Iteration 20012: with minibatch training loss = 0.606 and accuracy of 0.78\n",
      "Iteration 20013: with minibatch training loss = 0.767 and accuracy of 0.73\n",
      "Iteration 20014: with minibatch training loss = 0.765 and accuracy of 0.8\n",
      "Iteration 20015: with minibatch training loss = 0.511 and accuracy of 0.83\n",
      "Iteration 20016: with minibatch training loss = 0.629 and accuracy of 0.81\n",
      "Iteration 20017: with minibatch training loss = 0.714 and accuracy of 0.75\n",
      "Iteration 20018: with minibatch training loss = 0.527 and accuracy of 0.83\n",
      "Iteration 20019: with minibatch training loss = 0.815 and accuracy of 0.77\n",
      "Iteration 20020: with minibatch training loss = 0.47 and accuracy of 0.83\n",
      "Iteration 20021: with minibatch training loss = 0.702 and accuracy of 0.75\n",
      "Iteration 20022: with minibatch training loss = 0.416 and accuracy of 0.89\n",
      "Iteration 20023: with minibatch training loss = 0.367 and accuracy of 0.88\n",
      "Iteration 20024: with minibatch training loss = 0.589 and accuracy of 0.83\n",
      "Iteration 20025: with minibatch training loss = 0.82 and accuracy of 0.72\n",
      "Iteration 20026: with minibatch training loss = 0.808 and accuracy of 0.73\n",
      "Iteration 20027: with minibatch training loss = 0.91 and accuracy of 0.77\n",
      "Iteration 20028: with minibatch training loss = 0.753 and accuracy of 0.73\n",
      "Iteration 20029: with minibatch training loss = 0.564 and accuracy of 0.8\n",
      "Iteration 20030: with minibatch training loss = 0.648 and accuracy of 0.8\n",
      "Iteration 20031: with minibatch training loss = 0.645 and accuracy of 0.8\n",
      "Iteration 20032: with minibatch training loss = 0.592 and accuracy of 0.8\n",
      "Iteration 20033: with minibatch training loss = 0.411 and accuracy of 0.89\n",
      "Iteration 20034: with minibatch training loss = 0.577 and accuracy of 0.78\n",
      "Iteration 20035: with minibatch training loss = 0.966 and accuracy of 0.69\n",
      "Iteration 20036: with minibatch training loss = 0.416 and accuracy of 0.84\n",
      "Iteration 20037: with minibatch training loss = 0.546 and accuracy of 0.83\n",
      "Iteration 20038: with minibatch training loss = 0.698 and accuracy of 0.78\n",
      "Iteration 20039: with minibatch training loss = 0.47 and accuracy of 0.86\n",
      "Iteration 20040: with minibatch training loss = 0.614 and accuracy of 0.8\n",
      "Iteration 20041: with minibatch training loss = 0.567 and accuracy of 0.81\n",
      "Iteration 20042: with minibatch training loss = 0.65 and accuracy of 0.78\n",
      "Iteration 20043: with minibatch training loss = 0.546 and accuracy of 0.83\n",
      "Iteration 20044: with minibatch training loss = 0.627 and accuracy of 0.8\n",
      "Iteration 20045: with minibatch training loss = 0.528 and accuracy of 0.83\n",
      "Iteration 20046: with minibatch training loss = 0.849 and accuracy of 0.7\n",
      "Iteration 20047: with minibatch training loss = 0.767 and accuracy of 0.73\n",
      "Iteration 20048: with minibatch training loss = 0.507 and accuracy of 0.83\n",
      "Iteration 20049: with minibatch training loss = 0.661 and accuracy of 0.77\n",
      "Iteration 20050: with minibatch training loss = 0.676 and accuracy of 0.78\n",
      "Iteration 20051: with minibatch training loss = 0.708 and accuracy of 0.77\n",
      "Iteration 20052: with minibatch training loss = 0.902 and accuracy of 0.7\n",
      "Iteration 20053: with minibatch training loss = 0.642 and accuracy of 0.78\n",
      "Iteration 20054: with minibatch training loss = 0.393 and accuracy of 0.89\n",
      "Iteration 20055: with minibatch training loss = 0.636 and accuracy of 0.83\n",
      "Iteration 20056: with minibatch training loss = 0.784 and accuracy of 0.77\n",
      "Iteration 20057: with minibatch training loss = 0.633 and accuracy of 0.81\n",
      "Iteration 20058: with minibatch training loss = 0.493 and accuracy of 0.89\n",
      "Iteration 20059: with minibatch training loss = 0.599 and accuracy of 0.8\n",
      "Iteration 20060: with minibatch training loss = 0.721 and accuracy of 0.75\n",
      "Iteration 20061: with minibatch training loss = 0.698 and accuracy of 0.75\n",
      "Iteration 20062: with minibatch training loss = 0.5 and accuracy of 0.83\n",
      "Iteration 20063: with minibatch training loss = 0.607 and accuracy of 0.83\n",
      "Iteration 20064: with minibatch training loss = 0.379 and accuracy of 0.89\n",
      "Iteration 20065: with minibatch training loss = 0.486 and accuracy of 0.88\n",
      "Iteration 20066: with minibatch training loss = 0.693 and accuracy of 0.77\n",
      "Iteration 20067: with minibatch training loss = 0.661 and accuracy of 0.78\n",
      "Iteration 20068: with minibatch training loss = 0.582 and accuracy of 0.78\n",
      "Iteration 20069: with minibatch training loss = 0.52 and accuracy of 0.83\n",
      "Iteration 20070: with minibatch training loss = 0.502 and accuracy of 0.84\n",
      "Iteration 20071: with minibatch training loss = 0.561 and accuracy of 0.83\n",
      "Iteration 20072: with minibatch training loss = 0.536 and accuracy of 0.83\n",
      "Iteration 20073: with minibatch training loss = 0.661 and accuracy of 0.75\n",
      "Iteration 20074: with minibatch training loss = 0.655 and accuracy of 0.8\n",
      "Iteration 20075: with minibatch training loss = 0.673 and accuracy of 0.8\n",
      "Iteration 20076: with minibatch training loss = 0.475 and accuracy of 0.86\n",
      "Iteration 20077: with minibatch training loss = 0.696 and accuracy of 0.75\n",
      "Iteration 20078: with minibatch training loss = 0.756 and accuracy of 0.75\n",
      "Iteration 20079: with minibatch training loss = 0.797 and accuracy of 0.72\n",
      "Iteration 20080: with minibatch training loss = 0.61 and accuracy of 0.83\n",
      "Iteration 20081: with minibatch training loss = 0.699 and accuracy of 0.81\n",
      "Iteration 20082: with minibatch training loss = 0.66 and accuracy of 0.75\n",
      "Iteration 20083: with minibatch training loss = 0.753 and accuracy of 0.75\n",
      "Iteration 20084: with minibatch training loss = 0.715 and accuracy of 0.77\n",
      "Iteration 20085: with minibatch training loss = 0.572 and accuracy of 0.8\n",
      "Iteration 20086: with minibatch training loss = 0.549 and accuracy of 0.83\n",
      "Iteration 20087: with minibatch training loss = 0.691 and accuracy of 0.75\n",
      "Iteration 20088: with minibatch training loss = 0.644 and accuracy of 0.78\n",
      "Iteration 20089: with minibatch training loss = 0.647 and accuracy of 0.8\n",
      "Iteration 20090: with minibatch training loss = 0.539 and accuracy of 0.83\n",
      "Iteration 20091: with minibatch training loss = 0.478 and accuracy of 0.84\n",
      "Iteration 20092: with minibatch training loss = 0.72 and accuracy of 0.75\n",
      "Iteration 20093: with minibatch training loss = 0.658 and accuracy of 0.77\n",
      "Iteration 20094: with minibatch training loss = 0.519 and accuracy of 0.84\n",
      "Iteration 20095: with minibatch training loss = 0.705 and accuracy of 0.81\n",
      "Iteration 20096: with minibatch training loss = 0.472 and accuracy of 0.88\n",
      "Iteration 20097: with minibatch training loss = 0.569 and accuracy of 0.8\n",
      "Iteration 20098: with minibatch training loss = 0.477 and accuracy of 0.83\n",
      "Iteration 20099: with minibatch training loss = 0.669 and accuracy of 0.75\n",
      "Iteration 20100: with minibatch training loss = 0.522 and accuracy of 0.83\n",
      "Iteration 20101: with minibatch training loss = 0.441 and accuracy of 0.84\n",
      "Iteration 20102: with minibatch training loss = 0.664 and accuracy of 0.8\n",
      "Iteration 20103: with minibatch training loss = 0.801 and accuracy of 0.75\n",
      "Iteration 20104: with minibatch training loss = 0.435 and accuracy of 0.84\n",
      "Iteration 20105: with minibatch training loss = 0.576 and accuracy of 0.78\n",
      "Iteration 20106: with minibatch training loss = 0.605 and accuracy of 0.78\n",
      "Iteration 20107: with minibatch training loss = 0.469 and accuracy of 0.86\n",
      "Iteration 20108: with minibatch training loss = 0.571 and accuracy of 0.84\n",
      "Iteration 20109: with minibatch training loss = 0.702 and accuracy of 0.77\n",
      "Iteration 20110: with minibatch training loss = 0.587 and accuracy of 0.8\n",
      "Iteration 20111: with minibatch training loss = 0.566 and accuracy of 0.84\n",
      "Iteration 20112: with minibatch training loss = 0.401 and accuracy of 0.86\n",
      "Iteration 20113: with minibatch training loss = 0.465 and accuracy of 0.88\n",
      "Iteration 20114: with minibatch training loss = 0.572 and accuracy of 0.83\n",
      "Iteration 20115: with minibatch training loss = 0.594 and accuracy of 0.81\n",
      "Iteration 20116: with minibatch training loss = 0.668 and accuracy of 0.81\n",
      "Iteration 20117: with minibatch training loss = 0.47 and accuracy of 0.84\n",
      "Iteration 20118: with minibatch training loss = 0.838 and accuracy of 0.75\n",
      "Iteration 20119: with minibatch training loss = 0.544 and accuracy of 0.81\n",
      "Iteration 20120: with minibatch training loss = 0.665 and accuracy of 0.78\n",
      "Iteration 20121: with minibatch training loss = 0.753 and accuracy of 0.78\n",
      "Iteration 20122: with minibatch training loss = 0.6 and accuracy of 0.83\n",
      "Iteration 20123: with minibatch training loss = 0.474 and accuracy of 0.88\n",
      "Iteration 20124: with minibatch training loss = 0.496 and accuracy of 0.86\n",
      "Iteration 20125: with minibatch training loss = 0.581 and accuracy of 0.81\n",
      "Iteration 20126: with minibatch training loss = 0.722 and accuracy of 0.77\n",
      "Iteration 20127: with minibatch training loss = 0.684 and accuracy of 0.8\n",
      "Iteration 20128: with minibatch training loss = 0.731 and accuracy of 0.75\n",
      "Iteration 20129: with minibatch training loss = 0.478 and accuracy of 0.84\n",
      "Iteration 20130: with minibatch training loss = 0.746 and accuracy of 0.75\n",
      "Iteration 20131: with minibatch training loss = 0.588 and accuracy of 0.83\n",
      "Iteration 20132: with minibatch training loss = 0.388 and accuracy of 0.91\n",
      "Iteration 20133: with minibatch training loss = 0.615 and accuracy of 0.83\n",
      "Iteration 20134: with minibatch training loss = 0.513 and accuracy of 0.88\n",
      "Iteration 20135: with minibatch training loss = 0.561 and accuracy of 0.84\n",
      "Iteration 20136: with minibatch training loss = 0.901 and accuracy of 0.72\n",
      "Iteration 20137: with minibatch training loss = 0.541 and accuracy of 0.83\n",
      "Iteration 20138: with minibatch training loss = 0.687 and accuracy of 0.75\n",
      "Iteration 20139: with minibatch training loss = 0.412 and accuracy of 0.89\n",
      "Iteration 20140: with minibatch training loss = 0.693 and accuracy of 0.83\n",
      "Iteration 20141: with minibatch training loss = 0.981 and accuracy of 0.69\n",
      "Iteration 20142: with minibatch training loss = 0.8 and accuracy of 0.73\n",
      "Iteration 20143: with minibatch training loss = 0.63 and accuracy of 0.8\n",
      "Iteration 20144: with minibatch training loss = 0.565 and accuracy of 0.83\n",
      "Iteration 20145: with minibatch training loss = 0.824 and accuracy of 0.75\n",
      "Iteration 20146: with minibatch training loss = 0.714 and accuracy of 0.73\n",
      "Iteration 20147: with minibatch training loss = 0.484 and accuracy of 0.83\n",
      "Iteration 20148: with minibatch training loss = 0.597 and accuracy of 0.81\n",
      "Iteration 20149: with minibatch training loss = 0.981 and accuracy of 0.69\n",
      "Iteration 20150: with minibatch training loss = 0.686 and accuracy of 0.77\n",
      "Iteration 20151: with minibatch training loss = 0.619 and accuracy of 0.8\n",
      "Iteration 20152: with minibatch training loss = 0.412 and accuracy of 0.88\n",
      "Iteration 20153: with minibatch training loss = 0.312 and accuracy of 0.91\n",
      "Iteration 20154: with minibatch training loss = 0.664 and accuracy of 0.78\n",
      "Iteration 20155: with minibatch training loss = 0.314 and accuracy of 0.91\n",
      "Iteration 20156: with minibatch training loss = 0.511 and accuracy of 0.83\n",
      "Iteration 20157: with minibatch training loss = 0.913 and accuracy of 0.72\n",
      "Iteration 20158: with minibatch training loss = 0.437 and accuracy of 0.84\n",
      "Iteration 20159: with minibatch training loss = 0.513 and accuracy of 0.83\n",
      "Iteration 20160: with minibatch training loss = 0.809 and accuracy of 0.73\n",
      "Iteration 20161: with minibatch training loss = 0.715 and accuracy of 0.77\n",
      "Iteration 20162: with minibatch training loss = 0.483 and accuracy of 0.84\n",
      "Iteration 20163: with minibatch training loss = 0.384 and accuracy of 0.86\n",
      "Iteration 20164: with minibatch training loss = 0.769 and accuracy of 0.78\n",
      "Iteration 20165: with minibatch training loss = 0.284 and accuracy of 0.92\n",
      "Iteration 20166: with minibatch training loss = 0.719 and accuracy of 0.8\n",
      "Iteration 20167: with minibatch training loss = 0.54 and accuracy of 0.83\n",
      "Iteration 20168: with minibatch training loss = 0.473 and accuracy of 0.86\n",
      "Iteration 20169: with minibatch training loss = 0.545 and accuracy of 0.78\n",
      "Iteration 20170: with minibatch training loss = 0.598 and accuracy of 0.78\n",
      "Iteration 20171: with minibatch training loss = 0.772 and accuracy of 0.75\n",
      "Iteration 20172: with minibatch training loss = 0.916 and accuracy of 0.69\n",
      "Iteration 20173: with minibatch training loss = 0.528 and accuracy of 0.86\n",
      "Iteration 20174: with minibatch training loss = 0.537 and accuracy of 0.81\n",
      "Iteration 20175: with minibatch training loss = 0.451 and accuracy of 0.88\n",
      "Iteration 20176: with minibatch training loss = 0.441 and accuracy of 0.84\n",
      "Iteration 20177: with minibatch training loss = 0.671 and accuracy of 0.73\n",
      "Iteration 20178: with minibatch training loss = 0.613 and accuracy of 0.81\n",
      "Iteration 20179: with minibatch training loss = 0.659 and accuracy of 0.78\n",
      "Iteration 20180: with minibatch training loss = 0.568 and accuracy of 0.84\n",
      "Iteration 20181: with minibatch training loss = 0.842 and accuracy of 0.73\n",
      "Iteration 20182: with minibatch training loss = 0.605 and accuracy of 0.81\n",
      "Iteration 20183: with minibatch training loss = 0.538 and accuracy of 0.81\n",
      "Iteration 20184: with minibatch training loss = 0.844 and accuracy of 0.72\n",
      "Iteration 20185: with minibatch training loss = 0.571 and accuracy of 0.84\n",
      "Iteration 20186: with minibatch training loss = 0.632 and accuracy of 0.8\n",
      "Iteration 20187: with minibatch training loss = 0.455 and accuracy of 0.88\n",
      "Iteration 20188: with minibatch training loss = 0.281 and accuracy of 0.91\n",
      "Iteration 20189: with minibatch training loss = 0.598 and accuracy of 0.78\n",
      "Iteration 20190: with minibatch training loss = 0.763 and accuracy of 0.75\n",
      "Iteration 20191: with minibatch training loss = 0.404 and accuracy of 0.91\n",
      "Iteration 20192: with minibatch training loss = 0.506 and accuracy of 0.81\n",
      "Iteration 20193: with minibatch training loss = 0.321 and accuracy of 0.89\n",
      "Iteration 20194: with minibatch training loss = 0.671 and accuracy of 0.75\n",
      "Iteration 20195: with minibatch training loss = 0.554 and accuracy of 0.83\n",
      "Iteration 20196: with minibatch training loss = 0.468 and accuracy of 0.86\n",
      "Iteration 20197: with minibatch training loss = 0.825 and accuracy of 0.72\n",
      "Iteration 20198: with minibatch training loss = 0.815 and accuracy of 0.72\n",
      "Iteration 20199: with minibatch training loss = 0.531 and accuracy of 0.83\n",
      "Iteration 20200: with minibatch training loss = 0.655 and accuracy of 0.78\n",
      "Iteration 20201: with minibatch training loss = 0.566 and accuracy of 0.81\n",
      "Iteration 20202: with minibatch training loss = 0.553 and accuracy of 0.78\n",
      "Iteration 20203: with minibatch training loss = 0.571 and accuracy of 0.84\n",
      "Iteration 20204: with minibatch training loss = 0.49 and accuracy of 0.84\n",
      "Iteration 20205: with minibatch training loss = 0.57 and accuracy of 0.86\n",
      "Iteration 20206: with minibatch training loss = 0.536 and accuracy of 0.86\n",
      "Iteration 20207: with minibatch training loss = 0.677 and accuracy of 0.77\n",
      "Iteration 20208: with minibatch training loss = 0.682 and accuracy of 0.78\n",
      "Iteration 20209: with minibatch training loss = 0.287 and accuracy of 0.94\n",
      "Iteration 20210: with minibatch training loss = 0.539 and accuracy of 0.81\n",
      "Iteration 20211: with minibatch training loss = 0.42 and accuracy of 0.84\n",
      "Iteration 20212: with minibatch training loss = 0.519 and accuracy of 0.83\n",
      "Iteration 20213: with minibatch training loss = 0.705 and accuracy of 0.78\n",
      "Iteration 20214: with minibatch training loss = 0.536 and accuracy of 0.81\n",
      "Iteration 20215: with minibatch training loss = 0.709 and accuracy of 0.75\n",
      "Iteration 20216: with minibatch training loss = 0.541 and accuracy of 0.83\n",
      "Iteration 20217: with minibatch training loss = 0.807 and accuracy of 0.75\n",
      "Iteration 20218: with minibatch training loss = 0.424 and accuracy of 0.89\n",
      "Iteration 20219: with minibatch training loss = 0.477 and accuracy of 0.86\n",
      "Iteration 20220: with minibatch training loss = 0.692 and accuracy of 0.77\n",
      "Iteration 20221: with minibatch training loss = 0.432 and accuracy of 0.84\n",
      "Iteration 20222: with minibatch training loss = 0.538 and accuracy of 0.83\n",
      "Iteration 20223: with minibatch training loss = 0.826 and accuracy of 0.73\n",
      "Iteration 20224: with minibatch training loss = 0.48 and accuracy of 0.86\n",
      "Iteration 20225: with minibatch training loss = 0.66 and accuracy of 0.81\n",
      "Iteration 20226: with minibatch training loss = 0.489 and accuracy of 0.83\n",
      "Iteration 20227: with minibatch training loss = 0.693 and accuracy of 0.75\n",
      "Iteration 20228: with minibatch training loss = 0.57 and accuracy of 0.81\n",
      "Iteration 20229: with minibatch training loss = 0.298 and accuracy of 0.94\n",
      "Iteration 20230: with minibatch training loss = 0.615 and accuracy of 0.81\n",
      "Iteration 20231: with minibatch training loss = 0.535 and accuracy of 0.83\n",
      "Iteration 20232: with minibatch training loss = 0.64 and accuracy of 0.8\n",
      "Iteration 20233: with minibatch training loss = 0.581 and accuracy of 0.83\n",
      "Iteration 20234: with minibatch training loss = 0.378 and accuracy of 0.86\n",
      "Iteration 20235: with minibatch training loss = 0.571 and accuracy of 0.81\n",
      "Iteration 20236: with minibatch training loss = 0.631 and accuracy of 0.78\n",
      "Iteration 20237: with minibatch training loss = 0.832 and accuracy of 0.7\n",
      "Iteration 20238: with minibatch training loss = 0.551 and accuracy of 0.81\n",
      "Iteration 20239: with minibatch training loss = 0.556 and accuracy of 0.81\n",
      "Iteration 20240: with minibatch training loss = 0.618 and accuracy of 0.81\n",
      "Iteration 20241: with minibatch training loss = 0.601 and accuracy of 0.8\n",
      "Iteration 20242: with minibatch training loss = 0.784 and accuracy of 0.73\n",
      "Iteration 20243: with minibatch training loss = 0.553 and accuracy of 0.83\n",
      "Iteration 20244: with minibatch training loss = 0.503 and accuracy of 0.84\n",
      "Iteration 20245: with minibatch training loss = 0.303 and accuracy of 0.91\n",
      "Iteration 20246: with minibatch training loss = 0.648 and accuracy of 0.77\n",
      "Iteration 20247: with minibatch training loss = 0.723 and accuracy of 0.77\n",
      "Iteration 20248: with minibatch training loss = 0.553 and accuracy of 0.81\n",
      "Iteration 20249: with minibatch training loss = 0.663 and accuracy of 0.81\n",
      "Iteration 20250: with minibatch training loss = 0.676 and accuracy of 0.78\n",
      "Iteration 20251: with minibatch training loss = 0.584 and accuracy of 0.81\n",
      "Iteration 20252: with minibatch training loss = 0.735 and accuracy of 0.77\n",
      "Iteration 20253: with minibatch training loss = 0.654 and accuracy of 0.77\n",
      "Iteration 20254: with minibatch training loss = 0.588 and accuracy of 0.81\n",
      "Iteration 20255: with minibatch training loss = 0.639 and accuracy of 0.8\n",
      "Iteration 20256: with minibatch training loss = 0.626 and accuracy of 0.81\n",
      "Iteration 20257: with minibatch training loss = 0.604 and accuracy of 0.83\n",
      "Iteration 20258: with minibatch training loss = 0.625 and accuracy of 0.81\n",
      "Iteration 20259: with minibatch training loss = 0.663 and accuracy of 0.75\n",
      "Iteration 20260: with minibatch training loss = 0.456 and accuracy of 0.84\n",
      "Iteration 20261: with minibatch training loss = 0.567 and accuracy of 0.83\n",
      "Iteration 20262: with minibatch training loss = 0.828 and accuracy of 0.72\n",
      "Iteration 20263: with minibatch training loss = 0.77 and accuracy of 0.8\n",
      "Iteration 20264: with minibatch training loss = 0.698 and accuracy of 0.77\n",
      "Iteration 20265: with minibatch training loss = 0.651 and accuracy of 0.83\n",
      "Iteration 20266: with minibatch training loss = 0.753 and accuracy of 0.8\n",
      "Iteration 20267: with minibatch training loss = 0.652 and accuracy of 0.78\n",
      "Iteration 20268: with minibatch training loss = 0.573 and accuracy of 0.8\n",
      "Iteration 20269: with minibatch training loss = 0.418 and accuracy of 0.89\n",
      "Iteration 20270: with minibatch training loss = 0.572 and accuracy of 0.81\n",
      "Iteration 20271: with minibatch training loss = 0.659 and accuracy of 0.83\n",
      "Iteration 20272: with minibatch training loss = 0.569 and accuracy of 0.83\n",
      "Iteration 20273: with minibatch training loss = 0.517 and accuracy of 0.84\n",
      "Iteration 20274: with minibatch training loss = 0.637 and accuracy of 0.81\n",
      "Iteration 20275: with minibatch training loss = 0.837 and accuracy of 0.67\n",
      "Iteration 20276: with minibatch training loss = 0.544 and accuracy of 0.84\n",
      "Iteration 20277: with minibatch training loss = 0.734 and accuracy of 0.77\n",
      "Iteration 20278: with minibatch training loss = 0.474 and accuracy of 0.83\n",
      "Iteration 20279: with minibatch training loss = 0.719 and accuracy of 0.8\n",
      "Iteration 20280: with minibatch training loss = 0.751 and accuracy of 0.73\n",
      "Iteration 20281: with minibatch training loss = 0.414 and accuracy of 0.89\n",
      "Iteration 20282: with minibatch training loss = 0.569 and accuracy of 0.81\n",
      "Iteration 20283: with minibatch training loss = 0.483 and accuracy of 0.88\n",
      "Iteration 20284: with minibatch training loss = 0.642 and accuracy of 0.81\n",
      "Iteration 20285: with minibatch training loss = 0.562 and accuracy of 0.8\n",
      "Iteration 20286: with minibatch training loss = 0.547 and accuracy of 0.83\n",
      "Iteration 20287: with minibatch training loss = 0.674 and accuracy of 0.77\n",
      "Iteration 20288: with minibatch training loss = 0.814 and accuracy of 0.73\n",
      "Iteration 20289: with minibatch training loss = 0.463 and accuracy of 0.84\n",
      "Iteration 20290: with minibatch training loss = 0.698 and accuracy of 0.75\n",
      "Iteration 20291: with minibatch training loss = 0.707 and accuracy of 0.75\n",
      "Iteration 20292: with minibatch training loss = 0.62 and accuracy of 0.81\n",
      "Iteration 20293: with minibatch training loss = 0.657 and accuracy of 0.78\n",
      "Iteration 20294: with minibatch training loss = 0.621 and accuracy of 0.83\n",
      "Iteration 20295: with minibatch training loss = 0.857 and accuracy of 0.72\n",
      "Iteration 20296: with minibatch training loss = 0.55 and accuracy of 0.86\n",
      "Iteration 20297: with minibatch training loss = 0.806 and accuracy of 0.77\n",
      "Iteration 20298: with minibatch training loss = 0.443 and accuracy of 0.83\n",
      "Iteration 20299: with minibatch training loss = 0.734 and accuracy of 0.78\n",
      "Iteration 20300: with minibatch training loss = 0.749 and accuracy of 0.75\n",
      "Iteration 20301: with minibatch training loss = 0.519 and accuracy of 0.83\n",
      "Iteration 20302: with minibatch training loss = 0.735 and accuracy of 0.77\n",
      "Iteration 20303: with minibatch training loss = 0.619 and accuracy of 0.8\n",
      "Iteration 20304: with minibatch training loss = 0.534 and accuracy of 0.88\n",
      "Iteration 20305: with minibatch training loss = 0.626 and accuracy of 0.8\n",
      "Iteration 20306: with minibatch training loss = 0.525 and accuracy of 0.83\n",
      "Iteration 20307: with minibatch training loss = 0.529 and accuracy of 0.83\n",
      "Iteration 20308: with minibatch training loss = 0.521 and accuracy of 0.83\n",
      "Iteration 20309: with minibatch training loss = 0.593 and accuracy of 0.8\n",
      "Iteration 20310: with minibatch training loss = 0.408 and accuracy of 0.86\n",
      "Iteration 20311: with minibatch training loss = 0.572 and accuracy of 0.8\n",
      "Iteration 20312: with minibatch training loss = 0.575 and accuracy of 0.78\n",
      "Iteration 20313: with minibatch training loss = 0.727 and accuracy of 0.78\n",
      "Iteration 20314: with minibatch training loss = 0.675 and accuracy of 0.78\n",
      "Iteration 20315: with minibatch training loss = 0.383 and accuracy of 0.88\n",
      "Iteration 20316: with minibatch training loss = 0.736 and accuracy of 0.73\n",
      "Iteration 20317: with minibatch training loss = 0.533 and accuracy of 0.8\n",
      "Iteration 20318: with minibatch training loss = 0.575 and accuracy of 0.81\n",
      "Iteration 20319: with minibatch training loss = 0.562 and accuracy of 0.81\n",
      "Iteration 20320: with minibatch training loss = 0.523 and accuracy of 0.81\n",
      "Iteration 20321: with minibatch training loss = 0.611 and accuracy of 0.8\n",
      "Iteration 20322: with minibatch training loss = 0.596 and accuracy of 0.84\n",
      "Iteration 20323: with minibatch training loss = 0.501 and accuracy of 0.86\n",
      "Iteration 20324: with minibatch training loss = 0.475 and accuracy of 0.88\n",
      "Iteration 20325: with minibatch training loss = 0.532 and accuracy of 0.83\n",
      "Iteration 20326: with minibatch training loss = 0.606 and accuracy of 0.8\n",
      "Iteration 20327: with minibatch training loss = 0.513 and accuracy of 0.84\n",
      "Iteration 20328: with minibatch training loss = 0.395 and accuracy of 0.86\n",
      "Iteration 20329: with minibatch training loss = 0.689 and accuracy of 0.77\n",
      "Iteration 20330: with minibatch training loss = 0.877 and accuracy of 0.75\n",
      "Iteration 20331: with minibatch training loss = 0.633 and accuracy of 0.78\n",
      "Iteration 20332: with minibatch training loss = 0.602 and accuracy of 0.83\n",
      "Iteration 20333: with minibatch training loss = 0.884 and accuracy of 0.69\n",
      "Iteration 20334: with minibatch training loss = 0.586 and accuracy of 0.81\n",
      "Iteration 20335: with minibatch training loss = 0.403 and accuracy of 0.88\n",
      "Iteration 20336: with minibatch training loss = 0.455 and accuracy of 0.86\n",
      "Iteration 20337: with minibatch training loss = 0.443 and accuracy of 0.86\n",
      "Iteration 20338: with minibatch training loss = 0.697 and accuracy of 0.77\n",
      "Iteration 20339: with minibatch training loss = 0.619 and accuracy of 0.8\n",
      "Iteration 20340: with minibatch training loss = 0.549 and accuracy of 0.81\n",
      "Iteration 20341: with minibatch training loss = 0.39 and accuracy of 0.88\n",
      "Iteration 20342: with minibatch training loss = 0.619 and accuracy of 0.77\n",
      "Iteration 20343: with minibatch training loss = 0.678 and accuracy of 0.78\n",
      "Iteration 20344: with minibatch training loss = 0.712 and accuracy of 0.8\n",
      "Iteration 20345: with minibatch training loss = 0.354 and accuracy of 0.89\n",
      "Iteration 20346: with minibatch training loss = 0.737 and accuracy of 0.8\n",
      "Iteration 20347: with minibatch training loss = 0.564 and accuracy of 0.8\n",
      "Iteration 20348: with minibatch training loss = 1.02 and accuracy of 0.67\n",
      "Iteration 20349: with minibatch training loss = 0.846 and accuracy of 0.73\n",
      "Iteration 20350: with minibatch training loss = 0.725 and accuracy of 0.75\n",
      "Iteration 20351: with minibatch training loss = 0.547 and accuracy of 0.8\n",
      "Iteration 20352: with minibatch training loss = 0.408 and accuracy of 0.89\n",
      "Iteration 20353: with minibatch training loss = 0.641 and accuracy of 0.81\n",
      "Iteration 20354: with minibatch training loss = 0.51 and accuracy of 0.84\n",
      "Iteration 20355: with minibatch training loss = 0.57 and accuracy of 0.84\n",
      "Iteration 20356: with minibatch training loss = 0.559 and accuracy of 0.84\n",
      "Iteration 20357: with minibatch training loss = 0.565 and accuracy of 0.78\n",
      "Iteration 20358: with minibatch training loss = 0.671 and accuracy of 0.75\n",
      "Iteration 20359: with minibatch training loss = 0.417 and accuracy of 0.88\n",
      "Iteration 20360: with minibatch training loss = 0.34 and accuracy of 0.92\n",
      "Iteration 20361: with minibatch training loss = 0.599 and accuracy of 0.83\n",
      "Iteration 20362: with minibatch training loss = 0.487 and accuracy of 0.84\n",
      "Iteration 20363: with minibatch training loss = 0.54 and accuracy of 0.84\n",
      "Iteration 20364: with minibatch training loss = 0.792 and accuracy of 0.72\n",
      "Iteration 20365: with minibatch training loss = 0.628 and accuracy of 0.78\n",
      "Iteration 20366: with minibatch training loss = 0.503 and accuracy of 0.84\n",
      "Iteration 20367: with minibatch training loss = 0.498 and accuracy of 0.83\n",
      "Iteration 20368: with minibatch training loss = 0.632 and accuracy of 0.78\n",
      "Iteration 20369: with minibatch training loss = 0.579 and accuracy of 0.8\n",
      "Iteration 20370: with minibatch training loss = 0.52 and accuracy of 0.84\n",
      "Iteration 20371: with minibatch training loss = 0.663 and accuracy of 0.77\n",
      "Iteration 20372: with minibatch training loss = 0.842 and accuracy of 0.72\n",
      "Iteration 20373: with minibatch training loss = 0.668 and accuracy of 0.78\n",
      "Iteration 20374: with minibatch training loss = 0.614 and accuracy of 0.83\n",
      "Iteration 20375: with minibatch training loss = 0.539 and accuracy of 0.8\n",
      "Iteration 20376: with minibatch training loss = 0.915 and accuracy of 0.69\n",
      "Iteration 20377: with minibatch training loss = 0.708 and accuracy of 0.75\n",
      "Iteration 20378: with minibatch training loss = 0.567 and accuracy of 0.81\n",
      "Iteration 20379: with minibatch training loss = 0.501 and accuracy of 0.86\n",
      "Iteration 20380: with minibatch training loss = 0.519 and accuracy of 0.86\n",
      "Iteration 20381: with minibatch training loss = 0.57 and accuracy of 0.8\n",
      "Iteration 20382: with minibatch training loss = 0.594 and accuracy of 0.81\n",
      "Iteration 20383: with minibatch training loss = 0.853 and accuracy of 0.72\n",
      "Iteration 20384: with minibatch training loss = 0.689 and accuracy of 0.72\n",
      "Iteration 20385: with minibatch training loss = 0.586 and accuracy of 0.81\n",
      "Iteration 20386: with minibatch training loss = 0.6 and accuracy of 0.8\n",
      "Iteration 20387: with minibatch training loss = 0.739 and accuracy of 0.73\n",
      "Iteration 20388: with minibatch training loss = 0.703 and accuracy of 0.77\n",
      "Iteration 20389: with minibatch training loss = 0.701 and accuracy of 0.75\n",
      "Iteration 20390: with minibatch training loss = 0.523 and accuracy of 0.88\n",
      "Iteration 20391: with minibatch training loss = 0.489 and accuracy of 0.86\n",
      "Iteration 20392: with minibatch training loss = 0.615 and accuracy of 0.8\n",
      "Iteration 20393: with minibatch training loss = 0.481 and accuracy of 0.84\n",
      "Iteration 20394: with minibatch training loss = 0.824 and accuracy of 0.73\n",
      "Iteration 20395: with minibatch training loss = 0.971 and accuracy of 0.67\n",
      "Iteration 20396: with minibatch training loss = 0.593 and accuracy of 0.83\n",
      "Iteration 20397: with minibatch training loss = 0.655 and accuracy of 0.75\n",
      "Iteration 20398: with minibatch training loss = 0.783 and accuracy of 0.72\n",
      "Iteration 20399: with minibatch training loss = 0.699 and accuracy of 0.78\n",
      "Iteration 20400: with minibatch training loss = 0.573 and accuracy of 0.84\n",
      "Iteration 20401: with minibatch training loss = 0.372 and accuracy of 0.86\n",
      "Iteration 20402: with minibatch training loss = 0.608 and accuracy of 0.78\n",
      "Iteration 20403: with minibatch training loss = 0.526 and accuracy of 0.83\n",
      "Iteration 20404: with minibatch training loss = 0.607 and accuracy of 0.78\n",
      "Iteration 20405: with minibatch training loss = 0.502 and accuracy of 0.84\n",
      "Iteration 20406: with minibatch training loss = 0.692 and accuracy of 0.73\n",
      "Iteration 20407: with minibatch training loss = 0.401 and accuracy of 0.86\n",
      "Iteration 20408: with minibatch training loss = 0.629 and accuracy of 0.78\n",
      "Iteration 20409: with minibatch training loss = 0.521 and accuracy of 0.81\n",
      "Iteration 20410: with minibatch training loss = 0.595 and accuracy of 0.78\n",
      "Iteration 20411: with minibatch training loss = 0.758 and accuracy of 0.77\n",
      "Iteration 20412: with minibatch training loss = 0.832 and accuracy of 0.75\n",
      "Iteration 20413: with minibatch training loss = 0.894 and accuracy of 0.72\n",
      "Iteration 20414: with minibatch training loss = 0.539 and accuracy of 0.83\n",
      "Iteration 20415: with minibatch training loss = 0.577 and accuracy of 0.8\n",
      "Iteration 20416: with minibatch training loss = 0.714 and accuracy of 0.78\n",
      "Iteration 20417: with minibatch training loss = 0.803 and accuracy of 0.77\n",
      "Iteration 20418: with minibatch training loss = 0.668 and accuracy of 0.81\n",
      "Iteration 20419: with minibatch training loss = 0.557 and accuracy of 0.83\n",
      "Iteration 20420: with minibatch training loss = 0.678 and accuracy of 0.8\n",
      "Iteration 20421: with minibatch training loss = 0.578 and accuracy of 0.81\n",
      "Iteration 20422: with minibatch training loss = 0.818 and accuracy of 0.7\n",
      "Iteration 20423: with minibatch training loss = 0.421 and accuracy of 0.89\n",
      "Iteration 20424: with minibatch training loss = 0.524 and accuracy of 0.81\n",
      "Iteration 20425: with minibatch training loss = 0.562 and accuracy of 0.83\n",
      "Iteration 20426: with minibatch training loss = 0.524 and accuracy of 0.84\n",
      "Iteration 20427: with minibatch training loss = 0.861 and accuracy of 0.73\n",
      "Iteration 20428: with minibatch training loss = 0.584 and accuracy of 0.78\n",
      "Iteration 20429: with minibatch training loss = 0.708 and accuracy of 0.77\n",
      "Iteration 20430: with minibatch training loss = 0.687 and accuracy of 0.77\n",
      "Iteration 20431: with minibatch training loss = 0.931 and accuracy of 0.72\n",
      "Iteration 20432: with minibatch training loss = 0.793 and accuracy of 0.73\n",
      "Iteration 20433: with minibatch training loss = 0.603 and accuracy of 0.8\n",
      "Iteration 20434: with minibatch training loss = 0.707 and accuracy of 0.77\n",
      "Iteration 20435: with minibatch training loss = 0.535 and accuracy of 0.81\n",
      "Iteration 20436: with minibatch training loss = 0.517 and accuracy of 0.86\n",
      "Iteration 20437: with minibatch training loss = 0.626 and accuracy of 0.81\n",
      "Iteration 20438: with minibatch training loss = 0.488 and accuracy of 0.84\n",
      "Iteration 20439: with minibatch training loss = 0.335 and accuracy of 0.92\n",
      "Iteration 20440: with minibatch training loss = 0.61 and accuracy of 0.81\n",
      "Iteration 20441: with minibatch training loss = 0.551 and accuracy of 0.81\n",
      "Iteration 20442: with minibatch training loss = 0.588 and accuracy of 0.84\n",
      "Iteration 20443: with minibatch training loss = 0.527 and accuracy of 0.81\n",
      "Iteration 20444: with minibatch training loss = 0.451 and accuracy of 0.86\n",
      "Iteration 20445: with minibatch training loss = 0.657 and accuracy of 0.78\n",
      "Iteration 20446: with minibatch training loss = 0.559 and accuracy of 0.81\n",
      "Iteration 20447: with minibatch training loss = 0.418 and accuracy of 0.86\n",
      "Iteration 20448: with minibatch training loss = 0.411 and accuracy of 0.86\n",
      "Iteration 20449: with minibatch training loss = 0.486 and accuracy of 0.83\n",
      "Iteration 20450: with minibatch training loss = 0.548 and accuracy of 0.8\n",
      "Iteration 20451: with minibatch training loss = 0.336 and accuracy of 0.89\n",
      "Iteration 20452: with minibatch training loss = 0.52 and accuracy of 0.83\n",
      "Iteration 20453: with minibatch training loss = 0.547 and accuracy of 0.81\n",
      "Iteration 20454: with minibatch training loss = 0.528 and accuracy of 0.81\n",
      "Iteration 20455: with minibatch training loss = 0.573 and accuracy of 0.81\n",
      "Iteration 20456: with minibatch training loss = 0.617 and accuracy of 0.81\n",
      "Iteration 20457: with minibatch training loss = 0.801 and accuracy of 0.73\n",
      "Iteration 20458: with minibatch training loss = 0.479 and accuracy of 0.88\n",
      "Iteration 20459: with minibatch training loss = 0.506 and accuracy of 0.8\n",
      "Iteration 20460: with minibatch training loss = 0.957 and accuracy of 0.69\n",
      "Iteration 20461: with minibatch training loss = 0.615 and accuracy of 0.81\n",
      "Iteration 20462: with minibatch training loss = 0.635 and accuracy of 0.78\n",
      "Iteration 20463: with minibatch training loss = 0.599 and accuracy of 0.83\n",
      "Iteration 20464: with minibatch training loss = 0.583 and accuracy of 0.8\n",
      "Iteration 20465: with minibatch training loss = 0.756 and accuracy of 0.77\n",
      "Iteration 20466: with minibatch training loss = 0.337 and accuracy of 0.88\n",
      "Iteration 20467: with minibatch training loss = 0.756 and accuracy of 0.73\n",
      "Iteration 20468: with minibatch training loss = 0.548 and accuracy of 0.83\n",
      "Iteration 20469: with minibatch training loss = 0.571 and accuracy of 0.8\n",
      "Iteration 20470: with minibatch training loss = 0.737 and accuracy of 0.73\n",
      "Iteration 20471: with minibatch training loss = 0.491 and accuracy of 0.81\n",
      "Iteration 20472: with minibatch training loss = 0.55 and accuracy of 0.81\n",
      "Iteration 20473: with minibatch training loss = 0.584 and accuracy of 0.81\n",
      "Iteration 20474: with minibatch training loss = 0.586 and accuracy of 0.81\n",
      "Iteration 20475: with minibatch training loss = 0.924 and accuracy of 0.69\n",
      "Iteration 20476: with minibatch training loss = 0.589 and accuracy of 0.77\n",
      "Iteration 20477: with minibatch training loss = 0.703 and accuracy of 0.75\n",
      "Iteration 20478: with minibatch training loss = 0.526 and accuracy of 0.86\n",
      "Iteration 20479: with minibatch training loss = 0.588 and accuracy of 0.8\n",
      "Iteration 20480: with minibatch training loss = 0.475 and accuracy of 0.83\n",
      "Iteration 20481: with minibatch training loss = 0.727 and accuracy of 0.78\n",
      "Iteration 20482: with minibatch training loss = 0.773 and accuracy of 0.75\n",
      "Iteration 20483: with minibatch training loss = 0.488 and accuracy of 0.86\n",
      "Iteration 20484: with minibatch training loss = 0.634 and accuracy of 0.8\n",
      "Iteration 20485: with minibatch training loss = 0.584 and accuracy of 0.8\n",
      "Iteration 20486: with minibatch training loss = 0.746 and accuracy of 0.78\n",
      "Iteration 20487: with minibatch training loss = 0.59 and accuracy of 0.83\n",
      "Iteration 20488: with minibatch training loss = 0.636 and accuracy of 0.78\n",
      "Iteration 20489: with minibatch training loss = 0.424 and accuracy of 0.86\n",
      "Iteration 20490: with minibatch training loss = 0.462 and accuracy of 0.83\n",
      "Iteration 20491: with minibatch training loss = 0.635 and accuracy of 0.77\n",
      "Iteration 20492: with minibatch training loss = 0.459 and accuracy of 0.84\n",
      "Iteration 20493: with minibatch training loss = 0.499 and accuracy of 0.83\n",
      "Iteration 20494: with minibatch training loss = 0.555 and accuracy of 0.83\n",
      "Iteration 20495: with minibatch training loss = 0.691 and accuracy of 0.78\n",
      "Iteration 20496: with minibatch training loss = 0.377 and accuracy of 0.88\n",
      "Iteration 20497: with minibatch training loss = 0.624 and accuracy of 0.81\n",
      "Iteration 20498: with minibatch training loss = 0.392 and accuracy of 0.88\n",
      "Iteration 20499: with minibatch training loss = 0.663 and accuracy of 0.8\n",
      "Iteration 20500: with minibatch training loss = 0.872 and accuracy of 0.69\n",
      "Iteration 20501: with minibatch training loss = 0.605 and accuracy of 0.84\n",
      "Iteration 20502: with minibatch training loss = 0.541 and accuracy of 0.84\n",
      "Iteration 20503: with minibatch training loss = 0.563 and accuracy of 0.81\n",
      "Iteration 20504: with minibatch training loss = 0.402 and accuracy of 0.89\n",
      "Iteration 20505: with minibatch training loss = 0.527 and accuracy of 0.83\n",
      "Iteration 20506: with minibatch training loss = 0.683 and accuracy of 0.78\n",
      "Iteration 20507: with minibatch training loss = 0.806 and accuracy of 0.73\n",
      "Iteration 20508: with minibatch training loss = 0.483 and accuracy of 0.83\n",
      "Iteration 20509: with minibatch training loss = 0.432 and accuracy of 0.86\n",
      "Iteration 20510: with minibatch training loss = 0.517 and accuracy of 0.84\n",
      "Iteration 20511: with minibatch training loss = 0.549 and accuracy of 0.81\n",
      "Iteration 20512: with minibatch training loss = 0.462 and accuracy of 0.84\n",
      "Iteration 20513: with minibatch training loss = 0.569 and accuracy of 0.84\n",
      "Iteration 20514: with minibatch training loss = 0.942 and accuracy of 0.7\n",
      "Iteration 20515: with minibatch training loss = 0.57 and accuracy of 0.83\n",
      "Iteration 20516: with minibatch training loss = 0.758 and accuracy of 0.75\n",
      "Iteration 20517: with minibatch training loss = 0.485 and accuracy of 0.86\n",
      "Iteration 20518: with minibatch training loss = 0.898 and accuracy of 0.67\n",
      "Iteration 20519: with minibatch training loss = 0.423 and accuracy of 0.88\n",
      "Iteration 20520: with minibatch training loss = 0.552 and accuracy of 0.84\n",
      "Iteration 20521: with minibatch training loss = 0.545 and accuracy of 0.84\n",
      "Iteration 20522: with minibatch training loss = 0.721 and accuracy of 0.78\n",
      "Iteration 20523: with minibatch training loss = 0.354 and accuracy of 0.91\n",
      "Iteration 20524: with minibatch training loss = 0.573 and accuracy of 0.83\n",
      "Iteration 20525: with minibatch training loss = 0.573 and accuracy of 0.83\n",
      "Iteration 20526: with minibatch training loss = 0.768 and accuracy of 0.77\n",
      "Iteration 20527: with minibatch training loss = 0.586 and accuracy of 0.81\n",
      "Iteration 20528: with minibatch training loss = 0.915 and accuracy of 0.7\n",
      "Iteration 20529: with minibatch training loss = 0.806 and accuracy of 0.72\n",
      "Iteration 20530: with minibatch training loss = 0.759 and accuracy of 0.75\n",
      "Iteration 20531: with minibatch training loss = 0.669 and accuracy of 0.77\n",
      "Iteration 20532: with minibatch training loss = 0.627 and accuracy of 0.83\n",
      "Iteration 20533: with minibatch training loss = 0.865 and accuracy of 0.73\n",
      "Iteration 20534: with minibatch training loss = 0.452 and accuracy of 0.89\n",
      "Iteration 20535: with minibatch training loss = 0.572 and accuracy of 0.78\n",
      "Iteration 20536: with minibatch training loss = 0.889 and accuracy of 0.69\n",
      "Iteration 20537: with minibatch training loss = 0.671 and accuracy of 0.75\n",
      "Iteration 20538: with minibatch training loss = 0.522 and accuracy of 0.83\n",
      "Iteration 20539: with minibatch training loss = 0.483 and accuracy of 0.84\n",
      "Iteration 20540: with minibatch training loss = 0.692 and accuracy of 0.75\n",
      "Iteration 20541: with minibatch training loss = 0.44 and accuracy of 0.88\n",
      "Iteration 20542: with minibatch training loss = 0.6 and accuracy of 0.78\n",
      "Iteration 20543: with minibatch training loss = 0.395 and accuracy of 0.88\n",
      "Iteration 20544: with minibatch training loss = 0.643 and accuracy of 0.81\n",
      "Iteration 20545: with minibatch training loss = 0.654 and accuracy of 0.8\n",
      "Iteration 20546: with minibatch training loss = 0.601 and accuracy of 0.8\n",
      "Iteration 20547: with minibatch training loss = 0.599 and accuracy of 0.78\n",
      "Iteration 20548: with minibatch training loss = 0.533 and accuracy of 0.83\n",
      "Iteration 20549: with minibatch training loss = 0.836 and accuracy of 0.72\n",
      "Iteration 20550: with minibatch training loss = 0.608 and accuracy of 0.81\n",
      "Iteration 20551: with minibatch training loss = 0.588 and accuracy of 0.78\n",
      "Iteration 20552: with minibatch training loss = 0.444 and accuracy of 0.86\n",
      "Iteration 20553: with minibatch training loss = 0.614 and accuracy of 0.83\n",
      "Iteration 20554: with minibatch training loss = 0.506 and accuracy of 0.81\n",
      "Iteration 20555: with minibatch training loss = 0.393 and accuracy of 0.88\n",
      "Iteration 20556: with minibatch training loss = 0.522 and accuracy of 0.81\n",
      "Iteration 20557: with minibatch training loss = 0.624 and accuracy of 0.78\n",
      "Iteration 20558: with minibatch training loss = 0.72 and accuracy of 0.77\n",
      "Iteration 20559: with minibatch training loss = 0.353 and accuracy of 0.88\n",
      "Iteration 20560: with minibatch training loss = 0.576 and accuracy of 0.77\n",
      "Iteration 20561: with minibatch training loss = 0.422 and accuracy of 0.86\n",
      "Iteration 20562: with minibatch training loss = 0.497 and accuracy of 0.83\n",
      "Iteration 20563: with minibatch training loss = 0.57 and accuracy of 0.8\n",
      "Iteration 20564: with minibatch training loss = 0.605 and accuracy of 0.8\n",
      "Iteration 20565: with minibatch training loss = 0.563 and accuracy of 0.81\n",
      "Iteration 20566: with minibatch training loss = 0.325 and accuracy of 0.91\n",
      "Iteration 20567: with minibatch training loss = 0.57 and accuracy of 0.83\n",
      "Iteration 20568: with minibatch training loss = 0.401 and accuracy of 0.89\n",
      "Iteration 20569: with minibatch training loss = 0.601 and accuracy of 0.8\n",
      "Iteration 20570: with minibatch training loss = 0.444 and accuracy of 0.88\n",
      "Iteration 20571: with minibatch training loss = 0.684 and accuracy of 0.78\n",
      "Iteration 20572: with minibatch training loss = 0.507 and accuracy of 0.81\n",
      "Iteration 20573: with minibatch training loss = 0.542 and accuracy of 0.8\n",
      "Iteration 20574: with minibatch training loss = 0.78 and accuracy of 0.73\n",
      "Iteration 20575: with minibatch training loss = 0.599 and accuracy of 0.81\n",
      "Iteration 20576: with minibatch training loss = 0.583 and accuracy of 0.8\n",
      "Iteration 20577: with minibatch training loss = 0.325 and accuracy of 0.91\n",
      "Iteration 20578: with minibatch training loss = 0.456 and accuracy of 0.84\n",
      "Iteration 20579: with minibatch training loss = 0.468 and accuracy of 0.88\n",
      "Iteration 20580: with minibatch training loss = 0.439 and accuracy of 0.88\n",
      "Iteration 20581: with minibatch training loss = 0.401 and accuracy of 0.89\n",
      "Iteration 20582: with minibatch training loss = 0.643 and accuracy of 0.78\n",
      "Iteration 20583: with minibatch training loss = 0.922 and accuracy of 0.69\n",
      "Iteration 20584: with minibatch training loss = 0.604 and accuracy of 0.84\n",
      "Iteration 20585: with minibatch training loss = 0.719 and accuracy of 0.78\n",
      "Iteration 20586: with minibatch training loss = 0.522 and accuracy of 0.84\n",
      "Iteration 20587: with minibatch training loss = 0.543 and accuracy of 0.83\n",
      "Iteration 20588: with minibatch training loss = 0.589 and accuracy of 0.83\n",
      "Iteration 20589: with minibatch training loss = 0.474 and accuracy of 0.86\n",
      "Iteration 20590: with minibatch training loss = 0.662 and accuracy of 0.77\n",
      "Iteration 20591: with minibatch training loss = 0.613 and accuracy of 0.8\n",
      "Iteration 20592: with minibatch training loss = 0.58 and accuracy of 0.81\n",
      "Iteration 20593: with minibatch training loss = 0.44 and accuracy of 0.84\n",
      "Iteration 20594: with minibatch training loss = 0.583 and accuracy of 0.84\n",
      "Iteration 20595: with minibatch training loss = 0.576 and accuracy of 0.81\n",
      "Iteration 20596: with minibatch training loss = 0.696 and accuracy of 0.78\n",
      "Iteration 20597: with minibatch training loss = 0.61 and accuracy of 0.81\n",
      "Iteration 20598: with minibatch training loss = 0.352 and accuracy of 0.89\n",
      "Iteration 20599: with minibatch training loss = 0.681 and accuracy of 0.78\n",
      "Iteration 20600: with minibatch training loss = 0.569 and accuracy of 0.83\n",
      "Iteration 20601: with minibatch training loss = 0.543 and accuracy of 0.81\n",
      "Iteration 20602: with minibatch training loss = 0.512 and accuracy of 0.84\n",
      "Iteration 20603: with minibatch training loss = 0.921 and accuracy of 0.67\n",
      "Iteration 20604: with minibatch training loss = 0.43 and accuracy of 0.86\n",
      "Iteration 20605: with minibatch training loss = 0.421 and accuracy of 0.86\n",
      "Iteration 20606: with minibatch training loss = 0.582 and accuracy of 0.8\n",
      "Iteration 20607: with minibatch training loss = 0.685 and accuracy of 0.73\n",
      "Iteration 20608: with minibatch training loss = 0.581 and accuracy of 0.81\n",
      "Iteration 20609: with minibatch training loss = 0.682 and accuracy of 0.81\n",
      "Iteration 20610: with minibatch training loss = 0.55 and accuracy of 0.81\n",
      "Iteration 20611: with minibatch training loss = 0.829 and accuracy of 0.72\n",
      "Iteration 20612: with minibatch training loss = 0.852 and accuracy of 0.72\n",
      "Iteration 20613: with minibatch training loss = 0.814 and accuracy of 0.7\n",
      "Iteration 20614: with minibatch training loss = 0.642 and accuracy of 0.78\n",
      "Iteration 20615: with minibatch training loss = 0.6 and accuracy of 0.78\n",
      "Iteration 20616: with minibatch training loss = 0.653 and accuracy of 0.81\n",
      "Iteration 20617: with minibatch training loss = 0.82 and accuracy of 0.7\n",
      "Iteration 20618: with minibatch training loss = 0.471 and accuracy of 0.84\n",
      "Iteration 20619: with minibatch training loss = 0.513 and accuracy of 0.83\n",
      "Iteration 20620: with minibatch training loss = 0.703 and accuracy of 0.78\n",
      "Iteration 20621: with minibatch training loss = 0.49 and accuracy of 0.81\n",
      "Iteration 20622: with minibatch training loss = 0.56 and accuracy of 0.83\n",
      "Iteration 20623: with minibatch training loss = 0.393 and accuracy of 0.86\n",
      "Iteration 20624: with minibatch training loss = 0.463 and accuracy of 0.83\n",
      "Iteration 20625: with minibatch training loss = 0.703 and accuracy of 0.75\n",
      "Iteration 20626: with minibatch training loss = 0.485 and accuracy of 0.86\n",
      "Iteration 20627: with minibatch training loss = 0.559 and accuracy of 0.8\n",
      "Iteration 20628: with minibatch training loss = 0.486 and accuracy of 0.88\n",
      "Iteration 20629: with minibatch training loss = 0.504 and accuracy of 0.83\n",
      "Iteration 20630: with minibatch training loss = 0.336 and accuracy of 0.91\n",
      "Iteration 20631: with minibatch training loss = 0.531 and accuracy of 0.8\n",
      "Iteration 20632: with minibatch training loss = 0.464 and accuracy of 0.88\n",
      "Iteration 20633: with minibatch training loss = 0.827 and accuracy of 0.83\n",
      "Iteration 20634: with minibatch training loss = 0.593 and accuracy of 0.8\n",
      "Iteration 20635: with minibatch training loss = 0.618 and accuracy of 0.83\n",
      "Iteration 20636: with minibatch training loss = 0.632 and accuracy of 0.8\n",
      "Iteration 20637: with minibatch training loss = 0.346 and accuracy of 0.91\n",
      "Iteration 20638: with minibatch training loss = 0.717 and accuracy of 0.81\n",
      "Iteration 20639: with minibatch training loss = 0.448 and accuracy of 0.84\n",
      "Iteration 20640: with minibatch training loss = 0.376 and accuracy of 0.84\n",
      "Iteration 20641: with minibatch training loss = 0.451 and accuracy of 0.86\n",
      "Iteration 20642: with minibatch training loss = 0.547 and accuracy of 0.81\n",
      "Iteration 20643: with minibatch training loss = 0.592 and accuracy of 0.81\n",
      "Iteration 20644: with minibatch training loss = 0.741 and accuracy of 0.77\n",
      "Iteration 20645: with minibatch training loss = 0.517 and accuracy of 0.83\n",
      "Iteration 20646: with minibatch training loss = 0.488 and accuracy of 0.8\n",
      "Iteration 20647: with minibatch training loss = 0.74 and accuracy of 0.77\n",
      "Iteration 20648: with minibatch training loss = 0.585 and accuracy of 0.84\n",
      "Iteration 20649: with minibatch training loss = 0.873 and accuracy of 0.67\n",
      "Iteration 20650: with minibatch training loss = 0.413 and accuracy of 0.86\n",
      "Iteration 20651: with minibatch training loss = 0.779 and accuracy of 0.78\n",
      "Iteration 20652: with minibatch training loss = 0.63 and accuracy of 0.8\n",
      "Iteration 20653: with minibatch training loss = 0.603 and accuracy of 0.81\n",
      "Iteration 20654: with minibatch training loss = 0.547 and accuracy of 0.81\n",
      "Iteration 20655: with minibatch training loss = 0.572 and accuracy of 0.8\n",
      "Iteration 20656: with minibatch training loss = 0.606 and accuracy of 0.83\n",
      "Iteration 20657: with minibatch training loss = 0.549 and accuracy of 0.83\n",
      "Iteration 20658: with minibatch training loss = 0.625 and accuracy of 0.81\n",
      "Iteration 20659: with minibatch training loss = 0.429 and accuracy of 0.86\n",
      "Iteration 20660: with minibatch training loss = 0.731 and accuracy of 0.77\n",
      "Iteration 20661: with minibatch training loss = 0.612 and accuracy of 0.8\n",
      "Iteration 20662: with minibatch training loss = 0.578 and accuracy of 0.81\n",
      "Iteration 20663: with minibatch training loss = 0.393 and accuracy of 0.88\n",
      "Iteration 20664: with minibatch training loss = 0.873 and accuracy of 0.69\n",
      "Iteration 20665: with minibatch training loss = 0.588 and accuracy of 0.78\n",
      "Iteration 20666: with minibatch training loss = 0.524 and accuracy of 0.81\n",
      "Iteration 20667: with minibatch training loss = 0.432 and accuracy of 0.88\n",
      "Iteration 20668: with minibatch training loss = 0.544 and accuracy of 0.81\n",
      "Iteration 20669: with minibatch training loss = 0.56 and accuracy of 0.83\n",
      "Iteration 20670: with minibatch training loss = 0.825 and accuracy of 0.72\n",
      "Iteration 20671: with minibatch training loss = 0.64 and accuracy of 0.77\n",
      "Iteration 20672: with minibatch training loss = 0.392 and accuracy of 0.89\n",
      "Iteration 20673: with minibatch training loss = 0.551 and accuracy of 0.81\n",
      "Iteration 20674: with minibatch training loss = 0.762 and accuracy of 0.77\n",
      "Iteration 20675: with minibatch training loss = 0.305 and accuracy of 0.92\n",
      "Iteration 20676: with minibatch training loss = 0.526 and accuracy of 0.83\n",
      "Iteration 20677: with minibatch training loss = 0.611 and accuracy of 0.83\n",
      "Iteration 20678: with minibatch training loss = 0.721 and accuracy of 0.75\n",
      "Iteration 20679: with minibatch training loss = 0.756 and accuracy of 0.78\n",
      "Iteration 20680: with minibatch training loss = 0.567 and accuracy of 0.83\n",
      "Iteration 20681: with minibatch training loss = 0.716 and accuracy of 0.75\n",
      "Iteration 20682: with minibatch training loss = 0.636 and accuracy of 0.8\n",
      "Iteration 20683: with minibatch training loss = 0.776 and accuracy of 0.77\n",
      "Iteration 20684: with minibatch training loss = 0.498 and accuracy of 0.83\n",
      "Iteration 20685: with minibatch training loss = 0.827 and accuracy of 0.7\n",
      "Iteration 20686: with minibatch training loss = 0.674 and accuracy of 0.77\n",
      "Iteration 20687: with minibatch training loss = 0.305 and accuracy of 0.89\n",
      "Iteration 20688: with minibatch training loss = 0.53 and accuracy of 0.81\n",
      "Iteration 20689: with minibatch training loss = 0.979 and accuracy of 0.67\n",
      "Iteration 20690: with minibatch training loss = 0.643 and accuracy of 0.77\n",
      "Iteration 20691: with minibatch training loss = 0.646 and accuracy of 0.78\n",
      "Iteration 20692: with minibatch training loss = 0.657 and accuracy of 0.8\n",
      "Iteration 20693: with minibatch training loss = 0.569 and accuracy of 0.8\n",
      "Iteration 20694: with minibatch training loss = 0.629 and accuracy of 0.81\n",
      "Iteration 20695: with minibatch training loss = 0.582 and accuracy of 0.78\n",
      "Iteration 20696: with minibatch training loss = 0.48 and accuracy of 0.91\n",
      "Iteration 20697: with minibatch training loss = 0.642 and accuracy of 0.77\n",
      "Iteration 20698: with minibatch training loss = 0.658 and accuracy of 0.78\n",
      "Iteration 20699: with minibatch training loss = 0.485 and accuracy of 0.83\n",
      "Iteration 20700: with minibatch training loss = 0.489 and accuracy of 0.86\n",
      "Iteration 20701: with minibatch training loss = 0.954 and accuracy of 0.7\n",
      "Iteration 20702: with minibatch training loss = 0.384 and accuracy of 0.88\n",
      "Iteration 20703: with minibatch training loss = 0.733 and accuracy of 0.77\n",
      "Iteration 20704: with minibatch training loss = 0.562 and accuracy of 0.83\n",
      "Iteration 20705: with minibatch training loss = 0.41 and accuracy of 0.88\n",
      "Iteration 20706: with minibatch training loss = 0.576 and accuracy of 0.81\n",
      "Iteration 20707: with minibatch training loss = 0.563 and accuracy of 0.8\n",
      "Iteration 20708: with minibatch training loss = 0.665 and accuracy of 0.75\n",
      "Iteration 20709: with minibatch training loss = 0.446 and accuracy of 0.88\n",
      "Iteration 20710: with minibatch training loss = 0.575 and accuracy of 0.83\n",
      "Iteration 20711: with minibatch training loss = 0.595 and accuracy of 0.81\n",
      "Iteration 20712: with minibatch training loss = 0.612 and accuracy of 0.8\n",
      "Iteration 20713: with minibatch training loss = 0.673 and accuracy of 0.77\n",
      "Iteration 20714: with minibatch training loss = 0.704 and accuracy of 0.78\n",
      "Iteration 20715: with minibatch training loss = 0.588 and accuracy of 0.78\n",
      "Iteration 20716: with minibatch training loss = 0.794 and accuracy of 0.73\n",
      "Iteration 20717: with minibatch training loss = 1.05 and accuracy of 0.61\n",
      "Iteration 20718: with minibatch training loss = 0.766 and accuracy of 0.73\n",
      "Iteration 20719: with minibatch training loss = 0.646 and accuracy of 0.77\n",
      "Iteration 20720: with minibatch training loss = 0.515 and accuracy of 0.84\n",
      "Iteration 20721: with minibatch training loss = 0.332 and accuracy of 0.91\n",
      "Iteration 20722: with minibatch training loss = 0.454 and accuracy of 0.88\n",
      "Iteration 20723: with minibatch training loss = 0.72 and accuracy of 0.8\n",
      "Iteration 20724: with minibatch training loss = 0.579 and accuracy of 0.8\n",
      "Iteration 20725: with minibatch training loss = 0.645 and accuracy of 0.81\n",
      "Iteration 20726: with minibatch training loss = 0.576 and accuracy of 0.83\n",
      "Iteration 20727: with minibatch training loss = 0.491 and accuracy of 0.84\n",
      "Iteration 20728: with minibatch training loss = 0.531 and accuracy of 0.86\n",
      "Iteration 20729: with minibatch training loss = 0.775 and accuracy of 0.72\n",
      "Iteration 20730: with minibatch training loss = 0.657 and accuracy of 0.78\n",
      "Iteration 20731: with minibatch training loss = 0.745 and accuracy of 0.73\n",
      "Iteration 20732: with minibatch training loss = 0.514 and accuracy of 0.83\n",
      "Iteration 20733: with minibatch training loss = 0.33 and accuracy of 0.89\n",
      "Iteration 20734: with minibatch training loss = 0.601 and accuracy of 0.8\n",
      "Iteration 20735: with minibatch training loss = 0.48 and accuracy of 0.83\n",
      "Iteration 20736: with minibatch training loss = 0.729 and accuracy of 0.77\n",
      "Iteration 20737: with minibatch training loss = 0.658 and accuracy of 0.78\n",
      "Iteration 20738: with minibatch training loss = 0.569 and accuracy of 0.81\n",
      "Iteration 20739: with minibatch training loss = 0.657 and accuracy of 0.83\n",
      "Iteration 20740: with minibatch training loss = 0.681 and accuracy of 0.77\n",
      "Iteration 20741: with minibatch training loss = 0.827 and accuracy of 0.75\n",
      "Iteration 20742: with minibatch training loss = 0.515 and accuracy of 0.86\n",
      "Iteration 20743: with minibatch training loss = 0.642 and accuracy of 0.8\n",
      "Iteration 20744: with minibatch training loss = 0.485 and accuracy of 0.8\n",
      "Iteration 20745: with minibatch training loss = 0.58 and accuracy of 0.86\n",
      "Iteration 20746: with minibatch training loss = 0.595 and accuracy of 0.78\n",
      "Iteration 20747: with minibatch training loss = 0.486 and accuracy of 0.83\n",
      "Iteration 20748: with minibatch training loss = 0.882 and accuracy of 0.7\n",
      "Iteration 20749: with minibatch training loss = 0.841 and accuracy of 0.72\n",
      "Iteration 20750: with minibatch training loss = 0.686 and accuracy of 0.73\n",
      "Iteration 20751: with minibatch training loss = 0.609 and accuracy of 0.78\n",
      "Iteration 20752: with minibatch training loss = 0.493 and accuracy of 0.83\n",
      "Iteration 20753: with minibatch training loss = 0.354 and accuracy of 0.88\n",
      "Iteration 20754: with minibatch training loss = 0.613 and accuracy of 0.8\n",
      "Iteration 20755: with minibatch training loss = 0.595 and accuracy of 0.81\n",
      "Iteration 20756: with minibatch training loss = 0.689 and accuracy of 0.77\n",
      "Iteration 20757: with minibatch training loss = 0.615 and accuracy of 0.83\n",
      "Iteration 20758: with minibatch training loss = 0.66 and accuracy of 0.8\n",
      "Iteration 20759: with minibatch training loss = 0.771 and accuracy of 0.8\n",
      "Iteration 20760: with minibatch training loss = 0.51 and accuracy of 0.8\n",
      "Iteration 20761: with minibatch training loss = 0.514 and accuracy of 0.83\n",
      "Iteration 20762: with minibatch training loss = 0.536 and accuracy of 0.83\n",
      "Iteration 20763: with minibatch training loss = 0.71 and accuracy of 0.8\n",
      "Iteration 20764: with minibatch training loss = 0.362 and accuracy of 0.91\n",
      "Iteration 20765: with minibatch training loss = 0.386 and accuracy of 0.92\n",
      "Iteration 20766: with minibatch training loss = 0.484 and accuracy of 0.81\n",
      "Iteration 20767: with minibatch training loss = 0.817 and accuracy of 0.72\n",
      "Iteration 20768: with minibatch training loss = 0.634 and accuracy of 0.8\n",
      "Iteration 20769: with minibatch training loss = 0.672 and accuracy of 0.75\n",
      "Iteration 20770: with minibatch training loss = 0.516 and accuracy of 0.86\n",
      "Iteration 20771: with minibatch training loss = 0.751 and accuracy of 0.73\n",
      "Iteration 20772: with minibatch training loss = 0.608 and accuracy of 0.81\n",
      "Iteration 20773: with minibatch training loss = 0.512 and accuracy of 0.81\n",
      "Iteration 20774: with minibatch training loss = 0.39 and accuracy of 0.89\n",
      "Iteration 20775: with minibatch training loss = 0.61 and accuracy of 0.78\n",
      "Iteration 20776: with minibatch training loss = 0.467 and accuracy of 0.88\n",
      "Iteration 20777: with minibatch training loss = 0.625 and accuracy of 0.78\n",
      "Iteration 20778: with minibatch training loss = 0.842 and accuracy of 0.72\n",
      "Iteration 20779: with minibatch training loss = 0.564 and accuracy of 0.78\n",
      "Iteration 20780: with minibatch training loss = 0.654 and accuracy of 0.77\n",
      "Iteration 20781: with minibatch training loss = 0.547 and accuracy of 0.83\n",
      "Iteration 20782: with minibatch training loss = 0.715 and accuracy of 0.8\n",
      "Iteration 20783: with minibatch training loss = 0.734 and accuracy of 0.75\n",
      "Iteration 20784: with minibatch training loss = 0.652 and accuracy of 0.78\n",
      "Iteration 20785: with minibatch training loss = 0.471 and accuracy of 0.84\n",
      "Iteration 20786: with minibatch training loss = 0.79 and accuracy of 0.73\n",
      "Iteration 20787: with minibatch training loss = 0.483 and accuracy of 0.84\n",
      "Iteration 20788: with minibatch training loss = 0.677 and accuracy of 0.78\n",
      "Iteration 20789: with minibatch training loss = 0.635 and accuracy of 0.8\n",
      "Iteration 20790: with minibatch training loss = 0.773 and accuracy of 0.75\n",
      "Iteration 20791: with minibatch training loss = 0.506 and accuracy of 0.84\n",
      "Iteration 20792: with minibatch training loss = 0.524 and accuracy of 0.84\n",
      "Iteration 20793: with minibatch training loss = 0.546 and accuracy of 0.78\n",
      "Iteration 20794: with minibatch training loss = 0.627 and accuracy of 0.81\n",
      "Iteration 20795: with minibatch training loss = 0.618 and accuracy of 0.8\n",
      "Iteration 20796: with minibatch training loss = 0.511 and accuracy of 0.84\n",
      "Iteration 20797: with minibatch training loss = 0.548 and accuracy of 0.84\n",
      "Iteration 20798: with minibatch training loss = 0.632 and accuracy of 0.8\n",
      "Iteration 20799: with minibatch training loss = 0.485 and accuracy of 0.84\n",
      "Iteration 20800: with minibatch training loss = 0.672 and accuracy of 0.78\n",
      "Iteration 20801: with minibatch training loss = 0.559 and accuracy of 0.81\n",
      "Iteration 20802: with minibatch training loss = 0.583 and accuracy of 0.81\n",
      "Iteration 20803: with minibatch training loss = 0.656 and accuracy of 0.81\n",
      "Iteration 20804: with minibatch training loss = 0.588 and accuracy of 0.81\n",
      "Iteration 20805: with minibatch training loss = 0.621 and accuracy of 0.78\n",
      "Iteration 20806: with minibatch training loss = 0.575 and accuracy of 0.83\n",
      "Iteration 20807: with minibatch training loss = 0.952 and accuracy of 0.73\n",
      "Iteration 20808: with minibatch training loss = 0.493 and accuracy of 0.84\n",
      "Iteration 20809: with minibatch training loss = 0.734 and accuracy of 0.8\n",
      "Iteration 20810: with minibatch training loss = 0.295 and accuracy of 0.92\n",
      "Iteration 20811: with minibatch training loss = 0.577 and accuracy of 0.83\n",
      "Iteration 20812: with minibatch training loss = 0.557 and accuracy of 0.81\n",
      "Iteration 20813: with minibatch training loss = 0.343 and accuracy of 0.89\n",
      "Iteration 20814: with minibatch training loss = 0.786 and accuracy of 0.75\n",
      "Iteration 20815: with minibatch training loss = 0.707 and accuracy of 0.73\n",
      "Iteration 20816: with minibatch training loss = 0.738 and accuracy of 0.72\n",
      "Iteration 20817: with minibatch training loss = 0.354 and accuracy of 0.91\n",
      "Iteration 20818: with minibatch training loss = 0.598 and accuracy of 0.83\n",
      "Validation loss: 0.22196342\n",
      "Epoch 15, Overall loss = 0.611 and accuracy of 0.802\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ne6HsehkiAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13c5f3710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define hyperparameters:\n",
    "num_epoch = 15\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define model:\n",
    "model = SENet(inp_w = 28, inp_h = 28, inp_d = 1)\n",
    "model.fit(X_train, y_train_enc, num_epoch = num_epoch, batch_size = batch_size, weight_save_path = weight_save_path, plot_losses = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.439825\n",
      "Epoch 1, Overall loss = 0.335 and accuracy of 0.915\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Test model:\n",
    "print(model.evaluate(X_test, y_test_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
